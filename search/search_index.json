{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Swayer, Kim :) Taking Mathematical Statistics Theory & Practice Review The Introduction to Statistical Learning in R The Elements of Statistical Learning \ubd88\uac00\ub2a5 \ud560 \uac83 \uac19\uc9c0\ub9cc \ud30c\uc774\uc36c\ub3c4.. Yonsei Univ, Applied Statistics test","title":"Home"},{"location":"#swayer-kim","text":"Taking Mathematical Statistics Theory & Practice Review The Introduction to Statistical Learning in R The Elements of Statistical Learning \ubd88\uac00\ub2a5 \ud560 \uac83 \uac19\uc9c0\ub9cc \ud30c\uc774\uc36c\ub3c4.. Yonsei Univ, Applied Statistics test","title":"Swayer, Kim :)"},{"location":"%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6/","text":"\u314e\u3147\u314e\u3147 library(foreign) setwd(\"/Users/swayer/Desktop\") data <- read.spss(\"MethodData.sav\",reencode='utf-8') ## re-encoding from utf-8 data1 <- as.data.frame(data) AA <- (data1$AA) hist(AA) qqnorm(AA,main='Normal') qqline(AA,col='red') 1-c \ud788\uc2a4\ud1a0\uadf8\ub7a8\ub3c4 \ub300\uce6d\uc774\uace0, \uc815\uaddc\uc131\uac80\uc815 \ud655\uc778\ud574\ubcf4\uba74 \uc815\uaddc\ubd84\ud3ec\uc5d0 \uac00\uae5d\ub2e4\ubcfc \uc218 \uc788\uc74c. 2 library(dplyr) ## ## Attaching package: 'dplyr' ## The following objects are masked from 'package:stats': ## ## filter, lag ## The following objects are masked from 'package:base': ## ## intersect, setdiff, setequal, union ex<- data1 %>% filter(exercise=='Medium'|exercise=='much') ex_not <- data1 %>% filter(exercise=='A little'|exercise=='Not at all') 2-a ex_no <- ex_not[-24,] a<- cbind(ex$wt_ch,ex_no$wt_ch) t.test(a) ## ## One Sample t-test ## ## data: a ## t = 4.8255, df = 45, p-value = 1.641e-05 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 1.013246 2.465015 ## sample estimates: ## mean of x ## 1.73913 mean(ex$wt_ch) ## [1] 1.478261 mean(ex_no$wt_ch) ## [1] 2 \uc6b4\ub3d9\uc744 \ub9ce\uc774 \ud558\ub294 \ud559\uc0dd\ub4e4, \uc801\uac8c \ud558\ub294 \ud559\uc0dd\ub4e4 \ubf51\uc544\ubcf4\ub2c8 \uc801\uac8c\ud558\ub294 \ud559\uc0dd\ub4e4(ex_not)\uc774 \uc0d8\ud50c\uac2f\uc218\uac00 \ud55c\uac1c \ub354\ub9ce\uc544\uc11c \ub9c8\uc9c0\ub9c9 \ud589 \ud558\ub098 \uc0ad\uc81c\ud568 (23,23\uac1c\ub85c \ub9de\ucda4) \ud1b5\uacc4\uac00\uc124 \\(H_0 : \\mu_{1} = \\mu_2\\) \\(H_1 : \\mu_{1} \\neq \\mu_2\\) p-value\uac00 0.00001641.. \ub85c < 0.05 . \ud1b5\uacc4\uc801\uc73c\ub85c \uc720\uc758\ud558\ubbc0\ub85c \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud568 -> \ub450 \uc9d1\ub2e8 \ucc28\uc774 \uc874\uc7ac\ud568 \ub450 \uc9d1\ub2e8 \ud3c9\uade0 \ube44\uad50\ud574\ubcf4\uba74 \uc6b4\ub3d9\ud55c \uc0ac\ub78c\ub4e4 1.4782, \uc548\ud55c \uc0ac\ub78c\ub4e4 2\ub85c \uc6b4\ub3d9\ud55c \uc0ac\ub78c\ub4e4\uc758 \uccb4\uc911\ubcc0\ud654\uac00 \ub354 \uc791\uc74c. 2-b set <- cbind(data1$stress_r,data1$stress_g) t.test(set) ## ## One Sample t-test ## ## data: set ## t = 35.67, df = 93, p-value < 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 6.208458 6.940478 ## sample estimates: ## mean of x ## 6.574468 mean(set[,1]) ## [1] 7.297872 mean(set[,2]) ## [1] 5.851064 \uc774 \ubb38\uc81c\ub3c4 a\uc640 \uac19\uc774 \uc218\uc5c5\ubcc4 \uc9d1\ub2e8 \uac04 \uc2a4\ud2b8\ub808\uc2a4 \uc9c0\uc218\ub97c \ube44\uad50\ud574\ubcf4\uba74 \uadc0\ubb34, \ub300\ub9bd\uac00\uc124 \ub611\uac19\uc774 \uc138\uc6b0\uace0 t\uac80\uc815 \uacb0\uacfc \ub450 \uc9d1\ub2e8 \uac04 \ucc28\uc774\uac00 \uc720\uc758\ud558\uc5ec \uadc0\ubb34\uac00\uc124 \uae30\uac01 -> stress_r \uc218\uc5c5\uc758 \uc2a4\ud2b8\ub808\uc2a4 \uc9c0\uc218\uac00 \ud0c0\uc218\uc5c5\uc758 \uadf8 \uac83\ubcf4\ub2e4 \ub192\uc74c 3 cor(data1$HR1,data1$HR2) ## [1] 0.7168044 cor(data1$HR1,data1$HR3) ## [1] 0.7013617 cor(data1$HR2,data1$HR3) ## [1] 0.8755458 \ucc28\ub840\ub300\ub85c, \\(\\rho_{12}=0.7168,\\ \\rho_{13}=0.70136,\\ \\rho_{23}=0.8755\\) 4 \ucd5c\uc801\ud68c\uadc0\uc2dd \uacb0\uc815 nume_data <- data1[,c(6,7,9,10,11,14,15,16,17)] cat_data <- data1[,c(3,4,5,8,13,19)] head(cat_data) ## Prog Spec Gender Glasses Sit exercise ## 1 master's Philosophy Female Yes Front A little ## 2 Ph.D. Pedagogy Male No Front Medium ## 3 master's Medicine Female No Front Medium ## 4 master's Leisure and Recreation Female Yes Front Medium ## 5 master's Learning and Control Female No Front A little ## 6 master's Leisure and Recreation Male No Front Not at all final <- nume_data head(final) ## Age Hight AA wt_ch exp_wtch stress_r stress_g Beer Coffee ## 1 31 169 2.8 -2 -2 9 7 1.0 2 ## 2 38 172 4.1 0 0 3 3 0.5 1 ## 3 23 163 3.4 2 -5 8 5 0.0 0 ## 4 29 166 3.2 3 -5 8 6 5.0 3 ## 5 23 168 3.7 -1 -4 7 5 0.8 1 ## 6 28 176 3.2 0 0 10 5 20.0 7 lm(AA~Age+Hight+wt_ch+exp_wtch+stress_r+stress_g+Beer+Coffee, data=final) ## ## Call: ## lm(formula = AA ~ Age + Hight + wt_ch + exp_wtch + stress_r + ## stress_g + Beer + Coffee, data = final) ## ## Coefficients: ## (Intercept) Age Hight wt_ch exp_wtch stress_r ## 4.149441 0.012342 -0.004677 0.034349 0.016257 -0.027382 ## stress_g Beer Coffee ## 0.019618 -0.030005 0.005469 \uc218\uce58\ub97c \ucd94\uc815\ud558\ub294 \ud68c\uadc0\uc2dd\uc744 \uc791\uc131\ud560 \ub54c numeric data\ub294 \ubaa8\ub450 \uace0\ub824\ud558\uae30\ub85c \ud558\ub418, \uc2ec\ubc15\uc218 \ub370\uc774\ud130\ub294 \uc704\uc5d0\uc11c \ud655\uc778\ud588\ub4ef \uac01\uc790 \uc0c1\uad00\uacc4\uc218\uac00 \ub192\uc544 \ub3c5\ub9bd\ubcc0\uc218 \uac04 \ub2e4\uc911\uacf5\uc120\uc131 \ubb38\uc81c \uc5ec\uc9c0\uac00 \uc788\uc5b4\uc11c \uc81c\uc678 \ub2e8\uc21c\ud55c \uc124\uba85\uc744 \uc704\ud574 \uba85\ubaa9\ud615 \ubcc0\uc218\ub3c4 \uc81c\uc678 \uc885\ud569\ud558\uba74 \ud68c\uadc0\uc2dd\uc740 \uc544\ub798\uc640 \uac19\ub2e4 \\(AA = 4.149441+ 0.012342*Age -0.004677*Height + 0.034349*wt-ch + ..\\)","title":"R Notebook"},{"location":"%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6/#_1","text":"library(foreign) setwd(\"/Users/swayer/Desktop\") data <- read.spss(\"MethodData.sav\",reencode='utf-8') ## re-encoding from utf-8 data1 <- as.data.frame(data) AA <- (data1$AA) hist(AA) qqnorm(AA,main='Normal') qqline(AA,col='red')","title":"\u314e\u3147\u314e\u3147"},{"location":"%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6/#1-c","text":"\ud788\uc2a4\ud1a0\uadf8\ub7a8\ub3c4 \ub300\uce6d\uc774\uace0, \uc815\uaddc\uc131\uac80\uc815 \ud655\uc778\ud574\ubcf4\uba74 \uc815\uaddc\ubd84\ud3ec\uc5d0 \uac00\uae5d\ub2e4\ubcfc \uc218 \uc788\uc74c.","title":"1-c"},{"location":"%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6/#2","text":"library(dplyr) ## ## Attaching package: 'dplyr' ## The following objects are masked from 'package:stats': ## ## filter, lag ## The following objects are masked from 'package:base': ## ## intersect, setdiff, setequal, union ex<- data1 %>% filter(exercise=='Medium'|exercise=='much') ex_not <- data1 %>% filter(exercise=='A little'|exercise=='Not at all')","title":"2"},{"location":"%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6/#2-a","text":"ex_no <- ex_not[-24,] a<- cbind(ex$wt_ch,ex_no$wt_ch) t.test(a) ## ## One Sample t-test ## ## data: a ## t = 4.8255, df = 45, p-value = 1.641e-05 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 1.013246 2.465015 ## sample estimates: ## mean of x ## 1.73913 mean(ex$wt_ch) ## [1] 1.478261 mean(ex_no$wt_ch) ## [1] 2 \uc6b4\ub3d9\uc744 \ub9ce\uc774 \ud558\ub294 \ud559\uc0dd\ub4e4, \uc801\uac8c \ud558\ub294 \ud559\uc0dd\ub4e4 \ubf51\uc544\ubcf4\ub2c8 \uc801\uac8c\ud558\ub294 \ud559\uc0dd\ub4e4(ex_not)\uc774 \uc0d8\ud50c\uac2f\uc218\uac00 \ud55c\uac1c \ub354\ub9ce\uc544\uc11c \ub9c8\uc9c0\ub9c9 \ud589 \ud558\ub098 \uc0ad\uc81c\ud568 (23,23\uac1c\ub85c \ub9de\ucda4) \ud1b5\uacc4\uac00\uc124 \\(H_0 : \\mu_{1} = \\mu_2\\) \\(H_1 : \\mu_{1} \\neq \\mu_2\\) p-value\uac00 0.00001641.. \ub85c < 0.05 . \ud1b5\uacc4\uc801\uc73c\ub85c \uc720\uc758\ud558\ubbc0\ub85c \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud568 -> \ub450 \uc9d1\ub2e8 \ucc28\uc774 \uc874\uc7ac\ud568 \ub450 \uc9d1\ub2e8 \ud3c9\uade0 \ube44\uad50\ud574\ubcf4\uba74 \uc6b4\ub3d9\ud55c \uc0ac\ub78c\ub4e4 1.4782, \uc548\ud55c \uc0ac\ub78c\ub4e4 2\ub85c \uc6b4\ub3d9\ud55c \uc0ac\ub78c\ub4e4\uc758 \uccb4\uc911\ubcc0\ud654\uac00 \ub354 \uc791\uc74c.","title":"2-a"},{"location":"%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6/#2-b","text":"set <- cbind(data1$stress_r,data1$stress_g) t.test(set) ## ## One Sample t-test ## ## data: set ## t = 35.67, df = 93, p-value < 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 6.208458 6.940478 ## sample estimates: ## mean of x ## 6.574468 mean(set[,1]) ## [1] 7.297872 mean(set[,2]) ## [1] 5.851064 \uc774 \ubb38\uc81c\ub3c4 a\uc640 \uac19\uc774 \uc218\uc5c5\ubcc4 \uc9d1\ub2e8 \uac04 \uc2a4\ud2b8\ub808\uc2a4 \uc9c0\uc218\ub97c \ube44\uad50\ud574\ubcf4\uba74 \uadc0\ubb34, \ub300\ub9bd\uac00\uc124 \ub611\uac19\uc774 \uc138\uc6b0\uace0 t\uac80\uc815 \uacb0\uacfc \ub450 \uc9d1\ub2e8 \uac04 \ucc28\uc774\uac00 \uc720\uc758\ud558\uc5ec \uadc0\ubb34\uac00\uc124 \uae30\uac01 -> stress_r \uc218\uc5c5\uc758 \uc2a4\ud2b8\ub808\uc2a4 \uc9c0\uc218\uac00 \ud0c0\uc218\uc5c5\uc758 \uadf8 \uac83\ubcf4\ub2e4 \ub192\uc74c","title":"2-b"},{"location":"%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6/#3","text":"cor(data1$HR1,data1$HR2) ## [1] 0.7168044 cor(data1$HR1,data1$HR3) ## [1] 0.7013617 cor(data1$HR2,data1$HR3) ## [1] 0.8755458 \ucc28\ub840\ub300\ub85c, \\(\\rho_{12}=0.7168,\\ \\rho_{13}=0.70136,\\ \\rho_{23}=0.8755\\)","title":"3"},{"location":"%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6/#4","text":"nume_data <- data1[,c(6,7,9,10,11,14,15,16,17)] cat_data <- data1[,c(3,4,5,8,13,19)] head(cat_data) ## Prog Spec Gender Glasses Sit exercise ## 1 master's Philosophy Female Yes Front A little ## 2 Ph.D. Pedagogy Male No Front Medium ## 3 master's Medicine Female No Front Medium ## 4 master's Leisure and Recreation Female Yes Front Medium ## 5 master's Learning and Control Female No Front A little ## 6 master's Leisure and Recreation Male No Front Not at all final <- nume_data head(final) ## Age Hight AA wt_ch exp_wtch stress_r stress_g Beer Coffee ## 1 31 169 2.8 -2 -2 9 7 1.0 2 ## 2 38 172 4.1 0 0 3 3 0.5 1 ## 3 23 163 3.4 2 -5 8 5 0.0 0 ## 4 29 166 3.2 3 -5 8 6 5.0 3 ## 5 23 168 3.7 -1 -4 7 5 0.8 1 ## 6 28 176 3.2 0 0 10 5 20.0 7 lm(AA~Age+Hight+wt_ch+exp_wtch+stress_r+stress_g+Beer+Coffee, data=final) ## ## Call: ## lm(formula = AA ~ Age + Hight + wt_ch + exp_wtch + stress_r + ## stress_g + Beer + Coffee, data = final) ## ## Coefficients: ## (Intercept) Age Hight wt_ch exp_wtch stress_r ## 4.149441 0.012342 -0.004677 0.034349 0.016257 -0.027382 ## stress_g Beer Coffee ## 0.019618 -0.030005 0.005469 \uc218\uce58\ub97c \ucd94\uc815\ud558\ub294 \ud68c\uadc0\uc2dd\uc744 \uc791\uc131\ud560 \ub54c numeric data\ub294 \ubaa8\ub450 \uace0\ub824\ud558\uae30\ub85c \ud558\ub418, \uc2ec\ubc15\uc218 \ub370\uc774\ud130\ub294 \uc704\uc5d0\uc11c \ud655\uc778\ud588\ub4ef \uac01\uc790 \uc0c1\uad00\uacc4\uc218\uac00 \ub192\uc544 \ub3c5\ub9bd\ubcc0\uc218 \uac04 \ub2e4\uc911\uacf5\uc120\uc131 \ubb38\uc81c \uc5ec\uc9c0\uac00 \uc788\uc5b4\uc11c \uc81c\uc678 \ub2e8\uc21c\ud55c \uc124\uba85\uc744 \uc704\ud574 \uba85\ubaa9\ud615 \ubcc0\uc218\ub3c4 \uc81c\uc678 \uc885\ud569\ud558\uba74 \ud68c\uadc0\uc2dd\uc740 \uc544\ub798\uc640 \uac19\ub2e4 \\(AA = 4.149441+ 0.012342*Age -0.004677*Height + 0.034349*wt-ch + ..\\)","title":"4 \ucd5c\uc801\ud68c\uadc0\uc2dd \uacb0\uc815"},{"location":"00%20This%20and%20That/01_Copula/","text":"What is Copula? \ucf54\ud4f0\ub7ec(Copula)\ub294 \ubb34\uc5c7\uc778\uac00? Concept \ubd84\uba85 \ud1b5\uacc4\uc804\uc0b0 \uc218\uc5c5 \ub54c \uc5bc\ud54f\ub4e4\uc5c8\uc9c0\ub9cc \uae30\uc5b5\uc774 \uac00\ubb3c\uac00\ubb3c\ud574\uc838 \ub2e4\uc2dc \ubcf5\uc2b5(\uc0ac\uc2e4 \uac70\uc758 \ucc98\uc74c)\ud558\ub294 \ub9c8\uc74c\uc73c\ub85c \ucf54\ud4f0\ub7ec\uc758 \uac1c\ub150\uacfc \uc774\uc5d0 \ub300\ud55c \uac04\ub2e8\ud55c \uc2dc\ubbac\ub808\uc774\uc158\uc744 \uacf5\ubd80\ud574\ubcf4\uc558\ub2e4. \ucf54\ud4f0\ub7ec\ub780 \uac04\ub2e8\ud788 \ub9d0\uc744 \ud558\uc790\uba74 random variable\ub4e4 \uac04\uc758 \uc0c1\uad00\uad00\uacc4 \ud639\uc740 \uc885\uc18d\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \ud568\uc218\uc774\ub2e4. \uc6b0\ub9ac\uac00 random variable\ub4e4 \uac04\uc758 \uc885\uc18d \uad6c\uc870\ub97c \uc124\uba85\ud558\uace0 \uc2f6\uc744\ub54c, \ud639\uc740 \uadf8\ub7ec\ud55c \uad6c\uc870\ub97c \uac16\uace0 \uc788\ub294 \ub2e4\ubcc0\ub7c9\ud655\ub960\ubcc0\uc218\ub4e4\uc744 \ud45c\ud604\ud558\uace0 \uc2f6\uc744 \ub54c \uc6b0\ub9ac\ub294 copula\ub97c \ud65c\uc6a9\ud574\ubcfc \uc218 \uc788\ub2e4. \ub2e4\ubcc0\ub7c9\ud655\ub960\ubcc0\uc218\ub97c \ub2e4\ub8e8\uba74\uc11c \uc6b0\ub9ac\ub294 \uc774\ub97c \uc774\ub8e8\uace0 \uc788\ub294 \uac01\uac01\uc758 marginal random variable\ub9cc\uc73c\ub85c\ub294 \uc804\uccb4 \ub2e4\ubcc0\ub7c9\ud655\ub960\ubcc0\uc218\ub97c 100% \uc124\uba85\ud560 \uc218 \uc5c6\ub2e4. \\[X=\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\sim N_2(\\begin{bmatrix} {\\mu_1} \\\\ \\mu_2 \\end{bmatrix},\\begin{bmatrix} \\sum_{11} & {\\sum_{12}} \\\\ {\\sum_{21}} & \\sum_{22} \\end{bmatrix})\\] \uc704\uc640 \uac19\uc740 bivariate random variable\uc758 \ubaa8\uc591\uc744 \uc0dd\uac01\ud574\ubcf4\uc790. \uc6b0\ub9ac\ub294 Lienar Operater \\(\\mathbf{A}= [I_m\\ 0]\\) ( \\(in\\ this\\ case,\\ m=2\\) )\ub97c \uc774\uc6a9\ud558\uc5ec \uac04\ub2e8\ud55c \uc99d\uba85\uc744 \ud1b5\ud574 Marginal randomvariable \\(X_1\\) \uc774 $X_1 \\sim N_1(\\mu_1,\\sum_{11}) $\uc758 \ubd84\ud3ec\ub97c \ub530\ub974\ub294 \uac83\uc744 \ud655\uc778 \ud560 \uc218 \uc788\uc9c0\ub9cc, \uc774\ub7ec\ud55c marginal \ubd84\ud3ec \\(X_1\\) \uacfc \\(X_2\\) \ub85c\ub294 \uc804\uccb4 \uc774\ubcc0\ub7c9\uc815\uaddc\ud655\ub960\ubcc0\uc218\ub97c \uc124\uba85\ud558\uc9c0 \ubabb\ud55c\ub2e4. \\(X_1\\) \uacfc \\(X_2\\) \uac04\uc758 \uc0c1\uad00\uad00\uacc4\uc5d0 \ub300\ud55c \uc815\ubcf4\uac00 \uc788\uc744 \ub54c \uc6b0\ub9ac\ub294 \uc804\uccb4 \ud655\ub960\ubcc0\uc218\uc5d0 \ub300\ud574 \uc124\uba85\ud560 \uc218 \uc788\ub294\ub370, \uc774\ub7ec\ud55c \uc5ec\ub7ec\uac1c\uc758 \ubcc0\uc218\uac04(\uc5ec\uae30\uc11c\ub294 \\(X_1\\) , \\(X_2\\) \ub450\uac1c)\uc758 \uc885\uc18d\uad00\uacc4\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud574\uc8fc\ub294 \uac83\uc774 Copula\uc774\ub2e4. continuous random variable\uacfc \uadf8\uac83\uc758 cdf \ub610\ud55c \uc5f0\uc18d\uc778\uc778 \\(X\\) \ub97c \uc0dd\uac01\ud574\ubcfc \ub54c, \uc6b0\ub9ac\ub294 \uc774\uac83\uc758 cdf \\(F(X)\\) \uc758 \ubd84\ud3ec \uac00 \\([0,1]\\) \uc744 \ubc94\uc704\ub85c \uac16\ub294 Uniform Distribution\uc774\ub77c\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \uac04\ub2e8\ud788 \ud655\uc778\ud574\ubcf4\uc790\uba74, \uc704\uc5d0\uc11c \uc124\uba85\ud55c \ud655\ub960\ubcc0\uc218\uc758 \ub204\uc801\ud655\ub960\ubd84\ud3ec\ub97c \\(F(X)\\) \ub77c\uace0 \uce6d\ud558\uc790. \uadf8\ub807\ub2e4\uba74 \\(0<x<1\\) \uc744 \ub9cc\uc871\ud558\ub294 \uc784\uc758\uc758 \\(x\\) \uc5d0 \ub300\ud558\uc5ec \uc544\ub798\uc640 \uac19\uc740 \uc2dd\uc804\uac1c\uac00 \uac00\ub2a5\ud558\ub2e4. $ \\(P(F(X) \\leq x)\\) $ $ \\(=P(X \\leq F^{-1}(x))\\) $ $ \\(=F(F^{-1}(x))\\) $ $ \\(=x\\) $ \uace0\ub85c \\(F(X)\\) \uc758 pdf\ub294 \\(x\\) \uc778 Uniform Distribution\uc774\ub2e4. \ub450 \ud655\ub960\ubcc0\uc218 \\(X\\) \uc640 \\(Y\\) \uc5d0 \ub300\ud558\uc5ec \\(F_1\\) \uacfc \\(F_2\\) \ub97c \uac01\uac01\uc758 \ub204\uc801\ubd84\ud3ec\ud568\uc218\ub77c \ud558\uace0 \\(F\\) \ub97c \ub450 \ud655\ub960\ubcc0\uc218\uc758 joint cdf\ub77c\uace0 \ud55c\ub2e4\uba74 \uc55e\uc5d0\uc11c \uc124\uba85\ud55c \uac83\uacfc \uac19\uc774 \\(F_1\\) \uacfc \\(F_2\\) \ub294 \uac01\uac01 \\([0,1]\\) \uc5d0\uc11c\uc758 Uniform \ubd84\ud3ec\uac00 \ub41c\ub2e4. \ub610\ud55c Copula\ub780 \uc5ec\uae30\uc758 \ub450 \ud655\ub960\ubcc0\uc218 \\(X\\) \uc640 \\(Y\\) \uc5d0 \ub300\ud55c \\(F_1(X)\\) , \\(F_2(X)\\) \uc758 joint cdf\ub85c \uc815\uc758\ud55c\ub2e4. \uc989 \uc774\ubcc0\ub7c9\ud655\ub960\ubcc0\uc218\uc5d0 \ud55c\ud574\uc11c\ub294 Copula\ub294 \\([0,1]^2 \\rightarrow [0,1]\\) \ub85c \uac00\ub294 \ud568\uc218 \uc774\uba70 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\[C(x_1,x_2) = P(X_1 \\leq x_1, X_2 \\leq x_2)\\] \uc5ec\uae30\uc11c marginal cdf \\(F_i\\) \uac00 marginal distribution\uc5d0 \ub300\ud55c \ubaa8\ub4e0 \uc815\ubcf4\ub97c \uac16\uace0 \uc788\ub2e4\uba74, copula C\ub294 \uc774 \ub458 \uac04\uc758 \uc885\uc18d \uad6c\uc870 \ud639\uc740 \uc0c1\uad00\uad00\uacc4\uc5d0 \ub300\ud55c \ubaa8\ub4e0 \uc815\ubcf4\ub97c \uac16\uace0 \uc788\ub2e4\uace0 \ub9d0\ud560 \uc218 \uc788\ub2e4. \uc774\ub7ec\ud55c copula\uac00 \uc720\uc6a9\ud55c \uc774\uc720\ub294 \uc9c1\uad00\uc801\uc73c\ub85c \uc0dd\uac01\ud574\ubd10\ub3c4 \uc6b0\ub9ac\uac00 R\uc774\ub098 \ub2e4\ub978 \uc5ec\ub7ec \ud504\ub85c\uadf8\ub7a8\uc744 \uc774\uc6a9\ud574 simulation\uc744 \ud558\uba74\uc11c \uc885\uc18d\uad00\uacc4\ub97c \uc720\uc9c0\ud558\ub294 \ud655\ub960\ubcc0\uc218\ub4e4\uc744 \uc0dd\uc131\ud558\uace0 \uc2f6\uc744 \ub54c \uc544\uc8fc \uc720\uc6a9\ud558\ub2e4. \uc6b0\ub9ac\ub294 \ud754\ud788 \ud2b9\uc815\ud55c \ubd84\ud3ec\ub97c \uac16\uace0 \uc774 \ubd84\ud3ec\uc5d0\uc11c \ub098\uc624\ub294 \ub3c5\ub9bd\uc801\uc778 \ub09c\uc218\ub4e4\uc740 \uc27d\uac8c \uc0dd\uc131\ud560 \uc218 \uc788\uc9c0\ub9cc, \uc885\uc18d\uad00\uacc4\ub97c \uc720\uc9c0\ud558\ub294 \ubcc0\uc218\ub4e4\uc744 \ubf51\uc544\ub0b4\uae30\ub780 \uc27d\uc9c0\uc54a\uc740\ub370, \uc774\ub97c \ub3c4\uc640\uc8fc\ub294 \uac83\uc774 Copula\uc774\ub2e4. Simulation and check Copula \uc218\ub9ac\ud1b5\uacc4\ud559(1)\uc758 3\uc7a5 \ub0b4\uc6a9\uc744 \ubcf5\uae30\ud574\ubcf8\ub2e4\uba74 \uc6b0\ub9ac\ub294 \ud655\ub960\ubcc0\uc218 \\(X\\) \uc5d0 \ub300\ud574\uc11c \uc544\ub798\uc640 \uac19\uc740 \uc131\uc9c8\uc744 \ub5a0\uc62c\ub824\ubcfc \uc218 \uc788\ub2e4. \\[\\mathbf{X} \\sim N_{n}(\\mathbf{\\mu},\\mathbf{\\sum}),\\] \\[Let\\ \\mathbf{Y} = \\mathbf{A}\\mathbf{X}+\\mathbf{b},\\ where\\ \\mathbf{A}\\ is\\ an\\ m\\times{n}\\ matrix\\ and\\ \\mathbf{b} \\in \\mathbf{R}^m\\] \\[\\mathbf{Y} \\sim N_m(\\mathbf{A}\\mathbf{\\mu}+\\mathbf{b},\\mathbf{A}\\mathbf{\\sum}\\mathbf{A}^{'})\\] \uc5ec\uae30\uc11c \ud655\ub960\ubcc0\uc218 \\(X\\) \ub97c \ud45c\uc900\uc815\uaddc\ubd84\ud3ec\uc5d0\uc11c \ub098\uc628 \\(Z \\sim N(0,1)\\) \ub85c \uc815\uc758\ub97c \ud558\uba74 \uc6b0\ub9ac\ub294 \uc5ec\uae30\uc11c \uc5bb\ub294 Copula\ub294 Gaussian Copula\uac00 \ub41c\ub2e4. \uadf8\ub807\ub2e4\uba74 \uc774\ub54c\uc758 \uacf5\ubd84\uc0b0\ud589\ub82c\uc740 \\(\\mathbf{AA^{'}}\\) \uac00 \ub418\uba70 \uc774\ub97c \ud3b8\uc758\uc0c1 \\(\\mathbf{\\sum}\\) \uc774\ub77c \ud45c\ud604\ud558\uaca0\ub2e4. \uac04\ub2e8\ud558\uac8c \uc0dd\uac01\ud574\ubcf4\uba74 Gaussian Copula\ub97c \uc774\uc6a9\ud55c \uc0d8\ud50c\ub9c1\uc740, \ud45c\uc900\uc815\uaddc\ud655\ub960\ubcc0\uc218 \\(Z\\) \uc758 \uc120\ud615\uacb0\ud569\uc2dd\uc5d0\uc11c \uc6b0\ub9ac\ub294 \ud2b9\uc815\ud55c \uc885\uc18d\uad6c\uc870\ub97c \uac16\uace0 \uc788\ub294 scaling\ub41c \uacf5\ubd84\uc0b0\ud589\ub82c(\ud56d\uc0c1 scaling\ub418\uc9c0\ub294 \uc54a\uc73c\uba70, \uacc4\uc0b0\uc758 \ud3b8\uc758\uc0c1 \uacf5\ubd84\uc0b0\ud589\ub82c\uc758 \ubd84\uc0b0\uacfc \uacf5\ubd84\uc0b0 \uc6d0\uc18c\ub97c scaling\ud574\uc11c \uacc4\uc0b0\ud568.)\uc744 \uc774\uc6a9\ud558\uc5ec \uc11c\ub85c \uc885\uc18d\uad6c\uc870\ub97c \ub744\uace0 \uc788\ub294 \uc0c8\ub85c\uc6b4 \ud655\ub960\ubcc0\uc218\ub97c \uc0dd\uc131\ud558\ub294 \uc6d0\ub9ac\uc774\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \uc544\ub798\uc640 \uac19\uc740 \ud615\ud0dc\ub97c \ub744\ub294 \uc774\ubcc0\ub7c9\ud655\ub960\ubcc0\uc218\ub97c \ub9cc\ub4e4\uace0 \uc2f6\uc5b4\ud55c\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uc790. \\[X=\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\sim N_2(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\begin{bmatrix} 1 & 0.7 \\\\ 0.7 & 1 \\end{bmatrix})\\] \uc6b0\ub9ac\ub294 Choleski Decomposition\uc744 \ud1b5\ud574\uc11c \\(\\mathbf{AA^{'}}=\\mathbf{\\sum}\\) \ub97c \ub9cc\uc871\ud558\ub294 \\(\\mathbf{A}\\) \ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4. set.seed(2013122059) cov_matrix <- matrix(c(1,0.7,0.7,1),2,2) Z1 <- rnorm(5000,0,1) Z2 <- rnorm(5000,0,1) A <- chol(cov_matrix) #Generate bivariate random variable X X <- t(A)%*%rbind(Z1,Z2) X_t <- t(X) colnames(X_t) <- c('X1','X2') head(X_t) ## X1 X2 ## [1,] 0.171640092 -0.18734343 ## [2,] -0.004784986 0.06963441 ## [3,] 0.002310486 -0.24824690 ## [4,] 0.056441096 0.31423495 ## [5,] -1.495328424 -0.22135001 ## [6,] 1.701277385 1.19219915 cor(X_t) ## X1 X2 ## X1 1.0000000 0.6805573 ## X2 0.6805573 1.0000000 \ud45c\uc900\uc815\uaddc\ud655\ub960\ubcc0\uc218\ub97c random\ud558\uac8c \ucda9\ubd84\ud788 \ub9ce\uc774 \ubf51\uace0 \uc774\ub97c Choleski Decomposition\uc744 \ud1b5\ud574 \uc0c8\ub86d\uac8c \uc11c\ub85c \uc885\uc18d\uad6c\uc870\ub97c \uac16\ub294 bivariate normal random sample \\(X\\) \ub97c \ub9cc\ub4e4\uc5b4\ubcf4\uc558\ub2e4. \ub9c8\uc9c0\ub9c9\uc5d0 \ud655\uc778\ud560 \uc218 \uc788\ub4ef\uc774 \uc774\ub54c\uc758 correlation matrix\ub97c \uadf8\ub824\ubcf4\uba74 \\(\\begin{bmatrix} 1 & 0.7 \\\\ 0.7 & 1 \\end{bmatrix}\\) \uacfc \ub9e4\uc6b0 \uc720\uc0ac\ud558\uac8c \ub098\uc634\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. (\uc5ec\uae30\uc11c\ub294 \ucc98\uc74c\ubd80\ud130 \ud45c\uc900\uc815\uaddc\ud655\ub960\ubcc0\uc218\ub97c \ud1b5\ud574 covariance matrix\ub97c scaling\ud588\uae30 \ub54c\ubb38\uc5d0 \uacf5\ubd84\uc0b0\ud589\ub82c\uacfc \uc0c1\uad00\uacc4\uc218\ud589\ub82c\uc740 sample size\uac00 \ucee4\uc9c8\uc218\ub85d \ud55c\ucabd\uc73c\ub85c \uadfc\uc0ac\ud55c\ub2e4.) \ub2e4\uc2dc \uc815\ub9ac\ub97c \ud574\ubcf8\ub2e4\uba74, Gaussian Copula\ub97c \ud1b5\ud574 bivariate normal random sample\uc744 \ub9cc\ub4dc\ub294 \uac83\uc740 \uc544\ub798\uc640 \uac19\uc740 \ub2e8\uacc4\ub97c \uac70\uce5c\ub2e4\uace0 \ud560 \uc218 \uc788\ub2e4. Generate \\(Z_1, Z_2, ..., Z_{n} \\sim indep\\ N(0,1)\\) \\(\\mathbf{Y}=\\mathbf{A}\\mathbf{Z},\\ (\\mathbf{Y} \\sim N(\\mu,\\mathbf{\\sum})),\\ Choleski\\ decomposition\\ \\mathbf{\\sum}=\\mathbf{A}\\mathbf{A^{'}}\\) \\(Compute\\ {\\phi}(W_{i}),\\ i=1,..,n\\) Set \\(X_{i}= {F_{i}^{-1}}(\\phi(W_{i}))\\) \uc774\ub294 \uc704\uc5d0\uc11c \uac04\ub2e8\ud558\uac8c \uc2dc\ubbac\ub808\uc774\uc158\uc744 \ud1b5\ud574 \ud655\uc778\ud588\ub358 \ubc29\ubc95\uacfc\ub294 \uc57d\uac04 \ub2e4\ub978 \ubc29\ubc95\uc774\ub2e4. \uc704\uc5d0\uc11c\ub294 \ubbf8\ub9ac \uacf5\ubd84\uc0b0\ud589\ub82c\uc744 \uc2a4\ucf00\uc77c\ub9c1\ud55c \ud6c4\uc5d0 \ucd10\ub808\uc2a4\ud0a4 \ubd84\ud574\ub97c \ud1b5\ud574 \uc885\uc18d\uad6c\uc870\ub97c \uac16\ub294 \uc774\ubcc0\ub7c9\ud655\ub960\ubcc0\uc218 \\(X_{i}\\) \ub97c \ub9cc\ub4e4\uc5c8\uc9c0\ub9cc, \uc55e\uc5d0\uc11c \uc18c\uac1c\ud55c \ubc29\uc2dd\uc740 cdf\uc758 \ubd84\ud3ec\ub294 Uniform \ubd84\ud3ec\ub97c \uac16\ub294\ub2e4\ub294 \uc810\uc744 \ucc29\uc548\ud558\uc5ec \uc0c1\uad00\uad00\uacc4 \ud589\ub82c\uc744 \ucd08\uae30\uac12\uc73c\ub85c \uc2dc\uc791\ud558\uc5ec cdf\ub85c\uc758 \ubcc0\ud658\uc744 \ud1b5\ud574 \uc774\ubcc0\ub7c9\uade0\ub4f1 random sample\uc744 \ub9cc\ub4e0 \ud6c4, \uc774\ub97c \ubaa9\ud45c \ubd84\ud3ec\uc758 inverse cdf\ub85c \ubcc0\ud658\ud558\uc5ec \uc6d0\ud558\ub294 \\(X_{i}\\) \uc758 \uc885\uc18d\uad6c\uc870\ub97c \uac16\ub294 \ub2e4\ub978\ubd84\ud3ec\uc758 \uc774\ubcc0\ub7c9 random sample\uc744 \ub9cc\ub4dc\ub294 \ubc29\ubc95\uc774\ub2e4. \uc774\ub97c \uac04\ub2e8\ud788 \uc2dc\ubbac\ub808\uc774\uc158\uc744 \ud574\ubcf4\uc790. \uc774\ubcc0\ub7c9\ubfd0 \uc544\ub2c8\ub77c, \uadf8 \uc774\uc0c1\uc758 \ub2e4\ubcc0\ub7c9\ud655\ub960\ubcc0\uc218\ub97c sampling\uae4c\uc9c0 \ud560 \uc218 \uc788\ub3c4\ub85d \ud568\uc218\ub97c \uc9dc\ubcf4\uc558\ub2e4. \uc774\ub7ec\ud55c \ud568\uc218\ub97c \ud1b5\ud574 \ud655\ub960\ubcc0\uc218\ub4e4\uc744 \uc0d8\ud50c\ub9c1\ud560 \ub54c, correlation matrix\uc758 \ube44\ub300\uac01\uc131\ubd84\uc778 \\({\\rho}_{ij},\\ i\\neq{j}\\) \ub4e4\uc744 \uc77c\uc77c\ud788 \uc785\ub825\ud558\uc9c0\uc54a\uace0\ub3c4 \ud568\uc218\ub97c \uc9e4 \uc218 \uc788\uc9c0 \uc54a\uc744\uae4c \ud588\uc9c0\ub9cc, \ud544\uc790\uc758 \uba38\ub9ac\uc758 \ud55c\uacc4\ub85c \uc778\ud574.. \\(n\\times{n}\\ matrix\\) \uc5d0\uc11c \uc0c1\uad00\uacc4\uc218\ud589\ub82c\uc758 \ube44\ub300\uac01 \uc131\ubd84\uc758 \uac2f\uc218\ub294 \\(\\frac{n(n+1)}{2}\\) \uc774\uae30\uc5d0 \uc774 \uac2f\uc218\ub9cc\ud07c \\(\\rho\\) \ub4e4\uc744 \ubca1\ud130\ud615\ud0dc\ub85c \uc785\ub825\ud558\uc5ec \uc774\ubcc0\ub7c9\uc774\uc0c1\uc758 \ud655\ub960\ubcc0\uc218\ub97c \uc0d8\ud50c\ub9c1\ud558\ub294 \uc870\uae08\uc740 \uc218\ub3d9\uc801\uc778 \ubc29\ubc95\uc73c\ub85c\ub9cc \ud568\uc218\ub97c \uad6c\ud604\ud558\uc600\ub2e4. copula <- function(n,mu,var){ #C is correlation matrix C <- matrix(0,3,3) # function\uc548\uc5d0\uc11c \uc77c\uc77c\ud788 rho\ub97c \uc785\ub825\ud574\uc918\uc57c\ud558\ub294 \uac83\uc774 \uc544\uc26c\uc6b8 \ub530\ub984\uc774\ub2e4.. #\uc0c1\uad00\uacc4\uc218\ud589\ub82c\uc744 \ub9cc\ub4e4\uae30\uc704\ud574 \uc0ac\uc6a9\ud55c for\ubb38\uc758 \ud2b9\uc131\uc5d0 \ub530\ub77c rho\ub85c \uc9c0\uc815\ud558\ub294 \ubca1\ud130\uc6d0\uc18c\uc758 \uc21c\uc11c\ub294 4x4 \ud589\ub82c\uae30\uc900\uc73c\ub85c #rho_12, rho_13, rho_23, rho_14, rho_24, rho_34 \uc2dd\uc758 \uc21c\uc11c\uc774\ub2e4. rho <- c(0.7,0.6,0.5) C[upper.tri(C)] <- rho C <- C+t(C) for (i in 1:nrow(C)) { for (j in 1:ncol(C)) { if(i==j){C[i,j]=1} }} m <- ncol(C) A <- t(chol(C)) #U is Gaussian Copula & U1, U2 distributed Uniform U <- matrix(nrow=n,ncol=m) colnames(U)=paste0('U',1:m) W <- matrix(nrow=n,ncol=m) colnames(W)=paste0('W',1:m) for (i in 1:n){ Z <- rnorm(m,0,1) W <- A%*%Z U[i,] <- pnorm(W) } result <- list() result[['Gaussian Copula']] <- U X <- matrix(nrow=n,ncol=m) colnames(X) <- paste0('X',1:m) #Using inverse cdf, transform U to objective distribution for (j in 1:m){ X[,j] <- qnorm(U[,j],mean=mu[j], sd=sqrt(var[j])) } result[['Generated Multivariate Normal Random Sample']] <- X return(result) } n <- 5000 mu <- c(5,10,15) var <- c(9,15,30) result <- copula(n=n,mu=mu,var=var) U <- result[[1]] X <- result[[2]] cor(U) ## U1 U2 U3 ## U1 1.0000000 0.6824444 0.5935367 ## U2 0.6824444 1.0000000 0.4945416 ## U3 0.5935367 0.4945416 1.0000000 mydf<- as.data.frame(X) mydf2 <- as.data.frame(U) library(scatterplot3d) library(ggplot2) p2 <- scatterplot3d(mydf2[,1:3],pch=16, color=\"steelblue\",grid=T,box=F,angle=55, main=\"Generated Gaussian Copula\") my.lm2 <- lm(mydf2[,3]~mydf2[,1]+mydf2[,2]) p2$plane3d(my.lm2) U\uc758 \uc0c1\uad00\uacc4\uc218\ud589\ub82c\uc744 \uc0b4\ud3b4\ubcf4\uba74 \ucd08\uae30\uc5d0 \uc124\uc815\ud588\ub358 \uac12\uacfc \ub9e4\uc6b0 \uc720\uc0ac\ud55c \uac12\uc774 \ub098\uc634\uc744 \ud655\uc778 \ud560 \uc218 \uc788\ub2e4. p <- scatterplot3d(mydf[,1:3],pch=16, color=\"steelblue\",grid=T,box=F,angle=55, main='Generated 3-variate Normal Random Sample') my.lm <- lm(mydf[,3]~mydf[,1]+mydf[,2]) p$plane3d(my.lm) cor(X) ## X1 X2 X3 ## X1 1.0000000 0.7048168 0.6160424 ## X2 0.7048168 1.0000000 0.5171809 ## X3 0.6160424 0.5171809 1.0000000 cov(X) ## X1 X2 X3 ## X1 9.125139 8.484152 10.41140 ## X2 8.484152 15.879062 11.53012 ## X3 10.411404 11.530122 31.30098 apply(X,2,mean) ## X1 X2 X3 ## 5.081047 10.099912 15.009950 X \ub610\ud55c \uc0c1\uad00\uacc4\uc218\ud589\ub82c, \uacf5\ubd84\uc0b0\ud589\ub82c, \ud3c9\uade0\uc774 \ubaa9\ud45c\ubd84\ud3ec\uc758 \ubaa8\uc218\uc640 \ub9e4\uc6b0 \uc720\uc0ac\ud558\uac8c \uadfc\uc0ac\ud568\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. par(mfrow=c(2,3)) hist(U[,1]) hist(U[,2]) hist(U[,3]) hist(X[,1]) hist(X[,2]) hist(X[,3]) \uc0d8\ud50c\ub9c1\ub41c \uac12\ub4e4\uc758 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \uc0b4\ud3b4\ubcf4\uba74, \uc721\uc548\uc73c\ub85c \ud655\uc778\ud558\uc5ec\ub3c4 \uac00\uc6b0\uc2dc\uc548 \ucf54\ud4f0\ub7ec U\uc640 \uc0dd\uc131\ub41c 3\ubcc0\ub7c9\uc815\uaddc\ud655\ub960\ubcc0\uc218 X\uc758 marginal distribution\uc774 \uac01\uac01 \uade0\ub4f1\ubd84\ud3ec\uc640 \uc815\uaddc\ubd84\ud3ec\uc758 form\uc5d0 \uc720\uc0ac\ud558\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. Inverse cdf\ub97c \ud1b5\ud574 \uc6b0\ub9ac\uac00 \ub2e4\ub978 \uc5ec\ub7ec \ubaa9\ud45c \ubd84\ud3ec\ub97c \uc720\ub3c4\ud558\uace0\uc2f6\ub2e4\uba74, \uc5b8\uc81c\ub4e0\uc9c0 \uadf8 \ubaa8\uc218\ub4e4\ub9cc \uc9c0\uc815\uc744 \ud574\uc8fc\uc5b4\uc11c \uc0c8\ub86d\uac8c \ubf51\uc544\ub0bc \uc218 \uc788\ub2e4. \uc77c\ub840\ub85c \uc9c0\uc218\ubd84\ud3ec \\(Exp(\\lambda)\\) \uc758 inverse cdf\ub294 \\(F^{-1}(p;\\lambda)= -\\frac{1}{\\lambda}log(1-p)\\) \ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub294\ub370, \uc774\ub97c \uc704\uc5d0\uc11c \uc9e0 \ud568\uc218\uc5d0 \ub300\uc785\ud558\uc5ec \\(\\lambda=(1,0.7,0.3)\\) \uc778 \uc9c0\uc218\ubd84\ud3ec \\(X_1,X_2,X_3\\) \ub97c \uc0dd\uc131\ud574\ubcf4\uaca0\ub2e4. lambda1 <-1 lambda2 <-0.7 lambda3 <- 0.3 X1 <- -(1/lambda1)*log(1-U[,1]) X2 <- -(1/lambda2)*log(1-U[,2]) X3 <- -(1/lambda3)*log(1-U[,3]) multivariate_exp_random_sample <- cbind(X1,X2,X3) cor(multivariate_exp_random_sample) ## X1 X2 X3 ## X1 1.000000 0.671786 0.566118 ## X2 0.671786 1.000000 0.463459 ## X3 0.566118 0.463459 1.000000 par(mfrow=c(1,3)) hist(X1) hist(X2) hist(X3) library(MASS) fitdistr(X1,\"exponential\") ## rate ## 0.97348415 ## (0.01376714) fitdistr(X2,\"exponential\") ## rate ## 0.673762741 ## (0.009528444) fitdistr(X3,\"exponential\") ## rate ## 0.296073253 ## (0.004187108) \uc218\uce58\ub97c \ud655\uc778\ud560 \uc218 \uc788\ub4ef\uc774 \uae30\uc874\uc5d0 \uc124\uc815\ud588\ub358 \uc885\uc18d\uad6c\uc870\ub97c \uc798 \uc720\uc9c0\ud558\ub294 \uac83\ucc98\ub7fc \ubcf4\uc774\uace0, \uc0c8\ub86d\uac8c \uc0dd\uc131\ud55c \ubd84\ud3ec\uc758 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \ubcfc\ub54c \ubaa9\ud45c\ubd84\ud3ec\uc640 \uc798 \uc801\ud569\ud558\ub294 \uac83\ucc98\ub7fc \ubcf4\uc778\ub2e4. \ub610\ud55c \uc774\ub4e4\uc758 \ubaa8\uc218\ub97c \uadfc\uc0ac\uc801\uc73c\ub85c \ucd94\uc815\ud574\ubcf4\uc544\ub3c4 \uc6d0\ud558\ub294 \\(\\lambda\\) \uac12\uacfc \ub9e4\uc6b0 \uc720\uc0ac\ud55c \uac12\uc744 \ubaa8\uc218\ub85c \uac16\ub294\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \ub057~","title":"What is Copula?"},{"location":"00%20This%20and%20That/01_Copula/#what-is-copula","text":"\ucf54\ud4f0\ub7ec(Copula)\ub294 \ubb34\uc5c7\uc778\uac00?","title":"What is Copula?"},{"location":"00%20This%20and%20That/01_Copula/#concept","text":"\ubd84\uba85 \ud1b5\uacc4\uc804\uc0b0 \uc218\uc5c5 \ub54c \uc5bc\ud54f\ub4e4\uc5c8\uc9c0\ub9cc \uae30\uc5b5\uc774 \uac00\ubb3c\uac00\ubb3c\ud574\uc838 \ub2e4\uc2dc \ubcf5\uc2b5(\uc0ac\uc2e4 \uac70\uc758 \ucc98\uc74c)\ud558\ub294 \ub9c8\uc74c\uc73c\ub85c \ucf54\ud4f0\ub7ec\uc758 \uac1c\ub150\uacfc \uc774\uc5d0 \ub300\ud55c \uac04\ub2e8\ud55c \uc2dc\ubbac\ub808\uc774\uc158\uc744 \uacf5\ubd80\ud574\ubcf4\uc558\ub2e4. \ucf54\ud4f0\ub7ec\ub780 \uac04\ub2e8\ud788 \ub9d0\uc744 \ud558\uc790\uba74 random variable\ub4e4 \uac04\uc758 \uc0c1\uad00\uad00\uacc4 \ud639\uc740 \uc885\uc18d\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \ud568\uc218\uc774\ub2e4. \uc6b0\ub9ac\uac00 random variable\ub4e4 \uac04\uc758 \uc885\uc18d \uad6c\uc870\ub97c \uc124\uba85\ud558\uace0 \uc2f6\uc744\ub54c, \ud639\uc740 \uadf8\ub7ec\ud55c \uad6c\uc870\ub97c \uac16\uace0 \uc788\ub294 \ub2e4\ubcc0\ub7c9\ud655\ub960\ubcc0\uc218\ub4e4\uc744 \ud45c\ud604\ud558\uace0 \uc2f6\uc744 \ub54c \uc6b0\ub9ac\ub294 copula\ub97c \ud65c\uc6a9\ud574\ubcfc \uc218 \uc788\ub2e4. \ub2e4\ubcc0\ub7c9\ud655\ub960\ubcc0\uc218\ub97c \ub2e4\ub8e8\uba74\uc11c \uc6b0\ub9ac\ub294 \uc774\ub97c \uc774\ub8e8\uace0 \uc788\ub294 \uac01\uac01\uc758 marginal random variable\ub9cc\uc73c\ub85c\ub294 \uc804\uccb4 \ub2e4\ubcc0\ub7c9\ud655\ub960\ubcc0\uc218\ub97c 100% \uc124\uba85\ud560 \uc218 \uc5c6\ub2e4. \\[X=\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\sim N_2(\\begin{bmatrix} {\\mu_1} \\\\ \\mu_2 \\end{bmatrix},\\begin{bmatrix} \\sum_{11} & {\\sum_{12}} \\\\ {\\sum_{21}} & \\sum_{22} \\end{bmatrix})\\] \uc704\uc640 \uac19\uc740 bivariate random variable\uc758 \ubaa8\uc591\uc744 \uc0dd\uac01\ud574\ubcf4\uc790. \uc6b0\ub9ac\ub294 Lienar Operater \\(\\mathbf{A}= [I_m\\ 0]\\) ( \\(in\\ this\\ case,\\ m=2\\) )\ub97c \uc774\uc6a9\ud558\uc5ec \uac04\ub2e8\ud55c \uc99d\uba85\uc744 \ud1b5\ud574 Marginal randomvariable \\(X_1\\) \uc774 $X_1 \\sim N_1(\\mu_1,\\sum_{11}) $\uc758 \ubd84\ud3ec\ub97c \ub530\ub974\ub294 \uac83\uc744 \ud655\uc778 \ud560 \uc218 \uc788\uc9c0\ub9cc, \uc774\ub7ec\ud55c marginal \ubd84\ud3ec \\(X_1\\) \uacfc \\(X_2\\) \ub85c\ub294 \uc804\uccb4 \uc774\ubcc0\ub7c9\uc815\uaddc\ud655\ub960\ubcc0\uc218\ub97c \uc124\uba85\ud558\uc9c0 \ubabb\ud55c\ub2e4. \\(X_1\\) \uacfc \\(X_2\\) \uac04\uc758 \uc0c1\uad00\uad00\uacc4\uc5d0 \ub300\ud55c \uc815\ubcf4\uac00 \uc788\uc744 \ub54c \uc6b0\ub9ac\ub294 \uc804\uccb4 \ud655\ub960\ubcc0\uc218\uc5d0 \ub300\ud574 \uc124\uba85\ud560 \uc218 \uc788\ub294\ub370, \uc774\ub7ec\ud55c \uc5ec\ub7ec\uac1c\uc758 \ubcc0\uc218\uac04(\uc5ec\uae30\uc11c\ub294 \\(X_1\\) , \\(X_2\\) \ub450\uac1c)\uc758 \uc885\uc18d\uad00\uacc4\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud574\uc8fc\ub294 \uac83\uc774 Copula\uc774\ub2e4. continuous random variable\uacfc \uadf8\uac83\uc758 cdf \ub610\ud55c \uc5f0\uc18d\uc778\uc778 \\(X\\) \ub97c \uc0dd\uac01\ud574\ubcfc \ub54c, \uc6b0\ub9ac\ub294 \uc774\uac83\uc758 cdf \\(F(X)\\) \uc758 \ubd84\ud3ec \uac00 \\([0,1]\\) \uc744 \ubc94\uc704\ub85c \uac16\ub294 Uniform Distribution\uc774\ub77c\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \uac04\ub2e8\ud788 \ud655\uc778\ud574\ubcf4\uc790\uba74, \uc704\uc5d0\uc11c \uc124\uba85\ud55c \ud655\ub960\ubcc0\uc218\uc758 \ub204\uc801\ud655\ub960\ubd84\ud3ec\ub97c \\(F(X)\\) \ub77c\uace0 \uce6d\ud558\uc790. \uadf8\ub807\ub2e4\uba74 \\(0<x<1\\) \uc744 \ub9cc\uc871\ud558\ub294 \uc784\uc758\uc758 \\(x\\) \uc5d0 \ub300\ud558\uc5ec \uc544\ub798\uc640 \uac19\uc740 \uc2dd\uc804\uac1c\uac00 \uac00\ub2a5\ud558\ub2e4. $ \\(P(F(X) \\leq x)\\) $ $ \\(=P(X \\leq F^{-1}(x))\\) $ $ \\(=F(F^{-1}(x))\\) $ $ \\(=x\\) $ \uace0\ub85c \\(F(X)\\) \uc758 pdf\ub294 \\(x\\) \uc778 Uniform Distribution\uc774\ub2e4. \ub450 \ud655\ub960\ubcc0\uc218 \\(X\\) \uc640 \\(Y\\) \uc5d0 \ub300\ud558\uc5ec \\(F_1\\) \uacfc \\(F_2\\) \ub97c \uac01\uac01\uc758 \ub204\uc801\ubd84\ud3ec\ud568\uc218\ub77c \ud558\uace0 \\(F\\) \ub97c \ub450 \ud655\ub960\ubcc0\uc218\uc758 joint cdf\ub77c\uace0 \ud55c\ub2e4\uba74 \uc55e\uc5d0\uc11c \uc124\uba85\ud55c \uac83\uacfc \uac19\uc774 \\(F_1\\) \uacfc \\(F_2\\) \ub294 \uac01\uac01 \\([0,1]\\) \uc5d0\uc11c\uc758 Uniform \ubd84\ud3ec\uac00 \ub41c\ub2e4. \ub610\ud55c Copula\ub780 \uc5ec\uae30\uc758 \ub450 \ud655\ub960\ubcc0\uc218 \\(X\\) \uc640 \\(Y\\) \uc5d0 \ub300\ud55c \\(F_1(X)\\) , \\(F_2(X)\\) \uc758 joint cdf\ub85c \uc815\uc758\ud55c\ub2e4. \uc989 \uc774\ubcc0\ub7c9\ud655\ub960\ubcc0\uc218\uc5d0 \ud55c\ud574\uc11c\ub294 Copula\ub294 \\([0,1]^2 \\rightarrow [0,1]\\) \ub85c \uac00\ub294 \ud568\uc218 \uc774\uba70 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\[C(x_1,x_2) = P(X_1 \\leq x_1, X_2 \\leq x_2)\\] \uc5ec\uae30\uc11c marginal cdf \\(F_i\\) \uac00 marginal distribution\uc5d0 \ub300\ud55c \ubaa8\ub4e0 \uc815\ubcf4\ub97c \uac16\uace0 \uc788\ub2e4\uba74, copula C\ub294 \uc774 \ub458 \uac04\uc758 \uc885\uc18d \uad6c\uc870 \ud639\uc740 \uc0c1\uad00\uad00\uacc4\uc5d0 \ub300\ud55c \ubaa8\ub4e0 \uc815\ubcf4\ub97c \uac16\uace0 \uc788\ub2e4\uace0 \ub9d0\ud560 \uc218 \uc788\ub2e4. \uc774\ub7ec\ud55c copula\uac00 \uc720\uc6a9\ud55c \uc774\uc720\ub294 \uc9c1\uad00\uc801\uc73c\ub85c \uc0dd\uac01\ud574\ubd10\ub3c4 \uc6b0\ub9ac\uac00 R\uc774\ub098 \ub2e4\ub978 \uc5ec\ub7ec \ud504\ub85c\uadf8\ub7a8\uc744 \uc774\uc6a9\ud574 simulation\uc744 \ud558\uba74\uc11c \uc885\uc18d\uad00\uacc4\ub97c \uc720\uc9c0\ud558\ub294 \ud655\ub960\ubcc0\uc218\ub4e4\uc744 \uc0dd\uc131\ud558\uace0 \uc2f6\uc744 \ub54c \uc544\uc8fc \uc720\uc6a9\ud558\ub2e4. \uc6b0\ub9ac\ub294 \ud754\ud788 \ud2b9\uc815\ud55c \ubd84\ud3ec\ub97c \uac16\uace0 \uc774 \ubd84\ud3ec\uc5d0\uc11c \ub098\uc624\ub294 \ub3c5\ub9bd\uc801\uc778 \ub09c\uc218\ub4e4\uc740 \uc27d\uac8c \uc0dd\uc131\ud560 \uc218 \uc788\uc9c0\ub9cc, \uc885\uc18d\uad00\uacc4\ub97c \uc720\uc9c0\ud558\ub294 \ubcc0\uc218\ub4e4\uc744 \ubf51\uc544\ub0b4\uae30\ub780 \uc27d\uc9c0\uc54a\uc740\ub370, \uc774\ub97c \ub3c4\uc640\uc8fc\ub294 \uac83\uc774 Copula\uc774\ub2e4.","title":"Concept"},{"location":"00%20This%20and%20That/01_Copula/#simulation-and-check-copula","text":"\uc218\ub9ac\ud1b5\uacc4\ud559(1)\uc758 3\uc7a5 \ub0b4\uc6a9\uc744 \ubcf5\uae30\ud574\ubcf8\ub2e4\uba74 \uc6b0\ub9ac\ub294 \ud655\ub960\ubcc0\uc218 \\(X\\) \uc5d0 \ub300\ud574\uc11c \uc544\ub798\uc640 \uac19\uc740 \uc131\uc9c8\uc744 \ub5a0\uc62c\ub824\ubcfc \uc218 \uc788\ub2e4. \\[\\mathbf{X} \\sim N_{n}(\\mathbf{\\mu},\\mathbf{\\sum}),\\] \\[Let\\ \\mathbf{Y} = \\mathbf{A}\\mathbf{X}+\\mathbf{b},\\ where\\ \\mathbf{A}\\ is\\ an\\ m\\times{n}\\ matrix\\ and\\ \\mathbf{b} \\in \\mathbf{R}^m\\] \\[\\mathbf{Y} \\sim N_m(\\mathbf{A}\\mathbf{\\mu}+\\mathbf{b},\\mathbf{A}\\mathbf{\\sum}\\mathbf{A}^{'})\\] \uc5ec\uae30\uc11c \ud655\ub960\ubcc0\uc218 \\(X\\) \ub97c \ud45c\uc900\uc815\uaddc\ubd84\ud3ec\uc5d0\uc11c \ub098\uc628 \\(Z \\sim N(0,1)\\) \ub85c \uc815\uc758\ub97c \ud558\uba74 \uc6b0\ub9ac\ub294 \uc5ec\uae30\uc11c \uc5bb\ub294 Copula\ub294 Gaussian Copula\uac00 \ub41c\ub2e4. \uadf8\ub807\ub2e4\uba74 \uc774\ub54c\uc758 \uacf5\ubd84\uc0b0\ud589\ub82c\uc740 \\(\\mathbf{AA^{'}}\\) \uac00 \ub418\uba70 \uc774\ub97c \ud3b8\uc758\uc0c1 \\(\\mathbf{\\sum}\\) \uc774\ub77c \ud45c\ud604\ud558\uaca0\ub2e4. \uac04\ub2e8\ud558\uac8c \uc0dd\uac01\ud574\ubcf4\uba74 Gaussian Copula\ub97c \uc774\uc6a9\ud55c \uc0d8\ud50c\ub9c1\uc740, \ud45c\uc900\uc815\uaddc\ud655\ub960\ubcc0\uc218 \\(Z\\) \uc758 \uc120\ud615\uacb0\ud569\uc2dd\uc5d0\uc11c \uc6b0\ub9ac\ub294 \ud2b9\uc815\ud55c \uc885\uc18d\uad6c\uc870\ub97c \uac16\uace0 \uc788\ub294 scaling\ub41c \uacf5\ubd84\uc0b0\ud589\ub82c(\ud56d\uc0c1 scaling\ub418\uc9c0\ub294 \uc54a\uc73c\uba70, \uacc4\uc0b0\uc758 \ud3b8\uc758\uc0c1 \uacf5\ubd84\uc0b0\ud589\ub82c\uc758 \ubd84\uc0b0\uacfc \uacf5\ubd84\uc0b0 \uc6d0\uc18c\ub97c scaling\ud574\uc11c \uacc4\uc0b0\ud568.)\uc744 \uc774\uc6a9\ud558\uc5ec \uc11c\ub85c \uc885\uc18d\uad6c\uc870\ub97c \ub744\uace0 \uc788\ub294 \uc0c8\ub85c\uc6b4 \ud655\ub960\ubcc0\uc218\ub97c \uc0dd\uc131\ud558\ub294 \uc6d0\ub9ac\uc774\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \uc544\ub798\uc640 \uac19\uc740 \ud615\ud0dc\ub97c \ub744\ub294 \uc774\ubcc0\ub7c9\ud655\ub960\ubcc0\uc218\ub97c \ub9cc\ub4e4\uace0 \uc2f6\uc5b4\ud55c\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uc790. \\[X=\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\sim N_2(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\begin{bmatrix} 1 & 0.7 \\\\ 0.7 & 1 \\end{bmatrix})\\] \uc6b0\ub9ac\ub294 Choleski Decomposition\uc744 \ud1b5\ud574\uc11c \\(\\mathbf{AA^{'}}=\\mathbf{\\sum}\\) \ub97c \ub9cc\uc871\ud558\ub294 \\(\\mathbf{A}\\) \ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4. set.seed(2013122059) cov_matrix <- matrix(c(1,0.7,0.7,1),2,2) Z1 <- rnorm(5000,0,1) Z2 <- rnorm(5000,0,1) A <- chol(cov_matrix) #Generate bivariate random variable X X <- t(A)%*%rbind(Z1,Z2) X_t <- t(X) colnames(X_t) <- c('X1','X2') head(X_t) ## X1 X2 ## [1,] 0.171640092 -0.18734343 ## [2,] -0.004784986 0.06963441 ## [3,] 0.002310486 -0.24824690 ## [4,] 0.056441096 0.31423495 ## [5,] -1.495328424 -0.22135001 ## [6,] 1.701277385 1.19219915 cor(X_t) ## X1 X2 ## X1 1.0000000 0.6805573 ## X2 0.6805573 1.0000000 \ud45c\uc900\uc815\uaddc\ud655\ub960\ubcc0\uc218\ub97c random\ud558\uac8c \ucda9\ubd84\ud788 \ub9ce\uc774 \ubf51\uace0 \uc774\ub97c Choleski Decomposition\uc744 \ud1b5\ud574 \uc0c8\ub86d\uac8c \uc11c\ub85c \uc885\uc18d\uad6c\uc870\ub97c \uac16\ub294 bivariate normal random sample \\(X\\) \ub97c \ub9cc\ub4e4\uc5b4\ubcf4\uc558\ub2e4. \ub9c8\uc9c0\ub9c9\uc5d0 \ud655\uc778\ud560 \uc218 \uc788\ub4ef\uc774 \uc774\ub54c\uc758 correlation matrix\ub97c \uadf8\ub824\ubcf4\uba74 \\(\\begin{bmatrix} 1 & 0.7 \\\\ 0.7 & 1 \\end{bmatrix}\\) \uacfc \ub9e4\uc6b0 \uc720\uc0ac\ud558\uac8c \ub098\uc634\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. (\uc5ec\uae30\uc11c\ub294 \ucc98\uc74c\ubd80\ud130 \ud45c\uc900\uc815\uaddc\ud655\ub960\ubcc0\uc218\ub97c \ud1b5\ud574 covariance matrix\ub97c scaling\ud588\uae30 \ub54c\ubb38\uc5d0 \uacf5\ubd84\uc0b0\ud589\ub82c\uacfc \uc0c1\uad00\uacc4\uc218\ud589\ub82c\uc740 sample size\uac00 \ucee4\uc9c8\uc218\ub85d \ud55c\ucabd\uc73c\ub85c \uadfc\uc0ac\ud55c\ub2e4.) \ub2e4\uc2dc \uc815\ub9ac\ub97c \ud574\ubcf8\ub2e4\uba74, Gaussian Copula\ub97c \ud1b5\ud574 bivariate normal random sample\uc744 \ub9cc\ub4dc\ub294 \uac83\uc740 \uc544\ub798\uc640 \uac19\uc740 \ub2e8\uacc4\ub97c \uac70\uce5c\ub2e4\uace0 \ud560 \uc218 \uc788\ub2e4. Generate \\(Z_1, Z_2, ..., Z_{n} \\sim indep\\ N(0,1)\\) \\(\\mathbf{Y}=\\mathbf{A}\\mathbf{Z},\\ (\\mathbf{Y} \\sim N(\\mu,\\mathbf{\\sum})),\\ Choleski\\ decomposition\\ \\mathbf{\\sum}=\\mathbf{A}\\mathbf{A^{'}}\\) \\(Compute\\ {\\phi}(W_{i}),\\ i=1,..,n\\) Set \\(X_{i}= {F_{i}^{-1}}(\\phi(W_{i}))\\) \uc774\ub294 \uc704\uc5d0\uc11c \uac04\ub2e8\ud558\uac8c \uc2dc\ubbac\ub808\uc774\uc158\uc744 \ud1b5\ud574 \ud655\uc778\ud588\ub358 \ubc29\ubc95\uacfc\ub294 \uc57d\uac04 \ub2e4\ub978 \ubc29\ubc95\uc774\ub2e4. \uc704\uc5d0\uc11c\ub294 \ubbf8\ub9ac \uacf5\ubd84\uc0b0\ud589\ub82c\uc744 \uc2a4\ucf00\uc77c\ub9c1\ud55c \ud6c4\uc5d0 \ucd10\ub808\uc2a4\ud0a4 \ubd84\ud574\ub97c \ud1b5\ud574 \uc885\uc18d\uad6c\uc870\ub97c \uac16\ub294 \uc774\ubcc0\ub7c9\ud655\ub960\ubcc0\uc218 \\(X_{i}\\) \ub97c \ub9cc\ub4e4\uc5c8\uc9c0\ub9cc, \uc55e\uc5d0\uc11c \uc18c\uac1c\ud55c \ubc29\uc2dd\uc740 cdf\uc758 \ubd84\ud3ec\ub294 Uniform \ubd84\ud3ec\ub97c \uac16\ub294\ub2e4\ub294 \uc810\uc744 \ucc29\uc548\ud558\uc5ec \uc0c1\uad00\uad00\uacc4 \ud589\ub82c\uc744 \ucd08\uae30\uac12\uc73c\ub85c \uc2dc\uc791\ud558\uc5ec cdf\ub85c\uc758 \ubcc0\ud658\uc744 \ud1b5\ud574 \uc774\ubcc0\ub7c9\uade0\ub4f1 random sample\uc744 \ub9cc\ub4e0 \ud6c4, \uc774\ub97c \ubaa9\ud45c \ubd84\ud3ec\uc758 inverse cdf\ub85c \ubcc0\ud658\ud558\uc5ec \uc6d0\ud558\ub294 \\(X_{i}\\) \uc758 \uc885\uc18d\uad6c\uc870\ub97c \uac16\ub294 \ub2e4\ub978\ubd84\ud3ec\uc758 \uc774\ubcc0\ub7c9 random sample\uc744 \ub9cc\ub4dc\ub294 \ubc29\ubc95\uc774\ub2e4. \uc774\ub97c \uac04\ub2e8\ud788 \uc2dc\ubbac\ub808\uc774\uc158\uc744 \ud574\ubcf4\uc790. \uc774\ubcc0\ub7c9\ubfd0 \uc544\ub2c8\ub77c, \uadf8 \uc774\uc0c1\uc758 \ub2e4\ubcc0\ub7c9\ud655\ub960\ubcc0\uc218\ub97c sampling\uae4c\uc9c0 \ud560 \uc218 \uc788\ub3c4\ub85d \ud568\uc218\ub97c \uc9dc\ubcf4\uc558\ub2e4. \uc774\ub7ec\ud55c \ud568\uc218\ub97c \ud1b5\ud574 \ud655\ub960\ubcc0\uc218\ub4e4\uc744 \uc0d8\ud50c\ub9c1\ud560 \ub54c, correlation matrix\uc758 \ube44\ub300\uac01\uc131\ubd84\uc778 \\({\\rho}_{ij},\\ i\\neq{j}\\) \ub4e4\uc744 \uc77c\uc77c\ud788 \uc785\ub825\ud558\uc9c0\uc54a\uace0\ub3c4 \ud568\uc218\ub97c \uc9e4 \uc218 \uc788\uc9c0 \uc54a\uc744\uae4c \ud588\uc9c0\ub9cc, \ud544\uc790\uc758 \uba38\ub9ac\uc758 \ud55c\uacc4\ub85c \uc778\ud574.. \\(n\\times{n}\\ matrix\\) \uc5d0\uc11c \uc0c1\uad00\uacc4\uc218\ud589\ub82c\uc758 \ube44\ub300\uac01 \uc131\ubd84\uc758 \uac2f\uc218\ub294 \\(\\frac{n(n+1)}{2}\\) \uc774\uae30\uc5d0 \uc774 \uac2f\uc218\ub9cc\ud07c \\(\\rho\\) \ub4e4\uc744 \ubca1\ud130\ud615\ud0dc\ub85c \uc785\ub825\ud558\uc5ec \uc774\ubcc0\ub7c9\uc774\uc0c1\uc758 \ud655\ub960\ubcc0\uc218\ub97c \uc0d8\ud50c\ub9c1\ud558\ub294 \uc870\uae08\uc740 \uc218\ub3d9\uc801\uc778 \ubc29\ubc95\uc73c\ub85c\ub9cc \ud568\uc218\ub97c \uad6c\ud604\ud558\uc600\ub2e4. copula <- function(n,mu,var){ #C is correlation matrix C <- matrix(0,3,3) # function\uc548\uc5d0\uc11c \uc77c\uc77c\ud788 rho\ub97c \uc785\ub825\ud574\uc918\uc57c\ud558\ub294 \uac83\uc774 \uc544\uc26c\uc6b8 \ub530\ub984\uc774\ub2e4.. #\uc0c1\uad00\uacc4\uc218\ud589\ub82c\uc744 \ub9cc\ub4e4\uae30\uc704\ud574 \uc0ac\uc6a9\ud55c for\ubb38\uc758 \ud2b9\uc131\uc5d0 \ub530\ub77c rho\ub85c \uc9c0\uc815\ud558\ub294 \ubca1\ud130\uc6d0\uc18c\uc758 \uc21c\uc11c\ub294 4x4 \ud589\ub82c\uae30\uc900\uc73c\ub85c #rho_12, rho_13, rho_23, rho_14, rho_24, rho_34 \uc2dd\uc758 \uc21c\uc11c\uc774\ub2e4. rho <- c(0.7,0.6,0.5) C[upper.tri(C)] <- rho C <- C+t(C) for (i in 1:nrow(C)) { for (j in 1:ncol(C)) { if(i==j){C[i,j]=1} }} m <- ncol(C) A <- t(chol(C)) #U is Gaussian Copula & U1, U2 distributed Uniform U <- matrix(nrow=n,ncol=m) colnames(U)=paste0('U',1:m) W <- matrix(nrow=n,ncol=m) colnames(W)=paste0('W',1:m) for (i in 1:n){ Z <- rnorm(m,0,1) W <- A%*%Z U[i,] <- pnorm(W) } result <- list() result[['Gaussian Copula']] <- U X <- matrix(nrow=n,ncol=m) colnames(X) <- paste0('X',1:m) #Using inverse cdf, transform U to objective distribution for (j in 1:m){ X[,j] <- qnorm(U[,j],mean=mu[j], sd=sqrt(var[j])) } result[['Generated Multivariate Normal Random Sample']] <- X return(result) } n <- 5000 mu <- c(5,10,15) var <- c(9,15,30) result <- copula(n=n,mu=mu,var=var) U <- result[[1]] X <- result[[2]] cor(U) ## U1 U2 U3 ## U1 1.0000000 0.6824444 0.5935367 ## U2 0.6824444 1.0000000 0.4945416 ## U3 0.5935367 0.4945416 1.0000000 mydf<- as.data.frame(X) mydf2 <- as.data.frame(U) library(scatterplot3d) library(ggplot2) p2 <- scatterplot3d(mydf2[,1:3],pch=16, color=\"steelblue\",grid=T,box=F,angle=55, main=\"Generated Gaussian Copula\") my.lm2 <- lm(mydf2[,3]~mydf2[,1]+mydf2[,2]) p2$plane3d(my.lm2) U\uc758 \uc0c1\uad00\uacc4\uc218\ud589\ub82c\uc744 \uc0b4\ud3b4\ubcf4\uba74 \ucd08\uae30\uc5d0 \uc124\uc815\ud588\ub358 \uac12\uacfc \ub9e4\uc6b0 \uc720\uc0ac\ud55c \uac12\uc774 \ub098\uc634\uc744 \ud655\uc778 \ud560 \uc218 \uc788\ub2e4. p <- scatterplot3d(mydf[,1:3],pch=16, color=\"steelblue\",grid=T,box=F,angle=55, main='Generated 3-variate Normal Random Sample') my.lm <- lm(mydf[,3]~mydf[,1]+mydf[,2]) p$plane3d(my.lm) cor(X) ## X1 X2 X3 ## X1 1.0000000 0.7048168 0.6160424 ## X2 0.7048168 1.0000000 0.5171809 ## X3 0.6160424 0.5171809 1.0000000 cov(X) ## X1 X2 X3 ## X1 9.125139 8.484152 10.41140 ## X2 8.484152 15.879062 11.53012 ## X3 10.411404 11.530122 31.30098 apply(X,2,mean) ## X1 X2 X3 ## 5.081047 10.099912 15.009950 X \ub610\ud55c \uc0c1\uad00\uacc4\uc218\ud589\ub82c, \uacf5\ubd84\uc0b0\ud589\ub82c, \ud3c9\uade0\uc774 \ubaa9\ud45c\ubd84\ud3ec\uc758 \ubaa8\uc218\uc640 \ub9e4\uc6b0 \uc720\uc0ac\ud558\uac8c \uadfc\uc0ac\ud568\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. par(mfrow=c(2,3)) hist(U[,1]) hist(U[,2]) hist(U[,3]) hist(X[,1]) hist(X[,2]) hist(X[,3]) \uc0d8\ud50c\ub9c1\ub41c \uac12\ub4e4\uc758 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \uc0b4\ud3b4\ubcf4\uba74, \uc721\uc548\uc73c\ub85c \ud655\uc778\ud558\uc5ec\ub3c4 \uac00\uc6b0\uc2dc\uc548 \ucf54\ud4f0\ub7ec U\uc640 \uc0dd\uc131\ub41c 3\ubcc0\ub7c9\uc815\uaddc\ud655\ub960\ubcc0\uc218 X\uc758 marginal distribution\uc774 \uac01\uac01 \uade0\ub4f1\ubd84\ud3ec\uc640 \uc815\uaddc\ubd84\ud3ec\uc758 form\uc5d0 \uc720\uc0ac\ud558\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. Inverse cdf\ub97c \ud1b5\ud574 \uc6b0\ub9ac\uac00 \ub2e4\ub978 \uc5ec\ub7ec \ubaa9\ud45c \ubd84\ud3ec\ub97c \uc720\ub3c4\ud558\uace0\uc2f6\ub2e4\uba74, \uc5b8\uc81c\ub4e0\uc9c0 \uadf8 \ubaa8\uc218\ub4e4\ub9cc \uc9c0\uc815\uc744 \ud574\uc8fc\uc5b4\uc11c \uc0c8\ub86d\uac8c \ubf51\uc544\ub0bc \uc218 \uc788\ub2e4. \uc77c\ub840\ub85c \uc9c0\uc218\ubd84\ud3ec \\(Exp(\\lambda)\\) \uc758 inverse cdf\ub294 \\(F^{-1}(p;\\lambda)= -\\frac{1}{\\lambda}log(1-p)\\) \ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub294\ub370, \uc774\ub97c \uc704\uc5d0\uc11c \uc9e0 \ud568\uc218\uc5d0 \ub300\uc785\ud558\uc5ec \\(\\lambda=(1,0.7,0.3)\\) \uc778 \uc9c0\uc218\ubd84\ud3ec \\(X_1,X_2,X_3\\) \ub97c \uc0dd\uc131\ud574\ubcf4\uaca0\ub2e4. lambda1 <-1 lambda2 <-0.7 lambda3 <- 0.3 X1 <- -(1/lambda1)*log(1-U[,1]) X2 <- -(1/lambda2)*log(1-U[,2]) X3 <- -(1/lambda3)*log(1-U[,3]) multivariate_exp_random_sample <- cbind(X1,X2,X3) cor(multivariate_exp_random_sample) ## X1 X2 X3 ## X1 1.000000 0.671786 0.566118 ## X2 0.671786 1.000000 0.463459 ## X3 0.566118 0.463459 1.000000 par(mfrow=c(1,3)) hist(X1) hist(X2) hist(X3) library(MASS) fitdistr(X1,\"exponential\") ## rate ## 0.97348415 ## (0.01376714) fitdistr(X2,\"exponential\") ## rate ## 0.673762741 ## (0.009528444) fitdistr(X3,\"exponential\") ## rate ## 0.296073253 ## (0.004187108) \uc218\uce58\ub97c \ud655\uc778\ud560 \uc218 \uc788\ub4ef\uc774 \uae30\uc874\uc5d0 \uc124\uc815\ud588\ub358 \uc885\uc18d\uad6c\uc870\ub97c \uc798 \uc720\uc9c0\ud558\ub294 \uac83\ucc98\ub7fc \ubcf4\uc774\uace0, \uc0c8\ub86d\uac8c \uc0dd\uc131\ud55c \ubd84\ud3ec\uc758 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \ubcfc\ub54c \ubaa9\ud45c\ubd84\ud3ec\uc640 \uc798 \uc801\ud569\ud558\ub294 \uac83\ucc98\ub7fc \ubcf4\uc778\ub2e4. \ub610\ud55c \uc774\ub4e4\uc758 \ubaa8\uc218\ub97c \uadfc\uc0ac\uc801\uc73c\ub85c \ucd94\uc815\ud574\ubcf4\uc544\ub3c4 \uc6d0\ud558\ub294 \\(\\lambda\\) \uac12\uacfc \ub9e4\uc6b0 \uc720\uc0ac\ud55c \uac12\uc744 \ubaa8\uc218\ub85c \uac16\ub294\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.","title":"Simulation and check Copula"},{"location":"00%20This%20and%20That/01_Copula/#_1","text":"","title":"\ub057~"},{"location":"00%20This%20and%20That/02_Why_MLE_/","text":"\\[f(x;\\theta) = L(\\theta;x)= \\prod_{i=1}^nf(x_i;\\theta),\\ \\ \\ \\theta \\in \\Omega\\] \\[\\hat\\theta_{mle}=max_{\\theta}(\\theta)=max_{\\theta}\\prod_{i=1}^nf(x_i;\\theta)\\] \\[\\hat\\theta_{mle}=max_{\\theta}logL(\\theta)=max_{\\theta}\\sum_{i=1}^nlogf(x_i;\\theta)\\] \\[Mle\\ can\\ be\\ obtained\\ by\\ solving\\ this\\ equation\\ \\frac{\\partial{l(\\theta)}}{\\partial{\\theta}}=0,\\ where\\ l(\\theta)=logL(\\theta)\\] \\[Actually\\ need\\ to\\ check\\ \\frac{\\partial^2l(\\theta)}{\\partial{\\theta}}|_{\\theta=\\hat{\\theta}}<0\\] \\[Assume\\ that\\ X_1,...,X_n\\ satisfy\\ the \\ regularity\\ conditions\\ (R0)-(R2),\\ where\\ {\\theta}_0\\ is\\ the\\ true\\ parameter,\\\\ and\\ further\\ that\\ f(x;\\theta)\\ is\\ differentiable\\ with\\ respect\\ to\\ {\\theta}\\ in {\\Theta}.\\\\ Then,\\ \\frac{\\partial{l(\\theta)}}{\\partial{\\theta}}=0\\ is\\ a\\ solution\\ \\hat\\theta_n \\xrightarrow{P} {\\theta}_0\\] \\[(R0)-(R5)\\ are\\ satisfied,\\ suppose\\ further\\ that\\ 0<I(\\theta)<\\infty,\\\\ Then\\ any\\ consistent\\ sequence\\ if\\ solutions\\ of\\ the\\ MLE\\ equations\\ satisfies\\\\ \\sqrt{n}(\\hat\\theta_n-\\theta_0)\\xrightarrow{D}N(0,\\frac{1}{I(\\theta_0)}\\] \\[Applying\\ the\\ Taylor\\ Expansion\\ on\\ l'(\\hat\\theta_n),\\\\ l'(\\hat\\theta_n)= l'(\\theta_0)+l''(\\theta_0)(\\hat\\theta_n-\\theta_0)+\\frac{1}{2!}l'''(\\theta_n^*)(\\hat\\theta_n-\\theta_0)^2\\ when\\ \\theta_n^*\\ is\\ between\\ {\\theta_0} \\&\\ \\hat\\theta_n \\] \\[Because\\ l'(\\hat\\theta_n)=0,\\\\ (\\hat\\theta_n-\\theta_0)=\\frac{-l'(\\theta_0)}{l''(\\theta_0)+\\frac{1}{2}l'''(\\theta_n^*)(\\hat\\theta_n-\\theta_0)}\\\\ \\rightarrow \\sqrt{n}(\\hat\\theta_n-\\theta_0) = \\frac{\\frac{1}{\\sqrt{n}}l'(\\theta_0)}{\\frac{-1}{n}l''(\\theta_0)-\\frac{-1}{2n}l'''(\\theta_n^*)(\\hat\\theta_n-\\theta_0)}= \\frac{A_n}{B_n+C_n} \\] \\[A_n \\xrightarrow{d} N(0,I(\\theta_0)),\\\\ B_n \\xrightarrow{p} I(\\theta_0),\\\\ C_n\\ is\\ from\\ the\\ condition\\ (R5)\\ \\&\\ \\hat\\theta_n\\xrightarrow{p}\\theta_0,\\\\ Bounded*0 \\xrightarrow{p} 0\\\\ So,\\ \\frac{A_n}{B_n+C_n}=\\frac{1}{I(\\theta_0)}N(0,I(\\theta_0))= N(0,\\frac{1}{I(\\theta_0)})\\] \\[The\\ joint\\ pdf\\ is\\\\ L(\\theta;x_1,..,x_n)={\\theta}^n exp(-\\theta\\sum_{i=1}^nx_i),\\ for,\\ i=1,..,n.\\\\ From\\ the\\ factorization\\ theorem,\\ Y_1 = \\sum_{i=1}^nX_i\\ is\\ sufficient(does\\ not\\ depend\\ on\\ \\theta).\\\\ The\\ log\\ of\\ the\\ likelihood\\ is\\\\ l(\\theta)=nlog\\theta-\\theta\\sum_{i=1}^nx_i.\\\\ Then\\ the\\ MLE\\ Y_2=\\frac{1}{\\bar{X}}=n/Y_1\\ is\\ a\\ function\\ of\\ sufficient\\ statistic\\ Y_1. \\]","title":"R Notebook"},{"location":"00%20This%20and%20That/MM_takehome_exam/","text":"MatheMaticals Statistics 1st take home exam For each \\(n \\geq 1\\) , let \\(X_n\\) be a random variable with the Binomial \\((n,p_n)\\) distribution. Suppose that \\(p_n \\rightarrow 0\\) and \\(n{p_n} \\ rightarrow \\inf\\) as \\(n \\rightarrow \\infty\\) for sime \\(0< \\lambda< \\infty\\) Problem 1. Show that there exist \\(\\{ {{\\pi}_k} \\}_{k \\geq 0}\\) such that $\\sum_{k=0}^{\\infty}|p_{n,k}-\\pi_k|^2 \\rightarrow 0\\ as\\ n \\rightarrow \\infty $, where $p_{n,k} \\equiv P(X_n = k),\\ k \\geq 0,\\ n \\geq 1 $. Under Assumption, we can use Binomial and Poisson's mgfs link. mgf of \\(X_n\\) is computed as \\(M_{X_n}(t) = E[e^{tX_n}]= \\sum_{x_n}^ne^{tx_n}\\dbinom{n}{x_n}p_n^{x_n}(1-p_n)^{n-x_n}\\) Let \\(x_n = k\\) , \\(M_{X_n}(t)=\\sum_{x_n}^ne^{tk}\\dbinom{n}{k}p_n^{k}(1-p_n)^{n-k}\\) \\(=\\sum_{k=0}^n\\dbinom{n}{k}(p_ne^t)^k(1-p_n)^{n-k}= (p_ne^t+(1-p_n))^n,\\ ((a+b)^n=\\sum_{k}^n\\dbinom{n}{k}a^{k}b^{n-k} )\\) Mgf of \\(Poisson(\\lambda)\\) is expressed like \\(M_Y(t)=e^{\\lambda(e^t-1)}\\) Because \\(p_n \\rightarrow 0\\) and \\(n{p_n} \\rightarrow \\inf\\) as \\(n \\rightarrow \\infty\\) for sime \\(0< \\lambda< \\infty\\) , \\(lim_{n\\rightarrow \\infty}M_{X_n}(t)=lim_{n\\rightarrow \\infty}(p_ne^t+(1-p_n))^n= lim_{n\\rightarrow \\infty}(1+\\frac{np(e^t-1)}{n})^n= e^{\\lambda(e^t-1)}\\) Then \\(lim_{n\\rightarrow \\infty}E[e^{tk}]= lim_{n\\rightarrow \\infty}\\sum_{k}^ne^{tk}p_{n,k}\\) . So, $ lim_{n \\rightarrow \\infty}(\\sum_{k=0}^n({e^{tk}}p_{n,k}- \\frac{e^{\\lambda(e^t-1)}}{(n+1)e^{tk}}))=0$ \\(\\sum_{k=0}^\\infty|p_{n,k}-{\\pi}_k|^2 \\rightarrow 0\\ as\\ n \\rightarrow \\infty\\) \\(morbidity_{ij} = \\beta_0 + \\beta_1(low\\ temperature_{ij})+\\beta_2(low\\ temperature_{ij} - \\psi)_{+} + \\epsilon_{ij}\\) \\(where,\\ (high\\ temperature_{ij} - \\psi)_{+}= (high\\ temperature_{ij}-\\psi)\\times I(high\\ temperature_{ij} > \\psi), \\epsilon_{ij} \\sim N(0,\\sigma^2)\\) \\(mortality\\) \\(\\psi_{heat,i}=\\beta_0 + \\beta_1(high\\ temperature_i)+ \\beta_2(max\\ humidity_i)+ \\mathbf{\\beta_3}ses_i^T + \\epsilon_i\\) \\(where\\ \\epsilon_i \\sim N(0,\\sigma^2),\\ ses\\ is\\ Socioeconomic\\ Status\\)","title":"R Notebook"},{"location":"00%20This%20and%20That/MM_takehome_exam/#mathematicals-statistics-1st-take-home-exam","text":"","title":"MatheMaticals Statistics 1st take home exam"},{"location":"00%20This%20and%20That/MM_takehome_exam/#for-each-n-geq-1-let-x_n-be-a-random-variable-with-the-binomial-np_n-distribution-suppose-that-p_n-rightarrow-0-and-np_n-rightarrow-inf-as-n-rightarrow-infty-for-sime-0-lambda-infty","text":"","title":"For each \\(n \\geq 1\\), let \\(X_n\\) be a random variable with the Binomial \\((n,p_n)\\) distribution. Suppose that \\(p_n \\rightarrow 0\\) and \\(n{p_n} \\ rightarrow \\inf\\) as \\(n \\rightarrow \\infty\\) for sime \\(0&lt; \\lambda&lt; \\infty\\)"},{"location":"00%20This%20and%20That/MM_takehome_exam/#problem-1-show-that-there-exist-pi_k-_k-geq-0such-that","text":"$\\sum_{k=0}^{\\infty}|p_{n,k}-\\pi_k|^2 \\rightarrow 0\\ as\\ n \\rightarrow \\infty $, where $p_{n,k} \\equiv P(X_n = k),\\ k \\geq 0,\\ n \\geq 1 $. Under Assumption, we can use Binomial and Poisson's mgfs link. mgf of \\(X_n\\) is computed as \\(M_{X_n}(t) = E[e^{tX_n}]= \\sum_{x_n}^ne^{tx_n}\\dbinom{n}{x_n}p_n^{x_n}(1-p_n)^{n-x_n}\\) Let \\(x_n = k\\) , \\(M_{X_n}(t)=\\sum_{x_n}^ne^{tk}\\dbinom{n}{k}p_n^{k}(1-p_n)^{n-k}\\) \\(=\\sum_{k=0}^n\\dbinom{n}{k}(p_ne^t)^k(1-p_n)^{n-k}= (p_ne^t+(1-p_n))^n,\\ ((a+b)^n=\\sum_{k}^n\\dbinom{n}{k}a^{k}b^{n-k} )\\) Mgf of \\(Poisson(\\lambda)\\) is expressed like \\(M_Y(t)=e^{\\lambda(e^t-1)}\\) Because \\(p_n \\rightarrow 0\\) and \\(n{p_n} \\rightarrow \\inf\\) as \\(n \\rightarrow \\infty\\) for sime \\(0< \\lambda< \\infty\\) , \\(lim_{n\\rightarrow \\infty}M_{X_n}(t)=lim_{n\\rightarrow \\infty}(p_ne^t+(1-p_n))^n= lim_{n\\rightarrow \\infty}(1+\\frac{np(e^t-1)}{n})^n= e^{\\lambda(e^t-1)}\\) Then \\(lim_{n\\rightarrow \\infty}E[e^{tk}]= lim_{n\\rightarrow \\infty}\\sum_{k}^ne^{tk}p_{n,k}\\) . So, $ lim_{n \\rightarrow \\infty}(\\sum_{k=0}^n({e^{tk}}p_{n,k}- \\frac{e^{\\lambda(e^t-1)}}{(n+1)e^{tk}}))=0$ \\(\\sum_{k=0}^\\infty|p_{n,k}-{\\pi}_k|^2 \\rightarrow 0\\ as\\ n \\rightarrow \\infty\\) \\(morbidity_{ij} = \\beta_0 + \\beta_1(low\\ temperature_{ij})+\\beta_2(low\\ temperature_{ij} - \\psi)_{+} + \\epsilon_{ij}\\) \\(where,\\ (high\\ temperature_{ij} - \\psi)_{+}= (high\\ temperature_{ij}-\\psi)\\times I(high\\ temperature_{ij} > \\psi), \\epsilon_{ij} \\sim N(0,\\sigma^2)\\) \\(mortality\\) \\(\\psi_{heat,i}=\\beta_0 + \\beta_1(high\\ temperature_i)+ \\beta_2(max\\ humidity_i)+ \\mathbf{\\beta_3}ses_i^T + \\epsilon_i\\) \\(where\\ \\epsilon_i \\sim N(0,\\sigma^2),\\ ses\\ is\\ Socioeconomic\\ Status\\)","title":"Problem 1. Show that there exist \\(\\{ {{\\pi}_k} \\}_{k \\geq 0}\\)such that"},{"location":"000%20CPA%20Data%20Analysis/likelihood/","text":"nav: - Home: index.md - Mathematical Statistics: - Likelihood: 000 CPA Data Analysis /likelihood.md # Likelihood Example \ub098\ub294 \uae40\uc131\uc644\uc774\ub2e4. \uc774 \ud398\uc774\uc9c0\ub294 \ud14d\uc2a4\ud2b8\uc640 LaTeX \uc218\uc2dd \ud14c\uc2a4\ud2b8\uc6a9\uc774\ub2e4. \ud45c\ubcf8 \ud3c9\uade0\uc740 \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\) \ub85c \uc815\uc758\ub41c\ub2e4. \uc815\uaddc\ubd84\ud3ec\ub97c \uac00\uc815\ud558\uba74 \ub85c\uadf8\uc6b0\ub3c4\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\[ \\ell(\\mu) = -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2 \\] \uc774\ub97c \ucd5c\ub300\ud654\ud558\uba74 MLE\ub294 \\(\\hat{\\mu} = \\bar{X}\\) \uc774\ub2e4. git add .","title":"Likelihood"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/","text":"Two simple Approachs to Prediction \uc55e\uc758 Introduction\uc5d0\uc11c\ub294 \uac00\ubccd\uac8c \uc9c0\ub3c4\ud559\uc2b5\uacfc \ube44\uc9c0\ub3c4\ud559\uc2b5\uc744 \uc124\uba85\ud558\uba70 \uae00\uc744 \uc2dc\uc791\ud558\uc600\ub2e4. \uad50\uc7ac\uc758 \uc55e\ubd80\ubd84\uc5d0\uc11c \uc911\uc810\uc801\uc73c\ub85c \ub2e4\ub8f0 \uc9c0\ub3c4\ud559\uc2b5\uc740 \uac04\ub2e8\ud558\uac8c \uc785\ub825\uac12\uc5d0 \ub300\ud55c \ubaa9\ud45c\uce58\uac00 \uc8fc\uc5b4\uc838\uc788\uc73c\uba70, \uc774\ub97c \uac00\uc9c0\uace0 \uc788\ub294 \ub370\uc774\ud130\ub97c \uc774\uc6a9\ud574 \uc608\uce21\ud558\uac70\ub098 \ubd84\ub958\ud558\ub294 \uac83\uc774\ub77c\uace0 \ub9d0\ud560 \uc218 \uc788\ub2e4. 2\ub2e8\uc6d0\uc758 \uc2dc\uc791\uc73c\ub85c\ub294 \uc608\uce21\uc5d0 \uc788\uc5b4 \uac04\ub2e8\ud558\uac8c \uc0ac\uc6a9\ud558\ub294 \ub450 \uac00\uc9c0 \ubc29\ubc95\uc744 \uc18c\uac1c\ud55c\ub2e4. Least Squares Nearest Neighbors rule Least Squares \ub300\ud45c\uc801\uc73c\ub85c \uc6b0\ub9ac\uac00 \uc54c\uace0\uc788\ub294 \uc120\ud615\ubaa8\ub378\uc744 \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\[\\hat{Y}=\\hat{\\beta_0}+\\sum\\limits_{j=1}^p{X_{j}\\hat{\\beta_{j}}}\\] \uc704\uc758 \uc2dd\uc5d0\uc11c \\(X^T = (X_1,X_2,..,X_{p})\\) \uc758 \ud615\ud0dc\ub97c \ub744\ub294 \ubca1\ud130\uc774\ub2e4. \uc5ec\uae30\uc11c \\(\\hat{\\beta_0}\\) \ub294 intercept \uc774\uba70 \uc774\ub294 \uae30\uacc4\ud559\uc2b5\uc5d0\uc11c bias(\ud3b8\ud5a5) \uc774\ub77c \uce6d\ud558\uae30\ub3c4\ud55c\ub2e4. \uc6b0\ub9ac\ub294 \ud3b8\uc758\uc0c1 \\(\\hat{Y}\\) \ub97c \uae54\ub054\ud558\uac8c \ud45c\ud604\ud558\uae30 \uc704\ud574 \\({X}\\) \ubca1\ud130 \uc548\uc5d0 constant variable 1\uc744 \uc9d1\uc5b4 \ub123\uc5b4 \\(\\hat{\\beta_0}\\) \ub97c \\(\\hat{\\beta}\\) \uc5d0 \ud3ec\ud568\uc2dc\ucf1c \\(\\hat{Y}={X}^T\\hat{\\beta}\\) \ub85c \ud45c\uae30\ud55c\ub2e4. \uc774\ub294 \uc120\ud615\ubaa8\ub378\uc744 \ub0b4\uc801\uc758 \ud615\ud0dc\ub85c \ub9cc\ub4e4\uc5b4 \ud45c\uae30\ud55c\ub2e4\ub294 \uac83\uc5d0 \uc788\uc5b4 \ub610 \ub2e4\ub978 \ud3b8\ub9ac\ud568\uc774 \uc874\uc7ac\ud55c\ub2e4. \uc704\uc758 \uc120\ud615\ubaa8\ub378 \uc2dd\uc5d0 \ub530\ub974\uba74 \\({Y}\\) \ub294 scalar\uc9c0\ub9cc, \uc77c\ubc18\uc801\uc73c\ub85c K-vector\ub77c\uace0 \ud55c\ub2e4\uba74, \\({\\beta}\\) \ub294 \\(p\\times{K}\\) \uc758 \ud589\ub82c\uc774 \ub41c\ub2e4. \\(({X},{\\hat{Y}})\\) \ub294 \\(p+1\\) \ucc28\uc6d0\uc758 \uc785\ucd9c\ub825 \uacf5\uac04\uc5d0 \uc874\uc7ac\ud558\ub294 hyperplane\uc744 \ub098\ud0c0\ub0b4\uac8c \ub41c\ub2e4. \uc5b4\ub5bb\uac8c \ud558\uba74 \uac00\uc9c0\uace0 \uc788\ub294 trainig data\ub97c \uc120\ud615\ubaa8\ub378\uc5d0 \uc798 fitting \uc2dc\ud0ac \uc218 \uc788\uc744\uae4c? \ub2e4\uc591\ud55c \ubc29\ubc95\ub4e4\uc774 \uc874\uc7ac\ud558\uc9c0\ub9cc, \uc704\uc5d0\uc11c \uc5b8\uae09\ud588\ub4ef\uc774 \uac00\uc7a5 \ub300\uc911\uc801\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 \ubc29\uc2dd\uc740 least squares method\uc77c \uac83\uc774\ub2e4. \uc774 \ubc29\ubc95\uc740 \uc2e4\uc81c \\({y}\\) \uac12\uacfc \uc120\ud615 \ubaa8\ub378\uc744 \ud1b5\ud574 \ucd94\uc815\ud55c \\({\\hat{y}}\\) \uac04\uc758 \ucc28\uc774\uc778 \uc794\ucc28\uc758 \uc81c\uacf1\uc744 \ucd5c\uc18c\ud654\ud558\ub294 \\({\\beta}\\) \ub97c \ucc3e\ub294 \uc6d0\ub9ac\ub97c \uc774\uc6a9\ud558\uba70, \uc774\ub97c \uc218\uc2dd\uc73c\ub85c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(RSS({\\beta})= \\sum\\limits_{i=1}^{N}({y_{i}}-{x_{i}}^T{\\beta})^2\\) $ \\(RSS({\\beta})\\) \ub294 \ud30c\ub77c\ubbf8\ud130\uc5d0 \ub300\ud574 2\ucc28\ud56d\uc758 \ud568\uc218 \uaf34\uc774\uae30\uc5d0 \ucd5c\uc18c\uac12\uc774 \uc5b8\uc81c\ub098 \uc874\uc7ac\ud558\uc9c0\ub9cc, \uace0\uc720\uc758 \ud574\ub97c \uac16\uc9c4 \uc54a\uc744 \uc218\ub3c4 \uc788\ub2e4. \ud574\ub97c \uad6c\ud558\ub294 \uac83\uc744 \ud589\ub82c\ub85c \ub3c4\uc2dd\ud654 \ud55c\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. $ \\(RSS({\\beta})=(\\mathbf{y}-\\mathbf{X}{\\beta}^T)(\\mathbf{y}-\\mathbf{X}{\\beta})\\) $ \uc5ec\uae30\uc11c X \ub294 Nxp \ud589\ub82c\uc774\uba70 \uac01\uac01\uc758 \ud589\uc740 \uc785\ub825 \ubca1\ud130\ub97c \uc758\ubbf8\ud558\uace0, y \ub294\ud6c8\ub828 \ub370\uc774\ud130 \uc14b\uc758 \ucd9c\ub825\uac12\uc73c\ub85c N-vector\ub97c \uac16\ub294\ub2e4. \uc704\uc758 \uc2dd\uc744 {\\beta}\uc5d0 \ub300\ud574\uc11c \ubbf8\ubd84\uc744 \ud574\ubcf4\uba74 \uc6b0\ub9ac\ub294 \uc544\ub798\uc640 \uac19\uc740 normal equations\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4. $ \\(\\mathbf{X}^T(\\mathbf{y}-\\mathbf{X}{\\beta})=0\\) $ \ub9cc\uc57d \\(\\mathbf{X}^T\\mathbf{X}\\) \uac00 nonsingular \uc989 \\(det(\\mathbf{X}^T\\mathbf{X})=0\\) \uc774\ub77c\uba74 \uc720\ub2c8\ud06c\ud55c \uac12\uc744 \uac16\ub294 \ud574\ub294 \\({\\hat{\\beta}}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\) \uc774\uba70, i\ubc88\uc9f8 \uc785\ub825\uac12 \\({x_{i}}\\) \uc758 fitted value\ub294 \\(\\hat{y_{i}}=\\hat{y}(x_{i})=x_{i}^T\\hat{\\beta}\\) \uac00 \ub41c\ub2e4. \uc804\uccb4 fitted surface\ub294 p\uac1c\uc758 \ud30c\ub77c\ubbf8\ud130 \\(\\hat{\\beta}\\) \ub85c \uc774\ub8e8\uc5b4\uc838\uc788\ub2e4. \uc9c1\uad00\uc801\uc73c\ub85c \ubcfc \ub54c \uc774\ub294 \uc6b0\ub9ac\uac00 \uc9c0\ub098\uce58\uac8c \ub9ce\uc740 \ub370\uc774\ud130 \uc14b\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc544\ub3c4 \ud2b9\uc815 \ubaa8\ub378\uc5d0 fitting \uc2dc\ud0ac \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc900\ub2e4. Example \uc120\ud615 \ubaa8\ub378\uc744 \uc774\uc6a9\ud55c \uac04\ub2e8\ud55c \ubd84\ub958\uc758 \uc608\uc2dc\ub97c \uc0b4\ud3b4\ubcf4\uc790. \uc774\ub294 \uad50\uc7ac figure 2.1\uc5d0 \uc218\ub85d\ub41c \uc0b0\uc810\ub3c4\uc774\ub2e4. \uc774\ub294 \\(X_1\\) \uacfc \\(X_2\\) \uc30d\uc758 \uc0b0\uc810\ub3c4\ub97c \ub098\ud0c0\ub0b4\uba70, \ucd9c\ub825\ub418\ub294 class variable G\ub294 BLUE or ORANGE\ub85c \ub450 \uac00\uc9c0\uac00 \uc788\ub2e4. \uac01 \ud074\ub798\uc2a4 \ubcc4\ub85c\ub294 100\uac1c\uc758 \ub370\uc774\ud130\uac00 \uc874\uc7ac\ud558\uba70, \uc120\ud615\ubaa8\ub378\uc744 \uc801\uc6a9\ud558\uc5ec \uc774 \ub370\uc774\ud130\uc5d0 fitting\uc744 \ud55c\ub2e4. \ud68c\uadc0\ubd84\uc11d\uc744 \uc704\ud574 response Y\ub97c BLUE\uba74 0\uc73c\ub85c, ORANGE\uba74 1\ub85c \ud45c\uae30\ud558\uba70, fitted values\uc778 \\(\\hat{Y}\\) \ub294 \uc608\uc2dc\uc5d0 \uc81c\uc2dc\ub41c \uaddc\uce59\uc5d0 \ub530\ub77c\uc11c \\(\\hat{Y}\\) \uac00 0.5\ubcf4\ub2e4 \ud06c\ub2e4\uba74 fitted class variable\uc778 \\(\\hat{G}\\) \ub97c ORANGE(=1)\ub85c, \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74 BLUE(=0)\ub85c \ubc18\ud658\ud55c\ub2e4. \\(\\mathbf{R}^2\\) \uacf5\uac04\uc5d0\uc11c\uc758 \uc810\ub4e4\uc758 \uc14b\uc740 \uc704\uc5d0\uc11c \uc5b8\uae09\ud55c rule\uc5d0 \uc758\ud574 \ub450 \uac00\uc9c0\ub85c \ubd84\ub958\ub418\uba70, \uc774 \ub54c\uc758 \\({\\{x: x^T{\\hat{\\beta}}=0.5}\\}\\) \uc758 hyperplane\uc774 (\uc5ec\uae30\uc11c\ub294 1\ucc28\uc6d0\uc758 \uc9c1\uc120\uc744 \uc758\ubbf8\ud55c\ub2e4.) decision boundary \uac00 \ub41c\ub2e4. \uc774 \uacbd\uc6b0\uc5d0\uc11c \uc6b0\ub9ac\ub294 decision boundary \ub97c \uae30\uc900\uc73c\ub85c \uba87\uba87\uc758 \uc798\ubabb \ubd84\ub958\ub418\uc5b4\uc788\ub294 \uc810\ub4e4\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc774\ub7ec\ud55c \uc624\ubd84\ub958\uac00 \uc77c\uc5b4\ub098\ub294 \uac83\uc774 \ubaa8\ub378\uc774 \ub108\ubb34 \uc5c4\uaca9\ud574\uc11c \uc77c\uae4c ? \uc544\ub2c8\uba74 \uc774\ub7ec\ud55c \uba87\uba87 error\ub4e4\uc740 \ubd88\uac00\ud53c\ud55c \uc624\ub958\uc778 \uac83\uc77c\uae4c? \uc6b0\ub9ac\uac00 \uac00\uc9c0\uace0 \uc788\ub294 training data\ub97c \uc5b4\ub5bb\uac8c \uc5bb\uac8c \ub418\uc5c8\ub294\uc9c0\ub97c \uc5b8\uae09\ud558\uc9c0 \uc54a\uc740 \uc0c1\ud0dc\uc5d0\uc11c \ub450 \uac00\uc9c0 \uac00\ub2a5\ud55c \uc2dc\ub098\ub9ac\uc624\ub97c \uc0dd\uac01\ud574\ubcf4\uc790.(data\uac00 \uc5b4\ub5bb\uac8c gathering\ub418\uc5c8\ub294\uc9c0 \uc54c \uc218 \uc788\ub2e4\uba74 \ud310\ub2e8\uc740 \uc870\uae08 \ub2ec\ub77c\uc9c4\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud558\ub294 \uac83\uc77c\uae4c? \ucd94\ud6c4\uc5d0 \uc54c\uc544\ubcf4\uc790.) Scenario 1 : \uac01 \ubc94\uc8fc\uc5d0 \uc18d\ud558\ub294 training data\ub294 \uc11c\ub85c \ub3c5\ub9bd\uc774\uba70 \ub2e4\ub978 \ubaa8\ud3c9\uade0\uc744 \uac16\ub294 \uc774\ubcc0\ub7c9 \uac00\uc6b0\uc2dc\uc548 \ubd84\ud3ec\uc5d0\uc11c \uc0dd\uc131\ub418\uc5c8\ub2e4. Scenario 2: \uac01 \ubc94\uc8fc\uc5d0 \uc18d\ud558\ub294 training data\ub294 \uac01 \ubd84\ud3ec\uc758 \ubaa8\ud3c9\uade0\uc774 \uac00\uc6b0\uc2dc\uc548 \ubd84\ud3ec\ub97c \ub530\ub974\ub294 10\uac1c\uc758 low-variance \uac00\uc6b0\uc2dc\uc548 \ubd84\ud3ec\uc758 \ud63c\ud569\ubaa8\ub378\uc5d0\uc11c \uc0dd\uc131\ub418\uc5c8\ub2e4. \uac00\uc6b0\uc2dc\uc548 \ubd84\ud3ec\uc758 \ud63c\ud569\uc740 generative model\ub85c \uc798 \uc124\uba85\ud560 \uc218 \uc788\ub2e4. \uc5ec\uae30\uc11c generative model\uc774\ub77c \ud568\uc740, \uc8fc\uc5b4\uc9c4 \uc5ec\ub7ec\uac1c\uc758 \uac00\uc6b0\uc2dc\uc548 \ubd84\ud3ec \uc911\uc5d0\uc11c(\uc704\uc758 \uc2dc\ub098\ub9ac\uc6242\uc5d0\uc11c\ub294 10\uac1c\uc758 \uac00\uc6b0\uc2dc\uc548\ubd84\ud3ec.) \uc5b4\ub5a4 \ubd84\ud3ec\ub97c \uc0ac\uc6a9\ud560\uc9c0\ub97c \ud0dd\ud558\ub294 discrete\ud55c \ubcc0\uc218\ub97c \uc0dd\uc131\ud55c \ub4a4 \uc815\ud574\uc9c4 density\ub85c\ubd80\ud130 \uad00\uce21\uce58\ub97c \uc0dd\uc131\ud558\ub294 \ubaa8\ub378\uc774\ub77c\uace0 \uc124\uba85\ud560 \uc218 \uc788\ub2e4. \ud55c \ud074\ub798\uc2a4\ub9c8\ub2e4 \ud558\ub098\uc758 \uac00\uc6b0\uc2dc\uc548 \ubd84\ud3ec\ub97c \uc801\uc6a9\ud558\ub294 \uac83\uc740(=\ud55c \ud074\ub798\uc2a4\uc5d0 \uc18d\ud558\ub294 \uad00\uce21\uce58\uac00 \ud558\ub098\uc758 Normal\uc744 \ub530\ub974\ub294 \ud655\ub960\ubcc0\uc218\uc5d0\uc11c \ud30c\uc0dd\ub41c \uac83\uc774\ub77c\uba74,) \ub4a4\uc5d0 4\uc7a5\uc5d0\uc11c \ub2e4\uc2dc \ubc30\uc6b0\uaca0\uc9c0\ub9cc 1\ucc28\uc6d0\uc758 decision boundary\uac00 \ucd5c\uc120\uc758 \ubc29\ubc95\uc774\uba70, \uc774\ub54c \uc6b0\ub9ac\uac00 \uc5bb\uc740 \ucd94\uc815\uce58\uac00 \ucd5c\uc801\uc758 \uacb0\uacfc\uc77c \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub7ec\ud55c \uc120\ud615\uc801\uc778 decision boundary \ucf00\uc774\uc2a4\uc5d0\uc11c \ub450 \ubc94\uc8fc \uac04 \uc601\uc5ed\uc758 overlap\uc740 \ubd88\uac00\ud53c\ud558\uba70 \uc608\uce21\ud574\uc57c\ud558\ub294 \ubbf8\ub798\uc758 \ub370\uc774\ud130 \ub610\ud55c \uc774\ub7ec\ud55c overlap\uc758 \ub2aa\uc5d0\uc11c \uc790\uc720\ub85c\uc6b8 \uc218 \uc5c6\uc744 \uac83\uc774\ub2e4. \ub9cc\uc57d \uc5ec\ub7ec\uac1c\uc758 \ubc94\uc8fc\uac00 \ube7d\ube7d\ud558\uac8c \uc11c\ub85c \uac01\uac01\uc758 \ub2e4\ub978 \uc815\uaddc\ubd84\ud3ec\ub85c\ubd80\ud130 \ub098\uc628 \ud655\ub960\ubcc0\uc218\uc5d0\uc11c \ud30c\uc0dd\ub41c \uac83\uc774\ub77c\uba74, \uc774\uc57c\uae30\ub294 \uc870\uae08 \ub2ec\ub77c\uc9c4\ub2e4. \uc774 \ub54c\ub294 \uc9c1\uc120\uc73c\ub85c \uadf8\uc740 decision boundary\uac00 \ucd5c\uc801\uc758 \uc758\uc0ac\uacb0\uc815\uc744 \ub0b4\ub9ac\ub294\ub370\uc5d0 \uc801\uc808\ud558\uc9c0 \uc54a\uc744 \uac00\ub2a5\uc131\uc774 \ub192\uc73c\uba70, \uc2e4\uc81c\ub85c\ub3c4 \uadf8\ub807\ub2e4. \uc774\ub7f0 \ucf00\uc774\uc2a4\uc5d0\uc11c\uc758 \ucd5c\uc801\uc758 decision boundary\ub294 \uc704\uc640 \ub2e4\ub974\uac8c \ub354 nonlinear \ud560 \uac83\uc774\uba70, disjoint\ud558\uac8c \ud074\ub798\uc2a4 \uac04\uc758 \uad6c\ubd84\uc744 \uc798 \ud574\uc904 \uac83\uc774\uc9c0\ub9cc \uc774\ub97c \uc5bb\uae30\ub780 \ub9e4\uc6b0\ub9e4\uc6b0 \uc5b4\ub824\uc6b4 \uc77c\uc774\ub2e4.(\uc124\ub839 \ub108\ubb34\ub098\ub3c4 \uc798 \ubd84\ub958\ud558\uace0 \uc2f6\uc5b4\uc11c \uaf2c\ubd88\uaf2c\ubd88 \uad6c\uc5ed\uc744 \ub098\ub208\ub2e4 \ud558\ub354\ub77c\ub3c4 \uacfc\uc801\ud569\uc758 \ubb38\uc81c \ub610\ud55c \ubc1c\uc0dd\ud560 \uc218 \uc788\uaca0\uc9c0..) \uc774 \ub0b4\uc6a9\uc740 \ucd94\ud6c4\uc5d0 \uc870\uae08 \ub354 \ub2e4\ub904\ubcf4\ub3c4\ub85d \ud558\uace0 \uc704\uc5d0\uc11c \uc5b8\uae09\ud55c \ub450\ubc88\uc9f8 \uc2dc\ub098\ub9ac\uc624\uc5d0 \uc870\uae08 \ub354 \uc801\ud569\ud55c, \uc120\ud615\ud68c\uadc0\uc640\ub294 N\uadf9\uacfc S\uadf9 \ub9c8\ub0e5 \ubc18\ub300\uc120\uc0c1\uc5d0 \uc788\ub294 \ub2e4\ub978 \ubd84\ub958\ubc29\ubc95\uc744 \ubcf4\ub3c4\ub85d \ud558\uc790. Nearest-Neighbor Methods Nearest-Neighnor Methods\ub294 \uc785\ub825\uacf5\uac04\uc5d0\uc11c\uc758 x\uc640 \uac00\uc7a5 \uac00\uae4c\uc6b4 \uac70\ub9ac\uc5d0 \uc788\ub294 training set T \uc548\uc5d0 \uc788\ub294 \uad00\uce21\uce58\ub4e4\ub85c \\(\\hat{Y}\\) \ub97c \ud615\uc131\ud558\ub294 \ubc29\ubc95\uc774\ub2e4. \ud2b9\ud788 k\uac1c\uc758 \ucd5c\uadfc\uc811 \uc774\uc6c3\uc73c\ub85c fitting\ud55c \\(\\hat{Y}\\) \ub294 \uc544\ub798\uc640 \uac19\uc774 \uc815\uc758\ud560 \uc218 \uc788\ub2e4. $ \\(\\hat{Y}(x)=\\frac{1}{k}\\sum\\nolimits_{x_i \\in N_{k}(x)}{y_{i}}\\) $ \uc5ec\uae30\uc11c \\(N_{k}(x)\\) \ub780 training sample\uc548\uc5d0\uc11c \uc810 \\(x_{i}\\) \uacfc \uc81c\uc77c \uac00\uae4c\uc6b4 k\uac1c\uc758 \uad00\uce21\uce58\ub97c \uc758\ubbf8\ud55c\ub2e4. \uac00\uc7a5 nearest\ud55c \uac70\ub9ac\ub294 \uc5b4\ub5bb\uac8c \uc815\uc758\ub418\ub294\uac00? \uc774 \ubc29\ubc95\uc5d0\uc11c \uc6b0\ub9ac\ub294 Euclidean distance\ub97c \uac00\uc815\ud558\uba70, \uc774\ub294 x\uc5d0\uc11c \uc720\ud074\ub9ac\ub514\uc548 \uac70\ub9ac\ub85c \uc81c\uc77c \uac00\uae4c\uc6b4 k\uac1c\uc758 \uad00\uce21\uce58\ub4e4\uc744 \ucc3e\uace0, \uc774\ub4e4\uc758 \uc885\uc18d\ubcc0\uc218 (Response)\uc758 \ud3c9\uade0\uc744 \uad6c\ud558\ub294 \uac83\uc774\ub2e4. \uc55e\uc5d0\uc11c \uc5b8\uae09\ud55c binary classification\uc758 \uc608\uc81c\ub97c \uc774\ubc88\uc5d0\ub294 Nearest-Neighbor Methods\ub97c \uc774\uc6a9\ud558\uc5ec \uc811\uadfc\ud574\ubcf4\uace0\uc790 \ud55c\ub2e4. \ubb38\uc81c\uc5d0\uc11c k\ub294 15\ub85c \uc9c0\uc815\ud558\uc600\uc73c\uba70, binary\ub85c coded\ub41c \ucd9c\ub825\ubcc0\uc218\uc758 15\uac1c \ud3c9\uade0\uc73c\ub85c model fitting\uc744 \ud588\ub2e4. \uc704\uc758 \ubc29\ubc95\uc740 \uad00\uce21\ub41c \uac12\uc774 (0,1)\ub85c binary class\uc758 \ub450 \uac00\uc9c0 \uacb0\uacfc\uac12\uc73c\ub85c \ub098\uc624\uae30 \ub54c\ubb38\uc5d0, \ucd5c\uadfc\uc811 \uc774\uc6c3\uc758 \ubc29\ubc95\uc744 \ub530\ub974\uba74 \uc6b0\ub9ac\uac00 \uc5bb\uac8c\ub418\ub294 \\(\\hat{Y}\\) \ub294 \\(x_{i}\\) \uc8fc\ubcc0\uc758 15\uac1c\uc758 \uad00\uce21\uce58\uc758 \uc885\uc18d\ubcc0\uc218\ub4e4 \uc548\uc5d0\uc11c Orange\uc758 \ube44\uc728\uc744 \ub098\ud0c0\ub0bc \uac83\uc774\ub2e4. \uc55e\uc11c least squares \ubc29\ubc95\uc744 \uc4f4 \uac83\uc5d0 \ube44\ud574 \uc704\uc758 \ubc29\ubc95\uc740 \uc624\ubd84\ub958\ud55c \ud074\ub798\uc2a4\ub4e4\uc774 \ub2e4\uc18c \uc801\uc5b4 \ubcf4\uc778\ub2e4. \uadf8\ub9bc\ub9cc\uc73c\ub85c\ub3c4 \ud655\uc778\uc774 \uac00\ub2a5\ud558\ub4ef\uc774 \uc774 \uc608\uc81c\uc5d0 \ud55c\ud574\uc11c\ub294 \ucd5c\uc18c\uc81c\uacf1\ubc95\ubcf4\ub2e4\ub294 k=15\uc758 \ucd5c\uadfc\uc811\uc774\uc6c3\ubc29\ubc95\uc744 \uc801\uc6a9\ud558\ub294 \uac83\uc774 classification\uc5d0 \ub354 \uc88b\uc740 \ubaa8\ub378\uc784\uc744 \ubcf4\uc5ec\uc900\ub2e4. \ub610 \ub2e4\ub978 \uadf8\ub9bc\uc744 \ubcf4\uc790. \uc704\uc758 \uadf8\ub9bc k\ub97c 1\ub85c \uc124\uc815\ud558\uace0 \ucd5c\uadfc\uc811\uc774\uc6c3 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud55c \ubaa8\uc2b5\uc774\ub2e4. \uc77c\uc77c\uc774 \ud655\uc778\ud574\ubcf4\uba74 \ubaa8\ub4e0 \ub370\uc774\ud130\ub4e4\uc774 \ud55c\uac1c\ub3c4 \ube60\uc9d0\uc5c6\uc774 \ubc14\ub974\uac8c \ubd84\ub958\ub418\uc5b4\uc788\uc74c\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc6b0\ub9ac\ub294 \uc774\ub97c \ud1b5\ud574 \uba87\uac00\uc9c0 \uc0ac\uc2e4\uc744 \uc54c\uc558\ub2e4. k\ucd5c\uadfc\uc811\uc774\uc6c3 \ubc29\ubc95\uc5d0\uc11c\ub294 \ud6c8\ub828\ub370\uc774\ud130\ub4e4\uc758 \uc5d0\ub7ec\uac00 \ub300\ub7b5 k\uc5d0 \ub300\ud55c \uc99d\uac00\ud568\uc218\ub77c\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \ub610\ud55c k\uac00 1\uc774\ub77c\uba74 error\ub294 \ud56d\uc0c1 0\uc774\ub2e4. k\ucd5c\uadfc\uc811\uc774\uc6c3\uc740 p\uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uac16\ub294 \ucd5c\uc18c\uc81c\uacf1\ubc95\uacfc \ub2ec\ub9ac \uc624\ub85c\uc9c0 k\ud55c\uac1c\ub9cc\uc758 \ubaa8\uc218\ub97c \uac16\uace0 \uc774\ub97c \ud1b5\ud574 \ubaa8\ub378\uc774 \uacb0\uc815\ub41c\ub2e4. \uc6b0\ub9ac\ub294 \ud6a8\uacfc\uc801\uc778 k\uc758 \uac12\uc740 \ub300\uac1c p\ubcf4\ub2e4\ub294 \ud070 \\(N/k\\) \uc774\uba70, \uc774\ub294 k\uac12\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uac10\uc18c\ud568\uc744 \uc54c \uc218 \uc788\ub2e4. \ub9cc\uc57d \uc774\uc6c3\ub4e4\uac04\uc758 \uc601\uc5ed\uc774 \uacb9\uce58\uc9c0 \uc54a\ub294\ub2e4\uba74 \uc6b0\ub9ac\ub294 \uac01 region\ub9c8\ub2e4 \ud558\ub098\uc758 \ubaa8\uc218\ub97c fitting\ud560 \uac83\uc774\uae30 \ub54c\ubb38\uc774\ub2e4. \uc774\uc640 \uac19\uc740 \ud2b9\uc9d5\uc744 \uac16\ub294 \ucd5c\uadfc\uc811\uc774\uc6c3\uc758 \ubc29\ubc95\uc740 \uc6b0\ub9ac\uac00 \uc55e\uc11c \uc81c\uc2dc\ud55c \ub450\uac1c\uc758 \uc2dc\ub098\ub9ac\uc624 \uc0c1\ud669\uc5d0\uc11c \ud2b9\ud788 \uc2dc\ub098\ub9ac\uc6242\uc5d0 \ub354 \uc801\ud569\ud55c \ubd84\ub958\uae30\ubc95\uc774\ub2e4. \uc815\uaddc\ubd84\ud3ec\ub97c \ub530\ub974\ub294 \uc790\ub8cc\uc5d0\uc11c decision boundary\uac00 \ubd88\ud544\uc694\ud558\uac8c noisy\uac00 \ub9ce\uc558\uae30 \ub54c\ubb38\uc774\ub2e4. From Least Squares to Nearest Neighbors \ucd5c\uc18c\uc81c\uacf1\ubc95\uc740 \ubaa8\ub378 setting\uc774 \uc544\uc8fc smooth\ud558\uace0, fitting\uc5d0 \uc788\uc5b4\uc11c \uc548\uc815\uc801\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub294 \uc120\ud615 decision boudary\uac00 \ud0c0\ub2f9\ud558\ub2e4\ub294 \uc544\uc8fc \ubb34\uac70\uc6b4 \uac00\uc815\uc744 \ub9cc\uc871\ud574\uc57c\ud55c\ub2e4\ub294 \ub2e8\uc810\uc774 \uc788\ub2e4. \ubc18\ub300\ub85c \ucd5c\uadfc\uc811\uc774\uc6c3\ubc29\ubc95\uc740 \uac00\uc815\uc758 \uc81c\uc57d\uc740 \uac70\uc758 \uc5c6\uace0 \uc5b4\ub5a4 \uc0c1\ud669\uc5d0\uc11c\ub4e0 \uc801\uc808\ud558\uac8c k\ub97c \uc870\uc815\ud558\uc5ec \uc0ac\uc6a9\ud560 \uc218\uc788\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub294 decision boundary\uac00 \ub9e4\uc6b0 \ubcf5\uc7a1\ud558\uac8c \ube44\uc120\ud615\uc801\uc778 form\uc744 \ub744\uae30 \ub54c\ubb38\uc5d0, \uc801\uc740 \ud3b8\ud5a5\uc744 \uac16\uc9c0\ub9cc, \ub192\uc740 \ubd84\uc0b0\uc744 \uc9c0\ub2cc\ub2e4. variance-bias trade off \uc758 \ubb38\uc81c\ub85c\uc368, \ub450\uac00\uc9c0\uc758 \ubc29\ubc95\uc740 \uc21c\uc11c\ub300\ub85c (low variance-high bias), (high variance-low bias) \ub97c \uac16\ub294\ub2e4. \uac01\uac01\uc758 \ubc29\ubc95\uc740 \uac01\uc790 \uc801\ud569\ud55c \uc0c1\ud669\uc774 \uc788\uc73c\uba70, \uc2dc\ub098\ub9ac\uc6241\uc5d0\ub294 \ucd5c\uc18c\uc81c\uacf1\ubc95\uc774, \uc2dc\ub098\ub9ac\uc6242\uc5d0\uc11c\ub294 \ucd5c\uadfc\uc811\uc774\uc6c3\ubc95\uc774 \uc801\uc808\ud560 \uac83\uc73c\ub85c \ubcf4\uc5ec\uc9c4\ub2e4. \uc624\ub298\ub0a0 \uc774 \ub450\uac00\uc9c0 \ubc29\ubc95\uc740 \uac00\uc7a5 \ub300\uc911\uc801\uc73c\ub85c \uc4f0\uc774\ub294 \ub300\ud45c\uc801\uc778 \ubc29\ubc95\uc774\ub2e4. \uc77c\ub840\ub85c k=1 \ucd5c\uadfc\uc811\uc774\uc6c3\ubc29\ubc95\uc740 \uc2dc\uc7a5\uc758 \ub300\ubd80\ubd84\uc5d0\uc11c \uc800\ucc28\uc6d0\uc758 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\ub294\ub370 \uc0ac\uc6a9\ub418\ub294 \ubc29\ubc95\uc774\ub2e4. Statistical Decision Thoery \uc774\ubc88 \ub2e8\uc6d0\uc5d0\uc11c\ub294 \uc774 \uc804\uc5d0 \uc5b8\uae09\ud588\uc5c8\ub358 \ub450\uac00\uc9c0 \ubaa8\ub378 \ub4f1\uc744 \ud3ec\ud568\ud55c \uba87\uba87\uc758 \ubaa8\ub378\uc744 \ubc1c\uc804\uc2dc\ud0a4\ub294\ub370 \uc778\uc0ac\uc774\ud2b8\ub97c \uc81c\uacf5\ud574\uc904 \uba87\uac00\uc9c0 \uc774\ub860 \ub4f1\uc744 \ubc1c\uc804\uc2dc\ucf1c\ubcf4\uace0\uc790 \ud55c\ub2e4. \uba3c\uc800 quantative\ud55c output\ub4e4\uc5d0 \ub300\ud558\uc5ec \uc0dd\uac01\ud574\ubcfc \uac83\uc774\uba70, random variables(\ud655\ub960\ubcc0\uc218)\uacfc probability space(\ud655\ub960\uacf5\uac04)\uc5d0 \ub300\ud574\uc11c \ub17c\uc758\ub97c \ud560 \uac83\uc774\ub2e4. \uba3c\uc800 \\({X} \\in \\mathbf{R}^p\\) \ub294 \uc2e4\uc218\uc758 \uc785\ub825\ubca1\ud130\uc774\uba70, \\({Y} \\in \\mathbf{R}\\) \ub294 \ucd9c\ub825\ub418\ub294 \ud655\ub960\ubcc0\uc218\uc774\ub2e4. \ub610\ud55c \uc774\ub458\uc758 \uacb0\ud569\ubd84\ud3ec\ub294 \\(Pr(X,Y)\\) \ub85c \ud45c\uae30\ud55c\ub2e4. \uc6b0\ub9ac\ub294 \\(Y\\) \ub97c \uc608\uce21\ud558\uae30 \uc704\ud574 \uc8fc\uc5b4\uc9c4 \uc785\ub825\ubcc0\uc218 \\(X\\) \ub97c \uc774\uc6a9\ud558\uc5ec \\(f(X)\\) \ub77c\ub294 \ud568\uc218\ub97c \ucc3e\uace0\uc790\ud55c\ub2e4. \uc774 \uc774\ub860\uc740 \uc608\uce21\uc5d0 \uc788\uc5b4\uc11c error\ub97c penalize\ud558\ub294 loss function \\(L(Y,f(X)\\) \ub97c \ud544\uc694\ub85c\ud55c\ub2e4. \uc774\uc5d0 \ub300\ud55c \uc608\uc2dc\ub85c \uc6b0\ub9ac\uc5d0\uac8c \uac00\uc7a5 \uc798 \uc54c\ub824\uc9c4 squared error loss \uac00 \uc788\ub2e4. (= \\(L(Y,f(X)))= (Y-f(X))^2\\) ). \uc774\ub294 \uc6b0\ub9ac\uac00 f\ub97c \ucc3e\uc744 \uc218 \uc788\uac8c\ud558\uba70, \uc2dd\uc744 \uc804\uac1c\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(EPE(f) = E(Y-f(X))^2 = \\int[y-f(x)]^2f(x,y)dxdy\\) $ \uc774\ub97c X\uc5d0 \ub300\ud574 \uc870\uac74\ubd80\uc2dd\uc73c\ub85c \ud45c\ud604\ud558\uba74 $ \\(EPE(f) = {E}_{X}{E}_{Y|X}([Y-f(X)]^2|X)\\) $ \uac00 \ub41c\ub2e4. \uc704\uc758 \uc2dd\uc740 \uc870\uac74\ubd80 \ud655\ub960\uc744 \ud1b5\ud574 \uad6c\ud560 \uc218 \uc788\uc73c\uba70, \uc99d\uba85\uc774 \ub2e4\uc18c \uac04\ub2e8\ud558\ub2c8 \uc0b4\uc9dd\ub9cc \uc0b4\ud3b4\ubcf4\uc790. \\[EPE(f) = \\int[y-f(x)]^2Pr(dx,dy)\\] \\[= \\int[y-f(x)]^2f(x,y)dxdy\\] \\[=\\int_x\\int_y[y-f(x)]^2f(x,y)dxdy\\] \\[=\\int_x\\int_y[y-f(x)]^2f(x)f(y|x)dxdy\\] \\[=\\int_x(\\int_y[y-f(x)]^2f(y|x)dy)f(x)dx\\] \\[=\\int_x(E_{Y|X}([Y-f(X)^2|X=x))f(x)dx\\] \\[= E_XE_{Y|X}([Y-f(X)]^2|X=x)\\] \uc5ec\uae30\uc11c EPE\ub97c pointwise\ud558\uac8c \ub9cc\uc871\uc2dc\ud0a4\ub294 f(x)\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\uace0 \uadf8 \ubc11\uc5d0 \uc774\uc5b4\uc9c0\ub294 \uc2dd\uc774 \uadf8\ub54c\uc758 \ud574( \\(f(X)\\) )\uc774\ub2e4. \\[f(x)=argmin_{c}E_{Y|X}([Y-c]^2|X=x)\\] $ \\(f(x)=E(Y|X=x)\\) $ \uc774 \ud574\ub294 \uc870\uac74\ubd80\ud3c9\uade0\uc744 \uc758\ubbf8\ud558\uba70 \ub610\ud55c regression function\uc73c\ub85c \uc798 \uc54c\ub824\uc838\uc788\ub2e4. \ub2e4\uc2dc\ub9d0\ud574 average squared error\uac00 \ucd5c\uc801\uc758 \uc608\uce21\ucc99\ub3c4\ub85c \uc5ec\uaca8\uc9c8 \ub54c, best prediction of \\(Y\\) at any point \\(X=x\\) \ub294 Y\uc758 X\uc5d0 \ub300\ud55c \uc870\uac74\ubd80 \ud3c9\uade0\uc774\ub77c\ub294 \uac83\uc774\ub2e4. \uc218\ub9ac\ud1b5\uacc4\ud559\uc801\uc778 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud574\uc11c \uc6b0\ub9ac\ub294 \uc704\uc5d0\uc11c \uc5b8\uae09\ud55c \uc870\uac74\ubd80\ud3c9\uade0\uc774 \ucd5c\uc18c\uc790\uc2b9\uc608\uce21\uc790\ub77c\ub294 \uac83\uc744 \ubcf4\uc77c \uc218 \uc788\ub2e4. \uc774\ub97c \uc218\uc2dd\uc73c\ub85c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\(For\\ any\\ function\\ of\\ X,\\ say\\ u(x),\\) \\[E[(Y-u(X))^2|X] \\geq E[(Y-E[Y|X])^2|X]\\] \\(i.e.,\\ E[Y|X] is\\ equal\\ to\\ argmin_{u(X)}E[(Y-u)^2|X].\\) \uc774\ub97c \uc99d\uba85\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \uc2dd\uc804\uac1c\ub97c \ud560 \uc218\uc788\ub2e4. \\[E[(Y-u(X))^2|X]=E[Y-\\textbf{E[Y|X]}+\\textbf{E[Y|X]}-u(X))^2|X]\\] \\[=E[{(Y-E[Y|X])+(E[Y|X]-u(X))}^2|X]$$ $$=E[(Y-E(Y|X])^2|X)]+E[(E[Y|X]-u(X))^2|X]\\] \\[+2E[(Y-E[Y|X])(E[Y|X]-u(X))|X]\\] \\[=E[(Y-E[Y|X])^2|X]+(E[Y|X]-u(X))^2\\] $ \\(\\because E[(Y-E[Y|X])(E[Y|X]-u(X))|X]=(E[Y|X]-u(X))E[Y-E[Y|X]|X]=0\\) $ \uc5ec\uae30\uc11c \ub9c8\uc9c0\ub9c9 cross term\uc774 0\uc774 \ub418\ub294 \uc774\uc720\ub294 \\(E[Y-E[Y|X]|X]\\) \uc5d0 \ud78c\ud2b8\uac00 \uc788\ub2e4. \\(E[Y|X]\\) \uc758 \uc870\uac74\ubd80\ud3c9\uade0\uc740 random variable\uc758 \ud615\ud0dc\uc774\uc9c0\ub9cc \uc774 \uc2dd\uc758 \uc55e\uc5d0 X\uc5d0 \ub300\ud55c \uc870\uac74\ubd80 \ud3c9\uade0\uc774 \ud55c\ubc88 \ub354 \uc50c\uc5ec\uc788\ub294 \uac83 \ub54c\ubb38\uc778\ub370, \uc774\ub7f4\uacbd\uc6b0\uc5d0 E[Y|X]\ub294 constant\uaf34\uc744 \uac16\ub294\ub2e4. (Given X=x\uc774\uae30 \ub54c\ubb38\uc5d0) \uace0\ub85c \uc2dd\uc804\uac1c\ub97c \ud558\uba74 \\(E[Y|X]-E[Y|X]\\) \uaf34\uc774\ubbc0\ub85c 0\uc774\ub41c\ub2e4. \ub610\ub294 \ub2e4\ub974\uac8c \uc0dd\uac01\ud558\uc5ec E[Y|X]\ub294 X\uc5d0\ub300\ud55c \ud568\uc218\ud615\ud0dc\ub85c \ub098\uc624\uae30\ub54c\ubb38\uc5d0 \uc774\ub97c g(X)\ub85c \uc815\uc758\ud558\uc5ec \ubb38\uc81c\ub97c \ud480 \uc218\ub3c4 \uc788\ub2e4. \uadf8\ub807\ub2e4\uba74 \uc2dd\uc740 \\(E[Y-g(X)|X]\\) \uc758 \ud615\ud0dc\uac00 \ub418\uba70 \uc774\ub97c \uc804\uac1c\ud558\uba74 \\(E[Y|X]-g(X)=0, \\because g(X)=E[Y|X]\\) \uac00 \ub418\uc5b4 \uc704\uc640 \uac19\uc740 \uc2dd\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4. \ubd80\ub4f1\ud638\ub97c \uc815\ub9ac\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ub418\uba70, \\[E[(Y-u(X))^2|X] \\geq E[(Y-E[Y|X])^2|X]\\] \uadf8\ub7ec\ubbc0\ub85c \uc784\uc758\uc758 \\(X=x\\) \uc810\uc5d0 \ub300\ud55c best prediction of \\(Y\\) \ub294 \uc870\uac74\ubd80 \ud3c9\uade0\uc774 \ub41c\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. k-NN methods\uc5d0\uc11c\ub294 \ud574\uc758 \ucd94\uc815\uce58\ub97c \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud55c\ub2e4. $ \\(\\hat{f}(x)=Ave(y_{i}|x_{i} \\in N_{k}(x))\\) $ \uc704 \uc2dd\uc5d0\uc11c\ub294 \ub450\uac00\uc9c0\uc758 approximation\uc774 \uc874\uc7ac\ud558\ub294\ub370, \uae30\ub313\uac12\uc740 \uc0d8\ud50c \ub370\uc774\ud130\ub4e4\uc758 \ud3c9\uade0\uc73c\ub85c \uadfc\uc0ac\ud55c\ub2e4\ub294 \uac83\uacfc, \ud55c \uc810\uc5d0\uc11c\uc758 conditioning\uc740 \ubaa9\ud45c\uc9c0\uc810\uacfc \uac00\uae4c\uc6b4 \uc5b4\ub5a4 \uc9c0\uc5ed\uc758 conditioning\uc73c\ub85c relax\ub41c\ub2e4\ub294 \uac83\uc774\ub2e4. \ub9cc\uc57d knn\uc5d0\uc11c \uc0d8\ud50c\uc758 \uc0ac\uc774\uc988\uac00 \uc544\uc8fc\ud06c\uace0, k\ub610\ud55c \ud06c\ub2e4\uba74, \ud3c9\uade0\uc740 \ub354 \uc548\uc815\uc801\uc73c\ub85c \ubcc0\ud560 \uac83\uc774\uace0, \uacb0\ud569\ud655\ub960\ubd84\ud3ec\uc758 mild regularity conditions\ud558\uc5d0\uc11c \\(f(x)\\) \uac00 \\(E(Y|X=x)\\) \ub85c \uadfc\uc0ac\ud55c\ub2e4 . \ud558\uc9c0\ub9cc \uc885\uc885 \uc6b0\ub9ac\uac00 \ub2e4\ub8e8\ub294 \ub370\uc774\ud130\ub4e4\uc758 \ud45c\ubcf8\uc758 \ud06c\uae30\ub294 \ud56d\uc0c1 \ud06c\uc9c0\uac00 \uc54a\uc73c\uba70, \ucc28\uc6d0\uc774 \ucee4\uc9c0\uba74 \uc0c8\ub86d\uac8c \ubc1c\uc0dd\ud558\ub294 \ubb38\uc81c\ub4e4\ub3c4 \uc874\uc7ac\ud55c\ub2e4. \uc774\ub294 \ucd94\ud6c4 \ub4b7\uc7a5\uc5d0\uc11c \ub2e4\ub904\ubcf4\ub3c4\ub85d \ud558\uc790. \uc774\uc804 \ub2e8\uacc4\uc5d0\uc11c OLS estimator\uac00 linearity \uac00\uc815 \ud558\uc5d0\uc11c \\(E[Y|X=x]\\) \uc5d0 \ub300\ud55c consistent estimator\ub77c\ub294 \uac83\uc744 \ubcf4\uc600\ub2e4. KNN method\uc5d0 \ub300\ud574\uc11c\ub3c4 \uc774\uac00 \uc131\ub9bd\ud558\ub294\uc9c0 \uc99d\uba85\uc744 \ud574\ubcf4\uc790. mild regularity conditions\uc774 \ud0c0\ub2f9\ud558\ub2e4 \uac00\uc815\uc744 \ud558\uace0 \\(k,N \\rightarrow \\infty,\\ \\frac{k}{N} \\rightarrow 0\\) \uc774\ub77c\uba74, $ \\(avg(Y|X \\in N_{k}(x)) \\xrightarrow{P} E[Y|X=x]\\) $ \uc784\uc744 Empirical explanation\uc744 \ud1b5\ud574 \uc99d\uba85\ud574\ubcf4\uc790. KNN \ubc29\ubc95\uc744 \ud480\uc5b4\ud5e4\uccd0\uc4f0\uba74 \uc544\ub798\uc640 \uac19\uc740 \uc810\ub4e4\uc774 \ubaa8\uc5ec\uc788\ub294 \uac83\uc73c\ub85c \uc0dd\uac01\ud560 \uc218 \uc788\ub2e4. $ \\((x+\\Delta_1,y_1),\\ (x+\\Delta_2,y_2),\\ (x+\\Delta_{k-1},y_{k-1}),\\ (x+\\Delta_{k},y_k)\\) $ \ub610\ud55c \uc774 \uc810\ub4e4\uc5d0 \ub300\ud574\uc11c \uc544\ub798\uc640 \uac19\uc740 \uc2dd\uc744 \ub9cc\uc871\ud55c\ub2e4. $ \\((||x-a|| \\geq ||max_{j}(\\Delta_{j}))||, a\\in N_{k}(x)^c, j=1,...,k)\\) $ \uc5ec\uae30\uc11c \\(avg(Y|X \\in N_k(x)) \\equiv \\frac{y_1+..+y_{k}}{k} \\xrightarrow{P} E[Y]\\) \uc774 \uc2dd\uc774 \uc57d\ub300\uc218\uc758 \ubc95\uce59\uc5d0 \uc758\ud574 equivalent\ud558\ub2e4\uace0 \ubcfc \uc218 \uc788\uc744\uae4c? \uc544\uc27d\uac8c\ub3c4 \uadf8\ub807\uc9c0 \uc54a\ub2e4. \uc774\ub7ec\ud55c \uc2dd \ub3c4\ucd9c\uc758 \uacfc\uc815\uc740 \\(\\frac{k}{N} \\rightarrow \\infty\\) \ub77c\ub294 \uac00\uc815\uc744 \ubb34\uc2dc\ud574\uc11c \ub098\uc624\ub294 \uacb0\uacfc\uc774\ub2e4. \ub9cc\uc57d \\(k \\rightarrow \\infty\\) \ub77c\ub294 \uac00\uc815\ub9cc \uc788\uace0 data set\uc758 \uc0ac\uc774\uc988\uc5d0 \ub300\ud55c \uac00\uc815\uc774 \uc5c6\ub2e4\uba74 \uc704\uc758 \uc2dd\uc740 \ub9de\uc744\uc9c0\ub3c4 \ubaa8\ub978\ub2e4. \uc774\ub807\uac8c \uc0dd\uac01\ud574\ubcf4\uc790. \uc6b0\ub9ac\ub294 x\uc5d0 \ub300\ud55c \uc870\uac74\ubd80 \ud3c9\uade0 Y\ub97c \uad6c\ud558\ub294 \uac83\uc778\ub370, x\uc5d0 \uc885\uc18d\uc801\uc774\uac8c \ub300\uc751\ud558\ub294 Y\uc758 \uc810\ub4e4 k\uac1c\ub97c \uc0dd\uac01\ud574\ubcfc \uc218 \uc788\ub2e4. \uc5ec\uae30\uc11c k\uac00 \ubb34\ud55c\ub300\ub85c \uac04\ub2e4\uba74 \uc6b0\ub9ac\ub294 \uac70\uc758 \ubaa8\ub4e0 x\uc758 \uc810\ub4e4\uc5d0 \ub300\uc751\ud558\ub294 Y\ub97c \uace0\ub824\ud558\uace0 \uc774\ub7ec\ud55c y\ub4e4\uc758 x\uc5d0 \ub300\ud55c \uc870\uac74\ubd80\ud3c9\uade0\uc744 \uad6c\ud558\ub294 \uac83\uc778\ub370, \ubaa8\ub4e0 x\uc5d0 \ub300\ud558\uc5ec Y\ub97c \uad6c\ud588\uae30 \ub54c\ubb38\uc5d0 \uadf8\ub0e5 Y\uc5d0 \ub300\ud55c \ud3c9\uade0\uc744 \uad6c\ud558\ub294 \uac83\uacfc \ud655\ub960\uc801\uc73c\ub85c \uc77c\uce58\ud560 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc k\uc758 \uc0ac\uc774\uc988\uc5d0 \ube44\ud574 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c data set\uc758 \uc0ac\uc774\uc988\uac00 \ucee4\uc9c4\ub2e4\uba74 \uc774\ub294 data set\uc5d0\uc11c k\uac1c\uc758 \uad00\uce21\uce58\uac00 \ucc28\uc9c0\ud558\ub294 \uacf5\uac04\uc744 \ubaa8\ub4e0 x\uc5d0 \ub300\ud55c y\uc758 \ub300\uc751\uac12\uc73c\ub85c \uc0dd\uac01\ud560 \uc218 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \uc774\ub54c\ub294 X\uc5d0 \ub300\ud55c Y\uc758 \uc870\uac74\ubd80\ud3c9\uade0\uc73c\ub85c \ubcf4\ub294 \uac83\uc774 \ud569\ub2f9\ud560 \uac83\uc774\ub2e4. \uc774\ub97c \uac04\ub2e8\ud55c \uc2dc\ubbac\ub808\uc774\uc158\uc744 \ud1b5\ud574 \ud655\uc778\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\ub2e4. library(ggplot2) knn <- function(data,k,x){ od=order(abs(data[,2]-x)) new.data <- data[od,] Nk <- new.data[1:k,] Nk.range <- diff(range(Nk[,2])) Nk.avg <- mean(Nk[,1]) res <- c(Nk.range,Nk.avg) return(res) } L <- c(10,50,100,200,500,1000,2000) ; A <- chol(matrix(c(1,0.7,0.7,1),nrow=2)) y.bar <- matrix(rep(0,100*length(L)),nrow=100) range.bar <- matrix(rep(0,100*length(L)),nrow=100) for(l in 1:length(L)){ k <- L[l] for(i in 1:100){ temp.y <- c() temp.r <- c() for(j in 1:30){ X1 <- rnorm(k^2,0,1) Y1 <- rnorm(k^2,0,1) YX <- t(A)%*%rbind(Y1,X1) KNN <-knn(t(YX),k,0.5) C <- mean(rnorm(k,0.35,sqrt(.51))) temp.y <- append(temp.y,abs(KNN[2]-C)) temp.r <- append(temp.r,KNN[1]) } y.bar[i,l] <- mean(temp.y) range.bar[i,l] <- mean(temp.r) } } boxplot(y.bar,names=L,main='error') frame.y.bar <- stack(as.data.frame(y.bar)) frame.y.bar[,2] <- factor(frame.y.bar[,2],label=L) range(y.bar[,ncol(y.bar)]) ## [1] 0.01207920 0.02312542 p <- ggplot(frame.y.bar) + geom_boxplot(aes(x=ind,y=values)) p+labs(title=\"Errors\",x=\"Sample Size\",y=\"errors\")+ theme_classic() frame.range.bar <- stack(as.data.frame(range.bar)) frame.range.bar[,2] <- factor(frame.range.bar[,2],label=L) ggplot(frame.range.bar)+geom_boxplot(aes(x=ind,y=values)) + labs(title=\"Diameter of N_k\",x=\"Sample Size\", y=\"errors\")+theme_classic() \ud1b5\uacc4\uc804\uc0b0\uacfc \uc120\ud615\ub300\uc218\ub97c \uc18c\ud640\ud558\uac8c \ub4e4\uc740 \ud544\uc790\ub85c\uc11c.. \ud559\ud68c\uc5d0\uc11c \ud568\uaed8 ESL\uc744 \uc2a4\ud130\ub514\ud558\ub294 \uba4b\uc7c1\uc774 \uc774\ud615\uc131 \uce5c\uad6c\uac00 \uad6c\ud604\ud55c \ucf54\ub4dc\ub97c \uc778\uc6a9\ud558\uc600\ub2e4. \ucf54\ub4dc\ub97c \uc0b4\ud3b4\ubcf4\uba74 \uba3c\uc800 knn\uc774\ub77c\ub294 \uc774\ub984\uc73c\ub85c \uc0c8\ub85c\uc6b4 \ud568\uc218\ub97c \uc815\uc758\ud558\uc600\ub294\ub370, \ubd88\ub7ec\uc624\ub294 \ub370\uc774\ud130\uc758 \uaf34\uc744 1,2\uc5f4\uc774 \uac01\uac01 sample y,x\uc5d0 \ub300\uc751\ud558\ub3c4\ub85d \ub9cc\ub4e4\uc5b4 \ub193\uace0 \uc9e0 \ud568\uc218\uc5d0 \ub123\ub294 \uc6d0\ub9ac\uc774\ub2e4. \uc6b0\ub9ac\ub294 knn method\ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud558\uc5ec k\uac1c\uc758 sample\uc744 \ubf51\uc744 \ub54c, \ud2b9\uc815 \ud3ec\uc778\ud2b8 \uc9c0\uc810\uc5d0\uc11c \uac00\uc7a5 \uac00\uae4c\uc6b4 k\uac1c\ub97c \ubf51\uc544\uc57c\ud558\ubbc0\ub85c data\uc758 2\uc5f4\uc778 sample x\ub4e4\uacfc \uc9c0\uc815\ud55c \ud3ec\uc778\ud2b8 x=0.5\uc778 \uac12\uc758 \ucc28\uc758 \uc808\ub300\uac12\uc744 order\ud568\uc218\ub97c \uc774\uc6a9\ud558\uc5ec \uc778\ub371\uc2a4\ub97c \ucd94\ucd9c\ud558\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud588\ub2e4. sort\ub4f1\uc758 \ud568\uc218\ub97c \uc4f0\uc9c0\uc54a\uc740 \uc774\uc720\ub294 \uadf8 \ucc28\uc774\ub85c \ube44\uad50\ub97c \ud558\uac8c y,x\uac00 \ub300\uc751\ud558\ub294 \ud55c \ud589\uc744 \uc138\ud2b8\ub85c \ubf51\uc544\uc57c\ud558\uae30 \ub54c\ubb38\uc774\ub2e4. $ \\(\\begin{bmatrix} Y \\\\ X \\end{bmatrix} \\sim N_2(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\begin{bmatrix} 1 & 0.7 \\\\ 0.7 & 1 \\end{bmatrix} )\\) $ \uc704\uc758 \uc2dd\uc5d0\uc11c Y\ub294 \ucd9c\ub825\ubcc0\uc218\uc774\uace0 X\ub294 predictor variable\uc774\ub2e4. X\uc640 Y\ub97c \uc704\uc640 \uac19\uc774 \uac00\uc815\ud588\uc744 \ub54c $ \\(Y|X = x \\sim N(\\mu_{y}+\\sum_{yx}{\\sum_{xx}}^{-1}(x-\\mu_x),\\sum_{yy}-\\sum_{yx}{\\sum_{xx}}^{-1}\\sum_{xy})\\) $ \\(Y|X=x\\) \ub294 \uc704\uc640 \uac19\uc740 \ubd84\ud3ec\ub97c \ub530\ub974\uace0, \\(i.e.,\\ Y|X=0.5 \\sim N(0.35,0.51)\\) \uc758 \ubd84\ud3ec\ub97c \uac16\uace0 \uc774\ub97c \ucf54\ub4dc\uc548\uc5d0 \ub123\uc5b4\uc11c \uc608\uc81c\ub97c \ub9cc\ub4e4\uc5b4 \ubcf4\uc558\ub2e4. \uadf8\ub9bc\uc5d0\uc11c \ubcf4\uc774\ub2e4\uc2dc\ud53c sample size\uac00 \ucee4\uc9c8\uc218\ub85d \\(Loss(N.x) = |avg(Y|X \\in N_k(x))- E[Y|\\hat{X}=x]|\\) \uac00 0\uc5d0 \uc218\ub834\ud558\uace0 boxplot\uc758 \ud3ed \ub610\ud55c \uc791\uc544\uc9c0\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \ub610\ud55c \\(N_k(x)\\) \uc758 \uc9c1\uacbd \ub610\ud55c 0\uc5d0 \uc218\ub834\ud568\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc774 \ub458\uc744 \uc815\ub9ac\ud55c\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc740 \uacb0\ub860\uc744 \ub0b4\ub9b4 \uc218 \uc788\ub2e4. $$E[Y|\\hat{X}=x] \\xrightarrow{P} E[Y|X=x] (\\because WLLN) $$ $ \\(E[Y|\\hat{X}=x] -avg(Y|X \\in N_k(x))\\xrightarrow{P} 0\\) $ $ \\(\\therefore avg(Y|X \\in N_k(x)) \\xrightarrow{P} E[Y|X=x] (\\because Arithmetics \\ of\\ convergence\\ in\\ probability)\\) $ \\(i.e.\\ , K-NN\\ estimator\\ is\\ another\\ consistent\\ estimator\\ for\\ conditional\\ expectation(Not\\ E[Y])\\) \uc6b0\ub9ac\ub294 knn\uacfc \ucd5c\uc18c\uc81c\uacf1\ubc95\uc774 \ubaa8\ub450 \uc870\uac74\ubd80 \uae30\ub313\uac12\uc744 \ud1b5\ud574 \\(f(x)\\) \ub97c \ucc3e\uc544\uac00\ub294 \uac83\uc744 \uc54c\uc558\ub2e4. \ud558\uc9c0\ub9cc \ucd5c\uc18c\uc81c\uacf1\ubc95\uc5d0\uc11c\ub294 \ub450\uac00\uc9c0 \uc870\uac74\uc744 \uac00\uc815\ud574\uc57c\ud55c\ub2e4. \ud68c\uadc0\ud568\uc218 \\(f(x)\\) \uac00 globally linear function\uc5d0 \uc758\ud574 \uc798 \uadfc\uc0ac\ub418\uc5b4\uc57c\ud55c\ub2e4\ub294 \uac83. \\(\\beta=[E(XX^T)]^{-1}E(XY)\\) knn \ubc29\ubc95\uc740 \\(f(x)\\) \uac00 locally constant function\uc5d0 \uc758\ud574\uc11c \uc798 \uadfc\uc0ac\ub418\uc5b4\uc57c\ud55c\ub2e4\ub294 \uac00\uc815\uc744 \ud544\uc694\ub85c\ud55c\ub2e4. \\(L_2\\) loss function \ub300\uc2e0 \\(L_1: E|Y-f(X)|\\) \ub97c \uc4f8 \uc218\ub294 \uc5c6\ub294\uac78\uae4c? \uc774\uc5d0 \ub300\ud55c \ud574\ub2f5\uc740 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(\\hat{f}(x)=median(Y|X=x)\\) $ \uc774\ub294 location\uc5d0 \ub300\ud55c \ub2e4\ub978 \uce21\uc815\ubc29\ubc95\uc774\uba70 \uc870\uac74\ubd80 \ud3c9\uade0\uc5d0 \ube44\ud574\uc11c\ub294 \uc870\uae08\ub354 robust\ud55c estimates\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub7ec\ud55c loss function\uc5d0 \ub300\ud55c \uc811\uadfc\uc740 \ubbf8\ubd84\uc744 \ud568\uc5d0 \uc788\uc5b4\uc11c \ubd88\uc5f0\uc18d\uc810\uc744 \uac16\uace0\uc788\uae30 \ub54c\ubb38\uc5d0 \ub110\ub9ac \uc0ac\uc6a9\ub420 \uc218 \uc788\ub294 \ubc29\ubc95\uc740 \uc544\ub2c8\ub2e4. \uc678\ubd80\uc758 \ubcc0\ud654\uc5d0 \ub354 robust\ud55c loss function\ub4e4\uc740 \ub4b7 \uc7a5\uc5d0\uc11c \ucd94\ud6c4\uc5d0 \ucd94\uac00 \uc124\uba85\uc744 \ud560 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc \uadf8\ub7fc\uc5d0\ub3c4 \uc5ed\uc2dc squared error methods\uac00 \uac00\uc7a5 \ub110\ub9ac \uc54c\ub824\uc838\uc788\uace0 \ud3b8\ub9ac\ud55c \ubc29\ubc95\uc77c \uac83\uc774\ub2e4. \ucd9c\ub825\ubcc0\uc218\uac00 categorical output\uc774\ub77c\uba74 \uc5b4\ub5a8\uae4c? \uc774\ub294 \uc218\uce58\ud615 \uc790\ub8cc\uc77c \ub54c\uc640 \uac19\uc740 paradigm\uc744 \uacf5\uc720\ud558\uba70, \uc624\uc9c1 \uc608\uce21\uc624\ucc28\ub97c penalize\ud558\ub294 loss function\ub9cc \uc0b4\uc9dd \ubc14\uafd4\uc8fc\uba74 \ub41c\ub2e4. loss function\uc740 \\(K\\times{K}\\) \uc758 matrix \\(\\mathbf{L}\\) \uc774\uba70, \uc774\ub294 \ub300\uac01\uc6d0\uc18c\uac00 0\uc774\uace0 \ubaa8\ub4e0 \uc6d0\uc18c\uac00 nonnegative\ud55c \ud589\ub82c\uc774\ub2e4. \uc5ec\uae30\uc11c \\(L(k,l)\\) \uc740 K class\uc5d0 \uc18d\ud558\ub294 \uad00\uce21\uce58\ub97c L class\uc5d0 \ubd84\ub958\ud558\uc600\uc744 \ub54c \ub4dc\ub294 cost\ub97c \uc758\ubbf8\ud55c\ub2e4. \uc6b0\ub9ac\ub294 \ub300\uac1c loss funtion\uc73c\ub85c \\(zero-one\\) loss function\uc744 \uc0ac\uc6a9\ud558\uba70, \uc774\ub54c expected prediction error\ub294 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(EPE=E(L(G,\\hat{G}(X))]\\) $ \uc774\ub97c \uc55e\uc5d0\uc640 \ub611\uac19\uc740 \ubc29\ubc95\uc73c\ub85c X\uc5d0 \ub300\ud574 \uc870\uac74\ubd80 \uae30\ub300\uac12\uc744 \uac78\uba74 $ \\(EPE=E_{X}\\sum\\limits_{k=1}^{K}L[G_{k},\\hat{G}(X)]Pr(G_{k}|X)\\) $ \ub85c \ud45c\ud604\ub41c\ub2e4. \ub611\uac19\uc774 EPE pointwise\ub97c \ucd5c\uc18c\ud654\ud558\uac8c \ub9cc\uc871\ud558\ub294 \\(\\hat{G}(x)\\) \ub294 \\(argmin_{{g}\\in {G}}\\sum\\limits_{k=1}^KL(G_{k},g)Pr(G_{k}|X=x)\\) \uc774\uace0 0-1 loss function\uc5d0\uc11c \uc774\ub97c \uac04\ub7b5\ud558\uac8c \uc544\ub798\ucc98\ub7fc \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. $ \\(\\hat{G}(x)=argmin_{{g}\\in {G}}[1-Pr(g|X=x)]\\) $ \ub610\ub294, \\(\\hat{G}(x)=G_{k}\\) if \\(Pr(G_{k}|X=x)=max_g\\in{G}Pr(g|X=x)\\) \ub85c \ud45c\uae30\uac00\ub2a5\ud558\ub2e4. \uc704\uc758 \ud574\ub294 Bayes classifier \ub85c \uc54c\ub824\uc838 \uc788\uc73c\uba70, \uc774\ub294 \uc870\uac74\ubd80 \uc774\uc0b0\ubd84\ud3ec \\(Pr(G|X)\\) \ub97c \uc774\uc6a9\ud558\uc5ec \uac00\uc7a5 probable\ud55c class\uc5d0 \ubd84\ub958\ud558\ub294 \uac83\uc774\ub2e4. \ubca0\uc774\uc988 \ubd84\ub958\uae30\uc758 error rate\ub294 Bayes rate \ub85c \ubd88\ub9b0\ub2e4. \ub2e4\uc2dc k-\ucd5c\uadfc\uc811\uc774\uc6c3 \ubd84\ub958\uae30\ub97c \ub3cc\uc544\ubcf4\uba74, \uc6b0\ub9ac\ub294 \uc774 \ubd84\ub958\ubc29\ubc95\uc774 \uc9c1\uc811\uc801\uc73c\ub85c \uc704\uc758 \ud574\uc5d0 \uadfc\uc0ac\ud55c\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \uc774\ub294 \uc55e\uc5d0\uc11c \ud655\uc778\ud558\uc600\ub4ef\uc774, \uc8fc\ubcc0 \uc774\uc6c3\uc744 \uad6c\uc131\ud558\ub294 \ud074\ub798\uc2a4\ub4e4 \uc548\uc5d0\uc11c \ub2e4\uc218\uacb0\ub85c \uc608\uce21\uc744 \uc2dc\ud589\ud558\uba70, \ud55c \uc9c0\uc810\uc758 \uc870\uac74\ubd80 \ud655\ub960\uc740 \uc8fc\ubcc0 \uc810\uc758 \uc774\uc6c3\ub4e4\uc758 \uc870\uac74\ubd80 \ud655\ub960\ub85c \uc644\ud654\ub418\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \ub610\ud55c \uadf8\ub54c\uc758 \ud655\ub960\uc740 training-sample\uc758 proportions\uc73c\ub85c \uce21\uc815\ub41c\ub2e4. Binary classification \ubb38\uc81c\uc5d0\uc11c \uc6b0\ub9ac\ub294 error\uc758 loss function\uc774 \\(\\hat{f}(X)=E(Y|X)=Pr(G=G_{1}|X)\\) , if \\(G_{1}\\) corresponded to Y=1\uac00 \ub418\ub294 \uac83\uc744 \uc54c\uace0, \uc774 \uac83\uc774 K-class\ub85c \ud655\uc7a5\ub41c\ub2e4\uba74 \uc190\uc2e4\ud568\uc218\ub294 \\(E(Y_{k}|X)=Pr(G=G_{k}|X)\\) \uac00 \ub41c\ub2e4. \uc774 \uc218\uc2dd\uc740 \ub354\ubbf8\ubcc0\uc218\ub97c \ud65c\uc6a9\ud55c \ud68c\uadc0\uc758 \uacfc\uc815\uc774 \ubca0\uc774\uc988\ubd84\ub958\uae30\uc758 \ub610\ub2e4\ub978 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \ud558\uc9c0\ub9cc \uc2e4\uc81c \ubb38\uc81c\ub97c \ucc98\ub9ac\ud560 \ub54c \uc120\ud615\ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uba74\uc11c \\(\\hat{f}(X)\\) \ub294 \ubb34\uc870\uac74 \uc591\uc758 \uac12\uc744 \uac00\uc838\uc57c\ud558\uc9c4 \uc54a\uc73c\uba70, \uc774\ub7f4 \ub54c \uc6b0\ub9ac\ub294 \uc774\ub97c \ud655\ub960\uc758 \ucd94\uc815\uce58\ub85c \uc0ac\uc6a9\ud560 \ub54c \uc5b4\ub824\uc6c0\uc5d0 \ub2f9\ub3c4\ud560 \uc218\ub3c4 \uc788\ub2e4. \uc774\ub7ec\ud55c \ubb38\uc81c\ub294 \ucd94\ud6c4 \ucc55\ud130 4\uc7a5\uc5d0\uc11c \ub2e4\ub904\ubcf4\ub3c4\ub85d \ud558\uc790. Local Methods in High Dimensions knn averaging\uc744 \ud1b5\ud574 \ud45c\ubcf8\uc758 \ud06c\uae30\uac00 \uc810\uc810 \ucee4\uc838\ub3c4 \uc6b0\ub9ac\ub294 \uc5b8\uc81c\ub098 \uc774\uc0c1\uc801\uc778 \ucd5c\uc801\uc758 \uc870\uac74\ubd80 \ud3c9\uade0\uac12\uc73c\ub85c \uadfc\uc0ac\ud560\uc218\uc788\uc744\uae4c? \uc544\uc27d\uac8c\ub3c4 \uc774\ub7ec\ud55c \uc0dd\uac01\uc740 \uace0\ucc28\uc6d0\uc758 \ubb38\uc81c\uc5d0\uc11c \uac00\ubccd\uac8c \uae68\uc838\ubc84\ub9ac\ub294 \uae30\ub300 \uc774\uba70 \uc774\ub7ec\ud55c \ud604\uc0c1\uc744 \uc6b0\ub9ac\ub294 \ucc28\uc6d0\uc758 \uc800\uc8fc( curse of dimensionality )\ub77c\uace0 \uce6d\ud55c\ub2e4. \uc774\uc5d0 \ub300\ud55c \uc608\uc2dc\ub97c \uba87\uac1c\ub9cc \uc0b4\ud3b4\ubcf4\uaca0\ub2e4. p\ucc28\uc6d0\uc758 hypercube\uc548\uc5d0\uc11c \uade0\ub4f1\ud558\uac8c \ubd84\ud3ec\ub418\uc5b4 \uc788\ub294 input\uc5d0 \ub300\ud55c \ucd5c\uadfc\uc811\uc774\uc6c3 \uc808\ucc28\ub97c \uc0b4\ud3b4\ubcf4\uc790. \uad00\uce21\uce58\uc5d0\uc11c fraction r\uc744 \ud3ec\ucc29\ud558\uae30 \uc704\ud574 \ud0c0\uac9f \ud3ec\uc778\ud2b8\uc5d0 \ub300\ud55c hypercubical neighborhood\ub97c \ubcf4\ub0b8\ub2e4\uace0 \uc0dd\uac01\ud574\ubcf4\uc790. \uc774\ub294 \uacb0\uad6d \\(\\frac{r}{volume}\\) \uc774\uba70, expected dege legnth\ub294 \\(e_{p}(r)=r^{1/p}\\) \uac00 \ub420 \uac83\uc774\ub2e4. 10\ucc28\uc6d0\uc758 \uc608\uc2dc\uc5d0\uc11c r\uc744 \uc870\uc815\ud558\uba70 \uc608\uc2dc\ub97c \ub4e4\uc790\uba74, \\(e_{10}(0.01)=0.63\\) , \\(e_{10}(0.1)=0.8\\) \uc774 \ub418\uba70 \uac01 \uc785\ub825\ubcc0\uc218\uc5d0 \ub300\ud55c \uc804\uccb4 range\ub294 1.0\uc774\ub2e4. \uc774\ub294 \ub2e4\uc2dc\ub9d0\ud574 \uc6b0\ub9ac\uac00 1%\uc640 10%\ub85c \uac01\uac01\uc785\ub825\ubcc0\uc218\uc758 local average\ub97c \ud3ec\ucc29\ud558\uae30 \uc704\ud574 \ucee4\ubc84\ud574\uc57c\ud558\ub294 \uc785\ub825\ubcc0\uc218\uc758 \ubc94\uc704\uac00 63%\uc640 80%\ub098 \ud544\uc694\ud558\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc774\ub7ec\ud55c \uc774\uc6c3\uc740 \ub354\uc774\uc0c1 local \ud558\uc9c0 \uc54a\ub2e4. \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \uc5c6\uc560\uae30 \uc704\ud558\uc5ec \ucc28\uc6d0\uc744 \uc904\uc778\ub2e4\uace0 \ud55c\ub2e4\uba74 \uc774\ub294 \ub354 \uc801\uc740 \uad00\uce21\uce58\ub85c \ud3c9\uade0\uac12\uc744 \ub0b4\uac8c \ub418\ubbc0\ub85c, \ubd84\uc0b0\uc740 \ub354\uc6b1 \ub354 \ucee4\uc9c0\ub294 \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud55c\ub2e4. \ub610\ub2e4\ub978 \uace0\ucc28\uc6d0\uc5d0\uc11c\uc758 sparse sampling\uc758 \uacb0\uacfc\ub85c\ub294 \ubaa8\ub4e0 \ud45c\ubcf8 \uc9c0\uc810\ub4e4\uc774 \ud45c\ubcf8\uc758 \uacbd\uacc4\uc5d0 \uac00\uae5d\ub2e4\ub294 \uc810\uc774\ub2e4. \uc774\ub294 \ub2e4\uc2dc \ub9d0\ud574 \ucc28\uc6d0\uc774 \ub192\uc544\uc9c8\uc218\ub85d \uacf5\uac04\uc758 \uc0ac\uc774\uc988\ub294 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \ucee4\uc9c0\ub294\ub370\uc5d0 \ubc18\ud574 \uc624\ud788\ub824 sample point\ub4e4\uc774 \ud55c \ucabd \uad6c\uc11d\uc73c\ub85c \ubab0\ub824 \uc774\ub8e8\uc5b4\uc9c0\uac8c\ub41c\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc774\ub294 \ub300\ubd80\ubd84\uc758 \ub370\uc774\ud130\ub4e4\uc774 \ub2e4\ub978 data point\ub4e4 \uac04\uc758 \uac70\ub9ac\ubcf4\ub2e4 \ud45c\ubcf8\uacf5\uac04\uc758 \uacbd\uacc4\uc640 \ub354 \uac00\uae5d\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud558\uace0 training sample\uc758 \uacbd\uacc4\uc5d0\uc11c\ub294 \ub354\uc6b1 \ub354 \uc608\uce21\uc744 \uc5b4\ub835\uac8c\ud55c\ub2e4\ub294 \ubb38\uc81c\uc810\uc744 \ub0b3\ub294\ub2e4. \uc5ec\uae30\ub294 \ub118 \uc5b4\ub824\uc6cc\uc11c \uc774\ub530\uac00\ud558\uc790 Statistical Models, Supervised Learning and Function Approximation \uc6b0\ub9ac\uc758 \ubaa9\ud45c\ub294 input\uacfc output\uac04\uc758 \uc608\uce21\uc801\uc778 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud568\uc218 \\(f(x)\\) \uc5d0 \uc798 \uadfc\uc0ac\ud558\ub294 \\(\\hat{f}(x)\\) \ub97c \ucc3e\ub294 \uac83\uc774\ub2e4. \uc6b0\ub9ac\ub294 Section 2.4\ub97c \ud1b5\ud574 \uc774\ub860\uc801\uc73c\ub85c squared error loss\uac00 \uc591\uc801\ubcc0\uc218\uc5d0 \ub300\ud558\uc5ec \ud68c\uadc0\ud568\uc218 \\(f(x)=E(Y|X=x)\\) \ub97c \ub3c4\ucd9c\ud558\ub294 \uac83\uc744 \ubc30\uc6e0\ub2e4. knn methods\ub294 \uc774\ub7ec\ud55c \uc870\uac74\ubd80 \uae30\ub313\uac12\uc744 \uc9c1\uc811\uc801\uc73c\ub85c \ucd94\uc815\ud574\uc8fc\uc9c0\ub9cc \uc801\uc5b4\ub3c4 \uc544\ub798\uc758 \uc870\uac74\uc5d0\uc11c\ub294 \ub9cc\uc871\ud558\uc9c0 \uc54a\ub294\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc900\ub2e4. \uc785\ub825\uacf5\uac04\uc758 \ucc28\uc6d0\uc774 \ub108\ubb34 \ub192\ub2e4\uba74, \ucd5c\uadfc\uc811\uc774\uc6c3\ubc29\ubc95\uc740 \ud0c0\uac9f \ud3ec\uc778\ud2b8\uc5d0 \uac00\uae4c\uc6cc\uc9c0\uc9c0 \ubabb\ud558\uba70 \ud070 error\ub97c \ubc1c\uc0dd\uc2dc\ud0a8\ub2e4. \ub9cc\uc57d \ud2b9\ubcc4\ud55c \uad6c\uc870\uac00 \uc874\uc7ac\ud55c\ub2e4\uba74, \uc774\ub294 \ucd94\uc815\uce58\uc5d0 \ub300\ud55c \ud3b8\ud5a5\uacfc \ubd84\uc0b0\uc744 \uc904\uc774\ub294\ub370 \uc0ac\uc6a9\ub41c\ub2e4. A Statistical Model for the Joint Distribution Pr(X,Y) \\[Y=f(X)+\\epsilon\\] \uc774\ub7ec\ud55c \uc2dd\uc774 \uc788\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uc790. (\uc5ec\uae30\uc11c \uc784\uc758 \uc624\ucc28 \\(\\epsilon\\) \uc740 \\(E(\\epsilon)=0\\) \uc774\uba70, X\uc5d0 \ub300\ud574 \ub3c5\ub9bd\uc774\ub2e4.) \uc774\ub7ec\ud55c \uac00\ubc95\uc624\ucc28\ubaa8\ub378\uc740 \uc2e4\uc81c\ub85c \uc720\uc6a9\ud55c \uadfc\uc0ac\uc2dd\uc774\ub2e4. \uc2e4\uc81c\ub85c \ub300\ubd80\ubd84 \uc2dc\uc2a4\ud15c \uc548\uc5d0\uc11c\uc758 \uc785\ucd9c\ub825\uc30d \\((X,Y)\\) \ub294 determinisitic \ud55c \\(Y=f(X)\\) \uc758 \uad00\uacc4\ub97c \ub9cc\uc871\ud558\uc9c0 \uc54a\ub294\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c \uc774 \ub458\uc758 \uad00\uacc4\uc5d0\uc11c\ub294 \uce21\uc815\ub418\uc9c0 \uc548\ud754 \ubcc0\uc218\ub4e4\uc774 \uc874\uc7ac\ud558\uace0 \uc774\ub4e4\uc774 Y\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\uba70, \uce21\uc815\uc624\ucc28 \ub610\ud55c \ud3ec\ud568\ub41c\ub2e4. \uac00\ubc95 \ubaa8\ub378\uc758 \uac00\uc815\uc740 \uc774\ub7ec\ud55c deterministic \ud55c \uad00\uacc4 \uc678\uc758 error \\(\\epsilon\\) \uc744 \uc7a1\uc544\ub0bc \uc218 \uc788\ub2e4\ub294 \uac83\uc744 \uc804\uc81c\ub85c \ud55c\ub2e4. \ud2b9\uc815 \uba87\uba87\uc758 \ubb38\uc81c\ub4e4\uc740 deterministic \ud55c \uad00\uacc4\ub97c \uac16\uace0 \ub9cc\uc871\ud558\ub294 \ubaa8\uc2b5\uc744 \uac16\ub294\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub7ec\ud55c \ubb38\uc81c\ub4e4\uc73c \ub2e4\ub8e8\ub294 \uac83\uc774 \uc544\ub2cc \uc0c1\ud669\uc774\ub77c\uba74 \uc6b0\ub9ac\uac00 \uc624\ucc28\ud56d\uc5d0 \uae30\ubc18\ud55c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc801\uc808\ud560\uae4c? \uc5d0 \ub300\ud574 \uc0dd\uac01\uc744 \ud574\ubcfc \uc218 \uc788\ub2e4. error\uc5d0 \ub300\ud55c \ub3c5\ub9bd\uc131, \ub4f1\ubd84\uc0b0\uc131 \uc815\uaddc\uc131\ub4f1\uc758 \uac00\uc815\uc774 \uc5c4\uaca9\ud558\uac8c \ud544\uc694\ud55c \uac83\uc740 \uc544\ub2c8\uc9c0\ub9cc, \uc55e\uc11c \ubc30\uc6e0\ub358 EPE criterion\uc758 \uc608\ub97c \uc0dd\uac01\ud574\ubcf4\uba74 \uc774\ub7ec\ud55c \uac00\uc815\uc740 \uc0c1\ub2f9\ud788 \ud569\ub2f9\ud558\ub2e4. \uc774\ub7ec\ud55c \ubaa8\ub378\uc5d0\uc11c\ub294 \ucd5c\uc18c\uc81c\uacf1\ubc95\uc744 \uc774\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 estimation\ud558\ub294 \uac83\uc774 \ud569\ub2f9\ud574\ubcf4\uc778\ub2e4. \uc6b0\ub9ac\ub294 \ub3c5\ub9bd\uc131 \uac00\uc815\uc758 \uc81c\uc57d\uc744 \uac04\ub2e8\ud55c modifications\uc744 \ud1b5\ud574 \ud574\uacb0\ud560 \uc218\ub3c4 \uc788\ub2e4. \uc77c\ub840\ub85c \\(Var(Y|X=x)=\\sigma(x)\\) \ub97c \uc0dd\uac01\ud574\ubcf4\uc790. \uc6b0\ub9ac\ub294 Y\ub97c X\uc5d0 \ub300\ud55c \uc870\uac74\ubd80\ub97c \uac78\uc5b4 Y\uc758 \ubd84\uc0b0\uacfc \ud3c9\uade0\uc774 X\uc5d0 \uc885\uc18d\uc801\uc778 \uad00\uacc4\uac00 \ub418\ub3c4\ub85d \ubcc0\ud615\ud560 \uc218 \uc788\ub2e4. \uc77c\ubc18 \uc801\uc73c\ub85c \uc870\uac74\ubd80 \ubd84\ud3ec \\(Pr(Y|X)\\) \ub294 X\uc5d0 \uc885\uc18d\uc801\uc77c \uc218 \uc788\uc9c0\ub9cc, \uac00\ubc95\uc624\ucc28\ubaa8\ub378\uc5d0\uc11c \uc774 \ubd80\ubd84\uc740 \ubc30\uc81c\ud558\uc5ec \uc0dd\uac01\ud55c\ub2e4. \uac00\ubc95\ubaa8\ud615\uc740 \ud1b5\uc0c1\uc801\uc73c\ub85c \uc9c8\uc801 \ucd9c\ub825\ubcc0\uc218 \\(G\\) \uc5d0 \ub300\ud574\uc11c\ub294 \uc798 \uc0ac\uc6a9\ub418\uc9c0 \uc54a\ub294\ub2e4. \uc774\ub7ec\ud55c \ucf00\uc774\uc2a4\uc5d0\uc11c \ubaa9\ud45c\ud568\uc218 \\(p(X)\\) \ub294 \uc870\uac74\ubd80 \ubc00\ub3c4 \\(Pr(G|X)\\) \uc774\uba70 \uc9c1\uc811\uc801\uc73c\ub85c \ubaa8\ub378\ub9c1\ud560 \uc218 \uc788\ub2e4. \uadf8 \uc608\ub85c binary class \ub370\uc774\ud130\uc5d0\uc11c \uc790\ub8cc\ub4e4\uc774 \ub3c5\ub9bd\uc801\uc778 binary trials\uc5d0\uc11c \uc624\uace0 \ub450 \ubc94\uc8fc\uc5d0 \uc18d\ud560 \ud655\ub960\uc744 \uac01\uac01 \\(p(X)\\) , \\(1-p(X)\\) \ub85c \ub450\ub294 \uac83\uc774 \ud569\ub9ac\uc801\uc77c \uac83\uc774\ub2e4. \uadf8\ub7ec\ubbc0\ub85c Y\uac00 0-1 \ub85c \ub098\ub258\uc5b4\uc9c4 \uc9c8\uc801\ubcc0\uc218 G\uc778 \ucf00\uc774\uc2a4\uc5d0\uc11c\ub294 \\(E(Y|X=x)=p(x)\\) \uc774 \ub418\uc9c0\ub9cc, \ubd84\uc0b0\uc740 \\(Var(Y|X=x)=p(x)[1-p(x)]\\) \ub85c x\uc5d0 \ub300\ud574 \uc885\uc18d\uc801\uc774\ub2e4. Supervised Learning \uba38\uc2e0\ub7ec\ub2dd\uc758 \uad00\uc810\uc5d0\uc11c \ud568\uc218\uc801\ud569\uc758 \ud328\ub7ec\ub2e4\uc784\uc740 training set\uacfc input output\uc774 \uc874\uc7ac\ud558\ub294 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc77c\uc815\ud55c \uc54c\uace0\ub9ac\uc998\uc5d0 \uc758\ud574 \uadfc\uc0ac\ub41c\ub2e4. \uc774\ub7ec\ud55c \uc54c\uace0\ub9ac\uc998\uc740 \uc785\ucd9c\ub825\uac04\uc758 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \\(\\hat{f}\\) \ub97c \uc6d0\uc790\ub8cc\uc640 \uc0dd\uc131\ub41c \ucd9c\ub825\uac12 \uac04\uc758 \ucc28\uc774\uc778 \\(y_{i}-\\hat{f}(x_{i})\\) \ub97c \ud1b5\ud574 \ubcc0\ud615\ud560 \uc218 \uc788\ub294 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uace0, \uc774\ub97c learning by example \uc774\ub77c\uace0 \uce6d\ud55c\ub2e4. ####Function Approximation \ud559\uc2b5 \ud328\ub7ec\ub2e4\uc784\uc5d0 \ub300\ud55c \uc811\uadfc\uc740 \uc218\ud559, \ud1b5\uacc4\ud559\uc758 \ubd84\uc57c\uc5d0\uc11c\ub294 \ud568\uc218\uadfc\uc0ac\uc640 \ucd94\uc815\uc73c\ub85c \uc5ec\uaca8\uc9c4\ub2e4. \uc6b0\ub9ac\uac00 \uc870\uc6b0\ud558\uac8c \ub418\ub294 \ub9ce\uc740 \uadfc\uc0ac\ub4e4\uc740 \ub370\uc774\ud130\uc5d0 \ub530\ub77c \uc801\ud569\ud558\uac8c \ubcc0\ud615\ub418\ub294 \ubaa8\uc218 \\(\\theta\\) \uc758 set\uc744 \uac16\ub294\ub2e4. \uadf8 \uc608\ub85c \uc120\ud615\ubaa8\ub378 \\(f(x)=x^T\\beta\\) \ub294 \ubaa8\uc218 \\(\\theta=\\beta\\) \ub97c \uac16\ub294\ub2e4. \ub610\ub2e4\ub978 \uc720\uc6a9\ud55c \uadfc\uc0ac\ubd84\ub958\uae30\ubc95\uc740 linear basis expansions \uc73c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. $ \\(f_{\\theta}(x)=\\sum\\limits_{k=1}^Kh_{k}(x)\\theta_{k}\\) $ \uc5ec\uae30\uc11c \ub9d0\ud558\ub294 \\(h_{k}\\) \ub294 \uc801\uc808\ud55c \ud568\uc218\uc758 \uc9d1\ud569 \ud639\uc740 \uc785\ub825\ubca1\ud130 x\uc758 \ubcc0\ud658\ud615\ud0dc\uc774\ub2e4. \uc804\ud1b5\uc801\uc778 \uc608\ub85c polynomial \uacfc trigonometric expansions \ub4f1\uc774 \uc788\uace0 \uc774 \ub54c\uc758 \\(h_{k}\\) \ub294 \\(x_{1}^2, x_1x_2^2, cos(x_1)\\) \ub4f1\uc774 \uc788\ub2e4. \uc6b0\ub9ac\ub294 \ub610\ud55c \ube44\uc120\ud615\ud615\ud0dc\uc758 \ud655\uc7a5 \ub610\ud55c \uc0ac\uc6a9\ud560 \uc218\uac00 \uc788\ub294\ub370, \uc778\uacf5\uc2e0\uacbd\ub9dd \ubaa8\ub378\uc5d0\uc11c \uc77c\ubc18\uc801\uc73c\ub85c \uc4f0\uc774\ub294 sigmoid \ubcc0\ud658\uc744 \uc608\ub85c \ub4e4 \uc218 \uc788\uc73c\uba70, \uc774\ub7ec\ud55c form\uc740 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(h_{k}(x)=\\frac{1}{1+exp(-x^T\\beta_{k})}\\) $ \uc6b0\ub9ac\ub294 \ucd5c\uc18c\uc81c\uacf1\ubc95\uc744 \uc0ac\uc6a9\ud558\uba74\uc11c \uc120\ud615\ubaa8\ub378\uc5d0\uc11c \\(RSS(\\beta)\\) \ub97c \ucd5c\uc18c\ud654 \ud558\uc5ec \ubaa8\uc218 \\(\\theta\\) \uc758 \ud568\uc218\ub85c \ucde8\uae09\ud558\uace0 \ubaa8\uc218\ub97c \ucd94\uc815\ud558\ub294\ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. \uc120\ud615 \ubaa8\ub378\uc5d0\uc11c\ub294 \uc6b0\ub9ac\ub294 minimization problem\uc744 \ud1b5\ud574 \ub2e8\uc21c\ud55c closed form\uc758 \ud574\ub97c \uad6c\ud560 \uc218 \uc788\ub2e4. \uc774\ub294 basis function \ubc29\ubc95\ub860\uc5d0\uc11c\ub3c4 \ud1b5\uc6a9\ub418\uba70, \uc228\uaca8\uc9c4 \ub610\ub2e4\ub978 \ubaa8\uc218\ub97c \uac16\uc9c0\uc54a\ub294\ud55c \ub611\uac19\uc774 \uc801\uc6a9\ub41c\ub2e4. \uadf8\ub807\uc9c0 \uc54a\uc740 \uacbd\uc6b0\ub77c\uba74 \uc6b0\ub9ac\ub294 iterative methods\ub098 numerical optimazation\uc744 \ud1b5\ud574 \ud574\ub97c \uad6c\ud574\uc57c \ud55c\ub2e4. \ucd5c\uc18c\uc81c\uacf1\ubc95\uc740 \uc544\uc8fc \ud3b8\ub9ac\ud558\uace0 \ub300\uc911\uc801\uc774\uc9c0\ub9cc, \ud56d\uc0c1 \ud310\ub2e8\uc758 \ucc99\ub3c4\uac00 \ub418\ub294 \uac83\uc740 \uc544\ub2c8\ub2e4. \ub354 general\ud55c \ucd94\uc815\uc758 \uc6d0\ub9ac\uc758 \uc608\ub85c\ub294 maximum likelihood estimation \uc774 \uc788\ub2e4. $ \\(L(\\theta)=\\prod_{i=1}^Nf(x_{i};\\theta)\\) $ $ \\(l(\\theta)=\\sum\\limits_{i=1}^Nlogf(x_{i};\\theta)\\) $ MLE\ub97c \uc5bb\ub294 \uc6d0\ub9ac\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. \uac00\uc7a5 \ud569\ub9ac\uc801\uc778 \ubaa8\uc218 \\(\\theta\\) \ub294 \uad00\ucc30\ub41c \ud45c\ubcf8\uc744 \uc5bb\uc744 \ud655\ub960\uc774 \uac00\uc7a5\ub192\uc744 \uacbd\uc6b0\uc774\ub2e4. \uac00\ubc95\uc624\ucc28\ubaa8\ub378 \\(Y=f_{\\theta}(X)+{\\epsilon}\\) \uc5d0\uc11c\uc758 \ucd5c\uc18c\uc81c\uacf1\uc744 \ud1b5\ud574 \uc5bb\uc740 estimators\ub294 \uc870\uac74\ubd80 likelihood\ub97c \uc774\uc6a9\ud588\uc744 \ub54c\uc758 \ucd5c\ub300\uc6b0\ub3c4(maximum likelihood)\uc640 \uac19\ub2e4. $ \\(Pr(Y|X,\\theta)=N(f_{\\theta}(X),\\sigma^2)\\) $ \uace0\ub85c \ucd94\uac00\uc801\uc778 \uc815\uaddc\uc131 \uac00\uc815\uc744 \ub354\ud558\ub294 \uac83\uc740 \uc81c\ud55c\uc801\uc73c\ub85c \ubcf4\uc77c\uc9c0\ub77c\ub3c4, \uacb0\uacfc\ub294 \uac19\ub2e4. \uc774 \ub370\uc774\ud130\uc5d0\uc11c log-likelihood\ub294 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(L(\\theta)=-\\frac{N}{2}log(2{\\pi})-Nlog{\\sigma}-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^N(y_{i}-f_{\\theta}(x_{i}))^2\\) $ \uc704\uc758 \uc2dd\uc5d0\uc11c \\(\\theta\\) \ub97c \ud3ec\ud568\ud558\ub294 \ud56d\uc740 \uc624\uc9c1 \ub9c8\uc9c0\ub9c9 \ud56d\uc774\uba70, \uc774\ub294 \\(RSS(\\theta)\\) \uc758 scalar negative miltiplier\uc774\ub2e4. \uc870\uae08 \ub354 \ud765\ubbf8\ub85c\uc6b4 \uc608\uc81c\ub85c\ub294 \uc9c8\uc801 \ucd9c\ub825\ubcc0\uc218 \\(G\\) \uc5d0 \ub300\ud55c \ud68c\uadc0\ud568\uc218 \\(Pr(G=G_{k}|X=x)\\) \uc5d0 \ub300\ud55c mutinomial likelihood\uc774\ub2e4. \uc6b0\ub9ac\uac00 \uc8fc\uc5b4\uc9c4 X\uc5d0 \ub300\ud558\uc5ec \uac01 \ud074\ub798\uc2a4\uac00 \uc18d\ud560 \uc870\uac74\ubd80\ud655\ub960\uc774 \\(Pr(G=G_{k}|X=x)=p_{k,\\theta}(x), k=1,...,K\\) \ub85c \ud45c\uae30\ub418\ub294 \ubaa8\ub378\uc744 \uac16\uace0\uc788\ub2e4\uace0 \uc0dd\uac01\ud574\ubcf4\uc790. \uc774 \ub54c\uc758 \uc704 \uc2dd\uc758 log-likelihood(cross entropy)\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ub41c\ub2e4. (\ubbf8\uc548\ud55c\ub370 \uc9c4\uc9dc \ubb50\uac00 \ud765\ubbf8\ub85c\uc6b4\uc9c0 \ud558\ub098\ub3c4 \ubaa8\ub974\uaca0\ub2e4..) $ \\(L(\\theta)=\\sum\\limits_{i=1}^Nlogp_{g_{i},\\theta}(x_{i})\\) $ Structured Regression Models \uc6b0\ub9ac\ub294 \ucd5c\uadfc\uc811\uc774\uc6c3\uc774\ub098 \ub2e4\ub978 local methods\uac00 \uc9c1\uc811\uc801\uc73c\ub85c \ud2b9\uc815 \uc810\uc5d0\uc11c\uc758 \ud568\uc218\ub97c \ucd94\uc815\ud55c\ub2e4\ub294 \uac83\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd94\uace0 \uc788\uc9c0\ub9cc, \uc774\ub294 \uac00\ub054 \ucc28\uc6d0\uc73c\ub85c \uc778\ud574 \uc5b4\ub824\uc6c0\uc744 \ub9de\ub531\ub4e4\uc778\ub2e4\ub294 \uc0ac\uc2e4\uc744 \ubc30\uc6e0\ub2e4. \uc774\ub7ec\ud55c \uc811\uadfc\ubc95\uc740 \uc800\ucc28\uc6d0\uc758 \ubb38\uc81c\uc5d0\uc11c\ub294 \ubd80\uc801\uc808\ud560 \uc218\ub3c4 \uc788\uc73c\uba70, \uc624\ud788\ub824 \ub354 \uad6c\uc870\ud654\uace0 \uc81c\uc57d\ub41c \uc870\uac74\uc744 \uac78\uc5b4 \ubaa8\ub378\uc744 \uc9dc\ub294 \uac83\uc774 \ub370\uc774\ud130\ub97c \ub354 \ud6a8\uc728\uc801\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uc77c \uc218\ub3c4 \uc788\ub2e4. $ \\(RSS(f)=\\sum\\limits_{i=1}^N(y_{i}-f(x_{i}))^2\\) $ \uc704\uc758 \uc2dd\uc744 \ucd5c\uc18c\ud654 \ud558\ub294 \ud574\ub97c \uad6c\ud558\ub294 \uac83\uc740 training points \\(({x_{i}},y_{i})\\) \ub97c \uc9c0\ub098\ub294 \ud568\uc218 \\({\\hat{f}}\\) \uac00 \ubb34\ud55c\ud788 \uc874\uc7ac\ud560 \uc218\ub3c4\uc788\ub2e4. \uc774\ub7ec\ud55c \ud574\ub4e4 \uc911 \uba87\uba87\uc740 test points\uc5d0\uc11c \uc544\uc8fc \ud615\ud3b8\uc5c6\ub294 predictors\uac00 \ub420 \uac83\uc774\ub2e4. \ub9cc\uc57d \uad00\uce21\uce58 \uc30d\uc758 \uac2f\uc218\uac00 \ub9ce\ub2e4\uba74 \uc774\ub7ec\ud55c \ub9ac\uc2a4\ud06c\ub294 \uc81c\ud55c\ub420 \uc218\ub3c4 \uc787\ub2e4. \uc6b0\ub9ac\uac00 \uc870\uae08 \ub354 \ud6a8\uc728\uc801\uc73c\ub85c \uc774 \ubb38\uc81c\ub97c \uc811\uadfc\ud558\ub824\uba74 N\uc744 \ubb34\uc791\uc815 \ub298\ub9ac\ub294 \uac83\uc774 \uc544\ub2cc, \uc720\ud55c\ud55c N\uc548\uc5d0\uc11c \ucd5c\uc18c\ud654 \ubb38\uc81c\uc5d0 \uc870\uae08 \ub354 \uc81c\uc57d\uc744 \uac78\uc5b4 \ubcf4\ub2e4 \uc791\uc740 \ud568\uc218\uc758 \uc9d1\ud569\uc744 \uc5bb\uc5b4\uc57c\ud55c\ub2e4. \uc774\ub7ec\ud55c \ud574\ub4e4\uc5d0 \ub300\ud55c \uc81c\uc57d\uc774 \uc774 \ucc45\uc758 \uc8fc\uc694 \ud1a0\ud53d\uc774\ub2e4. \uc5ec\uae30\uc11c \ud558\ub098 \uba85\ud655\ud788 \ud574\uc57c\ud560 \uac83\uc740 \uc6b0\ub9ac\uac00 \uc720\uc77c\ud55c \ud574\ub97c \uc5bb\uae30 \uc704\ud574 \uc81c\uc57d\uc744 \uac70\ub294 \uac83\uc774 \ud574\uc758 \ubcf5\uc7a1\uc131\uc73c\ub85c \uc778\ud574 \ub098\uc624\ub294 \ubaa8\ud638\uc131\uc744 \uc5c6\uc568 \uc218\ub294 \uc5c6\ub2e4\ub294 \uac83\uc774\ub2e4. Classes of Restricted Estimators \ube44\ubaa8\uc218\ud68c\uadc0\ub098 \ub2e4\uc591\ud55c \ud559\uc2b5 \ubc29\ubc95\ub4e4\uc740 \uc6b0\ub9ac\uac00 \ucd94\uac00\ud558\ub294 \uc81c\uc57d\uc774 \uc5b4\ub5a4 nature\ub97c \uac16\ub290\ub0d0\uc5d0 \ub530\ub77c \ub2e4\ub974\uac8c \ubd84\ub958\ub41c\ub2e4. \uac01\uac01\uc758 \ud074\ub798\uc2a4\ub4e4\uc740 \ud55c\uac1c \ud639\uc740 \uadf8 \uc774\uc0c1\uc758 parameter\ub97c \uac16\ub294\ub370, \ub54c\ub54c\ub85c local neighborhood\uc758 \ud6a8\uacfc\uc801\uc778 size\ub97c \uc870\uc808\ud574\uc8fc\ub294 smoothing parameter\ub4e4\uc774 \uc788\ub2e4. \uc6b0\ub9ac\ub294 \uc5ec\uae30\uc11c class of restricted estimators\ub4e4\uc744 \ub113\uc740 \uc758\ubbf8\ub85c 3\uac00\uc9c0\ub85c \ubd84\ub958\ud558\uc5ec \uc124\uba85\uc744 \ud55c\ub2e4. 1.Roughness Penalty and Bayesian Methods \uba3c\uc800 \\(RSS(f)\\) \ub97c penalize\ud558\uc5ec \ucee8\ud2b8\ub864\ud558\ub294 \ud568\uc218\uac00 \uc788\ub2e4. \uc774\ub294 1\ucc28\uc6d0\uc758 \uc785\ub825\ubcc0\uc218\uc5d0 \ub300\ud55c cubic smoothing spline method\uc774\ub2e4. \uc27d\uac8c \uc124\uba85\ud558\uba74 cubic spline \uc774\ub780, \uc810\ub4e4\uc744 \ub9e4\ub044\ub7fd\uac8c \uc5f0\uacb0\ud558\ub294 \uc54c\uace0\ub9ac\uc998\uc778\ub370, \ub450 \uc810\uc744 \uc787\ub294 \uace1\uc120\uc744 3\ucc28 \ub2e4\ud56d\uc2dd\uc744 \uc774\uc6a9\ud558\uc5ec \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc5d0 cubic\uc774\ub77c \uce6d\ud55c\ub2e4. $ \\(PRSS(f;\\lambda)=RSS(f) + {\\lambda}J(f)\\) $ $ \\(PRSS(f;\\lambda)=\\sum\\limits_{i=1}^N(y_{i}-f(x_{i}))^2 + {\\lambda}\\int[f^{''}(x)]^2dx\\) $ roughness penalty\ub294 \\(f\\) \uc758 \ud070 2\ucc28\ubbf8\ubd84 \uac12\ub4e4\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\uba70, penalty\uc758 \uc815\ub3c4\ub294 \\(\\lambda \\geq 0\\) \uc778 \\(\\lambda\\) \uc5d0 \uc758\ud574 \ud45c\ud604\ub41c\ub2e4. \\(\\lambda=0\\) \ub294 no smoothing\uc744 \uc758\ubbf8\ud558\uba70 penalty\uac00 \uc874\uc7ac\ud558\uc9c0 \uc54a\uac8c \ub418\ub294 \uac83\uc774\uace0, \\(\\lambda = \\infty\\) \uc758 \uacbd\uc6b0\ub294 \uc624\ub85c\uc9c0 x\ub0b4\uc758 \uc120\ud615\ud568\uc218\ub9cc\uc744 \ud5c8\uc6a9\ud558\uac8c \ub41c\ub2e4. (smoothing\uc744 \ubb34\ud55c\ud788 \ub9ce\uc774\ud574\uc11c RSS(f)\uc640 \uac19\uc740 \uc2dd\uc774\ub77c\uace0 \ubcf4\uba74 \ub41c\ub2e4.) Penalty function J\ub294 \uc5b4\ub5a4 \ucc28\uc6d0\uc5d0\uc11c\ub4e0 \ud2b9\ubcc4\ud55c \ubc84\uc804\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc9c0\uace0 \ud2b9\ubcc4\ud55c \uad6c\uc870\ub97c \uc9e4 \uc218 \uc788\ub2e4. Penalty function \ub610\ub294 regularization methods\ub294 \uc774\ub7f0 \uc885\ub958\uc758 \ud568\uc218\uc758 smooth behavior\uc5d0 \ub300\ud55c \uc0ac\uc804\uc801 \ubbff\uc74c\uc744 \ub098\ud0c0\ub0b4\uba70, \uc774\ub294 Bayesian framework\ub0b4\uc5d0\uc11c \uc801\uc6a9\uac00\ub2a5\ud558\ub2e4. penalty J\ub294 log-prior\uc5d0 \ud574\ub2f9\ud558\uba70, \ud568\uc218 \\(PRSS(f;\\lambda)\\) \ub294 log-posterior distribution\uc744, \\(PRSS(f;\\lambda)\\) \ub97c \ucd5c\uc18c\ud654\ud558\ub294 \uac83\uc740 posterior mode\ub97c \ucc3e\ub294 \uac83\uacfc \uc0c1\uc751\ud55c\ub2e4. \uc774\uc5d0 \ub300\ud55c \uc2ec\ud654\ub0b4\uc6a9\uc740 \ub4a4\uc758 \ucc55\ud130 5\uc640 \ucc55\ud130 8 \uc5d0\uc11c \ud6c4\uc220\ud560 \uac83\uc774\ub2e4. 2.Kernal Methods and Local Regression \uc774 \ubc29\ubc95\uc740 local neighborhood\uc758 nature\ub97c \ud2b9\uc815\uc9c0\uc74c\uc73c\ub85c\uc368 \ud68c\uadc0\ud568\uc218 \ud639\uc740 \uc870\uac74\ubd80 \uae30\ub313\uac12\uc5d0 \ub300\ud55c \ucd94\uc815\uce58\ub97c \uc81c\uacf5\ud558\ub294 \ubc29\ubc95\uc774\ub2e4. local neighborhood\ub294 kernel function \\(K_{\\lambda}(x_0,x)\\) \uc5d0 \uc758\ud574 specify \ub418\ub294\ub370, \uc774 \ud568\uc218\ub294 \\(x_{0}\\) \uc8fc\ubcc0\uc758 x\uc810\ub4e4\uc5d0 \uac00\uc911\uce58\ub97c \ubd80\uc5ec\ud558\ub294 \ud568\uc218\uc774\ub2e4. \uac00\uc6b0\uc2dc\uc548 \ucee4\ub110\uc740 \uc77c\ub840\ub85c \uc815\uaddc\ud655\ub960\ubc00\ub3c4\ud568\uc218\uc5d0 \uae30\ubc18\ud558\uc5ec \uac00\uc911\ud568\uc218\ub97c \uac16\uac8c \ub418\ub294\ub370 \uadf8 \uc2dd\uc740 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(K_{\\lambda}(x_0,x)=\\frac{1}{\\lambda}exp[-\\frac{||x-x_0||^2}{2{\\lambda}}]\\) $ \uc774\ub294 \\(x_0\\) \uc73c\ub85c\ubd80\ud130 squared Euclidean \uac70\ub9ac\ub97c \ud1b5\ud574 \uba40\uc5b4\uc9c0\ub294 \uc9c0\uc810\ub4e4\uc5d0 \ub300\ud574 \uc9c0\uc218\uc801\uc73c\ub85c \uac00\uc911\uce58\ub97c \ubd80\uc5ec\ud558\ub294 \uc6d0\ub9ac\uc774\ub2e4. \uc5ec\uae30\uc11c \ud30c\ub77c\ubbf8\ud130 \\(\\lambda\\) \ub294 \uc815\uaddc\ubc00\ub3c4\uc758 \ubd84\uc0b0\uc744 \uc758\ubbf8\ud558\uba70, \uc774\uc6c3\uc758 \ub108\ube44\ub97c \uacb0\uc815\ud55c\ub2e4. \uac00\uc7a5 \uac04\uacb0\ud55c kernel estimate\uc758 form\uc740 Nadaraya-Watson weighted average \uc774\uba70 \uc2dd\uc740 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(\\hat{f}(x_0)=\\frac{\\sum\\limits_{i=1}^NK_{\\lambda}(x_0,x_{i})y_{i}}{\\sum\\limits_{i=1}^NK_{\\lambda}(x_0,x_{i})}\\) $ \uc704\uc758 \uc2dd\uc740 error\ub97c \ucd5c\uc18c\ud654 \ud558\ub294 \ud68c\uadc0\ud568\uc218 f(x)\uac00 \\(E[Y|X=x]\\) \uc778 \uc810\uc744 \uc774\uc6a9\ud558\uc5ec \uc774\ub97c \uc870\uac74\ubd80\ud655\ub960\ub85c \ucabc\uac20 \ud6c4, \uac01\uac01 marginal distribution\uc778 \\(f(x)\\) \uc640 joint distribution \uc778 \\(f(x,y)\\) \uc5d0 \ub450\uac1c\uc758 kernel density\ub97c plug\ud558\uc5ec \\(y_{i}\\) \uc5d0 \uac00\uc911\uce58\ub97c \ubd80\uc5ec\ud558\ub294 \ud568\uc218\ub77c\uace0 \ubcfc \uc218 \uc788\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c \uc6b0\ub9ac\ub294 \\(f(x_0)\\) \uc758 local \ud68c\uadc0 \ucd94\uc815\uce58\ub97c \\(f_{\\hat{\\theta}}(x_0)\\) \ub85c \ud45c\uae30\ud558\uace0, \\(\\hat{\\theta}\\) \ub97c \ucd5c\uc18c\ud654\ud558\ub294 \uc2dd\uc740 $ \\(RSS(f_{\\theta},x_0)=\\sum\\limits_{i=1}^NK_{\\lambda}(x_0,x_{i})(y_{i}-f_{\\theta}(x_{i}))^2\\) $ \uc774\ub2e4. \uc5ec\uae30\uc11c \\(f_{\\theta}\\) \ub294 \uc5b4\ub5a4 \ubaa8\uc218\ud654\ub41c \ud568\uc218\uc774\uba70 low-order polynomial \ub4f1\uc774 \uc788\ub2e4. \ub2e4\ub978 \uc608\ub97c \ubcf4\uc790. * \\(f_{\\theta}={\\theta}_0\\) \uc778 constant function\uc774\uba74 \uc774 \ud568\uc218\uc758 \uacb0\uacfc\uac12\uc740 \uc704\uc5d0\uc11c \uc5b8\uae09\ud55c Nadaraya-Watson estimate\ub97c \uac16\ub294\ub2e4. * \\(f_{\\theta}={\\theta}_0+{\\theta}_1x\\) \ub294 local linear regression model\uc744 \uac16\ub294\ub2e4. \ub4f1\uc774 \uc788\ub2e4. \uc815\ub9ac\ud558\uc790\uba74 \ucd5c\uadfc\uc811\uc774\uc6c3 \ubc29\ubc95\uc740 \ub354 \ub370\uc774\ud130\uc5d0 \uc758\uc874\uc801\uc778 metric\uc744 \uac16\ub294 kernel method\ub77c\uace0 \ubcfc \uc218 \uc788\ub2e4. \uc5ec\uae30\uc11c knn\uc758 metric\uc740 \uc544\ub798\uc640 \uac19\uc73c\uba70, $ \\(K_{k}(x,x_0)=I(||x-x_0|| \\leq||x_{(k)}-x_0||)\\) $ \uc5ec\uae30\uc11c \\(x_{(k)}\\) \ub294 \\(x_0\\) \uc73c\ub85c\ubd80\ud130 k \ubc88\uc9f8 \ub9cc\ud07c \ub5a8\uc5b4\uc9c4 training \uad00\uce21\uce58 \uc774\uace0, \\(I(S)\\) \ub294 \uc9d1\ud569 S\uc758 indicator \ud568\uc218\uc774\ub2e4. \uc774 \ubc29\ubc95\ub860\ub4e4 \uc5ed\uc2dc \uace0\ucc28\uc6d0\uc5d0\uc11c\ub294 \ucc28\uc6d0\uc758 \uc800\uc8fc\ub97c \ubc97\uc5b4\ub098\uae30 \uc704\ud574 \uc801\uc808\ud55c \ubcc0\ud615\uc774 \ud544\uc694\ud558\uba70, \uc774\ub294 6\uc7a5\uc5d0\uc11c \ud6c4\uc220\ud558\uaca0\ub2e4. 3. Basis Functions and Dictionary Methods \uc774\ubc88\uc5d0 \uc18c\uac1c\ud558\ub294 \ubc29\ubc95\uc740 \uce5c\uc219\ud55c \uc120\ud615\ubaa8\ub378\uacfc \ub2e4\ud56d\ubaa8\ub378\ub85c\uc758 \ud655\uc7a5\uc744 \ud3ec\ud568\ud558\uc9c0\ub9cc, \uadf8 \uc911\uc5d0\uc11c\ub3c4 \ub354 \uc720\uc5f0\ud558\uace0 \uc911\uc694\ud558\uac8c \uc5ec\uaca8\uc9c0\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc744 \ud3ec\ud568\ud55c\ub2e4. \ubaa8\ub378\uc5d0 \ub300\ud55c \ud568\uc218 f \ub294 basis function\uc758 \uc120\ud615\ud655\uc7a5 form\uc744 \ub744\uba70 \uc544\ub798\uc640 \uac19\uc740 \ud615\ud0dc\ub97c \uac16\ub294\ub2e4. $ \\(f_{\\theta}(x)=\\sum\\limits_{m=1}^M{\\theta}_{m}h_{m}(x)\\) $ \uc5ec\uae30\uc11c \\(h_{m}\\) \uc740 \uc785\ub825\ubcc0\uc218 x\uc5d0 \ub300\ud55c \ud568\uc218\uc774\uba70, linear term\uc740 \ubaa8\uc218 \\(\\theta\\) \uc5d0 \uc758\ud574 \uacb0\uc815\ub41c\ub2e4. \uc774 basis functions\uc758 \ud070 \ud2c0\uc740 \uad49\uc7a5\ud788 \ub2e4\uc591\ud55c \ubc29\ubc95\ub860\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc73c\uba70 \uc774\ub97c \uc124\uba85\ud560 \uc218 \uc788\ub294\ub370 \ub354 \uc790\uc138\ud55c \uc608\ub294 \ucd94\ud6c4\uc758 \ucc55\ud130\uc5d0\uc11c \ub2e4\ub904\ubcf8\ub2e4\uace0 \ud55c\ub2e4.(\ucc45\uc774 \ub108\ubb34 \ubd88\uce5c\uc808\ud558\uc8e0 \ucca8\uc5d0,,?) \uac04\ub2e8\ud55c \uc608\uc2dc\ub85c Radial basis functions \uac00 \uc788\ub294\ub370 \uc774\ub294 symmetric\ud55c p\ucc28\uc6e1\ub2ac \ucee4\ub110\uc744 \ud2b9\uc815 centroid\uc5d0 \uc704\uce58\uc2dc\ud0a4\ub294 \ubc29\ubc95\uc774\ub2e4. \uc774\ub294 \uc704\uc758 \uc2dd\uc5d0\uc11c \uc785\ub825\ubcc0\uc218 x\uc5d0 \ub300\ud55c \ud568\uc218\uac00 \\(K_{\\lambda{m}}({\\mu},x)\\) \uc758 \ud615\ud0dc\uc774\uba70 \uc5ec\uae30\uc11c kernal K\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \uc815\uaddc\ud655\ub960 kernel\uc778 \\(K_{\\lambda}({\\mu},x)=e^{-||x-{\\mu}||^2/2{\\lambda}}\\) \uac00 \ub9ce\uc774 \uc0ac\uc6a9\ub41c\ub2e4. \uc5b4\ub5a4 \uc758\ubbf8\ub97c \uac16\ub294\uc9c0, \uc5ec\uae30\uc11c \uc774 \ubaa8\ub378\uc758 \uc608\uc2dc\ub4e4\ub85c \uc5b4\ub5a0\ud55c \ud655\uc7a5\uc744 \ud560 \uc218 \uc788\ub294\uc9c0\ub294 \uc544\uc9c1\uae4c\uc9c0 \ucc55\ud1302\ub9cc\uc744 \uc77d\uace0\ub294 \uc774\ud574\uac00 \ud798\ub4e0 \uac83 \uac19\ub2e4. \ucd94\ud6c4 \ub4b7 \ucc55\ud130\uc5d0\uc11c \ub610 \ub2e4\ub8ec\ub2e4\uace0 \ud55c\ub2e4.. Model Selection and the Bias-Variance Tradeoff \ubaa8\ub4e0 \ubaa8\ub378\ub4e4\uc740 \ubaa8\uc218\uc5d0 \ub300\ud55c smoothing\uacfc complexity\uac00 \uacb0\uc815\ub418\uc5b4\uc57c\ud55c\ub2e4. penalty term\uc758 multiplier kernel\uc758 \ub108\ube44 basis functions\uc758 \uac2f\uc218 \ub4f1\uc73c\ub85c \ub9d0\uc774\ub2e4. \uc774\ub7ec\ud55c \uc870\uac74\ub4e4\uc774 \uc5b4\ub5bb\uac8c \uc6c0\uc9c1\uc774\ub0d0\uc5d0 \ub530\ub77c \uc6b0\ub9ac\ub294 training\uacfc test sample\uc5d0 \ub300\ud574 \uc801\uc808\ud55c bias\uc640 variance\uc5d0 \uc5b4\ub5bb\uac8c \uc81c\uc57d\uc744 \uac78\uace0 \ubaa8\ub378\uc744 \uafb8\ub824\ub098\uac08\uc9c0\ub97c \uacb0\uc815\ud560 \uc218 \uc788\ub2e4. knn \ud68c\uadc0 \uc54c\uace0\ub9ac\uc998\uc744 \uc608\uc2dc\ub85c \ud3b8\ud5a5 \ubd84\uc0b0 trade-off \uad00\uacc4\ub97c \uc0b4\ud3b4\ubcf4\uc790. \\(x_0\\) \uc5d0\uc11c\uc758 expected prediction error\ub294 test \ud639\uc740 generalization error \ub77c\uace0 \uc54c\ub824\uc838\uc788\uc73c\uba70 \uc2dd\uc744 \ubd84\ud574\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(EPE_{k}(x_0)=E[(T-\\hat{f}_{k}(x_0))^2|X=x_0]\\) $ $ \\(= {\\sigma}^2+[Bias^2(\\hat{f}_{k}(x_0))+Var_{\\tau}(\\hat{f}_{k}(x_0))]\\) $ $ \\(= {\\sigma}^2 + [f(x_0)-\\frac{1}{k}\\sum\\limits_{l=1}^kf(x_{(l)})]^2+ \\frac{{\\sigma}^2}{k}\\) $ \uc774\ub294 3 terms\ub85c \ub098\ub220\uc11c \ud45c\ud604\uc2dd\uc744 \uc0b4\ud3b4\ubcfc \uc218 \uc788\ub294\ub370, \uccab\ubc88\uc9f8 \ud140\uc758 \\({\\sigma}^2\\) \uc740 irreducible error\uc774\ub2e4. \uc989 \uc6b0\ub9ac\uac00 \uc124\ub839 \ucc38\uac12\uc758 \\(f(x_0)\\) \ub97c \uc54c\uace0\uc788\ub2e4\ud558\ub354\ub77c\ub3c4 \uc774\ub294 \uc0c8\ub85c\uc6b4 test target\uc758 \ubd84\uc0b0\uc774\uae30\uc5d0 \uc6b0\ub9ac\uc758 control\uc744 \ubc97\uc5b4\ub09c \ubcc0\uc218\uc774\ub2e4. \ub450\ubc88\uc9f8 term\uc740 \ud3b8\ud5a5\uc758 \uc81c\uacf1\uc744 \uc758\ubbf8\ud558\uba70 \ud3b8\ud5a5\uc774\ub780 \\(\\hat{f}_{k}(x_0)\\) \uc640 \uc2e4\uc81c \\(f({x_0})\\) \uac04\uc758 \ucc28\uc774\ub97c \ub9d0\ud55c\ub2e4. \uc774\ub294 k\uc5d0 \ub300\ud574 \ub2e8\uc870\uc99d\uac00 \ud568\uc218\uc774\uba70 k\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ucee4\uc9c4\ub2e4. \uc138\ubc88\uc9f8 term\uc740 new target\uc758 \ubd84\uc0b0\uc744 k\ub85c \ub098\ub208 \uac83\uc774\ubbc0\ub85c, k\uac00 \ucee4\uc9d0\uc5d0 \ub530\ub77c \uac12\uc774 \uac10\uc18c\ud55c\ub2e4. \uc704\uc758 \uad00\uacc4\ub97c \ud1b5\ud574 \uc6b0\ub9ac\ub294 \ubaa8\ub378\ub4e4\uc774 \uc608\uce21\uc5d0 \uc788\uc5b4\uc11c \uac16\uace0 \uc788\ub294 \ub51c\ub808\ub9c8 bias-variance trade off\ub97c \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc6b0\ub9ac\ub294 \ud1b5\uc0c1\uc801\uc73c\ub85c \ud3b8\ud5a5\ubd84\uc0b0\uac04\uc758 \ub51c\ub808\ub9c8\ub97c \uc904\uc774\uace0 test error\ub97c \ucd5c\uc18c\ud654 \ud558\uae30\uc704\ud574 \ubaa8\ub378\uc758 complexity\ub97c \uc815\ud558\ub294\ub370, \uc77c\ubc18\uc801\uc73c\ub85c \uc6b0\ub9ac\ub294 test error\uc5d0 \ub300\ud55c \ub69c\ub837\ud55c \ucd94\uc815\uac12\uc73c\ub85c MSE(=training error)\ub97c \uc0ac\uc6a9\ud55c\ub2e4. \ud558\uc9c0\ub9cc \ubd88\ud589\ud558\uac8c\ub3c4 \uc774\ub294 model complexity\uac00 \uc801\uc808\ud558\uac8c \uace0\ub824\ub418\uc9c0 \uc54a\ub294\ub2e4\uba74 training error\ub294 test error\uc5d0 \ub300\ud574 \uc88b\uc740 \ucd94\uc815\uac12\uc774 \ub418\uc9c0 \ubabb\ud55c\ub2e4. prediction \ubc29\ubc95\uc5d0\uc11c test error\ub97c \uce21\uc815\ud558\ub294 \ubc29\ubc95\ub4e4\uc740 \ucd94\ud6c4 7\uc7a5\uc5d0\uc11c \ub354 \ub17c\uc758 \ud574\ubcf4\uace0 \ub9e4\uc6b0 \ucc1c\ucc1c\ud558\uc9c0\ub9cc 2\uc7a5\uc740 \uc5ec\uae30\uc11c \ub9c8\ubb34\ub9ac \ud558\uaca0\ub2e4.","title":"02 Overview of Supervised Learning"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#two-simple-approachs-to-prediction","text":"\uc55e\uc758 Introduction\uc5d0\uc11c\ub294 \uac00\ubccd\uac8c \uc9c0\ub3c4\ud559\uc2b5\uacfc \ube44\uc9c0\ub3c4\ud559\uc2b5\uc744 \uc124\uba85\ud558\uba70 \uae00\uc744 \uc2dc\uc791\ud558\uc600\ub2e4. \uad50\uc7ac\uc758 \uc55e\ubd80\ubd84\uc5d0\uc11c \uc911\uc810\uc801\uc73c\ub85c \ub2e4\ub8f0 \uc9c0\ub3c4\ud559\uc2b5\uc740 \uac04\ub2e8\ud558\uac8c \uc785\ub825\uac12\uc5d0 \ub300\ud55c \ubaa9\ud45c\uce58\uac00 \uc8fc\uc5b4\uc838\uc788\uc73c\uba70, \uc774\ub97c \uac00\uc9c0\uace0 \uc788\ub294 \ub370\uc774\ud130\ub97c \uc774\uc6a9\ud574 \uc608\uce21\ud558\uac70\ub098 \ubd84\ub958\ud558\ub294 \uac83\uc774\ub77c\uace0 \ub9d0\ud560 \uc218 \uc788\ub2e4. 2\ub2e8\uc6d0\uc758 \uc2dc\uc791\uc73c\ub85c\ub294 \uc608\uce21\uc5d0 \uc788\uc5b4 \uac04\ub2e8\ud558\uac8c \uc0ac\uc6a9\ud558\ub294 \ub450 \uac00\uc9c0 \ubc29\ubc95\uc744 \uc18c\uac1c\ud55c\ub2e4. Least Squares Nearest Neighbors rule","title":"Two simple Approachs to Prediction"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#least-squares","text":"\ub300\ud45c\uc801\uc73c\ub85c \uc6b0\ub9ac\uac00 \uc54c\uace0\uc788\ub294 \uc120\ud615\ubaa8\ub378\uc744 \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\[\\hat{Y}=\\hat{\\beta_0}+\\sum\\limits_{j=1}^p{X_{j}\\hat{\\beta_{j}}}\\] \uc704\uc758 \uc2dd\uc5d0\uc11c \\(X^T = (X_1,X_2,..,X_{p})\\) \uc758 \ud615\ud0dc\ub97c \ub744\ub294 \ubca1\ud130\uc774\ub2e4. \uc5ec\uae30\uc11c \\(\\hat{\\beta_0}\\) \ub294 intercept \uc774\uba70 \uc774\ub294 \uae30\uacc4\ud559\uc2b5\uc5d0\uc11c bias(\ud3b8\ud5a5) \uc774\ub77c \uce6d\ud558\uae30\ub3c4\ud55c\ub2e4. \uc6b0\ub9ac\ub294 \ud3b8\uc758\uc0c1 \\(\\hat{Y}\\) \ub97c \uae54\ub054\ud558\uac8c \ud45c\ud604\ud558\uae30 \uc704\ud574 \\({X}\\) \ubca1\ud130 \uc548\uc5d0 constant variable 1\uc744 \uc9d1\uc5b4 \ub123\uc5b4 \\(\\hat{\\beta_0}\\) \ub97c \\(\\hat{\\beta}\\) \uc5d0 \ud3ec\ud568\uc2dc\ucf1c \\(\\hat{Y}={X}^T\\hat{\\beta}\\) \ub85c \ud45c\uae30\ud55c\ub2e4. \uc774\ub294 \uc120\ud615\ubaa8\ub378\uc744 \ub0b4\uc801\uc758 \ud615\ud0dc\ub85c \ub9cc\ub4e4\uc5b4 \ud45c\uae30\ud55c\ub2e4\ub294 \uac83\uc5d0 \uc788\uc5b4 \ub610 \ub2e4\ub978 \ud3b8\ub9ac\ud568\uc774 \uc874\uc7ac\ud55c\ub2e4. \uc704\uc758 \uc120\ud615\ubaa8\ub378 \uc2dd\uc5d0 \ub530\ub974\uba74 \\({Y}\\) \ub294 scalar\uc9c0\ub9cc, \uc77c\ubc18\uc801\uc73c\ub85c K-vector\ub77c\uace0 \ud55c\ub2e4\uba74, \\({\\beta}\\) \ub294 \\(p\\times{K}\\) \uc758 \ud589\ub82c\uc774 \ub41c\ub2e4. \\(({X},{\\hat{Y}})\\) \ub294 \\(p+1\\) \ucc28\uc6d0\uc758 \uc785\ucd9c\ub825 \uacf5\uac04\uc5d0 \uc874\uc7ac\ud558\ub294 hyperplane\uc744 \ub098\ud0c0\ub0b4\uac8c \ub41c\ub2e4. \uc5b4\ub5bb\uac8c \ud558\uba74 \uac00\uc9c0\uace0 \uc788\ub294 trainig data\ub97c \uc120\ud615\ubaa8\ub378\uc5d0 \uc798 fitting \uc2dc\ud0ac \uc218 \uc788\uc744\uae4c? \ub2e4\uc591\ud55c \ubc29\ubc95\ub4e4\uc774 \uc874\uc7ac\ud558\uc9c0\ub9cc, \uc704\uc5d0\uc11c \uc5b8\uae09\ud588\ub4ef\uc774 \uac00\uc7a5 \ub300\uc911\uc801\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 \ubc29\uc2dd\uc740 least squares method\uc77c \uac83\uc774\ub2e4. \uc774 \ubc29\ubc95\uc740 \uc2e4\uc81c \\({y}\\) \uac12\uacfc \uc120\ud615 \ubaa8\ub378\uc744 \ud1b5\ud574 \ucd94\uc815\ud55c \\({\\hat{y}}\\) \uac04\uc758 \ucc28\uc774\uc778 \uc794\ucc28\uc758 \uc81c\uacf1\uc744 \ucd5c\uc18c\ud654\ud558\ub294 \\({\\beta}\\) \ub97c \ucc3e\ub294 \uc6d0\ub9ac\ub97c \uc774\uc6a9\ud558\uba70, \uc774\ub97c \uc218\uc2dd\uc73c\ub85c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(RSS({\\beta})= \\sum\\limits_{i=1}^{N}({y_{i}}-{x_{i}}^T{\\beta})^2\\) $ \\(RSS({\\beta})\\) \ub294 \ud30c\ub77c\ubbf8\ud130\uc5d0 \ub300\ud574 2\ucc28\ud56d\uc758 \ud568\uc218 \uaf34\uc774\uae30\uc5d0 \ucd5c\uc18c\uac12\uc774 \uc5b8\uc81c\ub098 \uc874\uc7ac\ud558\uc9c0\ub9cc, \uace0\uc720\uc758 \ud574\ub97c \uac16\uc9c4 \uc54a\uc744 \uc218\ub3c4 \uc788\ub2e4. \ud574\ub97c \uad6c\ud558\ub294 \uac83\uc744 \ud589\ub82c\ub85c \ub3c4\uc2dd\ud654 \ud55c\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. $ \\(RSS({\\beta})=(\\mathbf{y}-\\mathbf{X}{\\beta}^T)(\\mathbf{y}-\\mathbf{X}{\\beta})\\) $ \uc5ec\uae30\uc11c X \ub294 Nxp \ud589\ub82c\uc774\uba70 \uac01\uac01\uc758 \ud589\uc740 \uc785\ub825 \ubca1\ud130\ub97c \uc758\ubbf8\ud558\uace0, y \ub294\ud6c8\ub828 \ub370\uc774\ud130 \uc14b\uc758 \ucd9c\ub825\uac12\uc73c\ub85c N-vector\ub97c \uac16\ub294\ub2e4. \uc704\uc758 \uc2dd\uc744 {\\beta}\uc5d0 \ub300\ud574\uc11c \ubbf8\ubd84\uc744 \ud574\ubcf4\uba74 \uc6b0\ub9ac\ub294 \uc544\ub798\uc640 \uac19\uc740 normal equations\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4. $ \\(\\mathbf{X}^T(\\mathbf{y}-\\mathbf{X}{\\beta})=0\\) $ \ub9cc\uc57d \\(\\mathbf{X}^T\\mathbf{X}\\) \uac00 nonsingular \uc989 \\(det(\\mathbf{X}^T\\mathbf{X})=0\\) \uc774\ub77c\uba74 \uc720\ub2c8\ud06c\ud55c \uac12\uc744 \uac16\ub294 \ud574\ub294 \\({\\hat{\\beta}}=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\) \uc774\uba70, i\ubc88\uc9f8 \uc785\ub825\uac12 \\({x_{i}}\\) \uc758 fitted value\ub294 \\(\\hat{y_{i}}=\\hat{y}(x_{i})=x_{i}^T\\hat{\\beta}\\) \uac00 \ub41c\ub2e4. \uc804\uccb4 fitted surface\ub294 p\uac1c\uc758 \ud30c\ub77c\ubbf8\ud130 \\(\\hat{\\beta}\\) \ub85c \uc774\ub8e8\uc5b4\uc838\uc788\ub2e4. \uc9c1\uad00\uc801\uc73c\ub85c \ubcfc \ub54c \uc774\ub294 \uc6b0\ub9ac\uac00 \uc9c0\ub098\uce58\uac8c \ub9ce\uc740 \ub370\uc774\ud130 \uc14b\uc744 \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\uc544\ub3c4 \ud2b9\uc815 \ubaa8\ub378\uc5d0 fitting \uc2dc\ud0ac \uc218 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc900\ub2e4. Example \uc120\ud615 \ubaa8\ub378\uc744 \uc774\uc6a9\ud55c \uac04\ub2e8\ud55c \ubd84\ub958\uc758 \uc608\uc2dc\ub97c \uc0b4\ud3b4\ubcf4\uc790. \uc774\ub294 \uad50\uc7ac figure 2.1\uc5d0 \uc218\ub85d\ub41c \uc0b0\uc810\ub3c4\uc774\ub2e4. \uc774\ub294 \\(X_1\\) \uacfc \\(X_2\\) \uc30d\uc758 \uc0b0\uc810\ub3c4\ub97c \ub098\ud0c0\ub0b4\uba70, \ucd9c\ub825\ub418\ub294 class variable G\ub294 BLUE or ORANGE\ub85c \ub450 \uac00\uc9c0\uac00 \uc788\ub2e4. \uac01 \ud074\ub798\uc2a4 \ubcc4\ub85c\ub294 100\uac1c\uc758 \ub370\uc774\ud130\uac00 \uc874\uc7ac\ud558\uba70, \uc120\ud615\ubaa8\ub378\uc744 \uc801\uc6a9\ud558\uc5ec \uc774 \ub370\uc774\ud130\uc5d0 fitting\uc744 \ud55c\ub2e4. \ud68c\uadc0\ubd84\uc11d\uc744 \uc704\ud574 response Y\ub97c BLUE\uba74 0\uc73c\ub85c, ORANGE\uba74 1\ub85c \ud45c\uae30\ud558\uba70, fitted values\uc778 \\(\\hat{Y}\\) \ub294 \uc608\uc2dc\uc5d0 \uc81c\uc2dc\ub41c \uaddc\uce59\uc5d0 \ub530\ub77c\uc11c \\(\\hat{Y}\\) \uac00 0.5\ubcf4\ub2e4 \ud06c\ub2e4\uba74 fitted class variable\uc778 \\(\\hat{G}\\) \ub97c ORANGE(=1)\ub85c, \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74 BLUE(=0)\ub85c \ubc18\ud658\ud55c\ub2e4. \\(\\mathbf{R}^2\\) \uacf5\uac04\uc5d0\uc11c\uc758 \uc810\ub4e4\uc758 \uc14b\uc740 \uc704\uc5d0\uc11c \uc5b8\uae09\ud55c rule\uc5d0 \uc758\ud574 \ub450 \uac00\uc9c0\ub85c \ubd84\ub958\ub418\uba70, \uc774 \ub54c\uc758 \\({\\{x: x^T{\\hat{\\beta}}=0.5}\\}\\) \uc758 hyperplane\uc774 (\uc5ec\uae30\uc11c\ub294 1\ucc28\uc6d0\uc758 \uc9c1\uc120\uc744 \uc758\ubbf8\ud55c\ub2e4.) decision boundary \uac00 \ub41c\ub2e4. \uc774 \uacbd\uc6b0\uc5d0\uc11c \uc6b0\ub9ac\ub294 decision boundary \ub97c \uae30\uc900\uc73c\ub85c \uba87\uba87\uc758 \uc798\ubabb \ubd84\ub958\ub418\uc5b4\uc788\ub294 \uc810\ub4e4\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc774\ub7ec\ud55c \uc624\ubd84\ub958\uac00 \uc77c\uc5b4\ub098\ub294 \uac83\uc774 \ubaa8\ub378\uc774 \ub108\ubb34 \uc5c4\uaca9\ud574\uc11c \uc77c\uae4c ? \uc544\ub2c8\uba74 \uc774\ub7ec\ud55c \uba87\uba87 error\ub4e4\uc740 \ubd88\uac00\ud53c\ud55c \uc624\ub958\uc778 \uac83\uc77c\uae4c? \uc6b0\ub9ac\uac00 \uac00\uc9c0\uace0 \uc788\ub294 training data\ub97c \uc5b4\ub5bb\uac8c \uc5bb\uac8c \ub418\uc5c8\ub294\uc9c0\ub97c \uc5b8\uae09\ud558\uc9c0 \uc54a\uc740 \uc0c1\ud0dc\uc5d0\uc11c \ub450 \uac00\uc9c0 \uac00\ub2a5\ud55c \uc2dc\ub098\ub9ac\uc624\ub97c \uc0dd\uac01\ud574\ubcf4\uc790.(data\uac00 \uc5b4\ub5bb\uac8c gathering\ub418\uc5c8\ub294\uc9c0 \uc54c \uc218 \uc788\ub2e4\uba74 \ud310\ub2e8\uc740 \uc870\uae08 \ub2ec\ub77c\uc9c4\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud558\ub294 \uac83\uc77c\uae4c? \ucd94\ud6c4\uc5d0 \uc54c\uc544\ubcf4\uc790.) Scenario 1 : \uac01 \ubc94\uc8fc\uc5d0 \uc18d\ud558\ub294 training data\ub294 \uc11c\ub85c \ub3c5\ub9bd\uc774\uba70 \ub2e4\ub978 \ubaa8\ud3c9\uade0\uc744 \uac16\ub294 \uc774\ubcc0\ub7c9 \uac00\uc6b0\uc2dc\uc548 \ubd84\ud3ec\uc5d0\uc11c \uc0dd\uc131\ub418\uc5c8\ub2e4. Scenario 2: \uac01 \ubc94\uc8fc\uc5d0 \uc18d\ud558\ub294 training data\ub294 \uac01 \ubd84\ud3ec\uc758 \ubaa8\ud3c9\uade0\uc774 \uac00\uc6b0\uc2dc\uc548 \ubd84\ud3ec\ub97c \ub530\ub974\ub294 10\uac1c\uc758 low-variance \uac00\uc6b0\uc2dc\uc548 \ubd84\ud3ec\uc758 \ud63c\ud569\ubaa8\ub378\uc5d0\uc11c \uc0dd\uc131\ub418\uc5c8\ub2e4. \uac00\uc6b0\uc2dc\uc548 \ubd84\ud3ec\uc758 \ud63c\ud569\uc740 generative model\ub85c \uc798 \uc124\uba85\ud560 \uc218 \uc788\ub2e4. \uc5ec\uae30\uc11c generative model\uc774\ub77c \ud568\uc740, \uc8fc\uc5b4\uc9c4 \uc5ec\ub7ec\uac1c\uc758 \uac00\uc6b0\uc2dc\uc548 \ubd84\ud3ec \uc911\uc5d0\uc11c(\uc704\uc758 \uc2dc\ub098\ub9ac\uc6242\uc5d0\uc11c\ub294 10\uac1c\uc758 \uac00\uc6b0\uc2dc\uc548\ubd84\ud3ec.) \uc5b4\ub5a4 \ubd84\ud3ec\ub97c \uc0ac\uc6a9\ud560\uc9c0\ub97c \ud0dd\ud558\ub294 discrete\ud55c \ubcc0\uc218\ub97c \uc0dd\uc131\ud55c \ub4a4 \uc815\ud574\uc9c4 density\ub85c\ubd80\ud130 \uad00\uce21\uce58\ub97c \uc0dd\uc131\ud558\ub294 \ubaa8\ub378\uc774\ub77c\uace0 \uc124\uba85\ud560 \uc218 \uc788\ub2e4. \ud55c \ud074\ub798\uc2a4\ub9c8\ub2e4 \ud558\ub098\uc758 \uac00\uc6b0\uc2dc\uc548 \ubd84\ud3ec\ub97c \uc801\uc6a9\ud558\ub294 \uac83\uc740(=\ud55c \ud074\ub798\uc2a4\uc5d0 \uc18d\ud558\ub294 \uad00\uce21\uce58\uac00 \ud558\ub098\uc758 Normal\uc744 \ub530\ub974\ub294 \ud655\ub960\ubcc0\uc218\uc5d0\uc11c \ud30c\uc0dd\ub41c \uac83\uc774\ub77c\uba74,) \ub4a4\uc5d0 4\uc7a5\uc5d0\uc11c \ub2e4\uc2dc \ubc30\uc6b0\uaca0\uc9c0\ub9cc 1\ucc28\uc6d0\uc758 decision boundary\uac00 \ucd5c\uc120\uc758 \ubc29\ubc95\uc774\uba70, \uc774\ub54c \uc6b0\ub9ac\uac00 \uc5bb\uc740 \ucd94\uc815\uce58\uac00 \ucd5c\uc801\uc758 \uacb0\uacfc\uc77c \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub7ec\ud55c \uc120\ud615\uc801\uc778 decision boundary \ucf00\uc774\uc2a4\uc5d0\uc11c \ub450 \ubc94\uc8fc \uac04 \uc601\uc5ed\uc758 overlap\uc740 \ubd88\uac00\ud53c\ud558\uba70 \uc608\uce21\ud574\uc57c\ud558\ub294 \ubbf8\ub798\uc758 \ub370\uc774\ud130 \ub610\ud55c \uc774\ub7ec\ud55c overlap\uc758 \ub2aa\uc5d0\uc11c \uc790\uc720\ub85c\uc6b8 \uc218 \uc5c6\uc744 \uac83\uc774\ub2e4. \ub9cc\uc57d \uc5ec\ub7ec\uac1c\uc758 \ubc94\uc8fc\uac00 \ube7d\ube7d\ud558\uac8c \uc11c\ub85c \uac01\uac01\uc758 \ub2e4\ub978 \uc815\uaddc\ubd84\ud3ec\ub85c\ubd80\ud130 \ub098\uc628 \ud655\ub960\ubcc0\uc218\uc5d0\uc11c \ud30c\uc0dd\ub41c \uac83\uc774\ub77c\uba74, \uc774\uc57c\uae30\ub294 \uc870\uae08 \ub2ec\ub77c\uc9c4\ub2e4. \uc774 \ub54c\ub294 \uc9c1\uc120\uc73c\ub85c \uadf8\uc740 decision boundary\uac00 \ucd5c\uc801\uc758 \uc758\uc0ac\uacb0\uc815\uc744 \ub0b4\ub9ac\ub294\ub370\uc5d0 \uc801\uc808\ud558\uc9c0 \uc54a\uc744 \uac00\ub2a5\uc131\uc774 \ub192\uc73c\uba70, \uc2e4\uc81c\ub85c\ub3c4 \uadf8\ub807\ub2e4. \uc774\ub7f0 \ucf00\uc774\uc2a4\uc5d0\uc11c\uc758 \ucd5c\uc801\uc758 decision boundary\ub294 \uc704\uc640 \ub2e4\ub974\uac8c \ub354 nonlinear \ud560 \uac83\uc774\uba70, disjoint\ud558\uac8c \ud074\ub798\uc2a4 \uac04\uc758 \uad6c\ubd84\uc744 \uc798 \ud574\uc904 \uac83\uc774\uc9c0\ub9cc \uc774\ub97c \uc5bb\uae30\ub780 \ub9e4\uc6b0\ub9e4\uc6b0 \uc5b4\ub824\uc6b4 \uc77c\uc774\ub2e4.(\uc124\ub839 \ub108\ubb34\ub098\ub3c4 \uc798 \ubd84\ub958\ud558\uace0 \uc2f6\uc5b4\uc11c \uaf2c\ubd88\uaf2c\ubd88 \uad6c\uc5ed\uc744 \ub098\ub208\ub2e4 \ud558\ub354\ub77c\ub3c4 \uacfc\uc801\ud569\uc758 \ubb38\uc81c \ub610\ud55c \ubc1c\uc0dd\ud560 \uc218 \uc788\uaca0\uc9c0..) \uc774 \ub0b4\uc6a9\uc740 \ucd94\ud6c4\uc5d0 \uc870\uae08 \ub354 \ub2e4\ub904\ubcf4\ub3c4\ub85d \ud558\uace0 \uc704\uc5d0\uc11c \uc5b8\uae09\ud55c \ub450\ubc88\uc9f8 \uc2dc\ub098\ub9ac\uc624\uc5d0 \uc870\uae08 \ub354 \uc801\ud569\ud55c, \uc120\ud615\ud68c\uadc0\uc640\ub294 N\uadf9\uacfc S\uadf9 \ub9c8\ub0e5 \ubc18\ub300\uc120\uc0c1\uc5d0 \uc788\ub294 \ub2e4\ub978 \ubd84\ub958\ubc29\ubc95\uc744 \ubcf4\ub3c4\ub85d \ud558\uc790.","title":"Least Squares"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#nearest-neighbor-methods","text":"Nearest-Neighnor Methods\ub294 \uc785\ub825\uacf5\uac04\uc5d0\uc11c\uc758 x\uc640 \uac00\uc7a5 \uac00\uae4c\uc6b4 \uac70\ub9ac\uc5d0 \uc788\ub294 training set T \uc548\uc5d0 \uc788\ub294 \uad00\uce21\uce58\ub4e4\ub85c \\(\\hat{Y}\\) \ub97c \ud615\uc131\ud558\ub294 \ubc29\ubc95\uc774\ub2e4. \ud2b9\ud788 k\uac1c\uc758 \ucd5c\uadfc\uc811 \uc774\uc6c3\uc73c\ub85c fitting\ud55c \\(\\hat{Y}\\) \ub294 \uc544\ub798\uc640 \uac19\uc774 \uc815\uc758\ud560 \uc218 \uc788\ub2e4. $ \\(\\hat{Y}(x)=\\frac{1}{k}\\sum\\nolimits_{x_i \\in N_{k}(x)}{y_{i}}\\) $ \uc5ec\uae30\uc11c \\(N_{k}(x)\\) \ub780 training sample\uc548\uc5d0\uc11c \uc810 \\(x_{i}\\) \uacfc \uc81c\uc77c \uac00\uae4c\uc6b4 k\uac1c\uc758 \uad00\uce21\uce58\ub97c \uc758\ubbf8\ud55c\ub2e4. \uac00\uc7a5 nearest\ud55c \uac70\ub9ac\ub294 \uc5b4\ub5bb\uac8c \uc815\uc758\ub418\ub294\uac00? \uc774 \ubc29\ubc95\uc5d0\uc11c \uc6b0\ub9ac\ub294 Euclidean distance\ub97c \uac00\uc815\ud558\uba70, \uc774\ub294 x\uc5d0\uc11c \uc720\ud074\ub9ac\ub514\uc548 \uac70\ub9ac\ub85c \uc81c\uc77c \uac00\uae4c\uc6b4 k\uac1c\uc758 \uad00\uce21\uce58\ub4e4\uc744 \ucc3e\uace0, \uc774\ub4e4\uc758 \uc885\uc18d\ubcc0\uc218 (Response)\uc758 \ud3c9\uade0\uc744 \uad6c\ud558\ub294 \uac83\uc774\ub2e4. \uc55e\uc5d0\uc11c \uc5b8\uae09\ud55c binary classification\uc758 \uc608\uc81c\ub97c \uc774\ubc88\uc5d0\ub294 Nearest-Neighbor Methods\ub97c \uc774\uc6a9\ud558\uc5ec \uc811\uadfc\ud574\ubcf4\uace0\uc790 \ud55c\ub2e4. \ubb38\uc81c\uc5d0\uc11c k\ub294 15\ub85c \uc9c0\uc815\ud558\uc600\uc73c\uba70, binary\ub85c coded\ub41c \ucd9c\ub825\ubcc0\uc218\uc758 15\uac1c \ud3c9\uade0\uc73c\ub85c model fitting\uc744 \ud588\ub2e4. \uc704\uc758 \ubc29\ubc95\uc740 \uad00\uce21\ub41c \uac12\uc774 (0,1)\ub85c binary class\uc758 \ub450 \uac00\uc9c0 \uacb0\uacfc\uac12\uc73c\ub85c \ub098\uc624\uae30 \ub54c\ubb38\uc5d0, \ucd5c\uadfc\uc811 \uc774\uc6c3\uc758 \ubc29\ubc95\uc744 \ub530\ub974\uba74 \uc6b0\ub9ac\uac00 \uc5bb\uac8c\ub418\ub294 \\(\\hat{Y}\\) \ub294 \\(x_{i}\\) \uc8fc\ubcc0\uc758 15\uac1c\uc758 \uad00\uce21\uce58\uc758 \uc885\uc18d\ubcc0\uc218\ub4e4 \uc548\uc5d0\uc11c Orange\uc758 \ube44\uc728\uc744 \ub098\ud0c0\ub0bc \uac83\uc774\ub2e4. \uc55e\uc11c least squares \ubc29\ubc95\uc744 \uc4f4 \uac83\uc5d0 \ube44\ud574 \uc704\uc758 \ubc29\ubc95\uc740 \uc624\ubd84\ub958\ud55c \ud074\ub798\uc2a4\ub4e4\uc774 \ub2e4\uc18c \uc801\uc5b4 \ubcf4\uc778\ub2e4. \uadf8\ub9bc\ub9cc\uc73c\ub85c\ub3c4 \ud655\uc778\uc774 \uac00\ub2a5\ud558\ub4ef\uc774 \uc774 \uc608\uc81c\uc5d0 \ud55c\ud574\uc11c\ub294 \ucd5c\uc18c\uc81c\uacf1\ubc95\ubcf4\ub2e4\ub294 k=15\uc758 \ucd5c\uadfc\uc811\uc774\uc6c3\ubc29\ubc95\uc744 \uc801\uc6a9\ud558\ub294 \uac83\uc774 classification\uc5d0 \ub354 \uc88b\uc740 \ubaa8\ub378\uc784\uc744 \ubcf4\uc5ec\uc900\ub2e4. \ub610 \ub2e4\ub978 \uadf8\ub9bc\uc744 \ubcf4\uc790. \uc704\uc758 \uadf8\ub9bc k\ub97c 1\ub85c \uc124\uc815\ud558\uace0 \ucd5c\uadfc\uc811\uc774\uc6c3 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud55c \ubaa8\uc2b5\uc774\ub2e4. \uc77c\uc77c\uc774 \ud655\uc778\ud574\ubcf4\uba74 \ubaa8\ub4e0 \ub370\uc774\ud130\ub4e4\uc774 \ud55c\uac1c\ub3c4 \ube60\uc9d0\uc5c6\uc774 \ubc14\ub974\uac8c \ubd84\ub958\ub418\uc5b4\uc788\uc74c\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc6b0\ub9ac\ub294 \uc774\ub97c \ud1b5\ud574 \uba87\uac00\uc9c0 \uc0ac\uc2e4\uc744 \uc54c\uc558\ub2e4. k\ucd5c\uadfc\uc811\uc774\uc6c3 \ubc29\ubc95\uc5d0\uc11c\ub294 \ud6c8\ub828\ub370\uc774\ud130\ub4e4\uc758 \uc5d0\ub7ec\uac00 \ub300\ub7b5 k\uc5d0 \ub300\ud55c \uc99d\uac00\ud568\uc218\ub77c\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \ub610\ud55c k\uac00 1\uc774\ub77c\uba74 error\ub294 \ud56d\uc0c1 0\uc774\ub2e4. k\ucd5c\uadfc\uc811\uc774\uc6c3\uc740 p\uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uac16\ub294 \ucd5c\uc18c\uc81c\uacf1\ubc95\uacfc \ub2ec\ub9ac \uc624\ub85c\uc9c0 k\ud55c\uac1c\ub9cc\uc758 \ubaa8\uc218\ub97c \uac16\uace0 \uc774\ub97c \ud1b5\ud574 \ubaa8\ub378\uc774 \uacb0\uc815\ub41c\ub2e4. \uc6b0\ub9ac\ub294 \ud6a8\uacfc\uc801\uc778 k\uc758 \uac12\uc740 \ub300\uac1c p\ubcf4\ub2e4\ub294 \ud070 \\(N/k\\) \uc774\uba70, \uc774\ub294 k\uac12\uc774 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \uac10\uc18c\ud568\uc744 \uc54c \uc218 \uc788\ub2e4. \ub9cc\uc57d \uc774\uc6c3\ub4e4\uac04\uc758 \uc601\uc5ed\uc774 \uacb9\uce58\uc9c0 \uc54a\ub294\ub2e4\uba74 \uc6b0\ub9ac\ub294 \uac01 region\ub9c8\ub2e4 \ud558\ub098\uc758 \ubaa8\uc218\ub97c fitting\ud560 \uac83\uc774\uae30 \ub54c\ubb38\uc774\ub2e4. \uc774\uc640 \uac19\uc740 \ud2b9\uc9d5\uc744 \uac16\ub294 \ucd5c\uadfc\uc811\uc774\uc6c3\uc758 \ubc29\ubc95\uc740 \uc6b0\ub9ac\uac00 \uc55e\uc11c \uc81c\uc2dc\ud55c \ub450\uac1c\uc758 \uc2dc\ub098\ub9ac\uc624 \uc0c1\ud669\uc5d0\uc11c \ud2b9\ud788 \uc2dc\ub098\ub9ac\uc6242\uc5d0 \ub354 \uc801\ud569\ud55c \ubd84\ub958\uae30\ubc95\uc774\ub2e4. \uc815\uaddc\ubd84\ud3ec\ub97c \ub530\ub974\ub294 \uc790\ub8cc\uc5d0\uc11c decision boundary\uac00 \ubd88\ud544\uc694\ud558\uac8c noisy\uac00 \ub9ce\uc558\uae30 \ub54c\ubb38\uc774\ub2e4. From Least Squares to Nearest Neighbors \ucd5c\uc18c\uc81c\uacf1\ubc95\uc740 \ubaa8\ub378 setting\uc774 \uc544\uc8fc smooth\ud558\uace0, fitting\uc5d0 \uc788\uc5b4\uc11c \uc548\uc815\uc801\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub294 \uc120\ud615 decision boudary\uac00 \ud0c0\ub2f9\ud558\ub2e4\ub294 \uc544\uc8fc \ubb34\uac70\uc6b4 \uac00\uc815\uc744 \ub9cc\uc871\ud574\uc57c\ud55c\ub2e4\ub294 \ub2e8\uc810\uc774 \uc788\ub2e4. \ubc18\ub300\ub85c \ucd5c\uadfc\uc811\uc774\uc6c3\ubc29\ubc95\uc740 \uac00\uc815\uc758 \uc81c\uc57d\uc740 \uac70\uc758 \uc5c6\uace0 \uc5b4\ub5a4 \uc0c1\ud669\uc5d0\uc11c\ub4e0 \uc801\uc808\ud558\uac8c k\ub97c \uc870\uc815\ud558\uc5ec \uc0ac\uc6a9\ud560 \uc218\uc788\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub294 decision boundary\uac00 \ub9e4\uc6b0 \ubcf5\uc7a1\ud558\uac8c \ube44\uc120\ud615\uc801\uc778 form\uc744 \ub744\uae30 \ub54c\ubb38\uc5d0, \uc801\uc740 \ud3b8\ud5a5\uc744 \uac16\uc9c0\ub9cc, \ub192\uc740 \ubd84\uc0b0\uc744 \uc9c0\ub2cc\ub2e4. variance-bias trade off \uc758 \ubb38\uc81c\ub85c\uc368, \ub450\uac00\uc9c0\uc758 \ubc29\ubc95\uc740 \uc21c\uc11c\ub300\ub85c (low variance-high bias), (high variance-low bias) \ub97c \uac16\ub294\ub2e4. \uac01\uac01\uc758 \ubc29\ubc95\uc740 \uac01\uc790 \uc801\ud569\ud55c \uc0c1\ud669\uc774 \uc788\uc73c\uba70, \uc2dc\ub098\ub9ac\uc6241\uc5d0\ub294 \ucd5c\uc18c\uc81c\uacf1\ubc95\uc774, \uc2dc\ub098\ub9ac\uc6242\uc5d0\uc11c\ub294 \ucd5c\uadfc\uc811\uc774\uc6c3\ubc95\uc774 \uc801\uc808\ud560 \uac83\uc73c\ub85c \ubcf4\uc5ec\uc9c4\ub2e4. \uc624\ub298\ub0a0 \uc774 \ub450\uac00\uc9c0 \ubc29\ubc95\uc740 \uac00\uc7a5 \ub300\uc911\uc801\uc73c\ub85c \uc4f0\uc774\ub294 \ub300\ud45c\uc801\uc778 \ubc29\ubc95\uc774\ub2e4. \uc77c\ub840\ub85c k=1 \ucd5c\uadfc\uc811\uc774\uc6c3\ubc29\ubc95\uc740 \uc2dc\uc7a5\uc758 \ub300\ubd80\ubd84\uc5d0\uc11c \uc800\ucc28\uc6d0\uc758 \ubb38\uc81c\ub97c \ud574\uacb0\ud558\ub294\ub370 \uc0ac\uc6a9\ub418\ub294 \ubc29\ubc95\uc774\ub2e4.","title":"Nearest-Neighbor Methods"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#statistical-decision-thoery","text":"\uc774\ubc88 \ub2e8\uc6d0\uc5d0\uc11c\ub294 \uc774 \uc804\uc5d0 \uc5b8\uae09\ud588\uc5c8\ub358 \ub450\uac00\uc9c0 \ubaa8\ub378 \ub4f1\uc744 \ud3ec\ud568\ud55c \uba87\uba87\uc758 \ubaa8\ub378\uc744 \ubc1c\uc804\uc2dc\ud0a4\ub294\ub370 \uc778\uc0ac\uc774\ud2b8\ub97c \uc81c\uacf5\ud574\uc904 \uba87\uac00\uc9c0 \uc774\ub860 \ub4f1\uc744 \ubc1c\uc804\uc2dc\ucf1c\ubcf4\uace0\uc790 \ud55c\ub2e4. \uba3c\uc800 quantative\ud55c output\ub4e4\uc5d0 \ub300\ud558\uc5ec \uc0dd\uac01\ud574\ubcfc \uac83\uc774\uba70, random variables(\ud655\ub960\ubcc0\uc218)\uacfc probability space(\ud655\ub960\uacf5\uac04)\uc5d0 \ub300\ud574\uc11c \ub17c\uc758\ub97c \ud560 \uac83\uc774\ub2e4. \uba3c\uc800 \\({X} \\in \\mathbf{R}^p\\) \ub294 \uc2e4\uc218\uc758 \uc785\ub825\ubca1\ud130\uc774\uba70, \\({Y} \\in \\mathbf{R}\\) \ub294 \ucd9c\ub825\ub418\ub294 \ud655\ub960\ubcc0\uc218\uc774\ub2e4. \ub610\ud55c \uc774\ub458\uc758 \uacb0\ud569\ubd84\ud3ec\ub294 \\(Pr(X,Y)\\) \ub85c \ud45c\uae30\ud55c\ub2e4. \uc6b0\ub9ac\ub294 \\(Y\\) \ub97c \uc608\uce21\ud558\uae30 \uc704\ud574 \uc8fc\uc5b4\uc9c4 \uc785\ub825\ubcc0\uc218 \\(X\\) \ub97c \uc774\uc6a9\ud558\uc5ec \\(f(X)\\) \ub77c\ub294 \ud568\uc218\ub97c \ucc3e\uace0\uc790\ud55c\ub2e4. \uc774 \uc774\ub860\uc740 \uc608\uce21\uc5d0 \uc788\uc5b4\uc11c error\ub97c penalize\ud558\ub294 loss function \\(L(Y,f(X)\\) \ub97c \ud544\uc694\ub85c\ud55c\ub2e4. \uc774\uc5d0 \ub300\ud55c \uc608\uc2dc\ub85c \uc6b0\ub9ac\uc5d0\uac8c \uac00\uc7a5 \uc798 \uc54c\ub824\uc9c4 squared error loss \uac00 \uc788\ub2e4. (= \\(L(Y,f(X)))= (Y-f(X))^2\\) ). \uc774\ub294 \uc6b0\ub9ac\uac00 f\ub97c \ucc3e\uc744 \uc218 \uc788\uac8c\ud558\uba70, \uc2dd\uc744 \uc804\uac1c\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(EPE(f) = E(Y-f(X))^2 = \\int[y-f(x)]^2f(x,y)dxdy\\) $ \uc774\ub97c X\uc5d0 \ub300\ud574 \uc870\uac74\ubd80\uc2dd\uc73c\ub85c \ud45c\ud604\ud558\uba74 $ \\(EPE(f) = {E}_{X}{E}_{Y|X}([Y-f(X)]^2|X)\\) $ \uac00 \ub41c\ub2e4. \uc704\uc758 \uc2dd\uc740 \uc870\uac74\ubd80 \ud655\ub960\uc744 \ud1b5\ud574 \uad6c\ud560 \uc218 \uc788\uc73c\uba70, \uc99d\uba85\uc774 \ub2e4\uc18c \uac04\ub2e8\ud558\ub2c8 \uc0b4\uc9dd\ub9cc \uc0b4\ud3b4\ubcf4\uc790. \\[EPE(f) = \\int[y-f(x)]^2Pr(dx,dy)\\] \\[= \\int[y-f(x)]^2f(x,y)dxdy\\] \\[=\\int_x\\int_y[y-f(x)]^2f(x,y)dxdy\\] \\[=\\int_x\\int_y[y-f(x)]^2f(x)f(y|x)dxdy\\] \\[=\\int_x(\\int_y[y-f(x)]^2f(y|x)dy)f(x)dx\\] \\[=\\int_x(E_{Y|X}([Y-f(X)^2|X=x))f(x)dx\\] \\[= E_XE_{Y|X}([Y-f(X)]^2|X=x)\\] \uc5ec\uae30\uc11c EPE\ub97c pointwise\ud558\uac8c \ub9cc\uc871\uc2dc\ud0a4\ub294 f(x)\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\uace0 \uadf8 \ubc11\uc5d0 \uc774\uc5b4\uc9c0\ub294 \uc2dd\uc774 \uadf8\ub54c\uc758 \ud574( \\(f(X)\\) )\uc774\ub2e4. \\[f(x)=argmin_{c}E_{Y|X}([Y-c]^2|X=x)\\] $ \\(f(x)=E(Y|X=x)\\) $ \uc774 \ud574\ub294 \uc870\uac74\ubd80\ud3c9\uade0\uc744 \uc758\ubbf8\ud558\uba70 \ub610\ud55c regression function\uc73c\ub85c \uc798 \uc54c\ub824\uc838\uc788\ub2e4. \ub2e4\uc2dc\ub9d0\ud574 average squared error\uac00 \ucd5c\uc801\uc758 \uc608\uce21\ucc99\ub3c4\ub85c \uc5ec\uaca8\uc9c8 \ub54c, best prediction of \\(Y\\) at any point \\(X=x\\) \ub294 Y\uc758 X\uc5d0 \ub300\ud55c \uc870\uac74\ubd80 \ud3c9\uade0\uc774\ub77c\ub294 \uac83\uc774\ub2e4. \uc218\ub9ac\ud1b5\uacc4\ud559\uc801\uc778 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud574\uc11c \uc6b0\ub9ac\ub294 \uc704\uc5d0\uc11c \uc5b8\uae09\ud55c \uc870\uac74\ubd80\ud3c9\uade0\uc774 \ucd5c\uc18c\uc790\uc2b9\uc608\uce21\uc790\ub77c\ub294 \uac83\uc744 \ubcf4\uc77c \uc218 \uc788\ub2e4. \uc774\ub97c \uc218\uc2dd\uc73c\ub85c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\(For\\ any\\ function\\ of\\ X,\\ say\\ u(x),\\) \\[E[(Y-u(X))^2|X] \\geq E[(Y-E[Y|X])^2|X]\\] \\(i.e.,\\ E[Y|X] is\\ equal\\ to\\ argmin_{u(X)}E[(Y-u)^2|X].\\) \uc774\ub97c \uc99d\uba85\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \uc2dd\uc804\uac1c\ub97c \ud560 \uc218\uc788\ub2e4. \\[E[(Y-u(X))^2|X]=E[Y-\\textbf{E[Y|X]}+\\textbf{E[Y|X]}-u(X))^2|X]\\] \\[=E[{(Y-E[Y|X])+(E[Y|X]-u(X))}^2|X]$$ $$=E[(Y-E(Y|X])^2|X)]+E[(E[Y|X]-u(X))^2|X]\\] \\[+2E[(Y-E[Y|X])(E[Y|X]-u(X))|X]\\] \\[=E[(Y-E[Y|X])^2|X]+(E[Y|X]-u(X))^2\\] $ \\(\\because E[(Y-E[Y|X])(E[Y|X]-u(X))|X]=(E[Y|X]-u(X))E[Y-E[Y|X]|X]=0\\) $ \uc5ec\uae30\uc11c \ub9c8\uc9c0\ub9c9 cross term\uc774 0\uc774 \ub418\ub294 \uc774\uc720\ub294 \\(E[Y-E[Y|X]|X]\\) \uc5d0 \ud78c\ud2b8\uac00 \uc788\ub2e4. \\(E[Y|X]\\) \uc758 \uc870\uac74\ubd80\ud3c9\uade0\uc740 random variable\uc758 \ud615\ud0dc\uc774\uc9c0\ub9cc \uc774 \uc2dd\uc758 \uc55e\uc5d0 X\uc5d0 \ub300\ud55c \uc870\uac74\ubd80 \ud3c9\uade0\uc774 \ud55c\ubc88 \ub354 \uc50c\uc5ec\uc788\ub294 \uac83 \ub54c\ubb38\uc778\ub370, \uc774\ub7f4\uacbd\uc6b0\uc5d0 E[Y|X]\ub294 constant\uaf34\uc744 \uac16\ub294\ub2e4. (Given X=x\uc774\uae30 \ub54c\ubb38\uc5d0) \uace0\ub85c \uc2dd\uc804\uac1c\ub97c \ud558\uba74 \\(E[Y|X]-E[Y|X]\\) \uaf34\uc774\ubbc0\ub85c 0\uc774\ub41c\ub2e4. \ub610\ub294 \ub2e4\ub974\uac8c \uc0dd\uac01\ud558\uc5ec E[Y|X]\ub294 X\uc5d0\ub300\ud55c \ud568\uc218\ud615\ud0dc\ub85c \ub098\uc624\uae30\ub54c\ubb38\uc5d0 \uc774\ub97c g(X)\ub85c \uc815\uc758\ud558\uc5ec \ubb38\uc81c\ub97c \ud480 \uc218\ub3c4 \uc788\ub2e4. \uadf8\ub807\ub2e4\uba74 \uc2dd\uc740 \\(E[Y-g(X)|X]\\) \uc758 \ud615\ud0dc\uac00 \ub418\uba70 \uc774\ub97c \uc804\uac1c\ud558\uba74 \\(E[Y|X]-g(X)=0, \\because g(X)=E[Y|X]\\) \uac00 \ub418\uc5b4 \uc704\uc640 \uac19\uc740 \uc2dd\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4. \ubd80\ub4f1\ud638\ub97c \uc815\ub9ac\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ub418\uba70, \\[E[(Y-u(X))^2|X] \\geq E[(Y-E[Y|X])^2|X]\\] \uadf8\ub7ec\ubbc0\ub85c \uc784\uc758\uc758 \\(X=x\\) \uc810\uc5d0 \ub300\ud55c best prediction of \\(Y\\) \ub294 \uc870\uac74\ubd80 \ud3c9\uade0\uc774 \ub41c\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. k-NN methods\uc5d0\uc11c\ub294 \ud574\uc758 \ucd94\uc815\uce58\ub97c \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud55c\ub2e4. $ \\(\\hat{f}(x)=Ave(y_{i}|x_{i} \\in N_{k}(x))\\) $ \uc704 \uc2dd\uc5d0\uc11c\ub294 \ub450\uac00\uc9c0\uc758 approximation\uc774 \uc874\uc7ac\ud558\ub294\ub370, \uae30\ub313\uac12\uc740 \uc0d8\ud50c \ub370\uc774\ud130\ub4e4\uc758 \ud3c9\uade0\uc73c\ub85c \uadfc\uc0ac\ud55c\ub2e4\ub294 \uac83\uacfc, \ud55c \uc810\uc5d0\uc11c\uc758 conditioning\uc740 \ubaa9\ud45c\uc9c0\uc810\uacfc \uac00\uae4c\uc6b4 \uc5b4\ub5a4 \uc9c0\uc5ed\uc758 conditioning\uc73c\ub85c relax\ub41c\ub2e4\ub294 \uac83\uc774\ub2e4. \ub9cc\uc57d knn\uc5d0\uc11c \uc0d8\ud50c\uc758 \uc0ac\uc774\uc988\uac00 \uc544\uc8fc\ud06c\uace0, k\ub610\ud55c \ud06c\ub2e4\uba74, \ud3c9\uade0\uc740 \ub354 \uc548\uc815\uc801\uc73c\ub85c \ubcc0\ud560 \uac83\uc774\uace0, \uacb0\ud569\ud655\ub960\ubd84\ud3ec\uc758 mild regularity conditions\ud558\uc5d0\uc11c \\(f(x)\\) \uac00 \\(E(Y|X=x)\\) \ub85c \uadfc\uc0ac\ud55c\ub2e4 . \ud558\uc9c0\ub9cc \uc885\uc885 \uc6b0\ub9ac\uac00 \ub2e4\ub8e8\ub294 \ub370\uc774\ud130\ub4e4\uc758 \ud45c\ubcf8\uc758 \ud06c\uae30\ub294 \ud56d\uc0c1 \ud06c\uc9c0\uac00 \uc54a\uc73c\uba70, \ucc28\uc6d0\uc774 \ucee4\uc9c0\uba74 \uc0c8\ub86d\uac8c \ubc1c\uc0dd\ud558\ub294 \ubb38\uc81c\ub4e4\ub3c4 \uc874\uc7ac\ud55c\ub2e4. \uc774\ub294 \ucd94\ud6c4 \ub4b7\uc7a5\uc5d0\uc11c \ub2e4\ub904\ubcf4\ub3c4\ub85d \ud558\uc790. \uc774\uc804 \ub2e8\uacc4\uc5d0\uc11c OLS estimator\uac00 linearity \uac00\uc815 \ud558\uc5d0\uc11c \\(E[Y|X=x]\\) \uc5d0 \ub300\ud55c consistent estimator\ub77c\ub294 \uac83\uc744 \ubcf4\uc600\ub2e4. KNN method\uc5d0 \ub300\ud574\uc11c\ub3c4 \uc774\uac00 \uc131\ub9bd\ud558\ub294\uc9c0 \uc99d\uba85\uc744 \ud574\ubcf4\uc790. mild regularity conditions\uc774 \ud0c0\ub2f9\ud558\ub2e4 \uac00\uc815\uc744 \ud558\uace0 \\(k,N \\rightarrow \\infty,\\ \\frac{k}{N} \\rightarrow 0\\) \uc774\ub77c\uba74, $ \\(avg(Y|X \\in N_{k}(x)) \\xrightarrow{P} E[Y|X=x]\\) $ \uc784\uc744 Empirical explanation\uc744 \ud1b5\ud574 \uc99d\uba85\ud574\ubcf4\uc790. KNN \ubc29\ubc95\uc744 \ud480\uc5b4\ud5e4\uccd0\uc4f0\uba74 \uc544\ub798\uc640 \uac19\uc740 \uc810\ub4e4\uc774 \ubaa8\uc5ec\uc788\ub294 \uac83\uc73c\ub85c \uc0dd\uac01\ud560 \uc218 \uc788\ub2e4. $ \\((x+\\Delta_1,y_1),\\ (x+\\Delta_2,y_2),\\ (x+\\Delta_{k-1},y_{k-1}),\\ (x+\\Delta_{k},y_k)\\) $ \ub610\ud55c \uc774 \uc810\ub4e4\uc5d0 \ub300\ud574\uc11c \uc544\ub798\uc640 \uac19\uc740 \uc2dd\uc744 \ub9cc\uc871\ud55c\ub2e4. $ \\((||x-a|| \\geq ||max_{j}(\\Delta_{j}))||, a\\in N_{k}(x)^c, j=1,...,k)\\) $ \uc5ec\uae30\uc11c \\(avg(Y|X \\in N_k(x)) \\equiv \\frac{y_1+..+y_{k}}{k} \\xrightarrow{P} E[Y]\\) \uc774 \uc2dd\uc774 \uc57d\ub300\uc218\uc758 \ubc95\uce59\uc5d0 \uc758\ud574 equivalent\ud558\ub2e4\uace0 \ubcfc \uc218 \uc788\uc744\uae4c? \uc544\uc27d\uac8c\ub3c4 \uadf8\ub807\uc9c0 \uc54a\ub2e4. \uc774\ub7ec\ud55c \uc2dd \ub3c4\ucd9c\uc758 \uacfc\uc815\uc740 \\(\\frac{k}{N} \\rightarrow \\infty\\) \ub77c\ub294 \uac00\uc815\uc744 \ubb34\uc2dc\ud574\uc11c \ub098\uc624\ub294 \uacb0\uacfc\uc774\ub2e4. \ub9cc\uc57d \\(k \\rightarrow \\infty\\) \ub77c\ub294 \uac00\uc815\ub9cc \uc788\uace0 data set\uc758 \uc0ac\uc774\uc988\uc5d0 \ub300\ud55c \uac00\uc815\uc774 \uc5c6\ub2e4\uba74 \uc704\uc758 \uc2dd\uc740 \ub9de\uc744\uc9c0\ub3c4 \ubaa8\ub978\ub2e4. \uc774\ub807\uac8c \uc0dd\uac01\ud574\ubcf4\uc790. \uc6b0\ub9ac\ub294 x\uc5d0 \ub300\ud55c \uc870\uac74\ubd80 \ud3c9\uade0 Y\ub97c \uad6c\ud558\ub294 \uac83\uc778\ub370, x\uc5d0 \uc885\uc18d\uc801\uc774\uac8c \ub300\uc751\ud558\ub294 Y\uc758 \uc810\ub4e4 k\uac1c\ub97c \uc0dd\uac01\ud574\ubcfc \uc218 \uc788\ub2e4. \uc5ec\uae30\uc11c k\uac00 \ubb34\ud55c\ub300\ub85c \uac04\ub2e4\uba74 \uc6b0\ub9ac\ub294 \uac70\uc758 \ubaa8\ub4e0 x\uc758 \uc810\ub4e4\uc5d0 \ub300\uc751\ud558\ub294 Y\ub97c \uace0\ub824\ud558\uace0 \uc774\ub7ec\ud55c y\ub4e4\uc758 x\uc5d0 \ub300\ud55c \uc870\uac74\ubd80\ud3c9\uade0\uc744 \uad6c\ud558\ub294 \uac83\uc778\ub370, \ubaa8\ub4e0 x\uc5d0 \ub300\ud558\uc5ec Y\ub97c \uad6c\ud588\uae30 \ub54c\ubb38\uc5d0 \uadf8\ub0e5 Y\uc5d0 \ub300\ud55c \ud3c9\uade0\uc744 \uad6c\ud558\ub294 \uac83\uacfc \ud655\ub960\uc801\uc73c\ub85c \uc77c\uce58\ud560 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc k\uc758 \uc0ac\uc774\uc988\uc5d0 \ube44\ud574 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c data set\uc758 \uc0ac\uc774\uc988\uac00 \ucee4\uc9c4\ub2e4\uba74 \uc774\ub294 data set\uc5d0\uc11c k\uac1c\uc758 \uad00\uce21\uce58\uac00 \ucc28\uc9c0\ud558\ub294 \uacf5\uac04\uc744 \ubaa8\ub4e0 x\uc5d0 \ub300\ud55c y\uc758 \ub300\uc751\uac12\uc73c\ub85c \uc0dd\uac01\ud560 \uc218 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \uc774\ub54c\ub294 X\uc5d0 \ub300\ud55c Y\uc758 \uc870\uac74\ubd80\ud3c9\uade0\uc73c\ub85c \ubcf4\ub294 \uac83\uc774 \ud569\ub2f9\ud560 \uac83\uc774\ub2e4. \uc774\ub97c \uac04\ub2e8\ud55c \uc2dc\ubbac\ub808\uc774\uc158\uc744 \ud1b5\ud574 \ud655\uc778\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\ub2e4. library(ggplot2) knn <- function(data,k,x){ od=order(abs(data[,2]-x)) new.data <- data[od,] Nk <- new.data[1:k,] Nk.range <- diff(range(Nk[,2])) Nk.avg <- mean(Nk[,1]) res <- c(Nk.range,Nk.avg) return(res) } L <- c(10,50,100,200,500,1000,2000) ; A <- chol(matrix(c(1,0.7,0.7,1),nrow=2)) y.bar <- matrix(rep(0,100*length(L)),nrow=100) range.bar <- matrix(rep(0,100*length(L)),nrow=100) for(l in 1:length(L)){ k <- L[l] for(i in 1:100){ temp.y <- c() temp.r <- c() for(j in 1:30){ X1 <- rnorm(k^2,0,1) Y1 <- rnorm(k^2,0,1) YX <- t(A)%*%rbind(Y1,X1) KNN <-knn(t(YX),k,0.5) C <- mean(rnorm(k,0.35,sqrt(.51))) temp.y <- append(temp.y,abs(KNN[2]-C)) temp.r <- append(temp.r,KNN[1]) } y.bar[i,l] <- mean(temp.y) range.bar[i,l] <- mean(temp.r) } } boxplot(y.bar,names=L,main='error') frame.y.bar <- stack(as.data.frame(y.bar)) frame.y.bar[,2] <- factor(frame.y.bar[,2],label=L) range(y.bar[,ncol(y.bar)]) ## [1] 0.01207920 0.02312542 p <- ggplot(frame.y.bar) + geom_boxplot(aes(x=ind,y=values)) p+labs(title=\"Errors\",x=\"Sample Size\",y=\"errors\")+ theme_classic() frame.range.bar <- stack(as.data.frame(range.bar)) frame.range.bar[,2] <- factor(frame.range.bar[,2],label=L) ggplot(frame.range.bar)+geom_boxplot(aes(x=ind,y=values)) + labs(title=\"Diameter of N_k\",x=\"Sample Size\", y=\"errors\")+theme_classic() \ud1b5\uacc4\uc804\uc0b0\uacfc \uc120\ud615\ub300\uc218\ub97c \uc18c\ud640\ud558\uac8c \ub4e4\uc740 \ud544\uc790\ub85c\uc11c.. \ud559\ud68c\uc5d0\uc11c \ud568\uaed8 ESL\uc744 \uc2a4\ud130\ub514\ud558\ub294 \uba4b\uc7c1\uc774 \uc774\ud615\uc131 \uce5c\uad6c\uac00 \uad6c\ud604\ud55c \ucf54\ub4dc\ub97c \uc778\uc6a9\ud558\uc600\ub2e4. \ucf54\ub4dc\ub97c \uc0b4\ud3b4\ubcf4\uba74 \uba3c\uc800 knn\uc774\ub77c\ub294 \uc774\ub984\uc73c\ub85c \uc0c8\ub85c\uc6b4 \ud568\uc218\ub97c \uc815\uc758\ud558\uc600\ub294\ub370, \ubd88\ub7ec\uc624\ub294 \ub370\uc774\ud130\uc758 \uaf34\uc744 1,2\uc5f4\uc774 \uac01\uac01 sample y,x\uc5d0 \ub300\uc751\ud558\ub3c4\ub85d \ub9cc\ub4e4\uc5b4 \ub193\uace0 \uc9e0 \ud568\uc218\uc5d0 \ub123\ub294 \uc6d0\ub9ac\uc774\ub2e4. \uc6b0\ub9ac\ub294 knn method\ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud558\uc5ec k\uac1c\uc758 sample\uc744 \ubf51\uc744 \ub54c, \ud2b9\uc815 \ud3ec\uc778\ud2b8 \uc9c0\uc810\uc5d0\uc11c \uac00\uc7a5 \uac00\uae4c\uc6b4 k\uac1c\ub97c \ubf51\uc544\uc57c\ud558\ubbc0\ub85c data\uc758 2\uc5f4\uc778 sample x\ub4e4\uacfc \uc9c0\uc815\ud55c \ud3ec\uc778\ud2b8 x=0.5\uc778 \uac12\uc758 \ucc28\uc758 \uc808\ub300\uac12\uc744 order\ud568\uc218\ub97c \uc774\uc6a9\ud558\uc5ec \uc778\ub371\uc2a4\ub97c \ucd94\ucd9c\ud558\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud588\ub2e4. sort\ub4f1\uc758 \ud568\uc218\ub97c \uc4f0\uc9c0\uc54a\uc740 \uc774\uc720\ub294 \uadf8 \ucc28\uc774\ub85c \ube44\uad50\ub97c \ud558\uac8c y,x\uac00 \ub300\uc751\ud558\ub294 \ud55c \ud589\uc744 \uc138\ud2b8\ub85c \ubf51\uc544\uc57c\ud558\uae30 \ub54c\ubb38\uc774\ub2e4. $ \\(\\begin{bmatrix} Y \\\\ X \\end{bmatrix} \\sim N_2(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\begin{bmatrix} 1 & 0.7 \\\\ 0.7 & 1 \\end{bmatrix} )\\) $ \uc704\uc758 \uc2dd\uc5d0\uc11c Y\ub294 \ucd9c\ub825\ubcc0\uc218\uc774\uace0 X\ub294 predictor variable\uc774\ub2e4. X\uc640 Y\ub97c \uc704\uc640 \uac19\uc774 \uac00\uc815\ud588\uc744 \ub54c $ \\(Y|X = x \\sim N(\\mu_{y}+\\sum_{yx}{\\sum_{xx}}^{-1}(x-\\mu_x),\\sum_{yy}-\\sum_{yx}{\\sum_{xx}}^{-1}\\sum_{xy})\\) $ \\(Y|X=x\\) \ub294 \uc704\uc640 \uac19\uc740 \ubd84\ud3ec\ub97c \ub530\ub974\uace0, \\(i.e.,\\ Y|X=0.5 \\sim N(0.35,0.51)\\) \uc758 \ubd84\ud3ec\ub97c \uac16\uace0 \uc774\ub97c \ucf54\ub4dc\uc548\uc5d0 \ub123\uc5b4\uc11c \uc608\uc81c\ub97c \ub9cc\ub4e4\uc5b4 \ubcf4\uc558\ub2e4. \uadf8\ub9bc\uc5d0\uc11c \ubcf4\uc774\ub2e4\uc2dc\ud53c sample size\uac00 \ucee4\uc9c8\uc218\ub85d \\(Loss(N.x) = |avg(Y|X \\in N_k(x))- E[Y|\\hat{X}=x]|\\) \uac00 0\uc5d0 \uc218\ub834\ud558\uace0 boxplot\uc758 \ud3ed \ub610\ud55c \uc791\uc544\uc9c0\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \ub610\ud55c \\(N_k(x)\\) \uc758 \uc9c1\uacbd \ub610\ud55c 0\uc5d0 \uc218\ub834\ud568\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc774 \ub458\uc744 \uc815\ub9ac\ud55c\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc740 \uacb0\ub860\uc744 \ub0b4\ub9b4 \uc218 \uc788\ub2e4. $$E[Y|\\hat{X}=x] \\xrightarrow{P} E[Y|X=x] (\\because WLLN) $$ $ \\(E[Y|\\hat{X}=x] -avg(Y|X \\in N_k(x))\\xrightarrow{P} 0\\) $ $ \\(\\therefore avg(Y|X \\in N_k(x)) \\xrightarrow{P} E[Y|X=x] (\\because Arithmetics \\ of\\ convergence\\ in\\ probability)\\) $ \\(i.e.\\ , K-NN\\ estimator\\ is\\ another\\ consistent\\ estimator\\ for\\ conditional\\ expectation(Not\\ E[Y])\\) \uc6b0\ub9ac\ub294 knn\uacfc \ucd5c\uc18c\uc81c\uacf1\ubc95\uc774 \ubaa8\ub450 \uc870\uac74\ubd80 \uae30\ub313\uac12\uc744 \ud1b5\ud574 \\(f(x)\\) \ub97c \ucc3e\uc544\uac00\ub294 \uac83\uc744 \uc54c\uc558\ub2e4. \ud558\uc9c0\ub9cc \ucd5c\uc18c\uc81c\uacf1\ubc95\uc5d0\uc11c\ub294 \ub450\uac00\uc9c0 \uc870\uac74\uc744 \uac00\uc815\ud574\uc57c\ud55c\ub2e4. \ud68c\uadc0\ud568\uc218 \\(f(x)\\) \uac00 globally linear function\uc5d0 \uc758\ud574 \uc798 \uadfc\uc0ac\ub418\uc5b4\uc57c\ud55c\ub2e4\ub294 \uac83. \\(\\beta=[E(XX^T)]^{-1}E(XY)\\) knn \ubc29\ubc95\uc740 \\(f(x)\\) \uac00 locally constant function\uc5d0 \uc758\ud574\uc11c \uc798 \uadfc\uc0ac\ub418\uc5b4\uc57c\ud55c\ub2e4\ub294 \uac00\uc815\uc744 \ud544\uc694\ub85c\ud55c\ub2e4. \\(L_2\\) loss function \ub300\uc2e0 \\(L_1: E|Y-f(X)|\\) \ub97c \uc4f8 \uc218\ub294 \uc5c6\ub294\uac78\uae4c? \uc774\uc5d0 \ub300\ud55c \ud574\ub2f5\uc740 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(\\hat{f}(x)=median(Y|X=x)\\) $ \uc774\ub294 location\uc5d0 \ub300\ud55c \ub2e4\ub978 \uce21\uc815\ubc29\ubc95\uc774\uba70 \uc870\uac74\ubd80 \ud3c9\uade0\uc5d0 \ube44\ud574\uc11c\ub294 \uc870\uae08\ub354 robust\ud55c estimates\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub7ec\ud55c loss function\uc5d0 \ub300\ud55c \uc811\uadfc\uc740 \ubbf8\ubd84\uc744 \ud568\uc5d0 \uc788\uc5b4\uc11c \ubd88\uc5f0\uc18d\uc810\uc744 \uac16\uace0\uc788\uae30 \ub54c\ubb38\uc5d0 \ub110\ub9ac \uc0ac\uc6a9\ub420 \uc218 \uc788\ub294 \ubc29\ubc95\uc740 \uc544\ub2c8\ub2e4. \uc678\ubd80\uc758 \ubcc0\ud654\uc5d0 \ub354 robust\ud55c loss function\ub4e4\uc740 \ub4b7 \uc7a5\uc5d0\uc11c \ucd94\ud6c4\uc5d0 \ucd94\uac00 \uc124\uba85\uc744 \ud560 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc \uadf8\ub7fc\uc5d0\ub3c4 \uc5ed\uc2dc squared error methods\uac00 \uac00\uc7a5 \ub110\ub9ac \uc54c\ub824\uc838\uc788\uace0 \ud3b8\ub9ac\ud55c \ubc29\ubc95\uc77c \uac83\uc774\ub2e4. \ucd9c\ub825\ubcc0\uc218\uac00 categorical output\uc774\ub77c\uba74 \uc5b4\ub5a8\uae4c? \uc774\ub294 \uc218\uce58\ud615 \uc790\ub8cc\uc77c \ub54c\uc640 \uac19\uc740 paradigm\uc744 \uacf5\uc720\ud558\uba70, \uc624\uc9c1 \uc608\uce21\uc624\ucc28\ub97c penalize\ud558\ub294 loss function\ub9cc \uc0b4\uc9dd \ubc14\uafd4\uc8fc\uba74 \ub41c\ub2e4. loss function\uc740 \\(K\\times{K}\\) \uc758 matrix \\(\\mathbf{L}\\) \uc774\uba70, \uc774\ub294 \ub300\uac01\uc6d0\uc18c\uac00 0\uc774\uace0 \ubaa8\ub4e0 \uc6d0\uc18c\uac00 nonnegative\ud55c \ud589\ub82c\uc774\ub2e4. \uc5ec\uae30\uc11c \\(L(k,l)\\) \uc740 K class\uc5d0 \uc18d\ud558\ub294 \uad00\uce21\uce58\ub97c L class\uc5d0 \ubd84\ub958\ud558\uc600\uc744 \ub54c \ub4dc\ub294 cost\ub97c \uc758\ubbf8\ud55c\ub2e4. \uc6b0\ub9ac\ub294 \ub300\uac1c loss funtion\uc73c\ub85c \\(zero-one\\) loss function\uc744 \uc0ac\uc6a9\ud558\uba70, \uc774\ub54c expected prediction error\ub294 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(EPE=E(L(G,\\hat{G}(X))]\\) $ \uc774\ub97c \uc55e\uc5d0\uc640 \ub611\uac19\uc740 \ubc29\ubc95\uc73c\ub85c X\uc5d0 \ub300\ud574 \uc870\uac74\ubd80 \uae30\ub300\uac12\uc744 \uac78\uba74 $ \\(EPE=E_{X}\\sum\\limits_{k=1}^{K}L[G_{k},\\hat{G}(X)]Pr(G_{k}|X)\\) $ \ub85c \ud45c\ud604\ub41c\ub2e4. \ub611\uac19\uc774 EPE pointwise\ub97c \ucd5c\uc18c\ud654\ud558\uac8c \ub9cc\uc871\ud558\ub294 \\(\\hat{G}(x)\\) \ub294 \\(argmin_{{g}\\in {G}}\\sum\\limits_{k=1}^KL(G_{k},g)Pr(G_{k}|X=x)\\) \uc774\uace0 0-1 loss function\uc5d0\uc11c \uc774\ub97c \uac04\ub7b5\ud558\uac8c \uc544\ub798\ucc98\ub7fc \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. $ \\(\\hat{G}(x)=argmin_{{g}\\in {G}}[1-Pr(g|X=x)]\\) $ \ub610\ub294, \\(\\hat{G}(x)=G_{k}\\) if \\(Pr(G_{k}|X=x)=max_g\\in{G}Pr(g|X=x)\\) \ub85c \ud45c\uae30\uac00\ub2a5\ud558\ub2e4. \uc704\uc758 \ud574\ub294 Bayes classifier \ub85c \uc54c\ub824\uc838 \uc788\uc73c\uba70, \uc774\ub294 \uc870\uac74\ubd80 \uc774\uc0b0\ubd84\ud3ec \\(Pr(G|X)\\) \ub97c \uc774\uc6a9\ud558\uc5ec \uac00\uc7a5 probable\ud55c class\uc5d0 \ubd84\ub958\ud558\ub294 \uac83\uc774\ub2e4. \ubca0\uc774\uc988 \ubd84\ub958\uae30\uc758 error rate\ub294 Bayes rate \ub85c \ubd88\ub9b0\ub2e4. \ub2e4\uc2dc k-\ucd5c\uadfc\uc811\uc774\uc6c3 \ubd84\ub958\uae30\ub97c \ub3cc\uc544\ubcf4\uba74, \uc6b0\ub9ac\ub294 \uc774 \ubd84\ub958\ubc29\ubc95\uc774 \uc9c1\uc811\uc801\uc73c\ub85c \uc704\uc758 \ud574\uc5d0 \uadfc\uc0ac\ud55c\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \uc774\ub294 \uc55e\uc5d0\uc11c \ud655\uc778\ud558\uc600\ub4ef\uc774, \uc8fc\ubcc0 \uc774\uc6c3\uc744 \uad6c\uc131\ud558\ub294 \ud074\ub798\uc2a4\ub4e4 \uc548\uc5d0\uc11c \ub2e4\uc218\uacb0\ub85c \uc608\uce21\uc744 \uc2dc\ud589\ud558\uba70, \ud55c \uc9c0\uc810\uc758 \uc870\uac74\ubd80 \ud655\ub960\uc740 \uc8fc\ubcc0 \uc810\uc758 \uc774\uc6c3\ub4e4\uc758 \uc870\uac74\ubd80 \ud655\ub960\ub85c \uc644\ud654\ub418\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \ub610\ud55c \uadf8\ub54c\uc758 \ud655\ub960\uc740 training-sample\uc758 proportions\uc73c\ub85c \uce21\uc815\ub41c\ub2e4. Binary classification \ubb38\uc81c\uc5d0\uc11c \uc6b0\ub9ac\ub294 error\uc758 loss function\uc774 \\(\\hat{f}(X)=E(Y|X)=Pr(G=G_{1}|X)\\) , if \\(G_{1}\\) corresponded to Y=1\uac00 \ub418\ub294 \uac83\uc744 \uc54c\uace0, \uc774 \uac83\uc774 K-class\ub85c \ud655\uc7a5\ub41c\ub2e4\uba74 \uc190\uc2e4\ud568\uc218\ub294 \\(E(Y_{k}|X)=Pr(G=G_{k}|X)\\) \uac00 \ub41c\ub2e4. \uc774 \uc218\uc2dd\uc740 \ub354\ubbf8\ubcc0\uc218\ub97c \ud65c\uc6a9\ud55c \ud68c\uadc0\uc758 \uacfc\uc815\uc774 \ubca0\uc774\uc988\ubd84\ub958\uae30\uc758 \ub610\ub2e4\ub978 \ubc29\ubc95\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \ud558\uc9c0\ub9cc \uc2e4\uc81c \ubb38\uc81c\ub97c \ucc98\ub9ac\ud560 \ub54c \uc120\ud615\ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uba74\uc11c \\(\\hat{f}(X)\\) \ub294 \ubb34\uc870\uac74 \uc591\uc758 \uac12\uc744 \uac00\uc838\uc57c\ud558\uc9c4 \uc54a\uc73c\uba70, \uc774\ub7f4 \ub54c \uc6b0\ub9ac\ub294 \uc774\ub97c \ud655\ub960\uc758 \ucd94\uc815\uce58\ub85c \uc0ac\uc6a9\ud560 \ub54c \uc5b4\ub824\uc6c0\uc5d0 \ub2f9\ub3c4\ud560 \uc218\ub3c4 \uc788\ub2e4. \uc774\ub7ec\ud55c \ubb38\uc81c\ub294 \ucd94\ud6c4 \ucc55\ud130 4\uc7a5\uc5d0\uc11c \ub2e4\ub904\ubcf4\ub3c4\ub85d \ud558\uc790.","title":"Statistical Decision Thoery"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#local-methods-in-high-dimensions","text":"knn averaging\uc744 \ud1b5\ud574 \ud45c\ubcf8\uc758 \ud06c\uae30\uac00 \uc810\uc810 \ucee4\uc838\ub3c4 \uc6b0\ub9ac\ub294 \uc5b8\uc81c\ub098 \uc774\uc0c1\uc801\uc778 \ucd5c\uc801\uc758 \uc870\uac74\ubd80 \ud3c9\uade0\uac12\uc73c\ub85c \uadfc\uc0ac\ud560\uc218\uc788\uc744\uae4c? \uc544\uc27d\uac8c\ub3c4 \uc774\ub7ec\ud55c \uc0dd\uac01\uc740 \uace0\ucc28\uc6d0\uc758 \ubb38\uc81c\uc5d0\uc11c \uac00\ubccd\uac8c \uae68\uc838\ubc84\ub9ac\ub294 \uae30\ub300 \uc774\uba70 \uc774\ub7ec\ud55c \ud604\uc0c1\uc744 \uc6b0\ub9ac\ub294 \ucc28\uc6d0\uc758 \uc800\uc8fc( curse of dimensionality )\ub77c\uace0 \uce6d\ud55c\ub2e4. \uc774\uc5d0 \ub300\ud55c \uc608\uc2dc\ub97c \uba87\uac1c\ub9cc \uc0b4\ud3b4\ubcf4\uaca0\ub2e4. p\ucc28\uc6d0\uc758 hypercube\uc548\uc5d0\uc11c \uade0\ub4f1\ud558\uac8c \ubd84\ud3ec\ub418\uc5b4 \uc788\ub294 input\uc5d0 \ub300\ud55c \ucd5c\uadfc\uc811\uc774\uc6c3 \uc808\ucc28\ub97c \uc0b4\ud3b4\ubcf4\uc790. \uad00\uce21\uce58\uc5d0\uc11c fraction r\uc744 \ud3ec\ucc29\ud558\uae30 \uc704\ud574 \ud0c0\uac9f \ud3ec\uc778\ud2b8\uc5d0 \ub300\ud55c hypercubical neighborhood\ub97c \ubcf4\ub0b8\ub2e4\uace0 \uc0dd\uac01\ud574\ubcf4\uc790. \uc774\ub294 \uacb0\uad6d \\(\\frac{r}{volume}\\) \uc774\uba70, expected dege legnth\ub294 \\(e_{p}(r)=r^{1/p}\\) \uac00 \ub420 \uac83\uc774\ub2e4. 10\ucc28\uc6d0\uc758 \uc608\uc2dc\uc5d0\uc11c r\uc744 \uc870\uc815\ud558\uba70 \uc608\uc2dc\ub97c \ub4e4\uc790\uba74, \\(e_{10}(0.01)=0.63\\) , \\(e_{10}(0.1)=0.8\\) \uc774 \ub418\uba70 \uac01 \uc785\ub825\ubcc0\uc218\uc5d0 \ub300\ud55c \uc804\uccb4 range\ub294 1.0\uc774\ub2e4. \uc774\ub294 \ub2e4\uc2dc\ub9d0\ud574 \uc6b0\ub9ac\uac00 1%\uc640 10%\ub85c \uac01\uac01\uc785\ub825\ubcc0\uc218\uc758 local average\ub97c \ud3ec\ucc29\ud558\uae30 \uc704\ud574 \ucee4\ubc84\ud574\uc57c\ud558\ub294 \uc785\ub825\ubcc0\uc218\uc758 \ubc94\uc704\uac00 63%\uc640 80%\ub098 \ud544\uc694\ud558\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc774\ub7ec\ud55c \uc774\uc6c3\uc740 \ub354\uc774\uc0c1 local \ud558\uc9c0 \uc54a\ub2e4. \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \uc5c6\uc560\uae30 \uc704\ud558\uc5ec \ucc28\uc6d0\uc744 \uc904\uc778\ub2e4\uace0 \ud55c\ub2e4\uba74 \uc774\ub294 \ub354 \uc801\uc740 \uad00\uce21\uce58\ub85c \ud3c9\uade0\uac12\uc744 \ub0b4\uac8c \ub418\ubbc0\ub85c, \ubd84\uc0b0\uc740 \ub354\uc6b1 \ub354 \ucee4\uc9c0\ub294 \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud55c\ub2e4. \ub610\ub2e4\ub978 \uace0\ucc28\uc6d0\uc5d0\uc11c\uc758 sparse sampling\uc758 \uacb0\uacfc\ub85c\ub294 \ubaa8\ub4e0 \ud45c\ubcf8 \uc9c0\uc810\ub4e4\uc774 \ud45c\ubcf8\uc758 \uacbd\uacc4\uc5d0 \uac00\uae5d\ub2e4\ub294 \uc810\uc774\ub2e4. \uc774\ub294 \ub2e4\uc2dc \ub9d0\ud574 \ucc28\uc6d0\uc774 \ub192\uc544\uc9c8\uc218\ub85d \uacf5\uac04\uc758 \uc0ac\uc774\uc988\ub294 \uae30\ud558\uae09\uc218\uc801\uc73c\ub85c \ucee4\uc9c0\ub294\ub370\uc5d0 \ubc18\ud574 \uc624\ud788\ub824 sample point\ub4e4\uc774 \ud55c \ucabd \uad6c\uc11d\uc73c\ub85c \ubab0\ub824 \uc774\ub8e8\uc5b4\uc9c0\uac8c\ub41c\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc774\ub294 \ub300\ubd80\ubd84\uc758 \ub370\uc774\ud130\ub4e4\uc774 \ub2e4\ub978 data point\ub4e4 \uac04\uc758 \uac70\ub9ac\ubcf4\ub2e4 \ud45c\ubcf8\uacf5\uac04\uc758 \uacbd\uacc4\uc640 \ub354 \uac00\uae5d\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud558\uace0 training sample\uc758 \uacbd\uacc4\uc5d0\uc11c\ub294 \ub354\uc6b1 \ub354 \uc608\uce21\uc744 \uc5b4\ub835\uac8c\ud55c\ub2e4\ub294 \ubb38\uc81c\uc810\uc744 \ub0b3\ub294\ub2e4.","title":"Local Methods in High Dimensions"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#_1","text":"","title":"\uc5ec\uae30\ub294 \ub118 \uc5b4\ub824\uc6cc\uc11c \uc774\ub530\uac00\ud558\uc790"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#statistical-models-supervised-learning-and-function-approximation","text":"\uc6b0\ub9ac\uc758 \ubaa9\ud45c\ub294 input\uacfc output\uac04\uc758 \uc608\uce21\uc801\uc778 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ud568\uc218 \\(f(x)\\) \uc5d0 \uc798 \uadfc\uc0ac\ud558\ub294 \\(\\hat{f}(x)\\) \ub97c \ucc3e\ub294 \uac83\uc774\ub2e4. \uc6b0\ub9ac\ub294 Section 2.4\ub97c \ud1b5\ud574 \uc774\ub860\uc801\uc73c\ub85c squared error loss\uac00 \uc591\uc801\ubcc0\uc218\uc5d0 \ub300\ud558\uc5ec \ud68c\uadc0\ud568\uc218 \\(f(x)=E(Y|X=x)\\) \ub97c \ub3c4\ucd9c\ud558\ub294 \uac83\uc744 \ubc30\uc6e0\ub2e4. knn methods\ub294 \uc774\ub7ec\ud55c \uc870\uac74\ubd80 \uae30\ub313\uac12\uc744 \uc9c1\uc811\uc801\uc73c\ub85c \ucd94\uc815\ud574\uc8fc\uc9c0\ub9cc \uc801\uc5b4\ub3c4 \uc544\ub798\uc758 \uc870\uac74\uc5d0\uc11c\ub294 \ub9cc\uc871\ud558\uc9c0 \uc54a\ub294\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc900\ub2e4. \uc785\ub825\uacf5\uac04\uc758 \ucc28\uc6d0\uc774 \ub108\ubb34 \ub192\ub2e4\uba74, \ucd5c\uadfc\uc811\uc774\uc6c3\ubc29\ubc95\uc740 \ud0c0\uac9f \ud3ec\uc778\ud2b8\uc5d0 \uac00\uae4c\uc6cc\uc9c0\uc9c0 \ubabb\ud558\uba70 \ud070 error\ub97c \ubc1c\uc0dd\uc2dc\ud0a8\ub2e4. \ub9cc\uc57d \ud2b9\ubcc4\ud55c \uad6c\uc870\uac00 \uc874\uc7ac\ud55c\ub2e4\uba74, \uc774\ub294 \ucd94\uc815\uce58\uc5d0 \ub300\ud55c \ud3b8\ud5a5\uacfc \ubd84\uc0b0\uc744 \uc904\uc774\ub294\ub370 \uc0ac\uc6a9\ub41c\ub2e4.","title":"Statistical Models, Supervised Learning and Function Approximation"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#a-statistical-model-for-the-joint-distribution-prxy","text":"\\[Y=f(X)+\\epsilon\\] \uc774\ub7ec\ud55c \uc2dd\uc774 \uc788\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uc790. (\uc5ec\uae30\uc11c \uc784\uc758 \uc624\ucc28 \\(\\epsilon\\) \uc740 \\(E(\\epsilon)=0\\) \uc774\uba70, X\uc5d0 \ub300\ud574 \ub3c5\ub9bd\uc774\ub2e4.) \uc774\ub7ec\ud55c \uac00\ubc95\uc624\ucc28\ubaa8\ub378\uc740 \uc2e4\uc81c\ub85c \uc720\uc6a9\ud55c \uadfc\uc0ac\uc2dd\uc774\ub2e4. \uc2e4\uc81c\ub85c \ub300\ubd80\ubd84 \uc2dc\uc2a4\ud15c \uc548\uc5d0\uc11c\uc758 \uc785\ucd9c\ub825\uc30d \\((X,Y)\\) \ub294 determinisitic \ud55c \\(Y=f(X)\\) \uc758 \uad00\uacc4\ub97c \ub9cc\uc871\ud558\uc9c0 \uc54a\ub294\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c \uc774 \ub458\uc758 \uad00\uacc4\uc5d0\uc11c\ub294 \uce21\uc815\ub418\uc9c0 \uc548\ud754 \ubcc0\uc218\ub4e4\uc774 \uc874\uc7ac\ud558\uace0 \uc774\ub4e4\uc774 Y\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\uba70, \uce21\uc815\uc624\ucc28 \ub610\ud55c \ud3ec\ud568\ub41c\ub2e4. \uac00\ubc95 \ubaa8\ub378\uc758 \uac00\uc815\uc740 \uc774\ub7ec\ud55c deterministic \ud55c \uad00\uacc4 \uc678\uc758 error \\(\\epsilon\\) \uc744 \uc7a1\uc544\ub0bc \uc218 \uc788\ub2e4\ub294 \uac83\uc744 \uc804\uc81c\ub85c \ud55c\ub2e4. \ud2b9\uc815 \uba87\uba87\uc758 \ubb38\uc81c\ub4e4\uc740 deterministic \ud55c \uad00\uacc4\ub97c \uac16\uace0 \ub9cc\uc871\ud558\ub294 \ubaa8\uc2b5\uc744 \uac16\ub294\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub7ec\ud55c \ubb38\uc81c\ub4e4\uc73c \ub2e4\ub8e8\ub294 \uac83\uc774 \uc544\ub2cc \uc0c1\ud669\uc774\ub77c\uba74 \uc6b0\ub9ac\uac00 \uc624\ucc28\ud56d\uc5d0 \uae30\ubc18\ud55c \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc801\uc808\ud560\uae4c? \uc5d0 \ub300\ud574 \uc0dd\uac01\uc744 \ud574\ubcfc \uc218 \uc788\ub2e4. error\uc5d0 \ub300\ud55c \ub3c5\ub9bd\uc131, \ub4f1\ubd84\uc0b0\uc131 \uc815\uaddc\uc131\ub4f1\uc758 \uac00\uc815\uc774 \uc5c4\uaca9\ud558\uac8c \ud544\uc694\ud55c \uac83\uc740 \uc544\ub2c8\uc9c0\ub9cc, \uc55e\uc11c \ubc30\uc6e0\ub358 EPE criterion\uc758 \uc608\ub97c \uc0dd\uac01\ud574\ubcf4\uba74 \uc774\ub7ec\ud55c \uac00\uc815\uc740 \uc0c1\ub2f9\ud788 \ud569\ub2f9\ud558\ub2e4. \uc774\ub7ec\ud55c \ubaa8\ub378\uc5d0\uc11c\ub294 \ucd5c\uc18c\uc81c\uacf1\ubc95\uc744 \uc774\uc6a9\ud558\uc5ec \ubaa8\ub378\uc744 estimation\ud558\ub294 \uac83\uc774 \ud569\ub2f9\ud574\ubcf4\uc778\ub2e4. \uc6b0\ub9ac\ub294 \ub3c5\ub9bd\uc131 \uac00\uc815\uc758 \uc81c\uc57d\uc744 \uac04\ub2e8\ud55c modifications\uc744 \ud1b5\ud574 \ud574\uacb0\ud560 \uc218\ub3c4 \uc788\ub2e4. \uc77c\ub840\ub85c \\(Var(Y|X=x)=\\sigma(x)\\) \ub97c \uc0dd\uac01\ud574\ubcf4\uc790. \uc6b0\ub9ac\ub294 Y\ub97c X\uc5d0 \ub300\ud55c \uc870\uac74\ubd80\ub97c \uac78\uc5b4 Y\uc758 \ubd84\uc0b0\uacfc \ud3c9\uade0\uc774 X\uc5d0 \uc885\uc18d\uc801\uc778 \uad00\uacc4\uac00 \ub418\ub3c4\ub85d \ubcc0\ud615\ud560 \uc218 \uc788\ub2e4. \uc77c\ubc18 \uc801\uc73c\ub85c \uc870\uac74\ubd80 \ubd84\ud3ec \\(Pr(Y|X)\\) \ub294 X\uc5d0 \uc885\uc18d\uc801\uc77c \uc218 \uc788\uc9c0\ub9cc, \uac00\ubc95\uc624\ucc28\ubaa8\ub378\uc5d0\uc11c \uc774 \ubd80\ubd84\uc740 \ubc30\uc81c\ud558\uc5ec \uc0dd\uac01\ud55c\ub2e4. \uac00\ubc95\ubaa8\ud615\uc740 \ud1b5\uc0c1\uc801\uc73c\ub85c \uc9c8\uc801 \ucd9c\ub825\ubcc0\uc218 \\(G\\) \uc5d0 \ub300\ud574\uc11c\ub294 \uc798 \uc0ac\uc6a9\ub418\uc9c0 \uc54a\ub294\ub2e4. \uc774\ub7ec\ud55c \ucf00\uc774\uc2a4\uc5d0\uc11c \ubaa9\ud45c\ud568\uc218 \\(p(X)\\) \ub294 \uc870\uac74\ubd80 \ubc00\ub3c4 \\(Pr(G|X)\\) \uc774\uba70 \uc9c1\uc811\uc801\uc73c\ub85c \ubaa8\ub378\ub9c1\ud560 \uc218 \uc788\ub2e4. \uadf8 \uc608\ub85c binary class \ub370\uc774\ud130\uc5d0\uc11c \uc790\ub8cc\ub4e4\uc774 \ub3c5\ub9bd\uc801\uc778 binary trials\uc5d0\uc11c \uc624\uace0 \ub450 \ubc94\uc8fc\uc5d0 \uc18d\ud560 \ud655\ub960\uc744 \uac01\uac01 \\(p(X)\\) , \\(1-p(X)\\) \ub85c \ub450\ub294 \uac83\uc774 \ud569\ub9ac\uc801\uc77c \uac83\uc774\ub2e4. \uadf8\ub7ec\ubbc0\ub85c Y\uac00 0-1 \ub85c \ub098\ub258\uc5b4\uc9c4 \uc9c8\uc801\ubcc0\uc218 G\uc778 \ucf00\uc774\uc2a4\uc5d0\uc11c\ub294 \\(E(Y|X=x)=p(x)\\) \uc774 \ub418\uc9c0\ub9cc, \ubd84\uc0b0\uc740 \\(Var(Y|X=x)=p(x)[1-p(x)]\\) \ub85c x\uc5d0 \ub300\ud574 \uc885\uc18d\uc801\uc774\ub2e4.","title":"A Statistical Model for the Joint Distribution Pr(X,Y)"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#supervised-learning","text":"\uba38\uc2e0\ub7ec\ub2dd\uc758 \uad00\uc810\uc5d0\uc11c \ud568\uc218\uc801\ud569\uc758 \ud328\ub7ec\ub2e4\uc784\uc740 training set\uacfc input output\uc774 \uc874\uc7ac\ud558\ub294 \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc77c\uc815\ud55c \uc54c\uace0\ub9ac\uc998\uc5d0 \uc758\ud574 \uadfc\uc0ac\ub41c\ub2e4. \uc774\ub7ec\ud55c \uc54c\uace0\ub9ac\uc998\uc740 \uc785\ucd9c\ub825\uac04\uc758 \uad00\uacc4\ub97c \ubcf4\uc5ec\uc8fc\ub294 \\(\\hat{f}\\) \ub97c \uc6d0\uc790\ub8cc\uc640 \uc0dd\uc131\ub41c \ucd9c\ub825\uac12 \uac04\uc758 \ucc28\uc774\uc778 \\(y_{i}-\\hat{f}(x_{i})\\) \ub97c \ud1b5\ud574 \ubcc0\ud615\ud560 \uc218 \uc788\ub294 \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc788\uace0, \uc774\ub97c learning by example \uc774\ub77c\uace0 \uce6d\ud55c\ub2e4. ####Function Approximation \ud559\uc2b5 \ud328\ub7ec\ub2e4\uc784\uc5d0 \ub300\ud55c \uc811\uadfc\uc740 \uc218\ud559, \ud1b5\uacc4\ud559\uc758 \ubd84\uc57c\uc5d0\uc11c\ub294 \ud568\uc218\uadfc\uc0ac\uc640 \ucd94\uc815\uc73c\ub85c \uc5ec\uaca8\uc9c4\ub2e4. \uc6b0\ub9ac\uac00 \uc870\uc6b0\ud558\uac8c \ub418\ub294 \ub9ce\uc740 \uadfc\uc0ac\ub4e4\uc740 \ub370\uc774\ud130\uc5d0 \ub530\ub77c \uc801\ud569\ud558\uac8c \ubcc0\ud615\ub418\ub294 \ubaa8\uc218 \\(\\theta\\) \uc758 set\uc744 \uac16\ub294\ub2e4. \uadf8 \uc608\ub85c \uc120\ud615\ubaa8\ub378 \\(f(x)=x^T\\beta\\) \ub294 \ubaa8\uc218 \\(\\theta=\\beta\\) \ub97c \uac16\ub294\ub2e4. \ub610\ub2e4\ub978 \uc720\uc6a9\ud55c \uadfc\uc0ac\ubd84\ub958\uae30\ubc95\uc740 linear basis expansions \uc73c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. $ \\(f_{\\theta}(x)=\\sum\\limits_{k=1}^Kh_{k}(x)\\theta_{k}\\) $ \uc5ec\uae30\uc11c \ub9d0\ud558\ub294 \\(h_{k}\\) \ub294 \uc801\uc808\ud55c \ud568\uc218\uc758 \uc9d1\ud569 \ud639\uc740 \uc785\ub825\ubca1\ud130 x\uc758 \ubcc0\ud658\ud615\ud0dc\uc774\ub2e4. \uc804\ud1b5\uc801\uc778 \uc608\ub85c polynomial \uacfc trigonometric expansions \ub4f1\uc774 \uc788\uace0 \uc774 \ub54c\uc758 \\(h_{k}\\) \ub294 \\(x_{1}^2, x_1x_2^2, cos(x_1)\\) \ub4f1\uc774 \uc788\ub2e4. \uc6b0\ub9ac\ub294 \ub610\ud55c \ube44\uc120\ud615\ud615\ud0dc\uc758 \ud655\uc7a5 \ub610\ud55c \uc0ac\uc6a9\ud560 \uc218\uac00 \uc788\ub294\ub370, \uc778\uacf5\uc2e0\uacbd\ub9dd \ubaa8\ub378\uc5d0\uc11c \uc77c\ubc18\uc801\uc73c\ub85c \uc4f0\uc774\ub294 sigmoid \ubcc0\ud658\uc744 \uc608\ub85c \ub4e4 \uc218 \uc788\uc73c\uba70, \uc774\ub7ec\ud55c form\uc740 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(h_{k}(x)=\\frac{1}{1+exp(-x^T\\beta_{k})}\\) $ \uc6b0\ub9ac\ub294 \ucd5c\uc18c\uc81c\uacf1\ubc95\uc744 \uc0ac\uc6a9\ud558\uba74\uc11c \uc120\ud615\ubaa8\ub378\uc5d0\uc11c \\(RSS(\\beta)\\) \ub97c \ucd5c\uc18c\ud654 \ud558\uc5ec \ubaa8\uc218 \\(\\theta\\) \uc758 \ud568\uc218\ub85c \ucde8\uae09\ud558\uace0 \ubaa8\uc218\ub97c \ucd94\uc815\ud558\ub294\ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. \uc120\ud615 \ubaa8\ub378\uc5d0\uc11c\ub294 \uc6b0\ub9ac\ub294 minimization problem\uc744 \ud1b5\ud574 \ub2e8\uc21c\ud55c closed form\uc758 \ud574\ub97c \uad6c\ud560 \uc218 \uc788\ub2e4. \uc774\ub294 basis function \ubc29\ubc95\ub860\uc5d0\uc11c\ub3c4 \ud1b5\uc6a9\ub418\uba70, \uc228\uaca8\uc9c4 \ub610\ub2e4\ub978 \ubaa8\uc218\ub97c \uac16\uc9c0\uc54a\ub294\ud55c \ub611\uac19\uc774 \uc801\uc6a9\ub41c\ub2e4. \uadf8\ub807\uc9c0 \uc54a\uc740 \uacbd\uc6b0\ub77c\uba74 \uc6b0\ub9ac\ub294 iterative methods\ub098 numerical optimazation\uc744 \ud1b5\ud574 \ud574\ub97c \uad6c\ud574\uc57c \ud55c\ub2e4. \ucd5c\uc18c\uc81c\uacf1\ubc95\uc740 \uc544\uc8fc \ud3b8\ub9ac\ud558\uace0 \ub300\uc911\uc801\uc774\uc9c0\ub9cc, \ud56d\uc0c1 \ud310\ub2e8\uc758 \ucc99\ub3c4\uac00 \ub418\ub294 \uac83\uc740 \uc544\ub2c8\ub2e4. \ub354 general\ud55c \ucd94\uc815\uc758 \uc6d0\ub9ac\uc758 \uc608\ub85c\ub294 maximum likelihood estimation \uc774 \uc788\ub2e4. $ \\(L(\\theta)=\\prod_{i=1}^Nf(x_{i};\\theta)\\) $ $ \\(l(\\theta)=\\sum\\limits_{i=1}^Nlogf(x_{i};\\theta)\\) $ MLE\ub97c \uc5bb\ub294 \uc6d0\ub9ac\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. \uac00\uc7a5 \ud569\ub9ac\uc801\uc778 \ubaa8\uc218 \\(\\theta\\) \ub294 \uad00\ucc30\ub41c \ud45c\ubcf8\uc744 \uc5bb\uc744 \ud655\ub960\uc774 \uac00\uc7a5\ub192\uc744 \uacbd\uc6b0\uc774\ub2e4. \uac00\ubc95\uc624\ucc28\ubaa8\ub378 \\(Y=f_{\\theta}(X)+{\\epsilon}\\) \uc5d0\uc11c\uc758 \ucd5c\uc18c\uc81c\uacf1\uc744 \ud1b5\ud574 \uc5bb\uc740 estimators\ub294 \uc870\uac74\ubd80 likelihood\ub97c \uc774\uc6a9\ud588\uc744 \ub54c\uc758 \ucd5c\ub300\uc6b0\ub3c4(maximum likelihood)\uc640 \uac19\ub2e4. $ \\(Pr(Y|X,\\theta)=N(f_{\\theta}(X),\\sigma^2)\\) $ \uace0\ub85c \ucd94\uac00\uc801\uc778 \uc815\uaddc\uc131 \uac00\uc815\uc744 \ub354\ud558\ub294 \uac83\uc740 \uc81c\ud55c\uc801\uc73c\ub85c \ubcf4\uc77c\uc9c0\ub77c\ub3c4, \uacb0\uacfc\ub294 \uac19\ub2e4. \uc774 \ub370\uc774\ud130\uc5d0\uc11c log-likelihood\ub294 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(L(\\theta)=-\\frac{N}{2}log(2{\\pi})-Nlog{\\sigma}-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^N(y_{i}-f_{\\theta}(x_{i}))^2\\) $ \uc704\uc758 \uc2dd\uc5d0\uc11c \\(\\theta\\) \ub97c \ud3ec\ud568\ud558\ub294 \ud56d\uc740 \uc624\uc9c1 \ub9c8\uc9c0\ub9c9 \ud56d\uc774\uba70, \uc774\ub294 \\(RSS(\\theta)\\) \uc758 scalar negative miltiplier\uc774\ub2e4. \uc870\uae08 \ub354 \ud765\ubbf8\ub85c\uc6b4 \uc608\uc81c\ub85c\ub294 \uc9c8\uc801 \ucd9c\ub825\ubcc0\uc218 \\(G\\) \uc5d0 \ub300\ud55c \ud68c\uadc0\ud568\uc218 \\(Pr(G=G_{k}|X=x)\\) \uc5d0 \ub300\ud55c mutinomial likelihood\uc774\ub2e4. \uc6b0\ub9ac\uac00 \uc8fc\uc5b4\uc9c4 X\uc5d0 \ub300\ud558\uc5ec \uac01 \ud074\ub798\uc2a4\uac00 \uc18d\ud560 \uc870\uac74\ubd80\ud655\ub960\uc774 \\(Pr(G=G_{k}|X=x)=p_{k,\\theta}(x), k=1,...,K\\) \ub85c \ud45c\uae30\ub418\ub294 \ubaa8\ub378\uc744 \uac16\uace0\uc788\ub2e4\uace0 \uc0dd\uac01\ud574\ubcf4\uc790. \uc774 \ub54c\uc758 \uc704 \uc2dd\uc758 log-likelihood(cross entropy)\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ub41c\ub2e4. (\ubbf8\uc548\ud55c\ub370 \uc9c4\uc9dc \ubb50\uac00 \ud765\ubbf8\ub85c\uc6b4\uc9c0 \ud558\ub098\ub3c4 \ubaa8\ub974\uaca0\ub2e4..) $ \\(L(\\theta)=\\sum\\limits_{i=1}^Nlogp_{g_{i},\\theta}(x_{i})\\) $","title":"Supervised Learning"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#structured-regression-models","text":"\uc6b0\ub9ac\ub294 \ucd5c\uadfc\uc811\uc774\uc6c3\uc774\ub098 \ub2e4\ub978 local methods\uac00 \uc9c1\uc811\uc801\uc73c\ub85c \ud2b9\uc815 \uc810\uc5d0\uc11c\uc758 \ud568\uc218\ub97c \ucd94\uc815\ud55c\ub2e4\ub294 \uac83\uc5d0 \ucd08\uc810\uc744 \ub9de\ucd94\uace0 \uc788\uc9c0\ub9cc, \uc774\ub294 \uac00\ub054 \ucc28\uc6d0\uc73c\ub85c \uc778\ud574 \uc5b4\ub824\uc6c0\uc744 \ub9de\ub531\ub4e4\uc778\ub2e4\ub294 \uc0ac\uc2e4\uc744 \ubc30\uc6e0\ub2e4. \uc774\ub7ec\ud55c \uc811\uadfc\ubc95\uc740 \uc800\ucc28\uc6d0\uc758 \ubb38\uc81c\uc5d0\uc11c\ub294 \ubd80\uc801\uc808\ud560 \uc218\ub3c4 \uc788\uc73c\uba70, \uc624\ud788\ub824 \ub354 \uad6c\uc870\ud654\uace0 \uc81c\uc57d\ub41c \uc870\uac74\uc744 \uac78\uc5b4 \ubaa8\ub378\uc744 \uc9dc\ub294 \uac83\uc774 \ub370\uc774\ud130\ub97c \ub354 \ud6a8\uc728\uc801\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uc77c \uc218\ub3c4 \uc788\ub2e4. $ \\(RSS(f)=\\sum\\limits_{i=1}^N(y_{i}-f(x_{i}))^2\\) $ \uc704\uc758 \uc2dd\uc744 \ucd5c\uc18c\ud654 \ud558\ub294 \ud574\ub97c \uad6c\ud558\ub294 \uac83\uc740 training points \\(({x_{i}},y_{i})\\) \ub97c \uc9c0\ub098\ub294 \ud568\uc218 \\({\\hat{f}}\\) \uac00 \ubb34\ud55c\ud788 \uc874\uc7ac\ud560 \uc218\ub3c4\uc788\ub2e4. \uc774\ub7ec\ud55c \ud574\ub4e4 \uc911 \uba87\uba87\uc740 test points\uc5d0\uc11c \uc544\uc8fc \ud615\ud3b8\uc5c6\ub294 predictors\uac00 \ub420 \uac83\uc774\ub2e4. \ub9cc\uc57d \uad00\uce21\uce58 \uc30d\uc758 \uac2f\uc218\uac00 \ub9ce\ub2e4\uba74 \uc774\ub7ec\ud55c \ub9ac\uc2a4\ud06c\ub294 \uc81c\ud55c\ub420 \uc218\ub3c4 \uc787\ub2e4. \uc6b0\ub9ac\uac00 \uc870\uae08 \ub354 \ud6a8\uc728\uc801\uc73c\ub85c \uc774 \ubb38\uc81c\ub97c \uc811\uadfc\ud558\ub824\uba74 N\uc744 \ubb34\uc791\uc815 \ub298\ub9ac\ub294 \uac83\uc774 \uc544\ub2cc, \uc720\ud55c\ud55c N\uc548\uc5d0\uc11c \ucd5c\uc18c\ud654 \ubb38\uc81c\uc5d0 \uc870\uae08 \ub354 \uc81c\uc57d\uc744 \uac78\uc5b4 \ubcf4\ub2e4 \uc791\uc740 \ud568\uc218\uc758 \uc9d1\ud569\uc744 \uc5bb\uc5b4\uc57c\ud55c\ub2e4. \uc774\ub7ec\ud55c \ud574\ub4e4\uc5d0 \ub300\ud55c \uc81c\uc57d\uc774 \uc774 \ucc45\uc758 \uc8fc\uc694 \ud1a0\ud53d\uc774\ub2e4. \uc5ec\uae30\uc11c \ud558\ub098 \uba85\ud655\ud788 \ud574\uc57c\ud560 \uac83\uc740 \uc6b0\ub9ac\uac00 \uc720\uc77c\ud55c \ud574\ub97c \uc5bb\uae30 \uc704\ud574 \uc81c\uc57d\uc744 \uac70\ub294 \uac83\uc774 \ud574\uc758 \ubcf5\uc7a1\uc131\uc73c\ub85c \uc778\ud574 \ub098\uc624\ub294 \ubaa8\ud638\uc131\uc744 \uc5c6\uc568 \uc218\ub294 \uc5c6\ub2e4\ub294 \uac83\uc774\ub2e4.","title":"Structured Regression Models"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#classes-of-restricted-estimators","text":"\ube44\ubaa8\uc218\ud68c\uadc0\ub098 \ub2e4\uc591\ud55c \ud559\uc2b5 \ubc29\ubc95\ub4e4\uc740 \uc6b0\ub9ac\uac00 \ucd94\uac00\ud558\ub294 \uc81c\uc57d\uc774 \uc5b4\ub5a4 nature\ub97c \uac16\ub290\ub0d0\uc5d0 \ub530\ub77c \ub2e4\ub974\uac8c \ubd84\ub958\ub41c\ub2e4. \uac01\uac01\uc758 \ud074\ub798\uc2a4\ub4e4\uc740 \ud55c\uac1c \ud639\uc740 \uadf8 \uc774\uc0c1\uc758 parameter\ub97c \uac16\ub294\ub370, \ub54c\ub54c\ub85c local neighborhood\uc758 \ud6a8\uacfc\uc801\uc778 size\ub97c \uc870\uc808\ud574\uc8fc\ub294 smoothing parameter\ub4e4\uc774 \uc788\ub2e4. \uc6b0\ub9ac\ub294 \uc5ec\uae30\uc11c class of restricted estimators\ub4e4\uc744 \ub113\uc740 \uc758\ubbf8\ub85c 3\uac00\uc9c0\ub85c \ubd84\ub958\ud558\uc5ec \uc124\uba85\uc744 \ud55c\ub2e4.","title":"Classes of Restricted Estimators"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#1roughness-penalty-and-bayesian-methods","text":"\uba3c\uc800 \\(RSS(f)\\) \ub97c penalize\ud558\uc5ec \ucee8\ud2b8\ub864\ud558\ub294 \ud568\uc218\uac00 \uc788\ub2e4. \uc774\ub294 1\ucc28\uc6d0\uc758 \uc785\ub825\ubcc0\uc218\uc5d0 \ub300\ud55c cubic smoothing spline method\uc774\ub2e4. \uc27d\uac8c \uc124\uba85\ud558\uba74 cubic spline \uc774\ub780, \uc810\ub4e4\uc744 \ub9e4\ub044\ub7fd\uac8c \uc5f0\uacb0\ud558\ub294 \uc54c\uace0\ub9ac\uc998\uc778\ub370, \ub450 \uc810\uc744 \uc787\ub294 \uace1\uc120\uc744 3\ucc28 \ub2e4\ud56d\uc2dd\uc744 \uc774\uc6a9\ud558\uc5ec \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc5d0 cubic\uc774\ub77c \uce6d\ud55c\ub2e4. $ \\(PRSS(f;\\lambda)=RSS(f) + {\\lambda}J(f)\\) $ $ \\(PRSS(f;\\lambda)=\\sum\\limits_{i=1}^N(y_{i}-f(x_{i}))^2 + {\\lambda}\\int[f^{''}(x)]^2dx\\) $ roughness penalty\ub294 \\(f\\) \uc758 \ud070 2\ucc28\ubbf8\ubd84 \uac12\ub4e4\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\uba70, penalty\uc758 \uc815\ub3c4\ub294 \\(\\lambda \\geq 0\\) \uc778 \\(\\lambda\\) \uc5d0 \uc758\ud574 \ud45c\ud604\ub41c\ub2e4. \\(\\lambda=0\\) \ub294 no smoothing\uc744 \uc758\ubbf8\ud558\uba70 penalty\uac00 \uc874\uc7ac\ud558\uc9c0 \uc54a\uac8c \ub418\ub294 \uac83\uc774\uace0, \\(\\lambda = \\infty\\) \uc758 \uacbd\uc6b0\ub294 \uc624\ub85c\uc9c0 x\ub0b4\uc758 \uc120\ud615\ud568\uc218\ub9cc\uc744 \ud5c8\uc6a9\ud558\uac8c \ub41c\ub2e4. (smoothing\uc744 \ubb34\ud55c\ud788 \ub9ce\uc774\ud574\uc11c RSS(f)\uc640 \uac19\uc740 \uc2dd\uc774\ub77c\uace0 \ubcf4\uba74 \ub41c\ub2e4.) Penalty function J\ub294 \uc5b4\ub5a4 \ucc28\uc6d0\uc5d0\uc11c\ub4e0 \ud2b9\ubcc4\ud55c \ubc84\uc804\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc9c0\uace0 \ud2b9\ubcc4\ud55c \uad6c\uc870\ub97c \uc9e4 \uc218 \uc788\ub2e4. Penalty function \ub610\ub294 regularization methods\ub294 \uc774\ub7f0 \uc885\ub958\uc758 \ud568\uc218\uc758 smooth behavior\uc5d0 \ub300\ud55c \uc0ac\uc804\uc801 \ubbff\uc74c\uc744 \ub098\ud0c0\ub0b4\uba70, \uc774\ub294 Bayesian framework\ub0b4\uc5d0\uc11c \uc801\uc6a9\uac00\ub2a5\ud558\ub2e4. penalty J\ub294 log-prior\uc5d0 \ud574\ub2f9\ud558\uba70, \ud568\uc218 \\(PRSS(f;\\lambda)\\) \ub294 log-posterior distribution\uc744, \\(PRSS(f;\\lambda)\\) \ub97c \ucd5c\uc18c\ud654\ud558\ub294 \uac83\uc740 posterior mode\ub97c \ucc3e\ub294 \uac83\uacfc \uc0c1\uc751\ud55c\ub2e4. \uc774\uc5d0 \ub300\ud55c \uc2ec\ud654\ub0b4\uc6a9\uc740 \ub4a4\uc758 \ucc55\ud130 5\uc640 \ucc55\ud130 8 \uc5d0\uc11c \ud6c4\uc220\ud560 \uac83\uc774\ub2e4.","title":"1.Roughness Penalty and Bayesian Methods"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#2kernal-methods-and-local-regression","text":"\uc774 \ubc29\ubc95\uc740 local neighborhood\uc758 nature\ub97c \ud2b9\uc815\uc9c0\uc74c\uc73c\ub85c\uc368 \ud68c\uadc0\ud568\uc218 \ud639\uc740 \uc870\uac74\ubd80 \uae30\ub313\uac12\uc5d0 \ub300\ud55c \ucd94\uc815\uce58\ub97c \uc81c\uacf5\ud558\ub294 \ubc29\ubc95\uc774\ub2e4. local neighborhood\ub294 kernel function \\(K_{\\lambda}(x_0,x)\\) \uc5d0 \uc758\ud574 specify \ub418\ub294\ub370, \uc774 \ud568\uc218\ub294 \\(x_{0}\\) \uc8fc\ubcc0\uc758 x\uc810\ub4e4\uc5d0 \uac00\uc911\uce58\ub97c \ubd80\uc5ec\ud558\ub294 \ud568\uc218\uc774\ub2e4. \uac00\uc6b0\uc2dc\uc548 \ucee4\ub110\uc740 \uc77c\ub840\ub85c \uc815\uaddc\ud655\ub960\ubc00\ub3c4\ud568\uc218\uc5d0 \uae30\ubc18\ud558\uc5ec \uac00\uc911\ud568\uc218\ub97c \uac16\uac8c \ub418\ub294\ub370 \uadf8 \uc2dd\uc740 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(K_{\\lambda}(x_0,x)=\\frac{1}{\\lambda}exp[-\\frac{||x-x_0||^2}{2{\\lambda}}]\\) $ \uc774\ub294 \\(x_0\\) \uc73c\ub85c\ubd80\ud130 squared Euclidean \uac70\ub9ac\ub97c \ud1b5\ud574 \uba40\uc5b4\uc9c0\ub294 \uc9c0\uc810\ub4e4\uc5d0 \ub300\ud574 \uc9c0\uc218\uc801\uc73c\ub85c \uac00\uc911\uce58\ub97c \ubd80\uc5ec\ud558\ub294 \uc6d0\ub9ac\uc774\ub2e4. \uc5ec\uae30\uc11c \ud30c\ub77c\ubbf8\ud130 \\(\\lambda\\) \ub294 \uc815\uaddc\ubc00\ub3c4\uc758 \ubd84\uc0b0\uc744 \uc758\ubbf8\ud558\uba70, \uc774\uc6c3\uc758 \ub108\ube44\ub97c \uacb0\uc815\ud55c\ub2e4. \uac00\uc7a5 \uac04\uacb0\ud55c kernel estimate\uc758 form\uc740 Nadaraya-Watson weighted average \uc774\uba70 \uc2dd\uc740 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(\\hat{f}(x_0)=\\frac{\\sum\\limits_{i=1}^NK_{\\lambda}(x_0,x_{i})y_{i}}{\\sum\\limits_{i=1}^NK_{\\lambda}(x_0,x_{i})}\\) $ \uc704\uc758 \uc2dd\uc740 error\ub97c \ucd5c\uc18c\ud654 \ud558\ub294 \ud68c\uadc0\ud568\uc218 f(x)\uac00 \\(E[Y|X=x]\\) \uc778 \uc810\uc744 \uc774\uc6a9\ud558\uc5ec \uc774\ub97c \uc870\uac74\ubd80\ud655\ub960\ub85c \ucabc\uac20 \ud6c4, \uac01\uac01 marginal distribution\uc778 \\(f(x)\\) \uc640 joint distribution \uc778 \\(f(x,y)\\) \uc5d0 \ub450\uac1c\uc758 kernel density\ub97c plug\ud558\uc5ec \\(y_{i}\\) \uc5d0 \uac00\uc911\uce58\ub97c \ubd80\uc5ec\ud558\ub294 \ud568\uc218\ub77c\uace0 \ubcfc \uc218 \uc788\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c \uc6b0\ub9ac\ub294 \\(f(x_0)\\) \uc758 local \ud68c\uadc0 \ucd94\uc815\uce58\ub97c \\(f_{\\hat{\\theta}}(x_0)\\) \ub85c \ud45c\uae30\ud558\uace0, \\(\\hat{\\theta}\\) \ub97c \ucd5c\uc18c\ud654\ud558\ub294 \uc2dd\uc740 $ \\(RSS(f_{\\theta},x_0)=\\sum\\limits_{i=1}^NK_{\\lambda}(x_0,x_{i})(y_{i}-f_{\\theta}(x_{i}))^2\\) $ \uc774\ub2e4. \uc5ec\uae30\uc11c \\(f_{\\theta}\\) \ub294 \uc5b4\ub5a4 \ubaa8\uc218\ud654\ub41c \ud568\uc218\uc774\uba70 low-order polynomial \ub4f1\uc774 \uc788\ub2e4. \ub2e4\ub978 \uc608\ub97c \ubcf4\uc790. * \\(f_{\\theta}={\\theta}_0\\) \uc778 constant function\uc774\uba74 \uc774 \ud568\uc218\uc758 \uacb0\uacfc\uac12\uc740 \uc704\uc5d0\uc11c \uc5b8\uae09\ud55c Nadaraya-Watson estimate\ub97c \uac16\ub294\ub2e4. * \\(f_{\\theta}={\\theta}_0+{\\theta}_1x\\) \ub294 local linear regression model\uc744 \uac16\ub294\ub2e4. \ub4f1\uc774 \uc788\ub2e4. \uc815\ub9ac\ud558\uc790\uba74 \ucd5c\uadfc\uc811\uc774\uc6c3 \ubc29\ubc95\uc740 \ub354 \ub370\uc774\ud130\uc5d0 \uc758\uc874\uc801\uc778 metric\uc744 \uac16\ub294 kernel method\ub77c\uace0 \ubcfc \uc218 \uc788\ub2e4. \uc5ec\uae30\uc11c knn\uc758 metric\uc740 \uc544\ub798\uc640 \uac19\uc73c\uba70, $ \\(K_{k}(x,x_0)=I(||x-x_0|| \\leq||x_{(k)}-x_0||)\\) $ \uc5ec\uae30\uc11c \\(x_{(k)}\\) \ub294 \\(x_0\\) \uc73c\ub85c\ubd80\ud130 k \ubc88\uc9f8 \ub9cc\ud07c \ub5a8\uc5b4\uc9c4 training \uad00\uce21\uce58 \uc774\uace0, \\(I(S)\\) \ub294 \uc9d1\ud569 S\uc758 indicator \ud568\uc218\uc774\ub2e4. \uc774 \ubc29\ubc95\ub860\ub4e4 \uc5ed\uc2dc \uace0\ucc28\uc6d0\uc5d0\uc11c\ub294 \ucc28\uc6d0\uc758 \uc800\uc8fc\ub97c \ubc97\uc5b4\ub098\uae30 \uc704\ud574 \uc801\uc808\ud55c \ubcc0\ud615\uc774 \ud544\uc694\ud558\uba70, \uc774\ub294 6\uc7a5\uc5d0\uc11c \ud6c4\uc220\ud558\uaca0\ub2e4.","title":"2.Kernal Methods and Local Regression"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#3-basis-functions-and-dictionary-methods","text":"\uc774\ubc88\uc5d0 \uc18c\uac1c\ud558\ub294 \ubc29\ubc95\uc740 \uce5c\uc219\ud55c \uc120\ud615\ubaa8\ub378\uacfc \ub2e4\ud56d\ubaa8\ub378\ub85c\uc758 \ud655\uc7a5\uc744 \ud3ec\ud568\ud558\uc9c0\ub9cc, \uadf8 \uc911\uc5d0\uc11c\ub3c4 \ub354 \uc720\uc5f0\ud558\uace0 \uc911\uc694\ud558\uac8c \uc5ec\uaca8\uc9c0\ub294 \ub2e4\uc591\ud55c \ubaa8\ub378\ub4e4\uc744 \ud3ec\ud568\ud55c\ub2e4. \ubaa8\ub378\uc5d0 \ub300\ud55c \ud568\uc218 f \ub294 basis function\uc758 \uc120\ud615\ud655\uc7a5 form\uc744 \ub744\uba70 \uc544\ub798\uc640 \uac19\uc740 \ud615\ud0dc\ub97c \uac16\ub294\ub2e4. $ \\(f_{\\theta}(x)=\\sum\\limits_{m=1}^M{\\theta}_{m}h_{m}(x)\\) $ \uc5ec\uae30\uc11c \\(h_{m}\\) \uc740 \uc785\ub825\ubcc0\uc218 x\uc5d0 \ub300\ud55c \ud568\uc218\uc774\uba70, linear term\uc740 \ubaa8\uc218 \\(\\theta\\) \uc5d0 \uc758\ud574 \uacb0\uc815\ub41c\ub2e4. \uc774 basis functions\uc758 \ud070 \ud2c0\uc740 \uad49\uc7a5\ud788 \ub2e4\uc591\ud55c \ubc29\ubc95\ub860\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc73c\uba70 \uc774\ub97c \uc124\uba85\ud560 \uc218 \uc788\ub294\ub370 \ub354 \uc790\uc138\ud55c \uc608\ub294 \ucd94\ud6c4\uc758 \ucc55\ud130\uc5d0\uc11c \ub2e4\ub904\ubcf8\ub2e4\uace0 \ud55c\ub2e4.(\ucc45\uc774 \ub108\ubb34 \ubd88\uce5c\uc808\ud558\uc8e0 \ucca8\uc5d0,,?) \uac04\ub2e8\ud55c \uc608\uc2dc\ub85c Radial basis functions \uac00 \uc788\ub294\ub370 \uc774\ub294 symmetric\ud55c p\ucc28\uc6e1\ub2ac \ucee4\ub110\uc744 \ud2b9\uc815 centroid\uc5d0 \uc704\uce58\uc2dc\ud0a4\ub294 \ubc29\ubc95\uc774\ub2e4. \uc774\ub294 \uc704\uc758 \uc2dd\uc5d0\uc11c \uc785\ub825\ubcc0\uc218 x\uc5d0 \ub300\ud55c \ud568\uc218\uac00 \\(K_{\\lambda{m}}({\\mu},x)\\) \uc758 \ud615\ud0dc\uc774\uba70 \uc5ec\uae30\uc11c kernal K\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \uc815\uaddc\ud655\ub960 kernel\uc778 \\(K_{\\lambda}({\\mu},x)=e^{-||x-{\\mu}||^2/2{\\lambda}}\\) \uac00 \ub9ce\uc774 \uc0ac\uc6a9\ub41c\ub2e4. \uc5b4\ub5a4 \uc758\ubbf8\ub97c \uac16\ub294\uc9c0, \uc5ec\uae30\uc11c \uc774 \ubaa8\ub378\uc758 \uc608\uc2dc\ub4e4\ub85c \uc5b4\ub5a0\ud55c \ud655\uc7a5\uc744 \ud560 \uc218 \uc788\ub294\uc9c0\ub294 \uc544\uc9c1\uae4c\uc9c0 \ucc55\ud1302\ub9cc\uc744 \uc77d\uace0\ub294 \uc774\ud574\uac00 \ud798\ub4e0 \uac83 \uac19\ub2e4. \ucd94\ud6c4 \ub4b7 \ucc55\ud130\uc5d0\uc11c \ub610 \ub2e4\ub8ec\ub2e4\uace0 \ud55c\ub2e4..","title":"3. Basis Functions and Dictionary Methods"},{"location":"02%20ESL/02_Overview_of_Supervised_Learning/#model-selection-and-the-bias-variance-tradeoff","text":"\ubaa8\ub4e0 \ubaa8\ub378\ub4e4\uc740 \ubaa8\uc218\uc5d0 \ub300\ud55c smoothing\uacfc complexity\uac00 \uacb0\uc815\ub418\uc5b4\uc57c\ud55c\ub2e4. penalty term\uc758 multiplier kernel\uc758 \ub108\ube44 basis functions\uc758 \uac2f\uc218 \ub4f1\uc73c\ub85c \ub9d0\uc774\ub2e4. \uc774\ub7ec\ud55c \uc870\uac74\ub4e4\uc774 \uc5b4\ub5bb\uac8c \uc6c0\uc9c1\uc774\ub0d0\uc5d0 \ub530\ub77c \uc6b0\ub9ac\ub294 training\uacfc test sample\uc5d0 \ub300\ud574 \uc801\uc808\ud55c bias\uc640 variance\uc5d0 \uc5b4\ub5bb\uac8c \uc81c\uc57d\uc744 \uac78\uace0 \ubaa8\ub378\uc744 \uafb8\ub824\ub098\uac08\uc9c0\ub97c \uacb0\uc815\ud560 \uc218 \uc788\ub2e4. knn \ud68c\uadc0 \uc54c\uace0\ub9ac\uc998\uc744 \uc608\uc2dc\ub85c \ud3b8\ud5a5 \ubd84\uc0b0 trade-off \uad00\uacc4\ub97c \uc0b4\ud3b4\ubcf4\uc790. \\(x_0\\) \uc5d0\uc11c\uc758 expected prediction error\ub294 test \ud639\uc740 generalization error \ub77c\uace0 \uc54c\ub824\uc838\uc788\uc73c\uba70 \uc2dd\uc744 \ubd84\ud574\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(EPE_{k}(x_0)=E[(T-\\hat{f}_{k}(x_0))^2|X=x_0]\\) $ $ \\(= {\\sigma}^2+[Bias^2(\\hat{f}_{k}(x_0))+Var_{\\tau}(\\hat{f}_{k}(x_0))]\\) $ $ \\(= {\\sigma}^2 + [f(x_0)-\\frac{1}{k}\\sum\\limits_{l=1}^kf(x_{(l)})]^2+ \\frac{{\\sigma}^2}{k}\\) $ \uc774\ub294 3 terms\ub85c \ub098\ub220\uc11c \ud45c\ud604\uc2dd\uc744 \uc0b4\ud3b4\ubcfc \uc218 \uc788\ub294\ub370, \uccab\ubc88\uc9f8 \ud140\uc758 \\({\\sigma}^2\\) \uc740 irreducible error\uc774\ub2e4. \uc989 \uc6b0\ub9ac\uac00 \uc124\ub839 \ucc38\uac12\uc758 \\(f(x_0)\\) \ub97c \uc54c\uace0\uc788\ub2e4\ud558\ub354\ub77c\ub3c4 \uc774\ub294 \uc0c8\ub85c\uc6b4 test target\uc758 \ubd84\uc0b0\uc774\uae30\uc5d0 \uc6b0\ub9ac\uc758 control\uc744 \ubc97\uc5b4\ub09c \ubcc0\uc218\uc774\ub2e4. \ub450\ubc88\uc9f8 term\uc740 \ud3b8\ud5a5\uc758 \uc81c\uacf1\uc744 \uc758\ubbf8\ud558\uba70 \ud3b8\ud5a5\uc774\ub780 \\(\\hat{f}_{k}(x_0)\\) \uc640 \uc2e4\uc81c \\(f({x_0})\\) \uac04\uc758 \ucc28\uc774\ub97c \ub9d0\ud55c\ub2e4. \uc774\ub294 k\uc5d0 \ub300\ud574 \ub2e8\uc870\uc99d\uac00 \ud568\uc218\uc774\uba70 k\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ucee4\uc9c4\ub2e4. \uc138\ubc88\uc9f8 term\uc740 new target\uc758 \ubd84\uc0b0\uc744 k\ub85c \ub098\ub208 \uac83\uc774\ubbc0\ub85c, k\uac00 \ucee4\uc9d0\uc5d0 \ub530\ub77c \uac12\uc774 \uac10\uc18c\ud55c\ub2e4. \uc704\uc758 \uad00\uacc4\ub97c \ud1b5\ud574 \uc6b0\ub9ac\ub294 \ubaa8\ub378\ub4e4\uc774 \uc608\uce21\uc5d0 \uc788\uc5b4\uc11c \uac16\uace0 \uc788\ub294 \ub51c\ub808\ub9c8 bias-variance trade off\ub97c \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc6b0\ub9ac\ub294 \ud1b5\uc0c1\uc801\uc73c\ub85c \ud3b8\ud5a5\ubd84\uc0b0\uac04\uc758 \ub51c\ub808\ub9c8\ub97c \uc904\uc774\uace0 test error\ub97c \ucd5c\uc18c\ud654 \ud558\uae30\uc704\ud574 \ubaa8\ub378\uc758 complexity\ub97c \uc815\ud558\ub294\ub370, \uc77c\ubc18\uc801\uc73c\ub85c \uc6b0\ub9ac\ub294 test error\uc5d0 \ub300\ud55c \ub69c\ub837\ud55c \ucd94\uc815\uac12\uc73c\ub85c MSE(=training error)\ub97c \uc0ac\uc6a9\ud55c\ub2e4. \ud558\uc9c0\ub9cc \ubd88\ud589\ud558\uac8c\ub3c4 \uc774\ub294 model complexity\uac00 \uc801\uc808\ud558\uac8c \uace0\ub824\ub418\uc9c0 \uc54a\ub294\ub2e4\uba74 training error\ub294 test error\uc5d0 \ub300\ud574 \uc88b\uc740 \ucd94\uc815\uac12\uc774 \ub418\uc9c0 \ubabb\ud55c\ub2e4. prediction \ubc29\ubc95\uc5d0\uc11c test error\ub97c \uce21\uc815\ud558\ub294 \ubc29\ubc95\ub4e4\uc740 \ucd94\ud6c4 7\uc7a5\uc5d0\uc11c \ub354 \ub17c\uc758 \ud574\ubcf4\uace0 \ub9e4\uc6b0 \ucc1c\ucc1c\ud558\uc9c0\ub9cc 2\uc7a5\uc740 \uc5ec\uae30\uc11c \ub9c8\ubb34\ub9ac \ud558\uaca0\ub2e4.","title":"Model Selection and the Bias-Variance Tradeoff"},{"location":"02%20ESL/03_Linear_Methods_for_Regression/","text":"Linear Methods for Regression 3.3 Subset Selection \ucd5c\uc18c\uc81c\uacf1\ubc95\uc744 \ud1b5\ud55c \ucd94\uc815\uc774 \uc885\uc885 \ub9cc\uc871\uc2a4\ub7fd\uc9c0 \uc54a\uc740 \uc774\uc720\ub294 \ub450\uac00\uc9c0\uac00 \uc788\ub2e4. prediction accuracy interpretation \ucd5c\uc18c\uc81c\uacf1\ubc95\uc740 \ub0ae\uc740 bias\ub97c \uac16\uc9c0\ub9cc \ub192\uc740 \ubd84\uc0b0\uc744 \uac16\uae30\ub3c4 \ud55c\ub2e4. prediction accuracy \ub294 \ub54c\ub54c\ub85c shrinking \uc774\ub098 \uba87 \ud68c\uadc0\uacc4\uc218\ub4e4\uc744 0\uc73c\ub85c \ucd95\uc18c\uc2dc\ud0a4\ub294 \ubc29\ubc95\uc744 \ud1b5\ud574 \uac1c\uc120\ub418\uae30\ub3c4 \ud558\ub294\ub370, \uc774\ub294 \ub0ae\uc740 bias\ub97c \ud3ec\uae30\ud558\ub294\ub300\uc2e0 predicted value\uc758 \ubd84\uc0b0\uc744 \uc904\uc784\uc73c\ub85c\uc368 \uc804\uccb4\uc801\uc778 \ubaa8\ub378\uc815\ud655\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4. \ub9cc\uc57d \ub9ce\uc740 predictors\uac00 \uc874\uc7ac\ud55c\ub2e4\uba74 \uc6b0\ub9ac\ub294 \uc885\uc885 \uc608\uce21\uc5d0 \uc880 \ub354 \uac15\ud55c \uc601\ud5a5\uc744 \uc8fc\ub294 smaller subset\uc73c\ub85c \ubaa8\ub378\uc744 \uafb8\ub9ac\ub824 \ud560 \uac83\uc774\ub2e4. \ud070 \uadf8\ub9bc\uc744 \uc704\ud574 \uc6b0\ub9ac\ub294 \uae30\uaebc\uc774 \uba87\uba87 \uc601\ud5a5\ub825\uc774 \uc801\uc740 detail\ud55c \ubd80\ubd84\ub4e4\uc744 \ubc84\ub9ac\uae30\ub3c4 \ud55c\ub2e4. \uc774\ubc88 section\uc5d0\uc11c\ub294 \uba87\uac00\uc9c0\uc758 variable subset selection \ubc29\ubc95\uc744 \uc0b4\ud3b4\ubcf4\ub3c4\ub85d \ud560 \uac83\uc774\ub2e4. \uc774\ub7ec\ud55c \uc5ec\ub7ec \ubc29\ubc95\ub4e4\uc740 \uacb0\uad6d model selection\uc758 \ud55c \uacfc\uc815\uc774\uba70 \uc774\ub294 \uc120\ud615 \ubaa8\ub378\uc5d0\ub9cc \uad6d\ud55c\ub418\uc5b4\uc788\uc9c0\uc54a\uace0, \ucd94\ud6c4\uc5d0 \ub2e4\ub978 \ubaa8\ub378\uc5d0\uc11c\uc758 \uc801\uc6a9\uc744 \ub2e4\ub8f0 \uac83\uc774\ub2e4. \uc774\uc81c\ubd80\ud130 \uba87\uac00\uc9c0 choosing subset method\ub97c \uc0b4\ud3b4\ubcf4\uc790. 1. Best-Subset Selection Best subset regression\uc740 \uac00\ub2a5\ud55c \ubaa8\ub4e0 \ubaa8\ub378\uc744 \uace0\ub824\ud558\uc5ec \uadf8 \uc911 \uac00\uc7a5 \uc88b\uc740 \ubaa8\ub378\uc774 \ubb34\uc5c7\uc778\uc9c0\ub97c subset size k\ub97c \ud1b5\ud574 \ucc3e\uc544\uc8fc\ub294 \ubc29\ubc95\uc774\ub2e4. \\(k \\in \\{0,1,2,..,p\\}\\) \uac04\ub2e8\ud55c \uc608\uc2dc\ub97c \ud1b5\ud574 \uc704\uc758 subset selection \ubc29\ubc95\uc744 \ud655\uc778\ud574\ubcf4\uc790. library(ISLR) names(Hitters) ## [1] \"AtBat\" \"Hits\" \"HmRun\" \"Runs\" \"RBI\" ## [6] \"Walks\" \"Years\" \"CAtBat\" \"CHits\" \"CHmRun\" ## [11] \"CRuns\" \"CRBI\" \"CWalks\" \"League\" \"Division\" ## [16] \"PutOuts\" \"Assists\" \"Errors\" \"Salary\" \"NewLeague\" Hitters <- na.omit(Hitters) #Best Subset Selection method! #Best Subset Selection\uc774 search\ud558\ub294 predictor \uac2f\uc218\uc758 \ub514\ud3f4\ud2b8\uac12\uc740 8 #\ubcc0\uc218\uc758 \uac2f\uc218\uac00 \ub9ce\uc9c0 \uc54a\uc73c\ub2c8 max\ub85c \ud0d0\uc0c9\ud574\ubcf4\uc790 library(leaps) bss <- regsubsets(Salary~.,Hitters,nvmax=length(names(Hitters))-1) summary(bss) ## Subset selection object ## Call: regsubsets.formula(Salary ~ ., Hitters, nvmax = length(names(Hitters)) - ## 1) ## 19 Variables (and intercept) ## Forced in Forced out ## AtBat FALSE FALSE ## Hits FALSE FALSE ## HmRun FALSE FALSE ## Runs FALSE FALSE ## RBI FALSE FALSE ## Walks FALSE FALSE ## Years FALSE FALSE ## CAtBat FALSE FALSE ## CHits FALSE FALSE ## CHmRun FALSE FALSE ## CRuns FALSE FALSE ## CRBI FALSE FALSE ## CWalks FALSE FALSE ## LeagueN FALSE FALSE ## DivisionW FALSE FALSE ## PutOuts FALSE FALSE ## Assists FALSE FALSE ## Errors FALSE FALSE ## NewLeagueN FALSE FALSE ## 1 subsets of each size up to 19 ## Selection Algorithm: exhaustive ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns ## 1 ( 1 ) \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ## 2 ( 1 ) \" \" \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ## 3 ( 1 ) \" \" \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ## 4 ( 1 ) \" \" \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ## 5 ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ## 6 ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \"*\" \" \" \" \" \" \" \" \" \" \" ## 7 ( 1 ) \" \" \"*\" \" \" \" \" \" \" \"*\" \" \" \"*\" \"*\" \"*\" \" \" ## 8 ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \"*\" \" \" \" \" \" \" \"*\" \"*\" ## 9 ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\" ## 10 ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\" ## 11 ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\" ## 12 ( 1 ) \"*\" \"*\" \" \" \"*\" \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\" ## 13 ( 1 ) \"*\" \"*\" \" \" \"*\" \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\" ## 14 ( 1 ) \"*\" \"*\" \"*\" \"*\" \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\" ## 15 ( 1 ) \"*\" \"*\" \"*\" \"*\" \" \" \"*\" \" \" \"*\" \"*\" \" \" \"*\" ## 16 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \"*\" \"*\" \" \" \"*\" ## 17 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \"*\" \"*\" \" \" \"*\" ## 18 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \"*\" ## 19 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" ## CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN ## 1 ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ## 2 ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ## 3 ( 1 ) \"*\" \" \" \" \" \" \" \"*\" \" \" \" \" \" \" ## 4 ( 1 ) \"*\" \" \" \" \" \"*\" \"*\" \" \" \" \" \" \" ## 5 ( 1 ) \"*\" \" \" \" \" \"*\" \"*\" \" \" \" \" \" \" ## 6 ( 1 ) \"*\" \" \" \" \" \"*\" \"*\" \" \" \" \" \" \" ## 7 ( 1 ) \" \" \" \" \" \" \"*\" \"*\" \" \" \" \" \" \" ## 8 ( 1 ) \" \" \"*\" \" \" \"*\" \"*\" \" \" \" \" \" \" ## 9 ( 1 ) \"*\" \"*\" \" \" \"*\" \"*\" \" \" \" \" \" \" ## 10 ( 1 ) \"*\" \"*\" \" \" \"*\" \"*\" \"*\" \" \" \" \" ## 11 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \" \" ## 12 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \" \" ## 13 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" ## 14 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" ## 15 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" ## 16 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" ## 17 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" ## 18 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" ## 19 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" bss.table <- summary(bss) names(bss.table) ## [1] \"which\" \"rsq\" \"rss\" \"adjr2\" \"cp\" \"bic\" \"outmat\" \"obj\" par(mfrow=c(2,2)) plot(bss.table$rss,xlab=\"# of Variables\",ylab=\"RSS\",type=\"l\") plot(bss.table$adjr2,xlab=\"# of Variables\", ylab =\"Adjusted R-square\",type=\"l\") points(which.max(bss.table$adjr2),bss.table$adjr2[which.max(bss.table$adjr2)],col=\"red\",cex=2,pch=20) plot(bss.table$cp,xlab=\"# of Variables\", ylab =\"Cp\",type=\"l\") points(which.min(bss.table$cp),bss.table$cp[which.min(bss.table$cp)],col=\"red\",cex=2,pch=20) plot(bss.table$bic,xlab=\"# of Variables\", ylab =\"BIC\",type=\"l\") points(which.min(bss.table$bic),bss.table$bic[which.min(bss.table$bic)],col=\"red\",cex=2,pch=20) \uc704\ub294 ISLR \ud328\ud0a4\uc9c0\uc758 Hitters \ub370\uc774\ud130\ub97c \uac00\uc838\uc654\uc73c\uba70 best subset selection\uc758 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\uae30 \uc704\ud574 leaps \ud328\ud0a4\uc9c0\uc758 regsubsets\ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc600\ub2e4. \uccab\ubc88\uc9f8 \ud50c\ub86f\uc744 \ud655\uc778\ud574\ubcf4\uba74 RSS\ub294 subset k\uc758 \uc0ac\uc774\uc988\uac00 \ucee4\uc9c8\uc218\ub85d \uac10\uc18c\ud558\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc774\ub294\ub370, \uc774\ub294 \uc6b0\ub9ac\uac00 \ubb34\uc791\uc815 \ubcc0\uc218\uc758 \uac2f\uc218\ub97c \ub298\ub9ac\ub294 \uac83\uc774 \uac00\uc7a5 \uc88b\uc740 subset selection\uc774\ub77c\ub294 \uac83\uc744 \uc758\ubbf8\ud558\uc9c0 \uc54a\ub294\ub2e4. training RSS\ub294 full model\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud68c\uadc0\ub97c \ub3cc\ub838\uc744 \ub54c \uac00\uc7a5 \ub0ae\uc740 \uac12\uc744 \uac16\uae30 \ub54c\ubb38\uc5d0 RSS\ub9cc\uc73c\ub85c\ub294 \uc88b\uc740 \ubaa8\ub378\uc774\ub77c \uacb0\ub860\uc9d3\uae30 \uc5b4\ub835\ub2e4. \ub610\ud55c \uc774\ub294 \uacfc\uc801\ud569\uc73c\ub85c \uc5f0\uacb0\ub418\uc5b4 test RSS\ub294 \uc815\ubc18\ub300\uc758 \uacb0\uacfc\ub97c \uac00\uc838\uc62c \uc218 \uc788\ub2e4. \uadf8\ub9ac\ud558\uc5ec \ub098\uba38\uc9c0 \uc218\uc815\uacb0\uc815\uacc4\uc218\uc640 mallow's Cp, BIC \uc9c0\ud45c\ub97c \ucd94\uac00\ub85c \ud655\uc778\ud558\uc5ec \uacb0\uc815\uacc4\uc218\uc758 \ucd5c\uace0\uc810\uacfc Cp, BIC\uc758 \ucd5c\uc18c\uc810\uc744 \ucc0d\uc5b4 \uadf8\ub54c\uc758 size k\ub97c \ube68\uac04 \uc810\uc73c\ub85c \ud45c\uc2dc\ud558\uc600\ub2e4. coef(bss,5) ## (Intercept) AtBat Hits CRBI DivisionW ## 97.7684116 -1.4401428 7.1753197 0.6882079 -129.7319386 ## PutOuts ## 0.2905164 \uc704\uc640 \uac19\uc774 \ud2b9\uc815 size k\uc5d0\uc11c\uc758 \ud68c\uadc0\uacc4\uc218 \ub610\ud55c \ud655\uc778\ud560 \uc218 \uc788\ub2e4. 2&3. Forward, backward Stepwise Selection Best Subset Selection\uae30\ubc95\uc740 predictors\uc758 \uac2f\uc218\uac00 40\uac1c \uc774\uc0c1\uc774 \ub118\uc5b4\uac00\uba74 \ub9e4\uc6b0 \ube44\ud6a8\uc728\uc801\uc774\uba70 \uacc4\uc0b0\uc774 \ubd88\uac00\ub2a5\ud558\ub2e4\ub294 \ub2e8\uc810\uc774 \uc788\ub2e4. \uadf8\ub9ac\ud558\uc5ec \uc774\uc5d0 \ub300\ud55c \ub300\uc548\uc73c\ub85c \uc0ac\uc6a9\ub418\ub294 \uac83\uc774 \uc804\uc9c4\uc120\ud0dd\ubc95\uacfc \ud6c4\uc9c4\uc81c\uac70\ubc95 \ub4f1\uc774 \uc788\ub2e4. \uc804\uc9c4\uc120\ud0dd\ubc95\uc740 intercept\ub9cc \uac16\uace0 \uc788\ub294 reduced model\uc5d0\uc11c \uc2dc\uc791\ud558\uc5ec model fit\uc744 \uac1c\uc120\uc2dc\ud0ac \uc218 \uc788\ub294 predictor\ub4e4\uc744 \ucd94\uac00\ud558\ub294 \ubc29\ubc95\uc774\uba70, \ud6c4\uc9c4\uc81c\uac70\ubc95\uc740 full model\uc5d0\uc11c \uc2dc\uc791\ud558\uc5ec \uc804\uc9c4\uc120\ud0dd\ubc95\uacfc \ubc18\ub300\uc758 \ubc29\ubc95\uc73c\ub85c \ubaa8\ub378\uc744 \ud615\uc131\ud55c\ub2e4. library(ggplot2) library(tidyverse) ## \u2500\u2500 Attaching packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 1.2.1 \u2500\u2500 ## \u2714 tibble 2.0.1 \u2714 purrr 0.3.0 ## \u2714 tidyr 0.8.2 \u2714 dplyr 0.7.8 ## \u2714 readr 1.3.1 \u2714 stringr 1.4.0 ## \u2714 tibble 2.0.1 \u2714 forcats 0.3.0 ## \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500 ## \u2716 dplyr::filter() masks stats::filter() ## \u2716 dplyr::lag() masks stats::lag() bss.f <- regsubsets(Salary~.,Hitters,nvmax=length(names(Hitters))-1,method=\"forward\") bss.b <- regsubsets(Salary~.,Hitters,nvmax=length(names(Hitters))-1,method=\"backward\") bss.f.table <- summary(bss.f) bss.b.table <- summary(bss.b) names(bss.f.table) ## [1] \"which\" \"rsq\" \"rss\" \"adjr2\" \"cp\" \"bic\" \"outmat\" \"obj\" x <- seq(1,19,1) y1 <- bss.table$bic y2 <- bss.f.table$bic y3 <- (bss.b.table$bic) df <- data.frame(x,y1,y2,y3) df2=gather(df,key,value,-x) g<- ggplot(df2,aes(x=x,y=value,color=key))+ geom_jitter() g<-g+ scale_colour_hue(labels=c('bss','forward','backward')) g <- g+ylab(\"BIC\")+xlab(\"Number of Variables\") g 4 Forward-Stagewise Regression \uc774 \ubc29\ubc95\uc740 \uc704\uc5d0\uc11c \uc18c\uac1c\ud55c Forward stepwise\ubcf4\ub2e4\ub294 \uc880 \ub354 \uc81c\uc57d\uc774 \ub9ce\ub2e4. Forward stepwise\uc640 \ube44\uc2b7\ud558\uac8c \uc774 \ub610\ud55c intercept\ub9cc \uc788\ub294 \ubaa8\ub378\uc5d0\uc11c \uc2dc\uc791\ud558\uc9c0\ub9cc \uc774\ub54c\uc758 intercept \uac00 \\(\\bar{Y}\\) \uc774\ub2e4. Centered\ub41c predictors\uc758 \uacc4\uc218\ub97c \uc2dc\uc791\uc774 0\uc774\ub3c4\ub85d \ud558\ub294 \uac83\uc774\ub2e4. \uac01 \ub2e8\uacc4\uc5d0\uc11c \uc54c\uace0\ub9ac\uc998\uc740 \uc9c0\uae08\uc758 \uc794\ucc28\uc640 \uac00\uc7a5 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 \ubcc0\uc218\ub97c \ud655\uc778\ud574 \uc774\uc5d0 \ub300\ud55c \uc794\ucc28\uc640 \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\ub97c \ud558\uc5ec \uc5bb\uc740 \uacc4\uc218\ub97c \uc0ac\uc6a9\ud55c\ub2e4. \uc774 \uc791\uc5c5\uc740 \uc794\ucc28\uac00 \uc5b4\ub290 \ubcc0\uc218\uc640\ub3c4 \uc720\uc758\ud55c \uad00\uacc4\ub97c \uac00\uc9c0\uc9c0 \uc54a\uc744\ub54c \uae4c\uc9c0 \uacc4\uc18d\ub41c\ub2e4. \uc774 \ubc29\ubc95\uc740 Stepwise \ubc29\ubc95\uacfc\ub294 \ub2ec\ub9ac \ud55c\ubc88 \ubaa8\ub378\uc5d0 \ud3ec\ud568\ub41c \ubcc0\uc218\ub4e4\uc740 \uc774\ud6c4\uc5d0 \uc81c\uac70\ub418\uc9c0 \uc54a\ub294\ub2e4. Shrinkage Methods \uac00\ub054\uc740 predictors\ub4e4\uc758 \ubd80\ubd84\uc9d1\ud569\uc744 \ub0a8\uae30\uace0 \ub098\uba38\uc9c0 \ubd80\ubd84\ub4e4\uc744 \ubc84\ub9bc\uc73c\ub85c\uc368 subset selection \ubc29\ubc95\uc740 full model\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c \ubcf4\ub2e4 \ub354 \ub0ae\uc740 prediction error\ub97c \uac16\uace0 \uc6a9\uc774\ud55c \ud574\uc11d\uc744 \uc774\ub04c\uae30\ub3c4 \ud55c\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub294 discrete process \uc778\ub370, (\ubcc0\uc218\ub97c \ubc84\ub9ac\uac70\ub098 \ub0a8\uae30\uac70\ub098 \ud558\ub294 \uac83\uc740) \uc774\ub294 \uc885\uc885 high variance\ub97c \uac16\uace0, prediction error of the full model\uc744 \uc904\uc774\uc9c0 \ubabb\ud558\uae30\ub3c4 \ud55c\ub2e4. Shrinkage methods\ub294 \uc880\ub354 continuous\ud558\uba70, \ub192\uc740 \ubcc0\ub3d9\uc131\uc5d0 \ud06c\uac8c \uc81c\uc57d\uc744 \ubc1b\uc9c0 \uc54a\ub294 \ubc29\ubc95\uc774\uae30\uc5d0, \uc774\ubc88 \uc18c\ub2e8\uc6d0\uc5d0\uc11c\ub294 \uc774\uc5d0 \ub300\ud574\uc11c \ub2e4\ub904\ubcf4\uaca0\ub2e4. Ridge Regression Ridge regression\uc740 coeeicients\uc758 size\uc5d0 penalty \ub97c \uac78\uc5b4 \uc774\ub4e4\uc744 \ucd95\uc18c\uc2dc\ud0a4\ub294 \ubc29\ubc95\uc774\ub2e4. Ridge \ud68c\uadc0\ub294 \uace0\ub85c penalized\ub41c residual sum of squares\ub97c \ucd5c\uc18c\ud654 \ud558\ub294 \ubc29\ud5a5\uc73c\ub85c \ubaa8\ub378\uc744 \ub9cc\ub4e4\uac8c \ub418\ub294\ub370 \uc774\ub97c \uc218\uc2dd\uc73c\ub85c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\[{\\hat{\\beta}}^{ridge}=argmin_{\\beta}\\{\\sum_{i=1}^N(y_{i}-{\\beta}_0-\\sum_{j=1}^px_{ij}{\\beta}_{j})^2+{\\lambda}\\sum_{j=1}^p{\\beta}_{j}^2\\}\\] \uc5ec\uae30\uc11c \\({\\lambda} \\geq 0\\) \uc740 \ucd95\uc18c\uc758 \uc815\ub3c4\ub97c \uacb0\uc815\ud558\ub294 complexity parameter\uc778\ub370, \uc774 \ub78c\ub2e4\uc758 \uac12\uc774 \ucee4\uc9c8\uc218\ub85d \uacc4\uc218\ub97c 0\uc73c\ub85c \ubcf4\ub0b4\ub294 \ucd95\uc18c\uac00 \ub354 \ub9ce\uc774 \uc774\ub8e8\uc5b4\uc9c4\ub2e4. \uc704 \uc2dd\uc740 \uc544\ub798\uc640 \uac19\uc774\ub3c4 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\[{\\hat{\\beta}}^{ridge}=argmin_{\\beta}\\sum_{i=1}^N(y_{i}-{\\beta}_0-\\sum_{j=1}^px_{ij}{\\beta}_{j})^2,\\] \\[subject\\ to\\ \\sum_{j=1}^p{\\beta}_{j}^2 \\leq t \\] \uac01 \uc2dd\uc5d0\uc11c \\({\\lambda}\\) \uc640 \\(t\\) \uc758 \uac12\uc740 1:1\ub300\uc751\uc774 \uc131\ub9bd\ud55c\ub2e4. \uc2dd\uc744 \ud655\uc778\ud574\ubcf4\uba74 \uc6b0\ub9ac\ub294 intercept \ud56d\uc5d0 \ub300\ud574\uc11c\ub294 penalization\uc774 \uc9c4\ud589\ub418\uc9c0 \uc54a\uc558\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub294\ub370, \uc774\ub294 target \\(y_{i}\\) \uc5d0 c\ub9cc\ud07c\uc758 \uc0c1\uc218\ub97c \ub354\ud574\uc8fc\ub294 \uac83\uc774 \uc608\uce21\uac12\uc774 c\ub9cc\ud07c \uc774\ub3d9\ud55c\ub2e4\ub294 \uc758\ubbf8\uc640 \ub2e4\ub974\uae30 \ub54c\ubb38\uc774\ub2e4. \\({\\lambda}\\) \ub97c \ud1b5\ud574 \uc5bb\uc740 penalized RSS\uc758 \ud574\ub294 centered\ub41c predictors\ub97c \uc0ac\uc6a9\ud55c \uc7ac\ubaa8\uc218\ud654\ub85c \ub450 \ubd80\ubd84\uc73c\ub85c \ucabc\uac1c\uc9c0\ub294\ub370, \uc774\ub294 \\(x_{ij}\\) \uac00 \\(x_{ij}-\\bar{x}_{j}\\) \ub85c \ub300\uccb4\ub428\uc744 \uc758\ubbf8\ud78c\ub2e4. \uc5ec\uae30\uc11c intercept \\({\\beta}_0\\) \ub97c \\(\\bar{Y}\\) \ub85c \ucd94\uc815\ud558\uba70, \ub098\uba38\uc9c0 \uacc4\uc218\ub4e4\uc740 intercept\uac00 \uc5c6\ub294 ridge regression \uc73c\ub85c centered\ub41c \\(x_{ij}\\) \ub4e4\uc744 \ucd94\uc815\ud558\uac8c \ub41c\ub2e4. \uc774\ub7ec\ud55c \uc911\uc2ec\ud654\uac00 \uc218\ud589\ub418\uc5c8\ub2e4\uace0 \uac00\uc815\ud588\uc744\ub54c, input matrix \\(\\mathbf{X}\\) \ub294 \\(p+1\\) \uac1c\uac00 \uc544\ub2cc \\(p\\) \uac1c\uc758 \uc5f4\uc744 \uac16\ub294\ub2e4. \uc774\ub7ec\ud55c \uc2dd\uacfc \uadf8\ub54c\uc758 \ud574\ub97c \ud589\ub82c\ub85c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(RSS({\\lambda})=(\\mathbf{y}-\\mathbf{X}{\\beta})^T(\\mathbf{y}-\\mathbf{X}{\\beta})+{\\lambda}{\\beta}^T{\\beta}\\) $ $ \\({\\hat{\\beta}}^{ridge}=(\\mathbf{X}^T{\\mathbf{X}}+{\\lambda}\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\) $ \\(\\mathbf{I}\\) \ub294 \\(p\\times{p}\\) \uc758 \ud56d\ub4f1\ud589\ub82c\uc774\ub2e4. \uc6b0\ub9ac\ub294 quadratic penalty \\({\\beta}^T{\\beta}\\) \ub97c \uc0ac\uc6a9\ud558\uc600\uae30 \ub54c\ubb38\uc5d0, ridge regression \uc758 \ud574\ub294 \\(\\mathbf{y}\\) \uc5d0 \ub300\ud55c \uc120\ud615 \ud568\uc218\uac00 \ub41c\ub2e4. \uc774 \ud574\ub294 \uc5ed\ud589\ub82c\uc744 \uad6c\ud558\uae30 \uc804\uc5d0 \\(\\mathbf{X}^T\\mathbf{X}\\) \uc758 \ub300\uac01\uc6d0\uc18c\uc5d0 \uc591\uc758 \uc0c1\uc218\ub97c \ub354\ud55c\ub2e4. \uc774\ub294 \ubb38\uc81c\ub97c nonsingular \ud558\uac8c \ud574\uc8fc\uba70 \\(\\mathbf{X}^T\\mathbf{X}\\) \uac00 full rank\uac00 \uc544\ub2c8\uc5b4\ub3c4 \uad1c\ucc2e\uac8c \ud574\uc900\ub2e4. \uc544\ub798\uc5d0 \ucca8\ubd80\ud55c \uadf8\ub9bc\uc740 prostate cancer \uc608\uc81c\ub97c \ud1b5\ud574 \uc5bb\uc740 \uadf8\ub9bc\uc778\ub370, \uc774\ub294 \\(df({\\lambda})\\) \uc5d0 \ub530\ub77c ridge \ud68c\uadc0\uacc4\uc218\uc758 \ucd94\uc815\uce58\uac00 \ubcc0\ud654\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc900\ub2e4. \\(df({\\lambda})\\) \ub294 penalty \\({\\lambda}\\) \uc5d0 \ub300\ud55c \ud568\uc218\ub85c effective degress of freedom \uc744 \ubcf4\uc5ec\uc900\ub2e4. \ub9cc\uc57d \uc785\ub825\ubcc0\uc218\ub4e4\uc774 orthonormal \ud558\ub2e4\uba74, ridge\uc758 \ucd94\uc815\uce58\ub294 \ucd5c\uc18c\uc81c\uacf1\ubc95 \ucd94\uc815\uce58\uc758 scaled version\uc774 \ub41c\ub2e4. \\({\\hat{\\beta}}^{ridge}=\\frac{\\hat{\\beta}}{1+{\\lambda}}\\) \uc911\uc2ec\ud654\ub41c \uc785\ub825\ud589\ub82c \\(\\mathbf{X}\\) \uc758 \uace0\uc720\uac12\ubd84\ud574( singular value decomposition , SVD)\ub97c \ud1b5\ud574 \uc6b0\ub9ac\ub294 ridge regression\uc758 \ud2b9\uc131\uc5d0 \ub300\ud574 \ucd94\uac00\uc801\uc778 insight\ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4. \uc774\ub7ec\ud55c \ubd84\ud574\ub294 \ub2e4\uc591\ud55c \ud1b5\uacc4\uc801 \ubc29\ubc95\uc5d0 \ub300\ud55c \ubd84\uc11d\uc5d0 \ub9e4\uc6b0 \uc6a9\uc774\ud55c \ubc29\ubc95\uc774\ub2e4. \\(N\\times{p}\\) \\(\\mathbf{X}\\) \ud589\ub82c\uc758 \uace0\uc720\uac12\ubd84\ud574\ub294 \uc544\ub798\uc640 \uac19\uc740 form\uc744 \uac16\ub294\ub2e4. \\[\\mathbf{X}=\\mathbf{U}\\mathbf{D}\\mathbf{V}^T\\] \uc5ec\uae30\uc11c \\(\\mathbf{U}\\) \uc640 \\(\\mathbf{V}\\) \ub294 \\(N\\times{p}\\) , \\(p\\times{p}\\) \uc758 \uc9c1\uad50\ud589\ub82c\uc774\uace0, \\(\\mathbf{U}\\) \uc758 \uc5f4\uc740 \\(\\mathbf{X}\\) \uc758 \uc5f4\uacf5\uac04\uc744 span\ud558\uba70, \\(\\mathbf{V}\\) \uc758 \uc5f4\ub4e4\uc774 \ud589\uacf5\uac04\uc744 span\ud55c\ub2e4. \\(\\mathbf{D}\\) \ub294 \\(p\\times{p}\\) \uc758 \ub300\uac01\ud589\ub82c\uc774\uba70, \ub300\uac01\uc6d0\uc18c\ub4e4\uc740 \\(d_1\\geq{d_2}\\geq...\\geq{d_{p}}\\geq0\\) \uc778 \\(\\mathbf{X}\\) \uc758 singular value\ub4e4\uc774\ub2e4. \ub9cc\uc57d \ud55c\uac1c\ub77c\ub3c4 \\(d_j=0\\) \uc774\ub77c\uba74, \\(\\mathbf{X}\\) \ub294 singular\uc774\ub2e4. SVD\ub97c \ud1b5\ud574 \uc6b0\ub9ac\ub294 least squares fitted vector\ub97c \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\[\\mathbf{X}{\\hat{\\beta}^{ls}}=\\mathbf{X}{(\\mathbf{X}^T\\mathbf{X})}^{-1}\\mathbf{X}^T\\mathbf{y}\\] \\[=\\mathbf{X}(\\mathbf{V}\\mathbf{D}^2\\mathbf{V}^T)^{-1}\\mathbf{X}^T\\mathbf{y}\\] \\[=\\mathbf{X}\\mathbf{V}\\mathbf{D}^{-2}\\mathbf{V}^T\\mathbf{X}^T\\mathbf{y}\\] \\[=\\mathbf{U}\\mathbf{D}\\mathbf{V}^T\\mathbf{V}\\mathbf{D}^{-2}\\mathbf{V}^T\\mathbf{X}^T\\mathbf{y}\\] \\[=\\mathbf{U}\\mathbf{D}^{-1}\\mathbf{V}^T\\mathbf{X}^T\\mathbf{y}\\] \\[=\\mathbf{U}\\mathbf{D}^{-1}\\mathbf{V}^T\\mathbf{V}\\mathbf{D}\\mathbf{U}^T\\mathbf{y}=\\mathbf{U}\\mathbf{U}^T\\mathbf{y}\\] \ub85c \ud45c\ud604\uac00\ub2a5\ud558\ub2e4. \uc5ec\uae30\uc11c \\(\\mathbf{U}^T\\mathbf{y}\\) \ub294 orthonormal basis \\(\\mathbf{U}\\) \uc5d0 \ub300\ud55c \\(\\mathbf{y}\\) \uc758 \uc88c\ud45c\ub4e4\uc774\ub2e4. \uc774 \uc99d\uba85\uc740 \\(\\mathbf{U}\\) \uc640 \\(\\mathbf{V}\\) \uc758 \uc5f4\ub4e4\uc774 orthogonal\ud558\ub2e4\ub294 \uc131\uc9c8\uacfc \uac01 \uc5f4\ub4e4\uc774 \\(\\mathbf{X}\\mathbf{X}^T\\) \uc640 \\(\\mathbf{X}^T\\mathbf{X}\\) \uc758 eigenvector\ub77c\ub294 \uc131\uc9c8\uc744 \uc774\uc6a9\ud558\uc5ec \uc2dd\uc804\uac1c\ub97c \ud55c \uac83\uc774\ub2e4. \uadf8\ub807\ub2e4\uba74 \uc774\uc81c ridge solutions\uc740 \uc544\ub798\uc640 \uac19\uc774 \uad6c\ud560 \uc218 \uc788\ub2e4. \\[\\mathbf{X}{\\hat{\\beta}}^{ridge}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X}+{\\lambda}\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\] \\[=\\mathbf{U}\\mathbf{D}\\mathbf{V}^T(\\mathbf{V}\\mathbf{D}^{-2}\\mathbf{V}^T+\\frac{{1}}{\\lambda}\\mathbf{I})\\mathbf{V}\\mathbf{D}\\mathbf{U}^T\\mathbf{y}\\] \\[=\\mathbf{U}\\mathbf{U}^T\\mathbf{y}+\\frac{1}{\\lambda}\\mathbf{U}\\mathbf{D}^2\\mathbf{U}^T\\mathbf{y}\\] \\[=\\mathbf{U}(\\mathbf{I}+\\frac{1}{\\lambda}\\mathbf{D}^2)\\mathbf{U}^T\\mathbf{y}\\] \\[=\\mathbf{U}\\mathbf{D}(\\mathbf{D}^{-2}+\\frac{1}{\\lambda})\\mathbf{D}\\mathbf{U}^T\\mathbf{y}\\] \\[=\\mathbf{U}\\mathbf{D}(\\mathbf{D}^2+\\lambda\\mathbf{I})^{-1}\\mathbf{D}\\mathbf{U}^T\\mathbf{y}\\] \\[=\\sum_{j=1}^p\\mathbf{u}_{j}\\frac{d_j^2}{d_j^2+\\lambda}\\mathbf{u}_j^T\\mathbf{y}\\] \uc5ec\uae30\uc11c \\(\\mathbf{u}_j\\) \ub4e4\uc740 \\(\\mathbf{U}\\) \uc758 \uc5f4\ub4e4\uc774\ub2e4. \\(\\lambda \\geq 0\\) \uc77c\ub54c \uc6b0\ub9ac\ub294 \\(d_j^2/(d_j^2+\\lambda) \\leq 0\\) \uc784\uc744 \uc54c \uc218 \uc788\ub2e4. \uc120\ud615 \ud68c\uadc0 \ucc98\ub7fc ridge\ud68c\uadc0\ub294 \\(\\mathbf{y}\\) \uc758 \uc88c\ud45c\ub4e4\uc744 \uc815\uaddc\uc9c1\uad50\uae30\uc800 \\(\\mathbf{U}\\) \uc5d0 \ub300\ud574 \uacc4\uc0b0\ud558\uac8c \ub41c\ub2e4. \uc774\ud6c4 \uc774\ub294 \uc774\ub7ec\ud55c \uc88c\ud45c\ub4e4\uc744 \\(d_j^2/(d_j^2+\\lambda)\\) \ub97c \uacf1\ud568\uc73c\ub85c\uc368 \ucd95\uc18c\uc2dc\ud0a4\ub294 \uac83\uc774\ub2e4. \uc774\ub294 \uc989 \ub354 \uc791\uc740 \\(d_j^2\\) \ub97c \uac16\ub294 \uae30\uc800 \ubca1\ud130\ub4e4\uc758 \uc88c\ud45c\uc5d0 \ub354 \ud070 \uc591\uc758 \ucd95\uc18c\uac00 \uc801\uc6a9\ub418\ub294 \uac83\uc774\ub2e4. \\(d_j^2\\) \uac00 \uc791\ub2e4\ub294 \ub9d0\uc740 \ubb34\uc2a8 \ub9d0\uc778\uac00? \uc911\uc2ec\ud654\ub41c \ud589\ub82c \\(\\mathbf{X}\\) \uc758 \\(\\mathbf{SVD}\\) \ub294 \\(\\mathbf{X}\\) \ub0b4\uc758 \ubcc0\uc218\ub4e4\uc758 principal components \ub97c \ud45c\ud604\ud558\ub294 \ub610 \ub2e4\ub978 \ubc29\ubc95\uc774\ub2e4. sample covariance matrix\ub294 \\(\\mathbf{S}=\\mathbf{X^TX}/n\\) \uc774\uba70, SVD\uc5d0 \uc758\ud574 \\(\\mathbf{X^TX}=\\mathbf{VD^2V^T}\\) \ub97c \uc5bb\uac8c \ub418\uba70, \uc774\ub294 \\(\\mathbf{X^TX}\\) \uc758 eigen decomposition\uc774\uba70, eigenvector\ub4e4\uc758 \\(v_j\\) \ub294 \\(\\mathbf{X}\\) \uc758 principal components directions\uc774\ub2e4. \uccab\ubc88\uc9f8 \uc8fc\uc131\ubd84 \ubc29\ud5a5 \\(v_1\\) \uc740 \\(\\mathbf{z}_1=\\mathbf{X}v_1\\) \uc774 \ubaa8\ub4e0 \\(\\mathbf{X}\\) \uc758 \uc5f4\ub4e4\uc758 \uc120\ud615 \uc870\ud569\ub4e4\uc744 normalize\ud55c \uac83\ub4e4 \uc911 \uac00\uc7a5 \ud070 sample variance\ub97c \uac16\ub294 \uc131\uc9c8\uc744 \uac16\ub294\ub2e4. \uc774\ub7ec\ud55c sample variance\ub294 \\(Var(\\mathbf{Z}_1)=Var(\\mathbf{X}v_1)=\\frac{d_1^2}{n}\\) \ub97c \ub9cc\uc871\ud558\uace0, \uc2e4\uc81c\ub85c \\(\\mathbf{z}_1=\\mathbf{X}v_1=\\mathbf{u}_1d_1\\) \uc784 \ub610\ud55c \ubcf4\uc77c \uc218 \uc788\ub2e4. \ub3c4\ucd9c\ub41c \ubcc0\uc218 \\(\\mathbf{z}_1\\) \ub294 \\(\\mathbf{X}\\) \uc758 \uccab\ubc88\uc9f8 \uc8fc\uc131\ubd84\uc73c\ub85c \ubd88\ub9ac\uba70, \ub530\ub77c\uc11c \\(\\mathbf{u}_1\\) \ub294 normalize\ub41c \uccab\ubc88\uc9f8 \uc8fc\uc131\ubd84\uc774\ub77c\uace0 \ubcfc \uc218 \uc788\ub2e4. \uace0\ub85c \uc791\uc740 singular value \\(d_j\\) \ub294 \\(\\mathbf{X}\\) \uc758 \uc5f4\uacf5\uac04\uc758 \ubc29\ud5a5\ub4e4 \uc911 \uc791\uc740 \ubd84\uc0b0\uc744 \uac16\ub294 \ubc29\ud5a5\uacfc \ub300\uc751\ub418\uace0 ridge regression\uc740 \uc774\ub7ec\ud55c \ubc29\ud5a5\ub4e4\uc744 \ub354\uc6b1 \ucd95\uc18c\uc2dc\ud0a8\ub2e4. \uacb0\uad6d \uc791\uc740 singular value\uac12\uc77c \uc218\ub85d \uacc4\uc218\uac00 \ub354\uc6b1 shrink\ub418\ub294 \uac83\uc774\ub2e4. \uc989 ridge regression\uc740 \uc791\uc740 \ubd84\uc0b0\uc744 \uac16\ub294 \ucd95\uc5d0 \ub300\ud574 \uacc4\uc218\ub97c \ucd95\uc18c\uc2dc\ud0b4\uc73c\ub85c\uc368 \uc791\uc740 \ubd84\uc0b0\uc744 \uac16\ub294 \ubc29\ud5a5\uc5d0\uc11c \ub192\uc740 \ubd84\uc0b0\uc744 \uac16\ub294 \uae30\uc6b8\uae30\uac00 \ucd94\uc815\ub418\uc9c0 \uc54a\uac8c\ub054 \ubcf4\ud638\ud558\ub294 \uc5ed\ud560\uc744 \ud574\uc8fc\ub294 \uac83\uc774\ub2e4. \uc774\ub7ec\ud55c \ud574\uc11d\uc5d0 \ud544\uc694\ud55c \uc228\uc740 \uac00\uc815\uc740 output\ubcc0\uc218 \ub610\ud55c \uc785\ub825\uac12\uc758 \ubcc0\ub3d9\uc131\uc744 \uac00\uc7a5 \ub9ce\uc774 \uc124\uba85\ud558\ub294 (\ubd84\uc0b0\uc774 \uc81c\uc77c \ud070) \ucd95\uc5d0\uc11c \uadf8 \ubcc0\ub3d9\uc774 \uac00\uc7a5 \ud074 \uac83\uc774\ub77c\ub294 \uac00\uc815\uc774\ub2e4. \uc774\ub294 \ud544\uc218\uc801\uc774\uc9c0\ub294 \uc54a\uc9c0\ub9cc \ucda9\ubd84\ud788 \ud569\ub9ac\uc801\uc778 \uac00\uc815\uc774\ub77c\uace0 \ud55c\ub2e4. #Do SVD! library(MASS) ## ## Attaching package: 'MASS' ## The following object is masked from 'package:dplyr': ## ## select X <- matrix(c(3,7,0,1,9,3,5,0,0,7),ncol = 2, nrow = 5) XtX <- t(X)%*%X eigen(XtX) ## eigen() decomposition ## $values ## [1] 222.2305288 0.7694712 ## ## $vectors ## [,1] [,2] ## [1,] -0.7929002 0.6093515 ## [2,] -0.6093515 -0.7929002 V <- eigen(XtX)$vectors Vt <- t(V) singular_values <- sqrt(eigen(XtX)$values) singular_values ## [1] 14.9073985 0.8771951 D <- diag(singular_values) U = vector() for(i in 1:length(V[1,])){ U = append(U,1/singular_values[i]*X %*% V[,i]) } U = matrix(U,ncol = length(V[1,])) SVD <- function(X){ XtX = t(X)%*%X V =eigen(XtX)$vectors Vt = t(V) singular_values =sqrt(eigen(XtX)$values) D=diag(singular_values) U=vector() for(i in 1:length(V[1,])){ U = append(U,1/singular_values[i]*X %*% V[,i]) } U=matrix(U,ncol = length(V[1,])) return(list(U=U,D=D,Vt=Vt)) } U <- SVD(X)$U D <- SVD(X)$D Vt<- SVD(X)$Vt U%*%D%*%Vt ## [,1] [,2] ## [1,] 3 3 ## [2,] 7 5 ## [3,] 0 0 ## [4,] 1 0 ## [5,] 9 7 X ## [,1] [,2] ## [1,] 3 3 ## [2,] 7 5 ## [3,] 0 0 ## [4,] 1 0 ## [5,] 9 7 Y <- c(1,0,5,3,2) # \\mathbf{X}{\\hat{\\beta}}^{ls} = UU^Ty check X%*%solve(XtX)%*%t(X)%*%Y ## [,1] ## [1,] -0.2631579 ## [2,] 1.5847953 ## [3,] 0.0000000 ## [4,] 1.0116959 ## [5,] 1.4093567 U%*%t(U)%*%Y ## [,1] ## [1,] -0.2631579 ## [2,] 1.5847953 ## [3,] 0.0000000 ## [4,] 1.0116959 ## [5,] 1.4093567 #ridge case lambda <- 500*diag(2) U%*%D%*%solve(D^2+lambda)%*%D%*%t(U)%*%Y ## [,1] ## [1,] 0.16991962 ## [2,] 0.35051469 ## [3,] 0.00000000 ## [4,] 0.03365766 ## [5,] 0.46379444 Xbeta_ridge <- matrix(0,5,2) for (j in 1:ncol(X)){ Xbeta_ridge[,j] <- ((diag(D)[j]^2)/(diag(D)[j]^2+500))*U[,j]%*%t(U[,j])%*%Y } as.matrix(rowSums(Xbeta_ridge)) ## [,1] ## [1,] 0.16991962 ## [2,] 0.35051469 ## [3,] 0.00000000 ## [4,] 0.03365766 ## [5,] 0.46379444 U%*%D%*%solve(D^2+lambda)%*%D%*%t(U)%*%Y ## [,1] ## [1,] 0.16991962 ## [2,] 0.35051469 ## [3,] 0.00000000 ## [4,] 0.03365766 ## [5,] 0.46379444 Lasso Regression Lasso Regression \ub610\ud55c Ridge\uc640 \uac19\uc740 shrinkage method\uc774\ub2e4. \ucc28\uc774\uac00 \uc788\ub2e4\uba74 Lasso\ub294 \\(L_2\\) \uac00 \uc544\ub2cc \\(L_1\\) Regularization\uc744 \uc218\ud589\ud55c \uac83\uc774\ub2e4. Lasso\ub97c \ud1b5\ud574 \uc5bb\uc740 \ubca0\ud0c0\uc758 \ucd94\uc815\uce58\ub97c \uc2dd\uc73c\ub85c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\[{\\hat{\\beta}}^{lasso}=argmin_{\\beta}\\sum_{i=1}^N(y_{i}-{\\beta}_0-\\sum_{j=1}^px_{ij}{\\beta}_j)^2\\] \\[subject\\ to\\ \\sum_{j=1}^p|{\\beta}_j|\\leq t\\] \ub610 Lasso\uc5d0\uc11c\ub294 Ridge\uc5d0\uc11c\ucc98\ub7fc input predictors\ub4e4\uc744 \ud45c\uc900\ud654\ud558\uc5ec \\({\\beta}_0\\) \uc5d0 \ub300\ud55c \ucd94\uc815\uce58\ub97c \\(\\bar{y}\\) \ub85c \uc5bb\uc5b4 \uc808\ud3b8 \uacc4\uc218\uc5c6\uc774 \ubaa8\ub378\uc801\ud569\uc744 \uc2dc\ud589\ud560 \uc218 \uc788\ub2e4. \uc704\uc758 \uc2dd\uc744 \uc6b0\ub9ac\ub294 Lagrangian form \uc758 \ud615\ud0dc\ub85c\ub3c4 \ud45c\ud604\uac00\ub2a5\ud55c\ub370, \uc774 \uc2dd\uc774 \uc880 \ub354 \uc774\ud574\uac00 \uc218\uc6d4\ud55c\ub4ef\ud558\ub2e4. \\[{\\hat{\\beta}}^{lasso}=argmin_{\\beta}\\{\\sum_{i=1}^N(y_{i}-{\\beta}_0-\\sum_{j=1}^px_{ij}{\\beta}_j)^2+{\\lambda}\\sum_{j=1}^p|{\\beta}_j|\\}\\] \ub300\uccb4\ub41c \\(L_1\\) penalty\ub294 \ud574\uac00 \\(y_i\\) \uc5d0 \ub300\ud574 \ube44\uc120\ud615\uc778 \uc2dd\uc744 \ub9cc\ub4e4\uba70, \uace0\ub85c ridge\uc640 \ub2e4\ub974\uac8c closed form\uc758 \ud615\ud0dc\uac00 \uc874\uc7ac\ud558\uc9c0 \uc54a\ub294\ub2e4. \uc774\ub7ec\ud55c \uc81c\uc57d\uc2dd\uc73c\ub85c \uc778\ud574 t\ub97c \ucda9\ubd84\ud788 \uc791\uac8c \uc904\uc774\ub294 \uac83\uc740 \uba87\uba87 coefficients\ub4e4\uc744 \uc815\ud655\ud558\uac8c 0\uc73c\ub85c \ub9cc\ub4e4 \uac83\uc774\ub2e4. \uc704 \uadf8\ub9bc\uc744 \ud1b5\ud574 \uc6b0\ub9ac\ub294 \ud0c0\uc6d0\ubaa8\uc591\uc758 \ub2a5\uc120\uc744 \uac16\ub294 RSS\uc640 \uadf8 \uc911\uc2ec\uc5d0\uc11c Full LSE\ub97c \uc5bb\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. ridge\uc758 \uc81c\uc57d\uc2dd\uc740 constraint region\uc744 \uc6d0\ubaa8\uc591\uc744 \ub9cc\ub4e4\uace0(Predictor\uac00 \ub450\uac1c\uc778 case), lasso\uc77c\ub54c\ub294 \ub9c8\ub984\ubaa8 \ud615\ud0dc\ub97c \ub748\ub2e4. \uc774\ub294 \ubaa8\uc591\uc744 \ub744\ub294 \uc774\uc720\ub294 \ub450 \ubc29\ubc95\uc758 regularization form\uc774 \ub2e4\ub974\uae30 \ub584\ubb38\uc774\ub2e4. \\[|{\\beta}_1|+|{\\beta}_2| \\leq t \\ , {\\beta}_1^2+{\\beta}_2^2 \\leq t^2\\] lasso\uc758 \uacbd\uc6b0\ub294 \uac01\uc838\uc788\ub294 \ubaa8\uc11c\ub9ac\uac00 \uc788\uae30\uc5d0 \ud574\uac00 \uadf8 \uacf3\uc5d0\uc11c \ubc1c\uc0dd\ud558\uba74 \ud558\ub098\uc758 parameter\ub294 0\uc774 \ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \ucc28\uc6d0\uc774 \ub2e4\ucc28\uc6d0\uc73c\ub85c \ud655\uc7a5\ub41c\ub2e4\uba74 \ub354 \ub9ce\uc740 \ubaa8\uc11c\ub9ac\uc640 \uba74\ub4e4\uc774 \uc874\uc7ac\ud558\uae30 \ub54c\ubb38\uc5d0 \ub354 \ub9ce\uc740 \uc811\uc810\uc774 \ub9cc\ub4e4\uc5b4\uc9c0\uace0 \ub354 \ub9ce\uc740 \uacc4\uc218\ub97c 0\uc73c\ub85c \ubcf4\ub0bc \uae30\ud68c\uac00 \ub298\uc5b4\ub09c\ub2e4.","title":"03 Linear Methods for Regression"},{"location":"02%20ESL/03_Linear_Methods_for_Regression/#linear-methods-for-regression","text":"","title":"Linear Methods for Regression"},{"location":"02%20ESL/03_Linear_Methods_for_Regression/#33-subset-selection","text":"\ucd5c\uc18c\uc81c\uacf1\ubc95\uc744 \ud1b5\ud55c \ucd94\uc815\uc774 \uc885\uc885 \ub9cc\uc871\uc2a4\ub7fd\uc9c0 \uc54a\uc740 \uc774\uc720\ub294 \ub450\uac00\uc9c0\uac00 \uc788\ub2e4. prediction accuracy interpretation \ucd5c\uc18c\uc81c\uacf1\ubc95\uc740 \ub0ae\uc740 bias\ub97c \uac16\uc9c0\ub9cc \ub192\uc740 \ubd84\uc0b0\uc744 \uac16\uae30\ub3c4 \ud55c\ub2e4. prediction accuracy \ub294 \ub54c\ub54c\ub85c shrinking \uc774\ub098 \uba87 \ud68c\uadc0\uacc4\uc218\ub4e4\uc744 0\uc73c\ub85c \ucd95\uc18c\uc2dc\ud0a4\ub294 \ubc29\ubc95\uc744 \ud1b5\ud574 \uac1c\uc120\ub418\uae30\ub3c4 \ud558\ub294\ub370, \uc774\ub294 \ub0ae\uc740 bias\ub97c \ud3ec\uae30\ud558\ub294\ub300\uc2e0 predicted value\uc758 \ubd84\uc0b0\uc744 \uc904\uc784\uc73c\ub85c\uc368 \uc804\uccb4\uc801\uc778 \ubaa8\ub378\uc815\ud655\uc131\uc744 \ud5a5\uc0c1\uc2dc\ud0a8\ub2e4. \ub9cc\uc57d \ub9ce\uc740 predictors\uac00 \uc874\uc7ac\ud55c\ub2e4\uba74 \uc6b0\ub9ac\ub294 \uc885\uc885 \uc608\uce21\uc5d0 \uc880 \ub354 \uac15\ud55c \uc601\ud5a5\uc744 \uc8fc\ub294 smaller subset\uc73c\ub85c \ubaa8\ub378\uc744 \uafb8\ub9ac\ub824 \ud560 \uac83\uc774\ub2e4. \ud070 \uadf8\ub9bc\uc744 \uc704\ud574 \uc6b0\ub9ac\ub294 \uae30\uaebc\uc774 \uba87\uba87 \uc601\ud5a5\ub825\uc774 \uc801\uc740 detail\ud55c \ubd80\ubd84\ub4e4\uc744 \ubc84\ub9ac\uae30\ub3c4 \ud55c\ub2e4. \uc774\ubc88 section\uc5d0\uc11c\ub294 \uba87\uac00\uc9c0\uc758 variable subset selection \ubc29\ubc95\uc744 \uc0b4\ud3b4\ubcf4\ub3c4\ub85d \ud560 \uac83\uc774\ub2e4. \uc774\ub7ec\ud55c \uc5ec\ub7ec \ubc29\ubc95\ub4e4\uc740 \uacb0\uad6d model selection\uc758 \ud55c \uacfc\uc815\uc774\uba70 \uc774\ub294 \uc120\ud615 \ubaa8\ub378\uc5d0\ub9cc \uad6d\ud55c\ub418\uc5b4\uc788\uc9c0\uc54a\uace0, \ucd94\ud6c4\uc5d0 \ub2e4\ub978 \ubaa8\ub378\uc5d0\uc11c\uc758 \uc801\uc6a9\uc744 \ub2e4\ub8f0 \uac83\uc774\ub2e4. \uc774\uc81c\ubd80\ud130 \uba87\uac00\uc9c0 choosing subset method\ub97c \uc0b4\ud3b4\ubcf4\uc790.","title":"3.3 Subset Selection"},{"location":"02%20ESL/03_Linear_Methods_for_Regression/#1-best-subset-selection","text":"Best subset regression\uc740 \uac00\ub2a5\ud55c \ubaa8\ub4e0 \ubaa8\ub378\uc744 \uace0\ub824\ud558\uc5ec \uadf8 \uc911 \uac00\uc7a5 \uc88b\uc740 \ubaa8\ub378\uc774 \ubb34\uc5c7\uc778\uc9c0\ub97c subset size k\ub97c \ud1b5\ud574 \ucc3e\uc544\uc8fc\ub294 \ubc29\ubc95\uc774\ub2e4. \\(k \\in \\{0,1,2,..,p\\}\\) \uac04\ub2e8\ud55c \uc608\uc2dc\ub97c \ud1b5\ud574 \uc704\uc758 subset selection \ubc29\ubc95\uc744 \ud655\uc778\ud574\ubcf4\uc790. library(ISLR) names(Hitters) ## [1] \"AtBat\" \"Hits\" \"HmRun\" \"Runs\" \"RBI\" ## [6] \"Walks\" \"Years\" \"CAtBat\" \"CHits\" \"CHmRun\" ## [11] \"CRuns\" \"CRBI\" \"CWalks\" \"League\" \"Division\" ## [16] \"PutOuts\" \"Assists\" \"Errors\" \"Salary\" \"NewLeague\" Hitters <- na.omit(Hitters) #Best Subset Selection method! #Best Subset Selection\uc774 search\ud558\ub294 predictor \uac2f\uc218\uc758 \ub514\ud3f4\ud2b8\uac12\uc740 8 #\ubcc0\uc218\uc758 \uac2f\uc218\uac00 \ub9ce\uc9c0 \uc54a\uc73c\ub2c8 max\ub85c \ud0d0\uc0c9\ud574\ubcf4\uc790 library(leaps) bss <- regsubsets(Salary~.,Hitters,nvmax=length(names(Hitters))-1) summary(bss) ## Subset selection object ## Call: regsubsets.formula(Salary ~ ., Hitters, nvmax = length(names(Hitters)) - ## 1) ## 19 Variables (and intercept) ## Forced in Forced out ## AtBat FALSE FALSE ## Hits FALSE FALSE ## HmRun FALSE FALSE ## Runs FALSE FALSE ## RBI FALSE FALSE ## Walks FALSE FALSE ## Years FALSE FALSE ## CAtBat FALSE FALSE ## CHits FALSE FALSE ## CHmRun FALSE FALSE ## CRuns FALSE FALSE ## CRBI FALSE FALSE ## CWalks FALSE FALSE ## LeagueN FALSE FALSE ## DivisionW FALSE FALSE ## PutOuts FALSE FALSE ## Assists FALSE FALSE ## Errors FALSE FALSE ## NewLeagueN FALSE FALSE ## 1 subsets of each size up to 19 ## Selection Algorithm: exhaustive ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns ## 1 ( 1 ) \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ## 2 ( 1 ) \" \" \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ## 3 ( 1 ) \" \" \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ## 4 ( 1 ) \" \" \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ## 5 ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ## 6 ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \"*\" \" \" \" \" \" \" \" \" \" \" ## 7 ( 1 ) \" \" \"*\" \" \" \" \" \" \" \"*\" \" \" \"*\" \"*\" \"*\" \" \" ## 8 ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \"*\" \" \" \" \" \" \" \"*\" \"*\" ## 9 ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\" ## 10 ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\" ## 11 ( 1 ) \"*\" \"*\" \" \" \" \" \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\" ## 12 ( 1 ) \"*\" \"*\" \" \" \"*\" \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\" ## 13 ( 1 ) \"*\" \"*\" \" \" \"*\" \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\" ## 14 ( 1 ) \"*\" \"*\" \"*\" \"*\" \" \" \"*\" \" \" \"*\" \" \" \" \" \"*\" ## 15 ( 1 ) \"*\" \"*\" \"*\" \"*\" \" \" \"*\" \" \" \"*\" \"*\" \" \" \"*\" ## 16 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \"*\" \"*\" \" \" \"*\" ## 17 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \"*\" \"*\" \" \" \"*\" ## 18 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \"*\" ## 19 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" ## CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN ## 1 ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ## 2 ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" ## 3 ( 1 ) \"*\" \" \" \" \" \" \" \"*\" \" \" \" \" \" \" ## 4 ( 1 ) \"*\" \" \" \" \" \"*\" \"*\" \" \" \" \" \" \" ## 5 ( 1 ) \"*\" \" \" \" \" \"*\" \"*\" \" \" \" \" \" \" ## 6 ( 1 ) \"*\" \" \" \" \" \"*\" \"*\" \" \" \" \" \" \" ## 7 ( 1 ) \" \" \" \" \" \" \"*\" \"*\" \" \" \" \" \" \" ## 8 ( 1 ) \" \" \"*\" \" \" \"*\" \"*\" \" \" \" \" \" \" ## 9 ( 1 ) \"*\" \"*\" \" \" \"*\" \"*\" \" \" \" \" \" \" ## 10 ( 1 ) \"*\" \"*\" \" \" \"*\" \"*\" \"*\" \" \" \" \" ## 11 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \" \" ## 12 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" \" \" ## 13 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" ## 14 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" ## 15 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" ## 16 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \" \" ## 17 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" ## 18 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" ## 19 ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" bss.table <- summary(bss) names(bss.table) ## [1] \"which\" \"rsq\" \"rss\" \"adjr2\" \"cp\" \"bic\" \"outmat\" \"obj\" par(mfrow=c(2,2)) plot(bss.table$rss,xlab=\"# of Variables\",ylab=\"RSS\",type=\"l\") plot(bss.table$adjr2,xlab=\"# of Variables\", ylab =\"Adjusted R-square\",type=\"l\") points(which.max(bss.table$adjr2),bss.table$adjr2[which.max(bss.table$adjr2)],col=\"red\",cex=2,pch=20) plot(bss.table$cp,xlab=\"# of Variables\", ylab =\"Cp\",type=\"l\") points(which.min(bss.table$cp),bss.table$cp[which.min(bss.table$cp)],col=\"red\",cex=2,pch=20) plot(bss.table$bic,xlab=\"# of Variables\", ylab =\"BIC\",type=\"l\") points(which.min(bss.table$bic),bss.table$bic[which.min(bss.table$bic)],col=\"red\",cex=2,pch=20) \uc704\ub294 ISLR \ud328\ud0a4\uc9c0\uc758 Hitters \ub370\uc774\ud130\ub97c \uac00\uc838\uc654\uc73c\uba70 best subset selection\uc758 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\uae30 \uc704\ud574 leaps \ud328\ud0a4\uc9c0\uc758 regsubsets\ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc600\ub2e4. \uccab\ubc88\uc9f8 \ud50c\ub86f\uc744 \ud655\uc778\ud574\ubcf4\uba74 RSS\ub294 subset k\uc758 \uc0ac\uc774\uc988\uac00 \ucee4\uc9c8\uc218\ub85d \uac10\uc18c\ud558\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc774\ub294\ub370, \uc774\ub294 \uc6b0\ub9ac\uac00 \ubb34\uc791\uc815 \ubcc0\uc218\uc758 \uac2f\uc218\ub97c \ub298\ub9ac\ub294 \uac83\uc774 \uac00\uc7a5 \uc88b\uc740 subset selection\uc774\ub77c\ub294 \uac83\uc744 \uc758\ubbf8\ud558\uc9c0 \uc54a\ub294\ub2e4. training RSS\ub294 full model\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud68c\uadc0\ub97c \ub3cc\ub838\uc744 \ub54c \uac00\uc7a5 \ub0ae\uc740 \uac12\uc744 \uac16\uae30 \ub54c\ubb38\uc5d0 RSS\ub9cc\uc73c\ub85c\ub294 \uc88b\uc740 \ubaa8\ub378\uc774\ub77c \uacb0\ub860\uc9d3\uae30 \uc5b4\ub835\ub2e4. \ub610\ud55c \uc774\ub294 \uacfc\uc801\ud569\uc73c\ub85c \uc5f0\uacb0\ub418\uc5b4 test RSS\ub294 \uc815\ubc18\ub300\uc758 \uacb0\uacfc\ub97c \uac00\uc838\uc62c \uc218 \uc788\ub2e4. \uadf8\ub9ac\ud558\uc5ec \ub098\uba38\uc9c0 \uc218\uc815\uacb0\uc815\uacc4\uc218\uc640 mallow's Cp, BIC \uc9c0\ud45c\ub97c \ucd94\uac00\ub85c \ud655\uc778\ud558\uc5ec \uacb0\uc815\uacc4\uc218\uc758 \ucd5c\uace0\uc810\uacfc Cp, BIC\uc758 \ucd5c\uc18c\uc810\uc744 \ucc0d\uc5b4 \uadf8\ub54c\uc758 size k\ub97c \ube68\uac04 \uc810\uc73c\ub85c \ud45c\uc2dc\ud558\uc600\ub2e4. coef(bss,5) ## (Intercept) AtBat Hits CRBI DivisionW ## 97.7684116 -1.4401428 7.1753197 0.6882079 -129.7319386 ## PutOuts ## 0.2905164 \uc704\uc640 \uac19\uc774 \ud2b9\uc815 size k\uc5d0\uc11c\uc758 \ud68c\uadc0\uacc4\uc218 \ub610\ud55c \ud655\uc778\ud560 \uc218 \uc788\ub2e4.","title":"1. Best-Subset Selection"},{"location":"02%20ESL/03_Linear_Methods_for_Regression/#23-forward-backward-stepwise-selection","text":"Best Subset Selection\uae30\ubc95\uc740 predictors\uc758 \uac2f\uc218\uac00 40\uac1c \uc774\uc0c1\uc774 \ub118\uc5b4\uac00\uba74 \ub9e4\uc6b0 \ube44\ud6a8\uc728\uc801\uc774\uba70 \uacc4\uc0b0\uc774 \ubd88\uac00\ub2a5\ud558\ub2e4\ub294 \ub2e8\uc810\uc774 \uc788\ub2e4. \uadf8\ub9ac\ud558\uc5ec \uc774\uc5d0 \ub300\ud55c \ub300\uc548\uc73c\ub85c \uc0ac\uc6a9\ub418\ub294 \uac83\uc774 \uc804\uc9c4\uc120\ud0dd\ubc95\uacfc \ud6c4\uc9c4\uc81c\uac70\ubc95 \ub4f1\uc774 \uc788\ub2e4. \uc804\uc9c4\uc120\ud0dd\ubc95\uc740 intercept\ub9cc \uac16\uace0 \uc788\ub294 reduced model\uc5d0\uc11c \uc2dc\uc791\ud558\uc5ec model fit\uc744 \uac1c\uc120\uc2dc\ud0ac \uc218 \uc788\ub294 predictor\ub4e4\uc744 \ucd94\uac00\ud558\ub294 \ubc29\ubc95\uc774\uba70, \ud6c4\uc9c4\uc81c\uac70\ubc95\uc740 full model\uc5d0\uc11c \uc2dc\uc791\ud558\uc5ec \uc804\uc9c4\uc120\ud0dd\ubc95\uacfc \ubc18\ub300\uc758 \ubc29\ubc95\uc73c\ub85c \ubaa8\ub378\uc744 \ud615\uc131\ud55c\ub2e4. library(ggplot2) library(tidyverse) ## \u2500\u2500 Attaching packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 1.2.1 \u2500\u2500 ## \u2714 tibble 2.0.1 \u2714 purrr 0.3.0 ## \u2714 tidyr 0.8.2 \u2714 dplyr 0.7.8 ## \u2714 readr 1.3.1 \u2714 stringr 1.4.0 ## \u2714 tibble 2.0.1 \u2714 forcats 0.3.0 ## \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500 ## \u2716 dplyr::filter() masks stats::filter() ## \u2716 dplyr::lag() masks stats::lag() bss.f <- regsubsets(Salary~.,Hitters,nvmax=length(names(Hitters))-1,method=\"forward\") bss.b <- regsubsets(Salary~.,Hitters,nvmax=length(names(Hitters))-1,method=\"backward\") bss.f.table <- summary(bss.f) bss.b.table <- summary(bss.b) names(bss.f.table) ## [1] \"which\" \"rsq\" \"rss\" \"adjr2\" \"cp\" \"bic\" \"outmat\" \"obj\" x <- seq(1,19,1) y1 <- bss.table$bic y2 <- bss.f.table$bic y3 <- (bss.b.table$bic) df <- data.frame(x,y1,y2,y3) df2=gather(df,key,value,-x) g<- ggplot(df2,aes(x=x,y=value,color=key))+ geom_jitter() g<-g+ scale_colour_hue(labels=c('bss','forward','backward')) g <- g+ylab(\"BIC\")+xlab(\"Number of Variables\") g","title":"2&amp;3. Forward, backward Stepwise Selection"},{"location":"02%20ESL/03_Linear_Methods_for_Regression/#4-forward-stagewise-regression","text":"\uc774 \ubc29\ubc95\uc740 \uc704\uc5d0\uc11c \uc18c\uac1c\ud55c Forward stepwise\ubcf4\ub2e4\ub294 \uc880 \ub354 \uc81c\uc57d\uc774 \ub9ce\ub2e4. Forward stepwise\uc640 \ube44\uc2b7\ud558\uac8c \uc774 \ub610\ud55c intercept\ub9cc \uc788\ub294 \ubaa8\ub378\uc5d0\uc11c \uc2dc\uc791\ud558\uc9c0\ub9cc \uc774\ub54c\uc758 intercept \uac00 \\(\\bar{Y}\\) \uc774\ub2e4. Centered\ub41c predictors\uc758 \uacc4\uc218\ub97c \uc2dc\uc791\uc774 0\uc774\ub3c4\ub85d \ud558\ub294 \uac83\uc774\ub2e4. \uac01 \ub2e8\uacc4\uc5d0\uc11c \uc54c\uace0\ub9ac\uc998\uc740 \uc9c0\uae08\uc758 \uc794\ucc28\uc640 \uac00\uc7a5 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 \ubcc0\uc218\ub97c \ud655\uc778\ud574 \uc774\uc5d0 \ub300\ud55c \uc794\ucc28\uc640 \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\ub97c \ud558\uc5ec \uc5bb\uc740 \uacc4\uc218\ub97c \uc0ac\uc6a9\ud55c\ub2e4. \uc774 \uc791\uc5c5\uc740 \uc794\ucc28\uac00 \uc5b4\ub290 \ubcc0\uc218\uc640\ub3c4 \uc720\uc758\ud55c \uad00\uacc4\ub97c \uac00\uc9c0\uc9c0 \uc54a\uc744\ub54c \uae4c\uc9c0 \uacc4\uc18d\ub41c\ub2e4. \uc774 \ubc29\ubc95\uc740 Stepwise \ubc29\ubc95\uacfc\ub294 \ub2ec\ub9ac \ud55c\ubc88 \ubaa8\ub378\uc5d0 \ud3ec\ud568\ub41c \ubcc0\uc218\ub4e4\uc740 \uc774\ud6c4\uc5d0 \uc81c\uac70\ub418\uc9c0 \uc54a\ub294\ub2e4.","title":"4 Forward-Stagewise Regression"},{"location":"02%20ESL/03_Linear_Methods_for_Regression/#shrinkage-methods","text":"\uac00\ub054\uc740 predictors\ub4e4\uc758 \ubd80\ubd84\uc9d1\ud569\uc744 \ub0a8\uae30\uace0 \ub098\uba38\uc9c0 \ubd80\ubd84\ub4e4\uc744 \ubc84\ub9bc\uc73c\ub85c\uc368 subset selection \ubc29\ubc95\uc740 full model\uc744 \uc0ac\uc6a9\ud588\uc744 \ub54c \ubcf4\ub2e4 \ub354 \ub0ae\uc740 prediction error\ub97c \uac16\uace0 \uc6a9\uc774\ud55c \ud574\uc11d\uc744 \uc774\ub04c\uae30\ub3c4 \ud55c\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub294 discrete process \uc778\ub370, (\ubcc0\uc218\ub97c \ubc84\ub9ac\uac70\ub098 \ub0a8\uae30\uac70\ub098 \ud558\ub294 \uac83\uc740) \uc774\ub294 \uc885\uc885 high variance\ub97c \uac16\uace0, prediction error of the full model\uc744 \uc904\uc774\uc9c0 \ubabb\ud558\uae30\ub3c4 \ud55c\ub2e4. Shrinkage methods\ub294 \uc880\ub354 continuous\ud558\uba70, \ub192\uc740 \ubcc0\ub3d9\uc131\uc5d0 \ud06c\uac8c \uc81c\uc57d\uc744 \ubc1b\uc9c0 \uc54a\ub294 \ubc29\ubc95\uc774\uae30\uc5d0, \uc774\ubc88 \uc18c\ub2e8\uc6d0\uc5d0\uc11c\ub294 \uc774\uc5d0 \ub300\ud574\uc11c \ub2e4\ub904\ubcf4\uaca0\ub2e4.","title":"Shrinkage Methods"},{"location":"02%20ESL/03_Linear_Methods_for_Regression/#ridge-regression","text":"Ridge regression\uc740 coeeicients\uc758 size\uc5d0 penalty \ub97c \uac78\uc5b4 \uc774\ub4e4\uc744 \ucd95\uc18c\uc2dc\ud0a4\ub294 \ubc29\ubc95\uc774\ub2e4. Ridge \ud68c\uadc0\ub294 \uace0\ub85c penalized\ub41c residual sum of squares\ub97c \ucd5c\uc18c\ud654 \ud558\ub294 \ubc29\ud5a5\uc73c\ub85c \ubaa8\ub378\uc744 \ub9cc\ub4e4\uac8c \ub418\ub294\ub370 \uc774\ub97c \uc218\uc2dd\uc73c\ub85c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\[{\\hat{\\beta}}^{ridge}=argmin_{\\beta}\\{\\sum_{i=1}^N(y_{i}-{\\beta}_0-\\sum_{j=1}^px_{ij}{\\beta}_{j})^2+{\\lambda}\\sum_{j=1}^p{\\beta}_{j}^2\\}\\] \uc5ec\uae30\uc11c \\({\\lambda} \\geq 0\\) \uc740 \ucd95\uc18c\uc758 \uc815\ub3c4\ub97c \uacb0\uc815\ud558\ub294 complexity parameter\uc778\ub370, \uc774 \ub78c\ub2e4\uc758 \uac12\uc774 \ucee4\uc9c8\uc218\ub85d \uacc4\uc218\ub97c 0\uc73c\ub85c \ubcf4\ub0b4\ub294 \ucd95\uc18c\uac00 \ub354 \ub9ce\uc774 \uc774\ub8e8\uc5b4\uc9c4\ub2e4. \uc704 \uc2dd\uc740 \uc544\ub798\uc640 \uac19\uc774\ub3c4 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\[{\\hat{\\beta}}^{ridge}=argmin_{\\beta}\\sum_{i=1}^N(y_{i}-{\\beta}_0-\\sum_{j=1}^px_{ij}{\\beta}_{j})^2,\\] \\[subject\\ to\\ \\sum_{j=1}^p{\\beta}_{j}^2 \\leq t \\] \uac01 \uc2dd\uc5d0\uc11c \\({\\lambda}\\) \uc640 \\(t\\) \uc758 \uac12\uc740 1:1\ub300\uc751\uc774 \uc131\ub9bd\ud55c\ub2e4. \uc2dd\uc744 \ud655\uc778\ud574\ubcf4\uba74 \uc6b0\ub9ac\ub294 intercept \ud56d\uc5d0 \ub300\ud574\uc11c\ub294 penalization\uc774 \uc9c4\ud589\ub418\uc9c0 \uc54a\uc558\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub294\ub370, \uc774\ub294 target \\(y_{i}\\) \uc5d0 c\ub9cc\ud07c\uc758 \uc0c1\uc218\ub97c \ub354\ud574\uc8fc\ub294 \uac83\uc774 \uc608\uce21\uac12\uc774 c\ub9cc\ud07c \uc774\ub3d9\ud55c\ub2e4\ub294 \uc758\ubbf8\uc640 \ub2e4\ub974\uae30 \ub54c\ubb38\uc774\ub2e4. \\({\\lambda}\\) \ub97c \ud1b5\ud574 \uc5bb\uc740 penalized RSS\uc758 \ud574\ub294 centered\ub41c predictors\ub97c \uc0ac\uc6a9\ud55c \uc7ac\ubaa8\uc218\ud654\ub85c \ub450 \ubd80\ubd84\uc73c\ub85c \ucabc\uac1c\uc9c0\ub294\ub370, \uc774\ub294 \\(x_{ij}\\) \uac00 \\(x_{ij}-\\bar{x}_{j}\\) \ub85c \ub300\uccb4\ub428\uc744 \uc758\ubbf8\ud78c\ub2e4. \uc5ec\uae30\uc11c intercept \\({\\beta}_0\\) \ub97c \\(\\bar{Y}\\) \ub85c \ucd94\uc815\ud558\uba70, \ub098\uba38\uc9c0 \uacc4\uc218\ub4e4\uc740 intercept\uac00 \uc5c6\ub294 ridge regression \uc73c\ub85c centered\ub41c \\(x_{ij}\\) \ub4e4\uc744 \ucd94\uc815\ud558\uac8c \ub41c\ub2e4. \uc774\ub7ec\ud55c \uc911\uc2ec\ud654\uac00 \uc218\ud589\ub418\uc5c8\ub2e4\uace0 \uac00\uc815\ud588\uc744\ub54c, input matrix \\(\\mathbf{X}\\) \ub294 \\(p+1\\) \uac1c\uac00 \uc544\ub2cc \\(p\\) \uac1c\uc758 \uc5f4\uc744 \uac16\ub294\ub2e4. \uc774\ub7ec\ud55c \uc2dd\uacfc \uadf8\ub54c\uc758 \ud574\ub97c \ud589\ub82c\ub85c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(RSS({\\lambda})=(\\mathbf{y}-\\mathbf{X}{\\beta})^T(\\mathbf{y}-\\mathbf{X}{\\beta})+{\\lambda}{\\beta}^T{\\beta}\\) $ $ \\({\\hat{\\beta}}^{ridge}=(\\mathbf{X}^T{\\mathbf{X}}+{\\lambda}\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\) $ \\(\\mathbf{I}\\) \ub294 \\(p\\times{p}\\) \uc758 \ud56d\ub4f1\ud589\ub82c\uc774\ub2e4. \uc6b0\ub9ac\ub294 quadratic penalty \\({\\beta}^T{\\beta}\\) \ub97c \uc0ac\uc6a9\ud558\uc600\uae30 \ub54c\ubb38\uc5d0, ridge regression \uc758 \ud574\ub294 \\(\\mathbf{y}\\) \uc5d0 \ub300\ud55c \uc120\ud615 \ud568\uc218\uac00 \ub41c\ub2e4. \uc774 \ud574\ub294 \uc5ed\ud589\ub82c\uc744 \uad6c\ud558\uae30 \uc804\uc5d0 \\(\\mathbf{X}^T\\mathbf{X}\\) \uc758 \ub300\uac01\uc6d0\uc18c\uc5d0 \uc591\uc758 \uc0c1\uc218\ub97c \ub354\ud55c\ub2e4. \uc774\ub294 \ubb38\uc81c\ub97c nonsingular \ud558\uac8c \ud574\uc8fc\uba70 \\(\\mathbf{X}^T\\mathbf{X}\\) \uac00 full rank\uac00 \uc544\ub2c8\uc5b4\ub3c4 \uad1c\ucc2e\uac8c \ud574\uc900\ub2e4. \uc544\ub798\uc5d0 \ucca8\ubd80\ud55c \uadf8\ub9bc\uc740 prostate cancer \uc608\uc81c\ub97c \ud1b5\ud574 \uc5bb\uc740 \uadf8\ub9bc\uc778\ub370, \uc774\ub294 \\(df({\\lambda})\\) \uc5d0 \ub530\ub77c ridge \ud68c\uadc0\uacc4\uc218\uc758 \ucd94\uc815\uce58\uac00 \ubcc0\ud654\ud558\ub294 \uac83\uc744 \ubcf4\uc5ec\uc900\ub2e4. \\(df({\\lambda})\\) \ub294 penalty \\({\\lambda}\\) \uc5d0 \ub300\ud55c \ud568\uc218\ub85c effective degress of freedom \uc744 \ubcf4\uc5ec\uc900\ub2e4. \ub9cc\uc57d \uc785\ub825\ubcc0\uc218\ub4e4\uc774 orthonormal \ud558\ub2e4\uba74, ridge\uc758 \ucd94\uc815\uce58\ub294 \ucd5c\uc18c\uc81c\uacf1\ubc95 \ucd94\uc815\uce58\uc758 scaled version\uc774 \ub41c\ub2e4. \\({\\hat{\\beta}}^{ridge}=\\frac{\\hat{\\beta}}{1+{\\lambda}}\\) \uc911\uc2ec\ud654\ub41c \uc785\ub825\ud589\ub82c \\(\\mathbf{X}\\) \uc758 \uace0\uc720\uac12\ubd84\ud574( singular value decomposition , SVD)\ub97c \ud1b5\ud574 \uc6b0\ub9ac\ub294 ridge regression\uc758 \ud2b9\uc131\uc5d0 \ub300\ud574 \ucd94\uac00\uc801\uc778 insight\ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4. \uc774\ub7ec\ud55c \ubd84\ud574\ub294 \ub2e4\uc591\ud55c \ud1b5\uacc4\uc801 \ubc29\ubc95\uc5d0 \ub300\ud55c \ubd84\uc11d\uc5d0 \ub9e4\uc6b0 \uc6a9\uc774\ud55c \ubc29\ubc95\uc774\ub2e4. \\(N\\times{p}\\) \\(\\mathbf{X}\\) \ud589\ub82c\uc758 \uace0\uc720\uac12\ubd84\ud574\ub294 \uc544\ub798\uc640 \uac19\uc740 form\uc744 \uac16\ub294\ub2e4. \\[\\mathbf{X}=\\mathbf{U}\\mathbf{D}\\mathbf{V}^T\\] \uc5ec\uae30\uc11c \\(\\mathbf{U}\\) \uc640 \\(\\mathbf{V}\\) \ub294 \\(N\\times{p}\\) , \\(p\\times{p}\\) \uc758 \uc9c1\uad50\ud589\ub82c\uc774\uace0, \\(\\mathbf{U}\\) \uc758 \uc5f4\uc740 \\(\\mathbf{X}\\) \uc758 \uc5f4\uacf5\uac04\uc744 span\ud558\uba70, \\(\\mathbf{V}\\) \uc758 \uc5f4\ub4e4\uc774 \ud589\uacf5\uac04\uc744 span\ud55c\ub2e4. \\(\\mathbf{D}\\) \ub294 \\(p\\times{p}\\) \uc758 \ub300\uac01\ud589\ub82c\uc774\uba70, \ub300\uac01\uc6d0\uc18c\ub4e4\uc740 \\(d_1\\geq{d_2}\\geq...\\geq{d_{p}}\\geq0\\) \uc778 \\(\\mathbf{X}\\) \uc758 singular value\ub4e4\uc774\ub2e4. \ub9cc\uc57d \ud55c\uac1c\ub77c\ub3c4 \\(d_j=0\\) \uc774\ub77c\uba74, \\(\\mathbf{X}\\) \ub294 singular\uc774\ub2e4. SVD\ub97c \ud1b5\ud574 \uc6b0\ub9ac\ub294 least squares fitted vector\ub97c \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\[\\mathbf{X}{\\hat{\\beta}^{ls}}=\\mathbf{X}{(\\mathbf{X}^T\\mathbf{X})}^{-1}\\mathbf{X}^T\\mathbf{y}\\] \\[=\\mathbf{X}(\\mathbf{V}\\mathbf{D}^2\\mathbf{V}^T)^{-1}\\mathbf{X}^T\\mathbf{y}\\] \\[=\\mathbf{X}\\mathbf{V}\\mathbf{D}^{-2}\\mathbf{V}^T\\mathbf{X}^T\\mathbf{y}\\] \\[=\\mathbf{U}\\mathbf{D}\\mathbf{V}^T\\mathbf{V}\\mathbf{D}^{-2}\\mathbf{V}^T\\mathbf{X}^T\\mathbf{y}\\] \\[=\\mathbf{U}\\mathbf{D}^{-1}\\mathbf{V}^T\\mathbf{X}^T\\mathbf{y}\\] \\[=\\mathbf{U}\\mathbf{D}^{-1}\\mathbf{V}^T\\mathbf{V}\\mathbf{D}\\mathbf{U}^T\\mathbf{y}=\\mathbf{U}\\mathbf{U}^T\\mathbf{y}\\] \ub85c \ud45c\ud604\uac00\ub2a5\ud558\ub2e4. \uc5ec\uae30\uc11c \\(\\mathbf{U}^T\\mathbf{y}\\) \ub294 orthonormal basis \\(\\mathbf{U}\\) \uc5d0 \ub300\ud55c \\(\\mathbf{y}\\) \uc758 \uc88c\ud45c\ub4e4\uc774\ub2e4. \uc774 \uc99d\uba85\uc740 \\(\\mathbf{U}\\) \uc640 \\(\\mathbf{V}\\) \uc758 \uc5f4\ub4e4\uc774 orthogonal\ud558\ub2e4\ub294 \uc131\uc9c8\uacfc \uac01 \uc5f4\ub4e4\uc774 \\(\\mathbf{X}\\mathbf{X}^T\\) \uc640 \\(\\mathbf{X}^T\\mathbf{X}\\) \uc758 eigenvector\ub77c\ub294 \uc131\uc9c8\uc744 \uc774\uc6a9\ud558\uc5ec \uc2dd\uc804\uac1c\ub97c \ud55c \uac83\uc774\ub2e4. \uadf8\ub807\ub2e4\uba74 \uc774\uc81c ridge solutions\uc740 \uc544\ub798\uc640 \uac19\uc774 \uad6c\ud560 \uc218 \uc788\ub2e4. \\[\\mathbf{X}{\\hat{\\beta}}^{ridge}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X}+{\\lambda}\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\] \\[=\\mathbf{U}\\mathbf{D}\\mathbf{V}^T(\\mathbf{V}\\mathbf{D}^{-2}\\mathbf{V}^T+\\frac{{1}}{\\lambda}\\mathbf{I})\\mathbf{V}\\mathbf{D}\\mathbf{U}^T\\mathbf{y}\\] \\[=\\mathbf{U}\\mathbf{U}^T\\mathbf{y}+\\frac{1}{\\lambda}\\mathbf{U}\\mathbf{D}^2\\mathbf{U}^T\\mathbf{y}\\] \\[=\\mathbf{U}(\\mathbf{I}+\\frac{1}{\\lambda}\\mathbf{D}^2)\\mathbf{U}^T\\mathbf{y}\\] \\[=\\mathbf{U}\\mathbf{D}(\\mathbf{D}^{-2}+\\frac{1}{\\lambda})\\mathbf{D}\\mathbf{U}^T\\mathbf{y}\\] \\[=\\mathbf{U}\\mathbf{D}(\\mathbf{D}^2+\\lambda\\mathbf{I})^{-1}\\mathbf{D}\\mathbf{U}^T\\mathbf{y}\\] \\[=\\sum_{j=1}^p\\mathbf{u}_{j}\\frac{d_j^2}{d_j^2+\\lambda}\\mathbf{u}_j^T\\mathbf{y}\\] \uc5ec\uae30\uc11c \\(\\mathbf{u}_j\\) \ub4e4\uc740 \\(\\mathbf{U}\\) \uc758 \uc5f4\ub4e4\uc774\ub2e4. \\(\\lambda \\geq 0\\) \uc77c\ub54c \uc6b0\ub9ac\ub294 \\(d_j^2/(d_j^2+\\lambda) \\leq 0\\) \uc784\uc744 \uc54c \uc218 \uc788\ub2e4. \uc120\ud615 \ud68c\uadc0 \ucc98\ub7fc ridge\ud68c\uadc0\ub294 \\(\\mathbf{y}\\) \uc758 \uc88c\ud45c\ub4e4\uc744 \uc815\uaddc\uc9c1\uad50\uae30\uc800 \\(\\mathbf{U}\\) \uc5d0 \ub300\ud574 \uacc4\uc0b0\ud558\uac8c \ub41c\ub2e4. \uc774\ud6c4 \uc774\ub294 \uc774\ub7ec\ud55c \uc88c\ud45c\ub4e4\uc744 \\(d_j^2/(d_j^2+\\lambda)\\) \ub97c \uacf1\ud568\uc73c\ub85c\uc368 \ucd95\uc18c\uc2dc\ud0a4\ub294 \uac83\uc774\ub2e4. \uc774\ub294 \uc989 \ub354 \uc791\uc740 \\(d_j^2\\) \ub97c \uac16\ub294 \uae30\uc800 \ubca1\ud130\ub4e4\uc758 \uc88c\ud45c\uc5d0 \ub354 \ud070 \uc591\uc758 \ucd95\uc18c\uac00 \uc801\uc6a9\ub418\ub294 \uac83\uc774\ub2e4. \\(d_j^2\\) \uac00 \uc791\ub2e4\ub294 \ub9d0\uc740 \ubb34\uc2a8 \ub9d0\uc778\uac00? \uc911\uc2ec\ud654\ub41c \ud589\ub82c \\(\\mathbf{X}\\) \uc758 \\(\\mathbf{SVD}\\) \ub294 \\(\\mathbf{X}\\) \ub0b4\uc758 \ubcc0\uc218\ub4e4\uc758 principal components \ub97c \ud45c\ud604\ud558\ub294 \ub610 \ub2e4\ub978 \ubc29\ubc95\uc774\ub2e4. sample covariance matrix\ub294 \\(\\mathbf{S}=\\mathbf{X^TX}/n\\) \uc774\uba70, SVD\uc5d0 \uc758\ud574 \\(\\mathbf{X^TX}=\\mathbf{VD^2V^T}\\) \ub97c \uc5bb\uac8c \ub418\uba70, \uc774\ub294 \\(\\mathbf{X^TX}\\) \uc758 eigen decomposition\uc774\uba70, eigenvector\ub4e4\uc758 \\(v_j\\) \ub294 \\(\\mathbf{X}\\) \uc758 principal components directions\uc774\ub2e4. \uccab\ubc88\uc9f8 \uc8fc\uc131\ubd84 \ubc29\ud5a5 \\(v_1\\) \uc740 \\(\\mathbf{z}_1=\\mathbf{X}v_1\\) \uc774 \ubaa8\ub4e0 \\(\\mathbf{X}\\) \uc758 \uc5f4\ub4e4\uc758 \uc120\ud615 \uc870\ud569\ub4e4\uc744 normalize\ud55c \uac83\ub4e4 \uc911 \uac00\uc7a5 \ud070 sample variance\ub97c \uac16\ub294 \uc131\uc9c8\uc744 \uac16\ub294\ub2e4. \uc774\ub7ec\ud55c sample variance\ub294 \\(Var(\\mathbf{Z}_1)=Var(\\mathbf{X}v_1)=\\frac{d_1^2}{n}\\) \ub97c \ub9cc\uc871\ud558\uace0, \uc2e4\uc81c\ub85c \\(\\mathbf{z}_1=\\mathbf{X}v_1=\\mathbf{u}_1d_1\\) \uc784 \ub610\ud55c \ubcf4\uc77c \uc218 \uc788\ub2e4. \ub3c4\ucd9c\ub41c \ubcc0\uc218 \\(\\mathbf{z}_1\\) \ub294 \\(\\mathbf{X}\\) \uc758 \uccab\ubc88\uc9f8 \uc8fc\uc131\ubd84\uc73c\ub85c \ubd88\ub9ac\uba70, \ub530\ub77c\uc11c \\(\\mathbf{u}_1\\) \ub294 normalize\ub41c \uccab\ubc88\uc9f8 \uc8fc\uc131\ubd84\uc774\ub77c\uace0 \ubcfc \uc218 \uc788\ub2e4. \uace0\ub85c \uc791\uc740 singular value \\(d_j\\) \ub294 \\(\\mathbf{X}\\) \uc758 \uc5f4\uacf5\uac04\uc758 \ubc29\ud5a5\ub4e4 \uc911 \uc791\uc740 \ubd84\uc0b0\uc744 \uac16\ub294 \ubc29\ud5a5\uacfc \ub300\uc751\ub418\uace0 ridge regression\uc740 \uc774\ub7ec\ud55c \ubc29\ud5a5\ub4e4\uc744 \ub354\uc6b1 \ucd95\uc18c\uc2dc\ud0a8\ub2e4. \uacb0\uad6d \uc791\uc740 singular value\uac12\uc77c \uc218\ub85d \uacc4\uc218\uac00 \ub354\uc6b1 shrink\ub418\ub294 \uac83\uc774\ub2e4. \uc989 ridge regression\uc740 \uc791\uc740 \ubd84\uc0b0\uc744 \uac16\ub294 \ucd95\uc5d0 \ub300\ud574 \uacc4\uc218\ub97c \ucd95\uc18c\uc2dc\ud0b4\uc73c\ub85c\uc368 \uc791\uc740 \ubd84\uc0b0\uc744 \uac16\ub294 \ubc29\ud5a5\uc5d0\uc11c \ub192\uc740 \ubd84\uc0b0\uc744 \uac16\ub294 \uae30\uc6b8\uae30\uac00 \ucd94\uc815\ub418\uc9c0 \uc54a\uac8c\ub054 \ubcf4\ud638\ud558\ub294 \uc5ed\ud560\uc744 \ud574\uc8fc\ub294 \uac83\uc774\ub2e4. \uc774\ub7ec\ud55c \ud574\uc11d\uc5d0 \ud544\uc694\ud55c \uc228\uc740 \uac00\uc815\uc740 output\ubcc0\uc218 \ub610\ud55c \uc785\ub825\uac12\uc758 \ubcc0\ub3d9\uc131\uc744 \uac00\uc7a5 \ub9ce\uc774 \uc124\uba85\ud558\ub294 (\ubd84\uc0b0\uc774 \uc81c\uc77c \ud070) \ucd95\uc5d0\uc11c \uadf8 \ubcc0\ub3d9\uc774 \uac00\uc7a5 \ud074 \uac83\uc774\ub77c\ub294 \uac00\uc815\uc774\ub2e4. \uc774\ub294 \ud544\uc218\uc801\uc774\uc9c0\ub294 \uc54a\uc9c0\ub9cc \ucda9\ubd84\ud788 \ud569\ub9ac\uc801\uc778 \uac00\uc815\uc774\ub77c\uace0 \ud55c\ub2e4. #Do SVD! library(MASS) ## ## Attaching package: 'MASS' ## The following object is masked from 'package:dplyr': ## ## select X <- matrix(c(3,7,0,1,9,3,5,0,0,7),ncol = 2, nrow = 5) XtX <- t(X)%*%X eigen(XtX) ## eigen() decomposition ## $values ## [1] 222.2305288 0.7694712 ## ## $vectors ## [,1] [,2] ## [1,] -0.7929002 0.6093515 ## [2,] -0.6093515 -0.7929002 V <- eigen(XtX)$vectors Vt <- t(V) singular_values <- sqrt(eigen(XtX)$values) singular_values ## [1] 14.9073985 0.8771951 D <- diag(singular_values) U = vector() for(i in 1:length(V[1,])){ U = append(U,1/singular_values[i]*X %*% V[,i]) } U = matrix(U,ncol = length(V[1,])) SVD <- function(X){ XtX = t(X)%*%X V =eigen(XtX)$vectors Vt = t(V) singular_values =sqrt(eigen(XtX)$values) D=diag(singular_values) U=vector() for(i in 1:length(V[1,])){ U = append(U,1/singular_values[i]*X %*% V[,i]) } U=matrix(U,ncol = length(V[1,])) return(list(U=U,D=D,Vt=Vt)) } U <- SVD(X)$U D <- SVD(X)$D Vt<- SVD(X)$Vt U%*%D%*%Vt ## [,1] [,2] ## [1,] 3 3 ## [2,] 7 5 ## [3,] 0 0 ## [4,] 1 0 ## [5,] 9 7 X ## [,1] [,2] ## [1,] 3 3 ## [2,] 7 5 ## [3,] 0 0 ## [4,] 1 0 ## [5,] 9 7 Y <- c(1,0,5,3,2) # \\mathbf{X}{\\hat{\\beta}}^{ls} = UU^Ty check X%*%solve(XtX)%*%t(X)%*%Y ## [,1] ## [1,] -0.2631579 ## [2,] 1.5847953 ## [3,] 0.0000000 ## [4,] 1.0116959 ## [5,] 1.4093567 U%*%t(U)%*%Y ## [,1] ## [1,] -0.2631579 ## [2,] 1.5847953 ## [3,] 0.0000000 ## [4,] 1.0116959 ## [5,] 1.4093567 #ridge case lambda <- 500*diag(2) U%*%D%*%solve(D^2+lambda)%*%D%*%t(U)%*%Y ## [,1] ## [1,] 0.16991962 ## [2,] 0.35051469 ## [3,] 0.00000000 ## [4,] 0.03365766 ## [5,] 0.46379444 Xbeta_ridge <- matrix(0,5,2) for (j in 1:ncol(X)){ Xbeta_ridge[,j] <- ((diag(D)[j]^2)/(diag(D)[j]^2+500))*U[,j]%*%t(U[,j])%*%Y } as.matrix(rowSums(Xbeta_ridge)) ## [,1] ## [1,] 0.16991962 ## [2,] 0.35051469 ## [3,] 0.00000000 ## [4,] 0.03365766 ## [5,] 0.46379444 U%*%D%*%solve(D^2+lambda)%*%D%*%t(U)%*%Y ## [,1] ## [1,] 0.16991962 ## [2,] 0.35051469 ## [3,] 0.00000000 ## [4,] 0.03365766 ## [5,] 0.46379444","title":"Ridge Regression"},{"location":"02%20ESL/03_Linear_Methods_for_Regression/#lasso-regression","text":"Lasso Regression \ub610\ud55c Ridge\uc640 \uac19\uc740 shrinkage method\uc774\ub2e4. \ucc28\uc774\uac00 \uc788\ub2e4\uba74 Lasso\ub294 \\(L_2\\) \uac00 \uc544\ub2cc \\(L_1\\) Regularization\uc744 \uc218\ud589\ud55c \uac83\uc774\ub2e4. Lasso\ub97c \ud1b5\ud574 \uc5bb\uc740 \ubca0\ud0c0\uc758 \ucd94\uc815\uce58\ub97c \uc2dd\uc73c\ub85c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\[{\\hat{\\beta}}^{lasso}=argmin_{\\beta}\\sum_{i=1}^N(y_{i}-{\\beta}_0-\\sum_{j=1}^px_{ij}{\\beta}_j)^2\\] \\[subject\\ to\\ \\sum_{j=1}^p|{\\beta}_j|\\leq t\\] \ub610 Lasso\uc5d0\uc11c\ub294 Ridge\uc5d0\uc11c\ucc98\ub7fc input predictors\ub4e4\uc744 \ud45c\uc900\ud654\ud558\uc5ec \\({\\beta}_0\\) \uc5d0 \ub300\ud55c \ucd94\uc815\uce58\ub97c \\(\\bar{y}\\) \ub85c \uc5bb\uc5b4 \uc808\ud3b8 \uacc4\uc218\uc5c6\uc774 \ubaa8\ub378\uc801\ud569\uc744 \uc2dc\ud589\ud560 \uc218 \uc788\ub2e4. \uc704\uc758 \uc2dd\uc744 \uc6b0\ub9ac\ub294 Lagrangian form \uc758 \ud615\ud0dc\ub85c\ub3c4 \ud45c\ud604\uac00\ub2a5\ud55c\ub370, \uc774 \uc2dd\uc774 \uc880 \ub354 \uc774\ud574\uac00 \uc218\uc6d4\ud55c\ub4ef\ud558\ub2e4. \\[{\\hat{\\beta}}^{lasso}=argmin_{\\beta}\\{\\sum_{i=1}^N(y_{i}-{\\beta}_0-\\sum_{j=1}^px_{ij}{\\beta}_j)^2+{\\lambda}\\sum_{j=1}^p|{\\beta}_j|\\}\\] \ub300\uccb4\ub41c \\(L_1\\) penalty\ub294 \ud574\uac00 \\(y_i\\) \uc5d0 \ub300\ud574 \ube44\uc120\ud615\uc778 \uc2dd\uc744 \ub9cc\ub4e4\uba70, \uace0\ub85c ridge\uc640 \ub2e4\ub974\uac8c closed form\uc758 \ud615\ud0dc\uac00 \uc874\uc7ac\ud558\uc9c0 \uc54a\ub294\ub2e4. \uc774\ub7ec\ud55c \uc81c\uc57d\uc2dd\uc73c\ub85c \uc778\ud574 t\ub97c \ucda9\ubd84\ud788 \uc791\uac8c \uc904\uc774\ub294 \uac83\uc740 \uba87\uba87 coefficients\ub4e4\uc744 \uc815\ud655\ud558\uac8c 0\uc73c\ub85c \ub9cc\ub4e4 \uac83\uc774\ub2e4. \uc704 \uadf8\ub9bc\uc744 \ud1b5\ud574 \uc6b0\ub9ac\ub294 \ud0c0\uc6d0\ubaa8\uc591\uc758 \ub2a5\uc120\uc744 \uac16\ub294 RSS\uc640 \uadf8 \uc911\uc2ec\uc5d0\uc11c Full LSE\ub97c \uc5bb\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. ridge\uc758 \uc81c\uc57d\uc2dd\uc740 constraint region\uc744 \uc6d0\ubaa8\uc591\uc744 \ub9cc\ub4e4\uace0(Predictor\uac00 \ub450\uac1c\uc778 case), lasso\uc77c\ub54c\ub294 \ub9c8\ub984\ubaa8 \ud615\ud0dc\ub97c \ub748\ub2e4. \uc774\ub294 \ubaa8\uc591\uc744 \ub744\ub294 \uc774\uc720\ub294 \ub450 \ubc29\ubc95\uc758 regularization form\uc774 \ub2e4\ub974\uae30 \ub584\ubb38\uc774\ub2e4. \\[|{\\beta}_1|+|{\\beta}_2| \\leq t \\ , {\\beta}_1^2+{\\beta}_2^2 \\leq t^2\\] lasso\uc758 \uacbd\uc6b0\ub294 \uac01\uc838\uc788\ub294 \ubaa8\uc11c\ub9ac\uac00 \uc788\uae30\uc5d0 \ud574\uac00 \uadf8 \uacf3\uc5d0\uc11c \ubc1c\uc0dd\ud558\uba74 \ud558\ub098\uc758 parameter\ub294 0\uc774 \ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \ucc28\uc6d0\uc774 \ub2e4\ucc28\uc6d0\uc73c\ub85c \ud655\uc7a5\ub41c\ub2e4\uba74 \ub354 \ub9ce\uc740 \ubaa8\uc11c\ub9ac\uc640 \uba74\ub4e4\uc774 \uc874\uc7ac\ud558\uae30 \ub54c\ubb38\uc5d0 \ub354 \ub9ce\uc740 \uc811\uc810\uc774 \ub9cc\ub4e4\uc5b4\uc9c0\uace0 \ub354 \ub9ce\uc740 \uacc4\uc218\ub97c 0\uc73c\ub85c \ubcf4\ub0bc \uae30\ud68c\uac00 \ub298\uc5b4\ub09c\ub2e4.","title":"Lasso Regression"},{"location":"02%20ESL/04_Linear_Methods_for_Classifications/","text":"04 Linear Methods for Classifications Introduction \ubcf8 \ub2e8\uc6d0\uc5d0\uc11c\ub294 classifications\uc5d0 \ub300\ud574\uc11c \ub2e4\ub8f0 \uc608\uc815\uc774\uba70 \ubd84\ub958 \ubb38\uc81c\uc5d0 \uad00\ud574\uc11c linear model\uc5d0 \ub300\ud574\uc11c \ub17c\ud55c\ub2e4. \uc6b0\ub9ac\ub294 \ub3c5\ub9bd\ubcc0\uc218 \\(G(x)\\) \uac00 \uc774\uc0b0\uc9d1\ud569\uc5d0\uc11c \uac12\uc744 \uac00\uc9c8 \ub54c, \uc785\ub825 \uacf5\uac04\uc744 labelling\ub41c \uc77c\uc885\uc758 region\ub4e4\uc758 \ubaa8\uc74c\uc73c\ub85c \ub098\ub20c \uc218 \uc788\ub2e4. \ubd84\ub958\ubb38\uc81c\uc5d0\uc11c \uc120\ud615\ubaa8\ub378\uc744 \ub17c\ud55c\ub2e4\ub294 \uac83\uc740, Decision Boundary \uac00 \uc120\ud615\uc774\ub77c\ub294 \uac83\uc774\ub2e4. \uc120\ud615 \uacb0\uc815\uacc4\ub294 \ub2e4\uc591\ud55c \ubc29\ubc95\uc744 \ud1b5\ud574 \ucc3e\uc744 \uc218 \uc788\ub294\ub370, 2\uc7a5\uc5d0\uc11c\ub294 \ubc94\uc8fc\ud615 \uc9c0\uc2dc \ubcc0\uc218\ub4e4\ub85c \uc120\ud615 \ud68c\uadc0 \ubaa8\ub378\uc744 \uc801\ud569\ud558\uc5ec \uc801\ud569\uac12\uc758 \ud06c\uae30\ub300\ub85c \ubd84\ub958\ud558\uae30\ub3c4 \ud558\uc600\ub2e4. \\(K\\) \uac1c\uc758 \ubc94\uc8fc\ud615 \ubcc0\uc218\uac00 \uc788\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uc790. \uadf8\ub9ac\uace0 k\ubc88\uc9f8 \uc9c0\uc2dc\uc885\uc18d\ubcc0\uc218\uc5d0 \ub300\ud574 \uc801\ud569\ud55c \uc120\ud615\ubaa8\ub378\uc774 \\(\\hat{f}_k(x)=\\hat{\\beta}_{k0}+\\hat{\\beta}^T_kx\\) \ub77c\uace0 \ud574\ubcf4\uc790. \ubc94\uc8fc k\uc640 l\uc744 \uad6c\ubd84\ud558\ub294 decision boundary\ub294 \\(\\hat{f}_k(x)=\\hat{f}_l(x)\\) \ub97c \ub9cc\uc871\ud558\ub294 \uc810\ub4e4\uc774 \ub420 \uac83\uc774\ub2e4. \uc774\ub294 \uc5b4\ub5a4 \ubc94\uc8fc\uc758 \uc30d\uc5d0\uc11c\ub4e0 \ucc38\uc774\uae30 \ub54c\ubb38\uc5d0, \uc785\ub825\uacf5\uac04\uc740 piecewise hyperplanar decision boundary\ub4e4\ub85c \ub098\ub258\uac8c \ub41c\ub2e4. \uc774\ub7ec\ud55c \ud68c\uadc0\uc801\uc778 \uc811\uadfc\uc740 \uac01 \ubc94\uc8fc\uc5d0 discriminant functions \\(\\sigma_k(x)\\) \ub97c \ubaa8\ub378\ub9c1\ud558\uace0 x\ub97c discriminant function\uc774 \uac00\uc7a5 \ud070 \uac12\uc744 \uac16\ub294 \ubc94\uc8fc\uc5d0 \ubd84\ub958\ud558\ub294 \ubc29\ubc95 \uc911 \ud558\ub098\uc774\ub2e4. posterior probability \\(Pr(G=k|X=x)\\) \ub97c \ubaa8\ub378\ub9c1 \ud558\ub294 \uac83 \ub610\ud55c \uc774\ub7ec\ud55c \ubc29\ubc95\uc758 \ud55c \ubd80\ub958\uc774\ub2e4. \\(\\sigma_k(x)\\) \ub610\ub294 \\(Pr(G=k|X=x)\\) \uac00 x\uc5d0 \ub300\ud574 \uc120\ud615\uc774\ub77c\uba74, decision boundary \ub610\ud55c \uc120\ud615\uc77c \uac83\uc774\ub2e4. \uc6b0\ub9ac\uac00 \uc5ec\uae30\uc11c \ud544\uc694\ub85c \ud558\ub294 \uc870\uac74\uc740, \\(\\sigma_k(x)\\) \ub610\ub294 \\(Pr(G=k|X=x)\\) \uc758 monotone transformation(ex. log\ubcc0\ud658)\uc758 \uacb0\uacfc\uac00 \uc120\ud615\uc131\ub9cc \ub744\uba74 \ub41c\ub2e4! \uc774\ub7ec\ud55c \uc608\ub85c \uac00\uc7a5 \uc720\uba85\ud55c monotone transformation\uc740 \ub2e4\ub4e4 \uc54c\ub2e4\uc2dc\ud53c logit \ubcc0\ud658\uc744 \uc608\ub85c \ub4e4 \uc218 \uc788\ub2e4. \\(Pr(G=1|X=x)=\\frac{exp(\\beta_0+\\beta^Tx)}{1+exp(\\beta_0+\\beta^Tx)}\\) \\(Pr(G=2|X=x)=1-\\frac{exp(\\beta_0+\\beta^Tx)}{1+exp(\\beta_0+\\beta^Tx)}\\) \\(logit(\\hat{p})=log\\frac{\\hat{p}}{1-\\hat{p}}\\) \uc774\ub7f0 \ubcc0\ud658\uc758 \uacb0\uacfc\uac00 linear decision boundary\ub97c \uc774\ub8e8\ub294 \uac83\uc740 \ub2e4\uc74c\uc744 \ubcf4\uba70 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \\(log\\frac{Pr(G=1|X=x)}{Pr(G=2|X=x)}=\\beta_0+\\beta^Tx\\) \uc774 \uacbd\uc6b0 decision boundary\ub294 log odds \uac00 0\uc774 \ub418\ub294 \uc9c0\uc810\ub4e4\uc774\uace0(odds= \\(\\frac{p}{1-p}\\) ), \uc774\ub294 \\({\\{x|\\beta_0+\\beta^Tx=0}\\}\\) \ub85c \uc815\uc758\ub418\ub294 \ucd08\ud3c9\uba74\uc77c \uac83\uc774\ub2e4. \uc6b0\ub9ac\ub294 \uc5ec\uae30\uc11c \ub450\uac1c\uc758 \ubc29\ubc95\ub860\uc744 \ub2e4\ub8f0 \uac83\uc774\ub2e4. \uc774\ub294 \uc720\uba85\ud558\uc9c0\ub9cc \uc5c4\uc5f0\ud788\ub294 \ub9e4\uc6b0 \ub2e4\ub978 \ubc29\ubc95\uc774\uba70, \uc774\ub294 \uc120\ud615\uc758 log odds \ud639\uc740 logits\uc744 \uac16\ub294 linear discriminant analysis(LDA), \uadf8\ub9ac\uace0 linear logistic regression\uc774\ub2e4. \uc774 \ub458\uc758 \ubcf8\uc9c8\uc801\uc778 \ucc28\uc774\ub294 linear function\uc774 training data\uc5d0 \uc801\ud569\ub418\ub294 \ubc29\uc2dd\uc5d0 \uc788\ub2e4. \uc9c1\uc811\uc801\uc778 \ubc29\ubc95\ub860\uc73c\ub85c\ub294 \ubc94\uc8fc\ub4e4 \uc0ac\uc774\uc758 decision boundary\ub97c \uc120\ud615\uc73c\ub85c \ubaa8\ub378\ub9c1\ud558\ub294 \uac83\uc778\ub370, \uc774\ub294 p\ucc28\uc6d0\uc758 \uc785\ub825 \uacf5\uac04\uc744 \uac16\ub294 \uc774\uc9c4 \ubd84\ub958 \ubb38\uc81c\uc5d0\uc11c \uacb0\uc815\uacc4\ub97c \ucd08\ud3c9\uba74\uc73c\ub85c \ubaa8\ub378\ub9c1 \ud558\ub294 \uac83\uc774 \ub41c\ub2e4. \uc774\ub294 normal vector \uc640 cut point\ub85c \ucd08\ud3c9\uba74\uc744 \ubd84\ub9ac\ud558\ub294 \uc6d0\ub9ac\uc778\ub370, \uc774\ub807\uac8c \ucd08\ud3c9\uba74\uc744 \ubd84\ub9ac\ud558\ub294 \ubc29\ubc95\uc911 \ub300\ud45c\uc801\uc778 \ub450 \uac00\uc9c0\ub294 \uc544\ub798\uc640 \uac19\ub2e4. \uba3c\uc800 perception \ubaa8\ub378\ub85c, \uc774\ub294 training data\ub0b4\uc5d0 \ub370\uc774\ud130\ub97c \ubd84\ub9ac\uc2dc\ud0a4\ub294 \ucd08\ud3c9\uba74\uc774 \uc788\ub2e4\uba74 \ucc3e\uc544\uc8fc\ub294 \uc54c\uace0\ub9ac\uc998\uc774\uace0, \ub450\ubc88\uca30\ub85c\ub294 optimally separating hyperplane\uc774 \uc874\uc7ac\ud55c\ub2e4\uba74 \uc774\ub97c \ucc3e\uace0, \uc544\ub2c8\ub77c\uba74 training data \ub0b4\uc5d0 \uacb9\uce58\ub294 \ubd80\ubd84\uc5d0 \ub300\ud55c \uce21\ub610\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ucd08\ud3c9\uba74\uc744 \ucc3e\ub294 \ubc29\ubc95 \uc774\ub2e4. \uc774 \ub2e8\uc6d0\uc5d0\uc11c\ub294 \uc120\ud615\uacb0\uc815\uacc4\uc5d0\uc11c \ub2e4\ub8e8\uc9c0\ub9cc, \uc774\ub294 \uc77c\ubc18\ud654\ub97c \ud55c \uacbd\uc6b0\uc774\uba70, \uc608\ub97c \ub4e4\uc5b4 \uc6b0\ub9ac\uac00 \ubcc0\uc218\uc9d1\ud569\uc744 \ub2e8\uc9c0 \ub2e8\uc77c\ubcc0\uc218\ub9cc \uce74\uc6b4\ud305\ud558\ub294 \uac8c \uc544\ub2c8\ub77c, cross-product(\ubca1\ud130\uacf1\uacfc \uc678\uc801)\uc774\ub098 \uc81c\uacf1\ud56d\ub4f1\uc744 \ucd94\uac00\ud558\uc5ec \ucda9\ubd84\ud788 \ud655\uc7a5\uc2dc\ud0ac \uc218 \uc788\ub2e4. \uc774\ub7f0 \ubc29\ubc95\uc740 \uc120\ud615\ud568\uc218\uac00 \uc774\ucc28\ud568\uc218\ub97c \ud1b5\ud574 \uc99d\uac15\ub41c \uacf5\uac04 \ub0b4\uc5d0 \uc874\uc7ac\ud558\uac8c \ub428\uc73c\ub85c\uc368, \uc774\ub7f0 \ucf00\uc774\uc2a4\uc5d0\uc11c linear decision boundarty\ub294 quadratic decision bounday\uac00 \ub418\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. Linear Regression of an Indicator Matrix \uc5ec\uae30\uc11c \uac01 \uc885\uc18d\ubcc0\uc218\uc758 \ubc94\uc8fc\ub4e4\uc740 \uc9c0\uc2dc\ubcc0\uc218\ub85c \ucf54\ub529\ub41c\ub2e4. \ub530\ub77c\uc11c \ubc94\uc8fc\uc758 \uac1c\ubcc4 \ud074\ub798\uc2a4\uc758 \uac2f\uc218\uac00 K\uac1c\ub77c\uba74, \uc9c0\uc2dc\ubcc0\uc218 \ub610\ud55c \\(Y_k\\) \ub85c K\uac1c\uac00 \uc788\uc744 \uac83\uc774\uba70 \\(G=k\\) \uba74 \\(Y_k=1 \\ or\\ Y_k=0\\) \uc77c \uac83\uc774\ub2e4. \uc774\ub4e4\uc740 \ubaa8\uc5ec\uc11c \\(Y=(Y_1,..,Y_k)\\) \uc778 \ubca1\ud130\ub97c \uc774\ub8e8\uace0 N\uac1c\uc758 training instances \ub4e4\uc740 \\(N \\times K\\) \uc758 indicatior response matrix Y \ub97c \uc774\ub8ec\ub2e4. \uc774\ub7ec\ud55c matrix\ub294 \uac01\ud589\uc5d0 1\uac1c\uc758 1\uc744 \uac16\uace0, 0\uacfc 1\ub85c\ub9cc \uc774\ub8e8\uc5b4\uc9c4 \ud589\ub82c\uc77c \uac83\uc774\ub2e4. \uc774\ub7ec\ud55c matrix Y\uc758 \uac01 \uc5f4\uc5d0 \uc120\ud615 \ud68c\uadc0 \ubaa8\ud615\uc744 \uc801\ud569\ud558\uba74 \uc6b0\ub9ac\ub294 \uc544\ub798\uc640 \uac19\uc740 \uacb0\uacfc\ub97c \uc5bb\uc744 \uac83\uc774\ub2e4. $ \\(\\hat{Y}=X(X^TX)^{-1}X^TY\\) $ \uac01 \uc885\uc18d \uc5f4 \\(y_k\\) \uc5d0 \ub300\ud55c \uacc4\uc218 \ubca1\ud130\uac00 \uc788\uace0, \\((p+1) \\times K\\) \uc758 \uacc4\uc218 \ud589\ub82c \\(\\hat{B}=(X^TX)^{-1}X^TY\\) \uc774 \uc874\uc7ac\ud55c\ub2e4. \ub530\ub77c\uc11c X\ub294 p\uac1c\uc758 \ub3c5\ub9bd\ubcc0\uc218\uac00 \uc788\uc744 \ub54c \uc808\ud3b8 \uacc4\uc218\ub97c \ud3ec\ud568\ud558\ub294 p+1\uac1c\uc758 \uc5f4\uc744 \uac16\ub294 model matrix\uc774\ub2e4. \uc785\ub825\uac12 x\ub97c \uac16\ub294 \uc0c8\ub85c\uc6b4 \uad00\uce21\uce58\uc5d0 \ub300\ud55c \ubd84\ub958\ub294 \uc544\ub798\uc640 \uac19\ub2e4. \ucd94\uc815\ub41c \ucd9c\ub825\uac12\uc778 K\uac1c\uc758 \ubca1\ud130 \\(\\hat{f}(x)^T=(1,x^T)\\hat{B}\\) \ub97c \uacc4\uc0b0\ud55c\ub2e4. \uac00\uc7a5 \ud070 \uac12\uc744 \ud30c\uc545\ud558\uc5ec \uc544\ub798\uc640 \uac19\uc774 \ubd84\ub958\ud55c\ub2e4. \\[\\hat{G}(x)=argmax_k\\hat{f}_k(x)\\] \ud5c8\ub098 \uacfc\uc5f0 \uc774\ub7ec\ud55c \uc811\uadfc\ubc29\ubc95\uc758 \uc815\ub2f9\uc131\uc740 \uc5b4\ub514\ub85c\ubd80\ud130 \ub098\uc62c\uae4c? \ud55c\uac00\uc9c0 \uc815\ub2f9\ud654\ub85c\ub294 \uc774\ub7ec\ud55c \ud68c\uadc0 \ubc29\ubc95\ub860\uc744 \uc870\uac74\ubd80 \uae30\ub313\uac12\uc5d0 \ub300\ud55c \ucd94\uc815\uce58\ub85c \ubcf4\ub294 \uac83\uc774\ub2e4. \uc6b0\ub9ac\uac00 2\uc7a5\uc5d0\uc11c \uc0b4\ud3c8\ub4ef\uc774, \uc6b0\ub9ac\ub294 KNN\uac19\uc740 classification\uc758 \uacbd\uc6b0\ub098 \uc120\ud615 \ud68c\uadc0 \ubaa8\ub378\uc5d0\uc11c \ud568\uc218\uadfc\uc0ac\uc758 \ucd5c\uc801\ud574\uac00 x\uc5d0 \ub300\ud55c Y\uc758 \uc870\uac74\ubd80\ud3c9\uade0\uc774\ub77c\ub294 \uac83\uc744 \ud655\uc778\ud55c \uc801\uc774 \uc788\uc5c8\ub2e4.(\ud68c\uadc0\uc5d0\uc11c\ub294 \ucd5c\uc18c\uc790\uc2b9\ubc95\uc744 \ud1b5\ud574 \uc870\uac74\ubd80\uae30\ub313\uac12\uc774 \ucd5c\uc801\uc758 \ud574\ub77c\ub294 \uac83\uc744 \ubc1d\ud614\ub358 \uc801\uc774 \uc788\ub2e4. \uc544\ubb34\ud2bc \uc788\ub2e4.) \uace0\ub85c \ud655\ub960\ubcc0\uc218 \\(Y_k\\) \uac00 \uc788\ub2e4\uace0 \ud560\ub54c, \\(E(Y_k|X=x)=Pr(G=k|X=x)\\) \uc774\uba70 \ub530\ub77c\uc11c \uac01 \\(Y_k\\) \uc5d0 \ub300\ud55c \uc870\uac74\ubd80 \uae30\ub313\uac12\uc774 \uad81\uadf9\uc801\uc778 \ubaa9\ud45c\uac00 \ub418\ub294 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub7ec\ud55c \ubc29\ubc95\uc758 real issue\ub294 \uc870\uac74\ubd80 \uae30\ub313\uac12\uc5d0 \ub300\ud55c \uadfc\uc0ac\uac00 \uc5c4\uaca9\ud55c \uc120\ud615 \ud68c\uadc0 \ubaa8\ud615\ubcf4\ub2e4 \uc5bc\ub9c8\ub098 \ub354 \uc88b\ub0d0\ub294 \uac83\uc774\ub2e4. \uc5ec\uae30\uc11c \ub2e4\ub8e8\ub294 \uc9c0\uc2dc\ubcc0\uc218\uc5d0 \ub300\ud55c \uc120\ud615\ud68c\uadc0\ub294 \uc0ac\uc2e4 \uadf8 \ucd94\uc815\uac12\uc774 \uc74c\uc218\uc774\uac70\ub098 1\ubcf4\ub2e4 \ud074 \uacbd\uc6b0\uac00 \uc874\uc7ac \ud55c\ub2e4. \uc774\ub294 \ud655\ub960\uc758 \uc815\uc758\uc5d0 \uc5b4\uae0b\ub098\ub294 \uac83\uc774\uc9c0\ub9cc, \ub610 \ub2e4\uc2dc \ubcf4\uba74 \uc774\ub7ec\ud55c \uadfc\uc0ac\ubc29\ubc95\uc774 \uc798 \uc791\ub3d9\ud558\uc9c0 \uc54a\uc744 \uac83\uc774\ub77c\ub294 \ubcf4\uc7a5\ub3c4 \uc5c6\uae34\ud558\ub2e4. \ub9cc\uc57d \uc6b0\ub9ac\uac00 \uc120\ud615\ud68c\uadc0\ub97c \ub3c5\ub9bd\ubcc0\uc218\ub4e4\uc758 \uae30\uc800 \ud655\uc7a5 \\(h(x)\\) \ub85c \uc801\ud569\ud55c\ub2e4\uba74, \uc774\ub7ec\ud55c \ubc29\ubc95\uc740 \ud655\ub960\uc5d0 \ub300\ud55c \uc77c\uce58\ucd94\uc815\uce58\ub85c \ub3c4\ucd9c\ub420 \uc218 \uc788\uc744 \uac83\uc774\ub2e4. \uc774\ub294 \ub9d0\ub85c\ub9cc \ud574\uc11c\ub294 \uc798 \uac10\uc774 \uc548\uc624\ub294\ub370 \ub2e4\ud589\ud788\ub3c4 5\uc7a5\uc5d0\uc11c \ub2e4\ub8ec\ub2e4\uace0 \ud55c\ub2e4.. \uac19\uc740 \ubb38\uc81c\uc5d0 \ub300\ud55c \ubcf4\ub2e4 \uc9c1\uad00\uc801\uc778 \uad00\uc810\uc740 \uac01 \ubc94\uc8fc\uc5d0 target \\(t_k\\) \ub97c \uc124\uc815\ud558\ub294 \uac83\uc774\ub2e4. \uc5ec\uae30\uc11c \\(t_k\\) \ub294 \\(K\\times K\\) \ud56d\ub4f1\ud589\ub82c\uc758 k\ubc88\uc9f8 \uc5f4\uc774\ub2e4. \uc6b0\ub9ac\uc758 \uc608\uce21\ubb38\uc81c\ub294 \uac01 \uad00\uce21\uce58\uc5d0 \uc801\ud569\ud55c \uc885\uc18d \ubcc0\uc218\uc758 \uac12\uc744 \uc0b0\ucd9c\ud558\ub294 \uac83\uc774\ub2e4. \uc774\ub294 \uad00\uce21\uce58 i\uc5d0 \ub300\ud55c \uc885\uc18d \ubca1\ud130 \\(y_i\\) (Y\uc758 i\ubc88\uc9f8 \ud589)\uac00 \\(g_i=k\\) \uba74 \\(y_i=t_k\\) \uc758 \uac12\uc744 \uac16\ub294 \uac83\uc774\ub2e4. \uc774\ub7ec\ud55c \ubaa8\ub378\uc5d0 \ub300\ud55c \ucd5c\uc18c\uc81c\uacf1\ubc95\uc5d0 \uc758\ud55c \uc120\ud615\ubaa8\ub378\uc744 \uc801\ud569\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\[min_B\\sum_{i=1}^N||y_i-[1,x_i^T)B]^T||^2\\] \uc774\ub7ec\ud55c \uc801\ud569\ub3c4\ub2c8 \ubca1\ud130\ub4e4\uc758 \uc885\uc18d\ubcc0\uc218 \uac12\ub4e4\ub85c\ubd80\ud130\uc758 \uc720\ud074\ub9ac\ub514\uc548 \uac70\ub9ac\ub4e4\uc758 \uc81c\uacf1\ud569\uc73c\ub85c \uc0c8\ub85c\uc6b4 \uad00\uce21\uce58\ub294 \uc801\ud569\ubca1\ud130 \\(\\hat{f}(x)\\) \ub97c \uacc4\uc0b0\ud558\uc5ec \uac00\uc7a5 \uac00\uae4c\uc6b4 \uc885\uc18d \ubcc0\uc218 \uac12\uc5d0 \ubd84\ub958\ud558\ub294 \ubc29\ubc95\uc744 \ud3ec\ud568\ud558\uac8c \ub420 \uac83\uc774\ub2e4. \\[\\hat{G}(x)=argmin_k||\\hat{f}(x)-t_k||^2\\] \uc774\ub7ec\ud55c \uc81c\uacf1\ud569 \uae30\uc900\uc740 multiple response linear regression\uacfc \ub9e4\uc6b0 \uc720\uc0ac\ud558\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub7ec\ud55c \ud68c\uadc0 \ubc29\ubc95\uc758 \uc2ec\uac01\ud55c \uacb0\uc810\uc740 \ubc94\uc8fc\uc758 \uac2f\uc218\uac00 3\uac1c \uc774\uc0c1\uc77c \ub54c \ubc1c\uc0dd\ud55c\ub2e4. \ud68c\uadc0 \ubaa8\ub378\uc758 \uc5c4\uaca9\ud55c \ubcf8\uc9c8\uc740 \ubc94\uc8fc\ub4e4\uc774 \ub2e4\ub978 \ubc94\uc8fc\ub4e4\uc5d0 \uc758\ud574 masked\ub420 \uc218 \uc788\ub2e4\ub294 \uac83\uc774\ub2e4. K\ub97c 3\uc73c\ub85c\ub9cc \ub193\uace0 LDA\uc640 linear regression of the indicator Variables\uc758 \ucc28\uc774\ub97c \ubd10\ub3c4, \uc120\ud615\ud68c\uadc0\uc5d0\uc11c middle class\ub97c \uc644\ubcbd\ud788 \ub193\uce58\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. Linear Discriminant Analysis(LDA) \ubd84\ub958 \ubb38\uc81c\uc5d0 \ub300\ud574 \uacb0\uc815\uacc4\ub294 \ucd5c\uc801\uc758 \ubd84\ub958\ub97c \uc704\ud574 \uc6b0\ub9ac\uac00 \ubc94\uc8fc\uc5d0 \uc18d\ud560 \uc0ac\ud6c4\ud655\ub960 \\(Pr(G|X)\\) \ub97c \uc54c\uc544\ub0b4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \\(f_k(x)\\) \ub97c \ubc94\uc8fc G=k\uc5d0\uc11c X\uc758 \ubc94\uc8fc \uc870\uac74\ubd80 \ubc00\ub3c4\ub77c\uace0 \ud558\uace0, \\(\\pi_k\\) \ub97c k\ubc94\uc8fc\uc5d0 \uc18d\ud560 \uc0ac\uc804 \ud655\ub960\uc774\ub77c\uace0 \ud558\uba74( \\(\\sum\\pi=1\\) ), \ubca0\uc774\uc988 \uc815\ub9ac\ub97c \ud1b5\ud574 \uc544\ub798\uc640 \uac19\uc740 \uacb0\uacfc\ub97c \ub3c4\ucd9c\ud560 \uc218 \uc788\ub2e4. \\[Pr(G=k|X=x)=\\frac{Pr(X|G)Pr(G)}{\\sum_{l=1}^GPr(X|G_l)PR(G_l)}\\] \ubd84\ub958\uc758 \ub2a5\ub825 \uce21\uba74\uc5d0\uc11c \\(f_k(x)\\) \ub97c \uc544\ub294 \uac83\uc774 \\(Pr(G=k|X=x)\\) \ub97c \uc544\ub294 \uac83\uacfc \uac19\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \ubc94\uc8fc \ubc00\ub3c4\uc5d0 \uae30\ubc18\ud55c \ubaa8\ub378\ub4e4\uc5d0 \ub300\ud55c \ub9ce\uc740 \uae30\uc220\ub4e4\uc774 \uc788\ub2e4. \ud558\uc9c0\ub9cc \uc6b0\ub9ac\ub294 \uc5ec\uae30\uc11c \uc77c\ub840\ub85c \uac01 \ubc94\uc8fc \ubc00\ub3c4\ub97c \ub2e4\ubcc0\ub7c9 \uc815\uaddc \ubd84\ud3ec\ub85c \ubaa8\ub378\ub9c1\ud55c\ub2e4\uace0 \ud574\ubcf4\uc790. \\[f_k(x)=\\frac{1}{(2\\pi)^{p2}|\\sum_k|^{1/2}}e^{-\\frac{1}{2}(x-\\mu_K)\\sum_k^{-1}(x-\\mu_k)}\\] LDA\ub294 \uc6b0\ub9ac\uac00 \ubc94\uc8fc\ub4e4\uc774 \uac19\uc740 \uacf5\ubd84\uc0b0 \ud589\ub82c\uc744 \uac16\ub294\ub2e4\ub294 \ud2b9\uc218\ud55c \uac00\uc815\uc744 \ud1b5\ud574 \uc2dc\uc791\ub41c\ub2e4. \uc989 LDA\ub294 \ub2e4\ubcc0\ub7c9 \uc815\uaddc\ubd84\ud3ec \uac00\uc815\uacfc \uc870\uac74\ubd80 \ubd84\ud3ec\ub4e4\uc758 \ub4f1\ubd84\uc0b0\uc131 \uac00\uc815\uc774 \uc874\uc7ac\ud55c\ub2e4. \uc5ec\uae30\uc11c \ub450 \ubc94\uc8fc k\uc640 l\uc744 \ube44\uad50\ud560\ub54c, log-ratio\ub97c \ub4e4\uc5ec\ub2e4\ubcf4\ub294 \uac83\uc774 \ucda9\ubd84\ud788 \uac00\ub2a5\ud55c\ub370, \uc2dd\uc744 \ud1b5\ud574 \uc6b0\ub9ac\ub294 log-ratio\uac00 x\uc5d0 \ub300\ud574 \uc120\ud615\uc784\uc744 \uc54c \uc218\uc788\ub2e4. \\[log\\frac{Pr(G=k|X=x)}{Pr(G=l|X=x)}=log\\frac{f_k(x)}{f_l(x)}+log\\frac{\\pi_k}{\\pi_l}\\] \\[=log(exp(-\\frac{1}{2}(x-\\mu_k)^T{\\sum}^{-1}(x-\\mu_k)+log\\frac{\\pi_k}{\\pi_l} \\frac{1}{2}(x-mu_l)^T{\\sum}^{-1}(x-\\mu_l)))\\] \\[=log\\frac{\\pi_k}{\\pi_l}-\\frac{1}{2}(\\mu_k+\\mu_l)^T{\\sum}^{-1}(\\mu_k-\\mu_l)+x^T{\\sum}^{-1}(\\mu_k-\\mu_l)\\] \ub3d9\ub4f1\ud55c \uacf5\ubd84\uc0b0 \ud589\ub82c\ub4e4\uc740 normalization factors\ub4e4\uc774 \uc18c\uac70 \ub418\uac8c \ud558\uba70, exponent\ub4e4\uc758 2\ucc28\ud56d \ub610\ud55c \uc18c\uac8c\ub418\uac8c \ub9cc\ub4e0\ub2e4. \uc774\ub7ec\ud55c \uc120\ud615 log-odds \ud568\uc218\ub294 k\uc640 l\uc744 \uad6c\ubd84\ud558\ub294 \uacb0\uc815\uacc4\uac00 \\(Pr(G=k|X=x)=Pr(G=l|X=x)\\) \uac00 \ub418\uac8c\ud558\ub294 \uc9c0\uc810\ub4e4\uc758 \uc9d1\ud569\uc774\uba70, x\uc5d0 \ub300\ud574 \uc120\ud615\uc778 p\ucc28\uc6d0\uc5d0\uc11c\uc758 \ucd08\ud3c9\uba74\uc784\uc744 \uc758\ubbf8\ud55c\ub2e4. \ub530\ub77c\uc11c \ubaa8\ub4e0 \uacb0\uc815\uacc4\ub4e4\uc774 \uc120\ud615\uc774\uba70 \\(R^p\\) \ub97c \ubc94\uc8fc\ub4e4\ub85c \ub098\ub258\ub294 \uc9c0\uc5ed\ub4e4\ub85c \ub098\ub204\uba74, \uc774\ub7ec\ud55c \uc9c0\uc5ed\ub4e4\uc774 \ucd08\ud3c9\uba74\uc5d0 \uc758\ud574 \ubd84\ub9ac \ub418\uac8c \ub41c\ub2e4. LDA\uc5d0\uc11c \ucd94\uc815\uc744 \ud560 \uacbd\uc6b0, \ub2e4\ubcc0\ub7c9 \uc815\uaddc\ubd84\ud3ec\ub4e4\uc758 \uacf5\uc720\ud558\ub294 \uacf5\ubd84\uc0b0 \ud589\ub82c\uacfc \ud3c9\uade0\uc5d0 \ub300\ud574\uc11c \ucd94\uc815\uc744 \ud574\uc57c\ud558\ub294\ub370, \uc774\ub294 \uc6b0\ub9ac\uac00 \ub2e4\ubcc0\ub7c9 \uc815\uaddc\ubd84\ud3ec\uc5d0\uc11c\uc758 MLE\ub97c \uc54c\uae30\ub54c\ubb38\uc5d0, \uc774\ub97c \uc774\uc6a9\ud558\uc5ec \ucd94\uc815\ud558\uba74 \ub41c\ub2e4. \uc774 \uacbd\uc6b0 \uc6b0\ub9ac\ub294 Linear Discriminant functions\uc774 \ub2e4\uc74c\uacfc \uac19\uc74c\uc744 \ud655\uc778 \uac00\ub2a5\ud558\ub2e4. \\[\\sigma_k(x)=x^T_{(1\\times p)}{\\sum}^{-1}_{(p\\times p)}\\mu_{k{(p\\times 1)}}-\\frac{1}{2}\\mu_{k{(1\\times p)}}^T{\\sum}^{-1}_{(p\\times p)}\\mu_{k{(p\\times 1)}}+ log\\pi_{k (p\\times 1)}\\] \uc0ac\uc804\ud655\ub960 \\(\\hat{\\pi_k}=N_k/N\\) \ud3c9\uade0 \\(\\hat{mu_k}=\\sum_{g_i=k}x_i/N_k\\) \uacf5\ubd84\uc0b0\ud589\ub82c \\(\\hat{\\sum}=\\sum_{g_i=k}(x_i-\\hat{\\mu_k})(x_i-\\hat{\\mu_k})^T/(N-k)\\) \uacf5\ubd84\uc0b0 \ud589\ub82c\uc758 \uacbd\uc6b0 \ubd84\ud3ec\uc758 N-K\ub97c \ud1b5\ud574 MLE\uac00 \uc544\ub2cc \ud45c\ubcf8\ubd84\uc0b0\uc744 \uc0ac\uc6a9\ud55c\ub2e4. \uacb0\uad6d \uac00\uc911\ud3c9\uade0\uc778 \uc148. example 3 classes and p=2 \uac19\uc740 \uacf5\ubd84\uc0b0 \ud589\ub82c\uc744 \uac16\ub294 \uc815\uaddc\ubd84\ud3ec\uc5d0\uc11c \uc790\ub8cc\uac00 \uc0d8\ud50c\ub9c1 \ub418\uc5c8\ub2e4\ub294 \uac00\uc815 \ud558\uc5d0\uc11c LDA\ub97c \uc2dc\ud589\ud574\ubcf4\uc558\ub2e4. \\(X|K = 1 \\sim N_2(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\begin{bmatrix} 1 & 0.4 \\\\ 0.4 & 1 \\end{bmatrix}), n_1= 500\\) \\(X|K = 2 \\sim N_2(\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix},\\begin{bmatrix} 1 & 0.4 \\\\ 0.4 & 1 \\end{bmatrix}), n_2= 1000\\) \\(X|K = 3 \\sim N_2(\\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix},\\begin{bmatrix} 1 & 0.4 \\\\ 0.4 & 1 \\end{bmatrix}), n_3= 1500\\) ## generating 3 bivariate normal distributions with a common covariance matrix using gaussian copula library(ggplot2) ## Warning: As of rlang 0.4.0, dplyr must be at least version 0.8.0. ## \u2716 dplyr 0.7.8 is too old for rlang 0.4.1. ## \u2139 Please update dplyr with `install.packages(\"dplyr\")`. set.seed(2013122059) # defining function using gaussian copula generator_gc=function(n,mu,rho,diag){ R=rbind(c(1,rho),c(rho,1)) A=t(chol(R)) U=matrix(nrow=n,ncol=2) W=matrix(nrow=n,ncol=2) # simulation for (i in 1:n){ Z=rnorm(2,0,1) W=A%*%Z U[i,]=pnorm(W) } X=matrix(nrow=n,ncol=2) colnames(X)=paste0('X',1:2) for (j in 1:2){ X[,j]=qnorm(U[,j],mean=mu[j],sd=sqrt(diag[j])) } return (X) } # parameters n=c(500,1000,1500) mu=rbind(c(0,0),c(2,2),c(4,0)) rho=0.4 var=c(1,1) for (i in 1:3){ class=paste0('k',i) K=factor(rep(i,n[i])) assign(class,data.frame(generator_gc(n=n[i],mu=mu[i,],rho=rho,diag=var),K)) } df3=data.frame(rbind(k1,k2,k3)) # visualization of simulated samples ggplot(df3,aes(x=X1,y=X2,color=K))+ geom_point()+ theme_bw()+ ggtitle('Simulated 3 Normal Bivariate Random Samples')+ theme(plot.title=element_text(face='bold')) # defining linear discriminant function for 3 classes ldf=function(df){ result=list() # getting prior, mu and common sigma estimates N=nrow(df) p=ncol(df)-1 k=length(unique(df$K)) nk=numeric(k) muhat=matrix(nrow=k,ncol=p) xk=list() ss=list() for (i in 1:k){ x=as.matrix(df[df$K==i,-ncol(df)]) xk[[i]]=x nk[i]=nrow(x) muhat[i,]=as.matrix(apply(x,2,mean)) dev=apply(x,1,function(x)x-muhat[i,]) ss[[i]]=dev%*%t(dev) } pihat=prop.table(nk) sigmahat=Reduce('+',ss)/(N-k) result[['pihat']]=pihat result[['muhat']]=muhat result[['sigmahat']]=sigmahat # discriminant functions for each class delta=function(x,sigmahat,muhat,pihat){ return (t(x)%*%solve(sigmahat)%*%muhat -0.5*(t(muhat)%*%solve(sigmahat)%*%muhat)+log(pihat)) } delta_mat=matrix(nrow=N,ncol=k) for (j in 1:k){ # xk should be matrix! delta_mat[,j]=apply(as.matrix(df[,1:p]), 1,delta, sigmahat=sigmahat, muhat=muhat[j,], pihat=pihat[j]) } result[['delta mat']]=delta_mat return (result) } result=ldf(df3) delta_mat=result[['delta mat']] colnames(delta_mat)=c('K1','K2','K3') # decision rule.. which class' value is maximum? df3$Khat=factor(apply(delta_mat,1,which.max)) ## using contour for visualization of decision boundary # using contour of posterior probabilities for visualization of decision boundary pihat=result[['pihat']] muhat=result[['muhat']] sigmahat=result[['sigmahat']] gridx=seq(min(df3[,1]),max(df3[,1]),length.out=100) gridy=seq(min(df3[,2]),max(df3[,2]),length.out=100) grid=expand.grid(gridx,gridy) likelihood=function(x,sigmahat,muhat){ return (exp(-0.5*t(x-muhat)%*%solve(sigmahat)%*%(x-muhat))) } posterior=matrix(nrow=nrow(grid), ncol=3) for (k in 1:3){ posterior[,k]=apply(as.matrix(grid[,1:2]),1,likelihood,sigmahat=sigmahat,muhat=muhat[k,])*pihat[k] } posterior=t(apply(posterior,1,prop.table)) posterior=cbind(grid,posterior) colnames(posterior)=c('x1','x2','k1','k2','k3') # final visualization using contour of posterior probabilities ggplot(df3,aes(x=X1,y=X2))+ geom_point(aes(color=K))+ geom_contour(data=posterior,aes(x=x1,y=x2,z=k1),breaks=c(0,0.5),color='black',size=3, lty=2)+ geom_contour(data=posterior,aes(x=x1,y=x2,z=k2),breaks=c(0,0.5),color='black',size=3, lty=2)+ theme_bw()+ ggtitle('Linear Decision Boundary of 3 Classes LDA Solution')+ theme(plot.title=element_text(face='bold')) \ucc45\uc5d0\uc11c\ub294 \uc0ac\ud6c4 \ud655\ub960\uc758 \ub300\uc18c\ub97c \ube44\uad50\ud558\uc5ec \ubc94\uc8fc\ub97c \uacb0\uc815\ud558\ub294 \uacfc\uc815\uc5d0\uc11c \uc874\uc7ac\ud558\ub294 \uc815\uaddc\uc131 \uac00\uc815\uc5d0 \ub300\ud55c \uac15\uc870\uc640 \ud568\uaed8 \uc815\uaddc\uc131 \uac00\uc815\uc744 \ub9cc\uc871\ud558\uc9c0 \uc54a\ub294 \uacbd\uc6b0 cut-point\ub97c empirically\ud558\uac8c \ud6c8\ub828 \ub370\uc774\ud130\uc758 \uc5d0\ub7ec\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ubc29\ud5a5\uc73c\ub85c \uc120\ud0dd\ud560\uc218\ub3c4 \uc788\uc74c\uc744 \uc774\uc57c\uae30\ud558\uace0 \uc788\ub2e4.","title":"04 Linear Methods for Classifications"},{"location":"02%20ESL/04_Linear_Methods_for_Classifications/#04-linear-methods-for-classifications","text":"","title":"04 Linear Methods for Classifications"},{"location":"02%20ESL/04_Linear_Methods_for_Classifications/#introduction","text":"\ubcf8 \ub2e8\uc6d0\uc5d0\uc11c\ub294 classifications\uc5d0 \ub300\ud574\uc11c \ub2e4\ub8f0 \uc608\uc815\uc774\uba70 \ubd84\ub958 \ubb38\uc81c\uc5d0 \uad00\ud574\uc11c linear model\uc5d0 \ub300\ud574\uc11c \ub17c\ud55c\ub2e4. \uc6b0\ub9ac\ub294 \ub3c5\ub9bd\ubcc0\uc218 \\(G(x)\\) \uac00 \uc774\uc0b0\uc9d1\ud569\uc5d0\uc11c \uac12\uc744 \uac00\uc9c8 \ub54c, \uc785\ub825 \uacf5\uac04\uc744 labelling\ub41c \uc77c\uc885\uc758 region\ub4e4\uc758 \ubaa8\uc74c\uc73c\ub85c \ub098\ub20c \uc218 \uc788\ub2e4. \ubd84\ub958\ubb38\uc81c\uc5d0\uc11c \uc120\ud615\ubaa8\ub378\uc744 \ub17c\ud55c\ub2e4\ub294 \uac83\uc740, Decision Boundary \uac00 \uc120\ud615\uc774\ub77c\ub294 \uac83\uc774\ub2e4. \uc120\ud615 \uacb0\uc815\uacc4\ub294 \ub2e4\uc591\ud55c \ubc29\ubc95\uc744 \ud1b5\ud574 \ucc3e\uc744 \uc218 \uc788\ub294\ub370, 2\uc7a5\uc5d0\uc11c\ub294 \ubc94\uc8fc\ud615 \uc9c0\uc2dc \ubcc0\uc218\ub4e4\ub85c \uc120\ud615 \ud68c\uadc0 \ubaa8\ub378\uc744 \uc801\ud569\ud558\uc5ec \uc801\ud569\uac12\uc758 \ud06c\uae30\ub300\ub85c \ubd84\ub958\ud558\uae30\ub3c4 \ud558\uc600\ub2e4. \\(K\\) \uac1c\uc758 \ubc94\uc8fc\ud615 \ubcc0\uc218\uac00 \uc788\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uc790. \uadf8\ub9ac\uace0 k\ubc88\uc9f8 \uc9c0\uc2dc\uc885\uc18d\ubcc0\uc218\uc5d0 \ub300\ud574 \uc801\ud569\ud55c \uc120\ud615\ubaa8\ub378\uc774 \\(\\hat{f}_k(x)=\\hat{\\beta}_{k0}+\\hat{\\beta}^T_kx\\) \ub77c\uace0 \ud574\ubcf4\uc790. \ubc94\uc8fc k\uc640 l\uc744 \uad6c\ubd84\ud558\ub294 decision boundary\ub294 \\(\\hat{f}_k(x)=\\hat{f}_l(x)\\) \ub97c \ub9cc\uc871\ud558\ub294 \uc810\ub4e4\uc774 \ub420 \uac83\uc774\ub2e4. \uc774\ub294 \uc5b4\ub5a4 \ubc94\uc8fc\uc758 \uc30d\uc5d0\uc11c\ub4e0 \ucc38\uc774\uae30 \ub54c\ubb38\uc5d0, \uc785\ub825\uacf5\uac04\uc740 piecewise hyperplanar decision boundary\ub4e4\ub85c \ub098\ub258\uac8c \ub41c\ub2e4. \uc774\ub7ec\ud55c \ud68c\uadc0\uc801\uc778 \uc811\uadfc\uc740 \uac01 \ubc94\uc8fc\uc5d0 discriminant functions \\(\\sigma_k(x)\\) \ub97c \ubaa8\ub378\ub9c1\ud558\uace0 x\ub97c discriminant function\uc774 \uac00\uc7a5 \ud070 \uac12\uc744 \uac16\ub294 \ubc94\uc8fc\uc5d0 \ubd84\ub958\ud558\ub294 \ubc29\ubc95 \uc911 \ud558\ub098\uc774\ub2e4. posterior probability \\(Pr(G=k|X=x)\\) \ub97c \ubaa8\ub378\ub9c1 \ud558\ub294 \uac83 \ub610\ud55c \uc774\ub7ec\ud55c \ubc29\ubc95\uc758 \ud55c \ubd80\ub958\uc774\ub2e4. \\(\\sigma_k(x)\\) \ub610\ub294 \\(Pr(G=k|X=x)\\) \uac00 x\uc5d0 \ub300\ud574 \uc120\ud615\uc774\ub77c\uba74, decision boundary \ub610\ud55c \uc120\ud615\uc77c \uac83\uc774\ub2e4. \uc6b0\ub9ac\uac00 \uc5ec\uae30\uc11c \ud544\uc694\ub85c \ud558\ub294 \uc870\uac74\uc740, \\(\\sigma_k(x)\\) \ub610\ub294 \\(Pr(G=k|X=x)\\) \uc758 monotone transformation(ex. log\ubcc0\ud658)\uc758 \uacb0\uacfc\uac00 \uc120\ud615\uc131\ub9cc \ub744\uba74 \ub41c\ub2e4! \uc774\ub7ec\ud55c \uc608\ub85c \uac00\uc7a5 \uc720\uba85\ud55c monotone transformation\uc740 \ub2e4\ub4e4 \uc54c\ub2e4\uc2dc\ud53c logit \ubcc0\ud658\uc744 \uc608\ub85c \ub4e4 \uc218 \uc788\ub2e4. \\(Pr(G=1|X=x)=\\frac{exp(\\beta_0+\\beta^Tx)}{1+exp(\\beta_0+\\beta^Tx)}\\) \\(Pr(G=2|X=x)=1-\\frac{exp(\\beta_0+\\beta^Tx)}{1+exp(\\beta_0+\\beta^Tx)}\\) \\(logit(\\hat{p})=log\\frac{\\hat{p}}{1-\\hat{p}}\\) \uc774\ub7f0 \ubcc0\ud658\uc758 \uacb0\uacfc\uac00 linear decision boundary\ub97c \uc774\ub8e8\ub294 \uac83\uc740 \ub2e4\uc74c\uc744 \ubcf4\uba70 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \\(log\\frac{Pr(G=1|X=x)}{Pr(G=2|X=x)}=\\beta_0+\\beta^Tx\\) \uc774 \uacbd\uc6b0 decision boundary\ub294 log odds \uac00 0\uc774 \ub418\ub294 \uc9c0\uc810\ub4e4\uc774\uace0(odds= \\(\\frac{p}{1-p}\\) ), \uc774\ub294 \\({\\{x|\\beta_0+\\beta^Tx=0}\\}\\) \ub85c \uc815\uc758\ub418\ub294 \ucd08\ud3c9\uba74\uc77c \uac83\uc774\ub2e4. \uc6b0\ub9ac\ub294 \uc5ec\uae30\uc11c \ub450\uac1c\uc758 \ubc29\ubc95\ub860\uc744 \ub2e4\ub8f0 \uac83\uc774\ub2e4. \uc774\ub294 \uc720\uba85\ud558\uc9c0\ub9cc \uc5c4\uc5f0\ud788\ub294 \ub9e4\uc6b0 \ub2e4\ub978 \ubc29\ubc95\uc774\uba70, \uc774\ub294 \uc120\ud615\uc758 log odds \ud639\uc740 logits\uc744 \uac16\ub294 linear discriminant analysis(LDA), \uadf8\ub9ac\uace0 linear logistic regression\uc774\ub2e4. \uc774 \ub458\uc758 \ubcf8\uc9c8\uc801\uc778 \ucc28\uc774\ub294 linear function\uc774 training data\uc5d0 \uc801\ud569\ub418\ub294 \ubc29\uc2dd\uc5d0 \uc788\ub2e4. \uc9c1\uc811\uc801\uc778 \ubc29\ubc95\ub860\uc73c\ub85c\ub294 \ubc94\uc8fc\ub4e4 \uc0ac\uc774\uc758 decision boundary\ub97c \uc120\ud615\uc73c\ub85c \ubaa8\ub378\ub9c1\ud558\ub294 \uac83\uc778\ub370, \uc774\ub294 p\ucc28\uc6d0\uc758 \uc785\ub825 \uacf5\uac04\uc744 \uac16\ub294 \uc774\uc9c4 \ubd84\ub958 \ubb38\uc81c\uc5d0\uc11c \uacb0\uc815\uacc4\ub97c \ucd08\ud3c9\uba74\uc73c\ub85c \ubaa8\ub378\ub9c1 \ud558\ub294 \uac83\uc774 \ub41c\ub2e4. \uc774\ub294 normal vector \uc640 cut point\ub85c \ucd08\ud3c9\uba74\uc744 \ubd84\ub9ac\ud558\ub294 \uc6d0\ub9ac\uc778\ub370, \uc774\ub807\uac8c \ucd08\ud3c9\uba74\uc744 \ubd84\ub9ac\ud558\ub294 \ubc29\ubc95\uc911 \ub300\ud45c\uc801\uc778 \ub450 \uac00\uc9c0\ub294 \uc544\ub798\uc640 \uac19\ub2e4. \uba3c\uc800 perception \ubaa8\ub378\ub85c, \uc774\ub294 training data\ub0b4\uc5d0 \ub370\uc774\ud130\ub97c \ubd84\ub9ac\uc2dc\ud0a4\ub294 \ucd08\ud3c9\uba74\uc774 \uc788\ub2e4\uba74 \ucc3e\uc544\uc8fc\ub294 \uc54c\uace0\ub9ac\uc998\uc774\uace0, \ub450\ubc88\uca30\ub85c\ub294 optimally separating hyperplane\uc774 \uc874\uc7ac\ud55c\ub2e4\uba74 \uc774\ub97c \ucc3e\uace0, \uc544\ub2c8\ub77c\uba74 training data \ub0b4\uc5d0 \uacb9\uce58\ub294 \ubd80\ubd84\uc5d0 \ub300\ud55c \uce21\ub610\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ucd08\ud3c9\uba74\uc744 \ucc3e\ub294 \ubc29\ubc95 \uc774\ub2e4. \uc774 \ub2e8\uc6d0\uc5d0\uc11c\ub294 \uc120\ud615\uacb0\uc815\uacc4\uc5d0\uc11c \ub2e4\ub8e8\uc9c0\ub9cc, \uc774\ub294 \uc77c\ubc18\ud654\ub97c \ud55c \uacbd\uc6b0\uc774\uba70, \uc608\ub97c \ub4e4\uc5b4 \uc6b0\ub9ac\uac00 \ubcc0\uc218\uc9d1\ud569\uc744 \ub2e8\uc9c0 \ub2e8\uc77c\ubcc0\uc218\ub9cc \uce74\uc6b4\ud305\ud558\ub294 \uac8c \uc544\ub2c8\ub77c, cross-product(\ubca1\ud130\uacf1\uacfc \uc678\uc801)\uc774\ub098 \uc81c\uacf1\ud56d\ub4f1\uc744 \ucd94\uac00\ud558\uc5ec \ucda9\ubd84\ud788 \ud655\uc7a5\uc2dc\ud0ac \uc218 \uc788\ub2e4. \uc774\ub7f0 \ubc29\ubc95\uc740 \uc120\ud615\ud568\uc218\uac00 \uc774\ucc28\ud568\uc218\ub97c \ud1b5\ud574 \uc99d\uac15\ub41c \uacf5\uac04 \ub0b4\uc5d0 \uc874\uc7ac\ud558\uac8c \ub428\uc73c\ub85c\uc368, \uc774\ub7f0 \ucf00\uc774\uc2a4\uc5d0\uc11c linear decision boundarty\ub294 quadratic decision bounday\uac00 \ub418\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4.","title":"Introduction"},{"location":"02%20ESL/04_Linear_Methods_for_Classifications/#linear-regression-of-an-indicator-matrix","text":"\uc5ec\uae30\uc11c \uac01 \uc885\uc18d\ubcc0\uc218\uc758 \ubc94\uc8fc\ub4e4\uc740 \uc9c0\uc2dc\ubcc0\uc218\ub85c \ucf54\ub529\ub41c\ub2e4. \ub530\ub77c\uc11c \ubc94\uc8fc\uc758 \uac1c\ubcc4 \ud074\ub798\uc2a4\uc758 \uac2f\uc218\uac00 K\uac1c\ub77c\uba74, \uc9c0\uc2dc\ubcc0\uc218 \ub610\ud55c \\(Y_k\\) \ub85c K\uac1c\uac00 \uc788\uc744 \uac83\uc774\uba70 \\(G=k\\) \uba74 \\(Y_k=1 \\ or\\ Y_k=0\\) \uc77c \uac83\uc774\ub2e4. \uc774\ub4e4\uc740 \ubaa8\uc5ec\uc11c \\(Y=(Y_1,..,Y_k)\\) \uc778 \ubca1\ud130\ub97c \uc774\ub8e8\uace0 N\uac1c\uc758 training instances \ub4e4\uc740 \\(N \\times K\\) \uc758 indicatior response matrix Y \ub97c \uc774\ub8ec\ub2e4. \uc774\ub7ec\ud55c matrix\ub294 \uac01\ud589\uc5d0 1\uac1c\uc758 1\uc744 \uac16\uace0, 0\uacfc 1\ub85c\ub9cc \uc774\ub8e8\uc5b4\uc9c4 \ud589\ub82c\uc77c \uac83\uc774\ub2e4. \uc774\ub7ec\ud55c matrix Y\uc758 \uac01 \uc5f4\uc5d0 \uc120\ud615 \ud68c\uadc0 \ubaa8\ud615\uc744 \uc801\ud569\ud558\uba74 \uc6b0\ub9ac\ub294 \uc544\ub798\uc640 \uac19\uc740 \uacb0\uacfc\ub97c \uc5bb\uc744 \uac83\uc774\ub2e4. $ \\(\\hat{Y}=X(X^TX)^{-1}X^TY\\) $ \uac01 \uc885\uc18d \uc5f4 \\(y_k\\) \uc5d0 \ub300\ud55c \uacc4\uc218 \ubca1\ud130\uac00 \uc788\uace0, \\((p+1) \\times K\\) \uc758 \uacc4\uc218 \ud589\ub82c \\(\\hat{B}=(X^TX)^{-1}X^TY\\) \uc774 \uc874\uc7ac\ud55c\ub2e4. \ub530\ub77c\uc11c X\ub294 p\uac1c\uc758 \ub3c5\ub9bd\ubcc0\uc218\uac00 \uc788\uc744 \ub54c \uc808\ud3b8 \uacc4\uc218\ub97c \ud3ec\ud568\ud558\ub294 p+1\uac1c\uc758 \uc5f4\uc744 \uac16\ub294 model matrix\uc774\ub2e4. \uc785\ub825\uac12 x\ub97c \uac16\ub294 \uc0c8\ub85c\uc6b4 \uad00\uce21\uce58\uc5d0 \ub300\ud55c \ubd84\ub958\ub294 \uc544\ub798\uc640 \uac19\ub2e4. \ucd94\uc815\ub41c \ucd9c\ub825\uac12\uc778 K\uac1c\uc758 \ubca1\ud130 \\(\\hat{f}(x)^T=(1,x^T)\\hat{B}\\) \ub97c \uacc4\uc0b0\ud55c\ub2e4. \uac00\uc7a5 \ud070 \uac12\uc744 \ud30c\uc545\ud558\uc5ec \uc544\ub798\uc640 \uac19\uc774 \ubd84\ub958\ud55c\ub2e4. \\[\\hat{G}(x)=argmax_k\\hat{f}_k(x)\\] \ud5c8\ub098 \uacfc\uc5f0 \uc774\ub7ec\ud55c \uc811\uadfc\ubc29\ubc95\uc758 \uc815\ub2f9\uc131\uc740 \uc5b4\ub514\ub85c\ubd80\ud130 \ub098\uc62c\uae4c? \ud55c\uac00\uc9c0 \uc815\ub2f9\ud654\ub85c\ub294 \uc774\ub7ec\ud55c \ud68c\uadc0 \ubc29\ubc95\ub860\uc744 \uc870\uac74\ubd80 \uae30\ub313\uac12\uc5d0 \ub300\ud55c \ucd94\uc815\uce58\ub85c \ubcf4\ub294 \uac83\uc774\ub2e4. \uc6b0\ub9ac\uac00 2\uc7a5\uc5d0\uc11c \uc0b4\ud3c8\ub4ef\uc774, \uc6b0\ub9ac\ub294 KNN\uac19\uc740 classification\uc758 \uacbd\uc6b0\ub098 \uc120\ud615 \ud68c\uadc0 \ubaa8\ub378\uc5d0\uc11c \ud568\uc218\uadfc\uc0ac\uc758 \ucd5c\uc801\ud574\uac00 x\uc5d0 \ub300\ud55c Y\uc758 \uc870\uac74\ubd80\ud3c9\uade0\uc774\ub77c\ub294 \uac83\uc744 \ud655\uc778\ud55c \uc801\uc774 \uc788\uc5c8\ub2e4.(\ud68c\uadc0\uc5d0\uc11c\ub294 \ucd5c\uc18c\uc790\uc2b9\ubc95\uc744 \ud1b5\ud574 \uc870\uac74\ubd80\uae30\ub313\uac12\uc774 \ucd5c\uc801\uc758 \ud574\ub77c\ub294 \uac83\uc744 \ubc1d\ud614\ub358 \uc801\uc774 \uc788\ub2e4. \uc544\ubb34\ud2bc \uc788\ub2e4.) \uace0\ub85c \ud655\ub960\ubcc0\uc218 \\(Y_k\\) \uac00 \uc788\ub2e4\uace0 \ud560\ub54c, \\(E(Y_k|X=x)=Pr(G=k|X=x)\\) \uc774\uba70 \ub530\ub77c\uc11c \uac01 \\(Y_k\\) \uc5d0 \ub300\ud55c \uc870\uac74\ubd80 \uae30\ub313\uac12\uc774 \uad81\uadf9\uc801\uc778 \ubaa9\ud45c\uac00 \ub418\ub294 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub7ec\ud55c \ubc29\ubc95\uc758 real issue\ub294 \uc870\uac74\ubd80 \uae30\ub313\uac12\uc5d0 \ub300\ud55c \uadfc\uc0ac\uac00 \uc5c4\uaca9\ud55c \uc120\ud615 \ud68c\uadc0 \ubaa8\ud615\ubcf4\ub2e4 \uc5bc\ub9c8\ub098 \ub354 \uc88b\ub0d0\ub294 \uac83\uc774\ub2e4. \uc5ec\uae30\uc11c \ub2e4\ub8e8\ub294 \uc9c0\uc2dc\ubcc0\uc218\uc5d0 \ub300\ud55c \uc120\ud615\ud68c\uadc0\ub294 \uc0ac\uc2e4 \uadf8 \ucd94\uc815\uac12\uc774 \uc74c\uc218\uc774\uac70\ub098 1\ubcf4\ub2e4 \ud074 \uacbd\uc6b0\uac00 \uc874\uc7ac \ud55c\ub2e4. \uc774\ub294 \ud655\ub960\uc758 \uc815\uc758\uc5d0 \uc5b4\uae0b\ub098\ub294 \uac83\uc774\uc9c0\ub9cc, \ub610 \ub2e4\uc2dc \ubcf4\uba74 \uc774\ub7ec\ud55c \uadfc\uc0ac\ubc29\ubc95\uc774 \uc798 \uc791\ub3d9\ud558\uc9c0 \uc54a\uc744 \uac83\uc774\ub77c\ub294 \ubcf4\uc7a5\ub3c4 \uc5c6\uae34\ud558\ub2e4. \ub9cc\uc57d \uc6b0\ub9ac\uac00 \uc120\ud615\ud68c\uadc0\ub97c \ub3c5\ub9bd\ubcc0\uc218\ub4e4\uc758 \uae30\uc800 \ud655\uc7a5 \\(h(x)\\) \ub85c \uc801\ud569\ud55c\ub2e4\uba74, \uc774\ub7ec\ud55c \ubc29\ubc95\uc740 \ud655\ub960\uc5d0 \ub300\ud55c \uc77c\uce58\ucd94\uc815\uce58\ub85c \ub3c4\ucd9c\ub420 \uc218 \uc788\uc744 \uac83\uc774\ub2e4. \uc774\ub294 \ub9d0\ub85c\ub9cc \ud574\uc11c\ub294 \uc798 \uac10\uc774 \uc548\uc624\ub294\ub370 \ub2e4\ud589\ud788\ub3c4 5\uc7a5\uc5d0\uc11c \ub2e4\ub8ec\ub2e4\uace0 \ud55c\ub2e4.. \uac19\uc740 \ubb38\uc81c\uc5d0 \ub300\ud55c \ubcf4\ub2e4 \uc9c1\uad00\uc801\uc778 \uad00\uc810\uc740 \uac01 \ubc94\uc8fc\uc5d0 target \\(t_k\\) \ub97c \uc124\uc815\ud558\ub294 \uac83\uc774\ub2e4. \uc5ec\uae30\uc11c \\(t_k\\) \ub294 \\(K\\times K\\) \ud56d\ub4f1\ud589\ub82c\uc758 k\ubc88\uc9f8 \uc5f4\uc774\ub2e4. \uc6b0\ub9ac\uc758 \uc608\uce21\ubb38\uc81c\ub294 \uac01 \uad00\uce21\uce58\uc5d0 \uc801\ud569\ud55c \uc885\uc18d \ubcc0\uc218\uc758 \uac12\uc744 \uc0b0\ucd9c\ud558\ub294 \uac83\uc774\ub2e4. \uc774\ub294 \uad00\uce21\uce58 i\uc5d0 \ub300\ud55c \uc885\uc18d \ubca1\ud130 \\(y_i\\) (Y\uc758 i\ubc88\uc9f8 \ud589)\uac00 \\(g_i=k\\) \uba74 \\(y_i=t_k\\) \uc758 \uac12\uc744 \uac16\ub294 \uac83\uc774\ub2e4. \uc774\ub7ec\ud55c \ubaa8\ub378\uc5d0 \ub300\ud55c \ucd5c\uc18c\uc81c\uacf1\ubc95\uc5d0 \uc758\ud55c \uc120\ud615\ubaa8\ub378\uc744 \uc801\ud569\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\[min_B\\sum_{i=1}^N||y_i-[1,x_i^T)B]^T||^2\\] \uc774\ub7ec\ud55c \uc801\ud569\ub3c4\ub2c8 \ubca1\ud130\ub4e4\uc758 \uc885\uc18d\ubcc0\uc218 \uac12\ub4e4\ub85c\ubd80\ud130\uc758 \uc720\ud074\ub9ac\ub514\uc548 \uac70\ub9ac\ub4e4\uc758 \uc81c\uacf1\ud569\uc73c\ub85c \uc0c8\ub85c\uc6b4 \uad00\uce21\uce58\ub294 \uc801\ud569\ubca1\ud130 \\(\\hat{f}(x)\\) \ub97c \uacc4\uc0b0\ud558\uc5ec \uac00\uc7a5 \uac00\uae4c\uc6b4 \uc885\uc18d \ubcc0\uc218 \uac12\uc5d0 \ubd84\ub958\ud558\ub294 \ubc29\ubc95\uc744 \ud3ec\ud568\ud558\uac8c \ub420 \uac83\uc774\ub2e4. \\[\\hat{G}(x)=argmin_k||\\hat{f}(x)-t_k||^2\\] \uc774\ub7ec\ud55c \uc81c\uacf1\ud569 \uae30\uc900\uc740 multiple response linear regression\uacfc \ub9e4\uc6b0 \uc720\uc0ac\ud558\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub7ec\ud55c \ud68c\uadc0 \ubc29\ubc95\uc758 \uc2ec\uac01\ud55c \uacb0\uc810\uc740 \ubc94\uc8fc\uc758 \uac2f\uc218\uac00 3\uac1c \uc774\uc0c1\uc77c \ub54c \ubc1c\uc0dd\ud55c\ub2e4. \ud68c\uadc0 \ubaa8\ub378\uc758 \uc5c4\uaca9\ud55c \ubcf8\uc9c8\uc740 \ubc94\uc8fc\ub4e4\uc774 \ub2e4\ub978 \ubc94\uc8fc\ub4e4\uc5d0 \uc758\ud574 masked\ub420 \uc218 \uc788\ub2e4\ub294 \uac83\uc774\ub2e4. K\ub97c 3\uc73c\ub85c\ub9cc \ub193\uace0 LDA\uc640 linear regression of the indicator Variables\uc758 \ucc28\uc774\ub97c \ubd10\ub3c4, \uc120\ud615\ud68c\uadc0\uc5d0\uc11c middle class\ub97c \uc644\ubcbd\ud788 \ub193\uce58\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.","title":"Linear Regression of an Indicator Matrix"},{"location":"02%20ESL/04_Linear_Methods_for_Classifications/#linear-discriminant-analysislda","text":"\ubd84\ub958 \ubb38\uc81c\uc5d0 \ub300\ud574 \uacb0\uc815\uacc4\ub294 \ucd5c\uc801\uc758 \ubd84\ub958\ub97c \uc704\ud574 \uc6b0\ub9ac\uac00 \ubc94\uc8fc\uc5d0 \uc18d\ud560 \uc0ac\ud6c4\ud655\ub960 \\(Pr(G|X)\\) \ub97c \uc54c\uc544\ub0b4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \\(f_k(x)\\) \ub97c \ubc94\uc8fc G=k\uc5d0\uc11c X\uc758 \ubc94\uc8fc \uc870\uac74\ubd80 \ubc00\ub3c4\ub77c\uace0 \ud558\uace0, \\(\\pi_k\\) \ub97c k\ubc94\uc8fc\uc5d0 \uc18d\ud560 \uc0ac\uc804 \ud655\ub960\uc774\ub77c\uace0 \ud558\uba74( \\(\\sum\\pi=1\\) ), \ubca0\uc774\uc988 \uc815\ub9ac\ub97c \ud1b5\ud574 \uc544\ub798\uc640 \uac19\uc740 \uacb0\uacfc\ub97c \ub3c4\ucd9c\ud560 \uc218 \uc788\ub2e4. \\[Pr(G=k|X=x)=\\frac{Pr(X|G)Pr(G)}{\\sum_{l=1}^GPr(X|G_l)PR(G_l)}\\] \ubd84\ub958\uc758 \ub2a5\ub825 \uce21\uba74\uc5d0\uc11c \\(f_k(x)\\) \ub97c \uc544\ub294 \uac83\uc774 \\(Pr(G=k|X=x)\\) \ub97c \uc544\ub294 \uac83\uacfc \uac19\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \ubc94\uc8fc \ubc00\ub3c4\uc5d0 \uae30\ubc18\ud55c \ubaa8\ub378\ub4e4\uc5d0 \ub300\ud55c \ub9ce\uc740 \uae30\uc220\ub4e4\uc774 \uc788\ub2e4. \ud558\uc9c0\ub9cc \uc6b0\ub9ac\ub294 \uc5ec\uae30\uc11c \uc77c\ub840\ub85c \uac01 \ubc94\uc8fc \ubc00\ub3c4\ub97c \ub2e4\ubcc0\ub7c9 \uc815\uaddc \ubd84\ud3ec\ub85c \ubaa8\ub378\ub9c1\ud55c\ub2e4\uace0 \ud574\ubcf4\uc790. \\[f_k(x)=\\frac{1}{(2\\pi)^{p2}|\\sum_k|^{1/2}}e^{-\\frac{1}{2}(x-\\mu_K)\\sum_k^{-1}(x-\\mu_k)}\\] LDA\ub294 \uc6b0\ub9ac\uac00 \ubc94\uc8fc\ub4e4\uc774 \uac19\uc740 \uacf5\ubd84\uc0b0 \ud589\ub82c\uc744 \uac16\ub294\ub2e4\ub294 \ud2b9\uc218\ud55c \uac00\uc815\uc744 \ud1b5\ud574 \uc2dc\uc791\ub41c\ub2e4. \uc989 LDA\ub294 \ub2e4\ubcc0\ub7c9 \uc815\uaddc\ubd84\ud3ec \uac00\uc815\uacfc \uc870\uac74\ubd80 \ubd84\ud3ec\ub4e4\uc758 \ub4f1\ubd84\uc0b0\uc131 \uac00\uc815\uc774 \uc874\uc7ac\ud55c\ub2e4. \uc5ec\uae30\uc11c \ub450 \ubc94\uc8fc k\uc640 l\uc744 \ube44\uad50\ud560\ub54c, log-ratio\ub97c \ub4e4\uc5ec\ub2e4\ubcf4\ub294 \uac83\uc774 \ucda9\ubd84\ud788 \uac00\ub2a5\ud55c\ub370, \uc2dd\uc744 \ud1b5\ud574 \uc6b0\ub9ac\ub294 log-ratio\uac00 x\uc5d0 \ub300\ud574 \uc120\ud615\uc784\uc744 \uc54c \uc218\uc788\ub2e4. \\[log\\frac{Pr(G=k|X=x)}{Pr(G=l|X=x)}=log\\frac{f_k(x)}{f_l(x)}+log\\frac{\\pi_k}{\\pi_l}\\] \\[=log(exp(-\\frac{1}{2}(x-\\mu_k)^T{\\sum}^{-1}(x-\\mu_k)+log\\frac{\\pi_k}{\\pi_l} \\frac{1}{2}(x-mu_l)^T{\\sum}^{-1}(x-\\mu_l)))\\] \\[=log\\frac{\\pi_k}{\\pi_l}-\\frac{1}{2}(\\mu_k+\\mu_l)^T{\\sum}^{-1}(\\mu_k-\\mu_l)+x^T{\\sum}^{-1}(\\mu_k-\\mu_l)\\] \ub3d9\ub4f1\ud55c \uacf5\ubd84\uc0b0 \ud589\ub82c\ub4e4\uc740 normalization factors\ub4e4\uc774 \uc18c\uac70 \ub418\uac8c \ud558\uba70, exponent\ub4e4\uc758 2\ucc28\ud56d \ub610\ud55c \uc18c\uac8c\ub418\uac8c \ub9cc\ub4e0\ub2e4. \uc774\ub7ec\ud55c \uc120\ud615 log-odds \ud568\uc218\ub294 k\uc640 l\uc744 \uad6c\ubd84\ud558\ub294 \uacb0\uc815\uacc4\uac00 \\(Pr(G=k|X=x)=Pr(G=l|X=x)\\) \uac00 \ub418\uac8c\ud558\ub294 \uc9c0\uc810\ub4e4\uc758 \uc9d1\ud569\uc774\uba70, x\uc5d0 \ub300\ud574 \uc120\ud615\uc778 p\ucc28\uc6d0\uc5d0\uc11c\uc758 \ucd08\ud3c9\uba74\uc784\uc744 \uc758\ubbf8\ud55c\ub2e4. \ub530\ub77c\uc11c \ubaa8\ub4e0 \uacb0\uc815\uacc4\ub4e4\uc774 \uc120\ud615\uc774\uba70 \\(R^p\\) \ub97c \ubc94\uc8fc\ub4e4\ub85c \ub098\ub258\ub294 \uc9c0\uc5ed\ub4e4\ub85c \ub098\ub204\uba74, \uc774\ub7ec\ud55c \uc9c0\uc5ed\ub4e4\uc774 \ucd08\ud3c9\uba74\uc5d0 \uc758\ud574 \ubd84\ub9ac \ub418\uac8c \ub41c\ub2e4. LDA\uc5d0\uc11c \ucd94\uc815\uc744 \ud560 \uacbd\uc6b0, \ub2e4\ubcc0\ub7c9 \uc815\uaddc\ubd84\ud3ec\ub4e4\uc758 \uacf5\uc720\ud558\ub294 \uacf5\ubd84\uc0b0 \ud589\ub82c\uacfc \ud3c9\uade0\uc5d0 \ub300\ud574\uc11c \ucd94\uc815\uc744 \ud574\uc57c\ud558\ub294\ub370, \uc774\ub294 \uc6b0\ub9ac\uac00 \ub2e4\ubcc0\ub7c9 \uc815\uaddc\ubd84\ud3ec\uc5d0\uc11c\uc758 MLE\ub97c \uc54c\uae30\ub54c\ubb38\uc5d0, \uc774\ub97c \uc774\uc6a9\ud558\uc5ec \ucd94\uc815\ud558\uba74 \ub41c\ub2e4. \uc774 \uacbd\uc6b0 \uc6b0\ub9ac\ub294 Linear Discriminant functions\uc774 \ub2e4\uc74c\uacfc \uac19\uc74c\uc744 \ud655\uc778 \uac00\ub2a5\ud558\ub2e4. \\[\\sigma_k(x)=x^T_{(1\\times p)}{\\sum}^{-1}_{(p\\times p)}\\mu_{k{(p\\times 1)}}-\\frac{1}{2}\\mu_{k{(1\\times p)}}^T{\\sum}^{-1}_{(p\\times p)}\\mu_{k{(p\\times 1)}}+ log\\pi_{k (p\\times 1)}\\] \uc0ac\uc804\ud655\ub960 \\(\\hat{\\pi_k}=N_k/N\\) \ud3c9\uade0 \\(\\hat{mu_k}=\\sum_{g_i=k}x_i/N_k\\) \uacf5\ubd84\uc0b0\ud589\ub82c \\(\\hat{\\sum}=\\sum_{g_i=k}(x_i-\\hat{\\mu_k})(x_i-\\hat{\\mu_k})^T/(N-k)\\) \uacf5\ubd84\uc0b0 \ud589\ub82c\uc758 \uacbd\uc6b0 \ubd84\ud3ec\uc758 N-K\ub97c \ud1b5\ud574 MLE\uac00 \uc544\ub2cc \ud45c\ubcf8\ubd84\uc0b0\uc744 \uc0ac\uc6a9\ud55c\ub2e4. \uacb0\uad6d \uac00\uc911\ud3c9\uade0\uc778 \uc148.","title":"Linear Discriminant Analysis(LDA)"},{"location":"02%20ESL/04_Linear_Methods_for_Classifications/#example-3-classes-and-p2","text":"\uac19\uc740 \uacf5\ubd84\uc0b0 \ud589\ub82c\uc744 \uac16\ub294 \uc815\uaddc\ubd84\ud3ec\uc5d0\uc11c \uc790\ub8cc\uac00 \uc0d8\ud50c\ub9c1 \ub418\uc5c8\ub2e4\ub294 \uac00\uc815 \ud558\uc5d0\uc11c LDA\ub97c \uc2dc\ud589\ud574\ubcf4\uc558\ub2e4. \\(X|K = 1 \\sim N_2(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\begin{bmatrix} 1 & 0.4 \\\\ 0.4 & 1 \\end{bmatrix}), n_1= 500\\) \\(X|K = 2 \\sim N_2(\\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix},\\begin{bmatrix} 1 & 0.4 \\\\ 0.4 & 1 \\end{bmatrix}), n_2= 1000\\) \\(X|K = 3 \\sim N_2(\\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix},\\begin{bmatrix} 1 & 0.4 \\\\ 0.4 & 1 \\end{bmatrix}), n_3= 1500\\) ## generating 3 bivariate normal distributions with a common covariance matrix using gaussian copula library(ggplot2) ## Warning: As of rlang 0.4.0, dplyr must be at least version 0.8.0. ## \u2716 dplyr 0.7.8 is too old for rlang 0.4.1. ## \u2139 Please update dplyr with `install.packages(\"dplyr\")`. set.seed(2013122059) # defining function using gaussian copula generator_gc=function(n,mu,rho,diag){ R=rbind(c(1,rho),c(rho,1)) A=t(chol(R)) U=matrix(nrow=n,ncol=2) W=matrix(nrow=n,ncol=2) # simulation for (i in 1:n){ Z=rnorm(2,0,1) W=A%*%Z U[i,]=pnorm(W) } X=matrix(nrow=n,ncol=2) colnames(X)=paste0('X',1:2) for (j in 1:2){ X[,j]=qnorm(U[,j],mean=mu[j],sd=sqrt(diag[j])) } return (X) } # parameters n=c(500,1000,1500) mu=rbind(c(0,0),c(2,2),c(4,0)) rho=0.4 var=c(1,1) for (i in 1:3){ class=paste0('k',i) K=factor(rep(i,n[i])) assign(class,data.frame(generator_gc(n=n[i],mu=mu[i,],rho=rho,diag=var),K)) } df3=data.frame(rbind(k1,k2,k3)) # visualization of simulated samples ggplot(df3,aes(x=X1,y=X2,color=K))+ geom_point()+ theme_bw()+ ggtitle('Simulated 3 Normal Bivariate Random Samples')+ theme(plot.title=element_text(face='bold')) # defining linear discriminant function for 3 classes ldf=function(df){ result=list() # getting prior, mu and common sigma estimates N=nrow(df) p=ncol(df)-1 k=length(unique(df$K)) nk=numeric(k) muhat=matrix(nrow=k,ncol=p) xk=list() ss=list() for (i in 1:k){ x=as.matrix(df[df$K==i,-ncol(df)]) xk[[i]]=x nk[i]=nrow(x) muhat[i,]=as.matrix(apply(x,2,mean)) dev=apply(x,1,function(x)x-muhat[i,]) ss[[i]]=dev%*%t(dev) } pihat=prop.table(nk) sigmahat=Reduce('+',ss)/(N-k) result[['pihat']]=pihat result[['muhat']]=muhat result[['sigmahat']]=sigmahat # discriminant functions for each class delta=function(x,sigmahat,muhat,pihat){ return (t(x)%*%solve(sigmahat)%*%muhat -0.5*(t(muhat)%*%solve(sigmahat)%*%muhat)+log(pihat)) } delta_mat=matrix(nrow=N,ncol=k) for (j in 1:k){ # xk should be matrix! delta_mat[,j]=apply(as.matrix(df[,1:p]), 1,delta, sigmahat=sigmahat, muhat=muhat[j,], pihat=pihat[j]) } result[['delta mat']]=delta_mat return (result) } result=ldf(df3) delta_mat=result[['delta mat']] colnames(delta_mat)=c('K1','K2','K3') # decision rule.. which class' value is maximum? df3$Khat=factor(apply(delta_mat,1,which.max)) ## using contour for visualization of decision boundary # using contour of posterior probabilities for visualization of decision boundary pihat=result[['pihat']] muhat=result[['muhat']] sigmahat=result[['sigmahat']] gridx=seq(min(df3[,1]),max(df3[,1]),length.out=100) gridy=seq(min(df3[,2]),max(df3[,2]),length.out=100) grid=expand.grid(gridx,gridy) likelihood=function(x,sigmahat,muhat){ return (exp(-0.5*t(x-muhat)%*%solve(sigmahat)%*%(x-muhat))) } posterior=matrix(nrow=nrow(grid), ncol=3) for (k in 1:3){ posterior[,k]=apply(as.matrix(grid[,1:2]),1,likelihood,sigmahat=sigmahat,muhat=muhat[k,])*pihat[k] } posterior=t(apply(posterior,1,prop.table)) posterior=cbind(grid,posterior) colnames(posterior)=c('x1','x2','k1','k2','k3') # final visualization using contour of posterior probabilities ggplot(df3,aes(x=X1,y=X2))+ geom_point(aes(color=K))+ geom_contour(data=posterior,aes(x=x1,y=x2,z=k1),breaks=c(0,0.5),color='black',size=3, lty=2)+ geom_contour(data=posterior,aes(x=x1,y=x2,z=k2),breaks=c(0,0.5),color='black',size=3, lty=2)+ theme_bw()+ ggtitle('Linear Decision Boundary of 3 Classes LDA Solution')+ theme(plot.title=element_text(face='bold')) \ucc45\uc5d0\uc11c\ub294 \uc0ac\ud6c4 \ud655\ub960\uc758 \ub300\uc18c\ub97c \ube44\uad50\ud558\uc5ec \ubc94\uc8fc\ub97c \uacb0\uc815\ud558\ub294 \uacfc\uc815\uc5d0\uc11c \uc874\uc7ac\ud558\ub294 \uc815\uaddc\uc131 \uac00\uc815\uc5d0 \ub300\ud55c \uac15\uc870\uc640 \ud568\uaed8 \uc815\uaddc\uc131 \uac00\uc815\uc744 \ub9cc\uc871\ud558\uc9c0 \uc54a\ub294 \uacbd\uc6b0 cut-point\ub97c empirically\ud558\uac8c \ud6c8\ub828 \ub370\uc774\ud130\uc758 \uc5d0\ub7ec\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ubc29\ud5a5\uc73c\ub85c \uc120\ud0dd\ud560\uc218\ub3c4 \uc788\uc74c\uc744 \uc774\uc57c\uae30\ud558\uace0 \uc788\ub2e4.","title":"example 3 classes and p=2"},{"location":"03%20Theoretical%20Statistics/Bayes_Homework/","text":"1. \\(Beta(\\alpha,\\beta)\\) \ubd84\ud3ec\uc758 \uacbd\uc6b0 \\(\\alpha=\\beta=1\\) \uc774\uba74, \uadf8 beta\ubd84\ud3ec\ub294 (0,1) \uc0ac\uc774\uc758 Uniform distribution\uc774 \ub428\uc744 \ubcf4\uc774\uc2dc\uc624. Beta \ubd84\ud3ec\uc758 pdf\ub294 \uc544\ub798\uc640 \uac19\ub2e4. \\(Beta(\\alpha,\\beta) \\sim \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}{x}^{\\alpha-1}(1-x)^{\\beta-1},\\ 0<x<1\\) \uc5ec\uae30\uc11c \\(\\alpha, \\beta\\) \uc5d0 \uac01\uac01 1\uc744 \ub300\uc785\ud558\uba74 pdf\uc758 \uc0c1\uc218\ud56d \uaf34\uc778 \\(\\Gamma()\\) \ubd80\ubd84\ub4e4\uc774 1\ub85c \uacc4\uc0b0\ub418\uace0, \\(x^0*(1-x)^0=1\\) \uc758 \uaf34\ub85c \ubc14\ub00c\uc5b4 pdf\uac00 \\(0<x<1\\) \uc0ac\uc774\uc5d0\uc11c 1\uc778 \ud615\ud0dc\uac00 \ub418\ub294\ub370, \uc774\ub294 \\(Unif(0,1)\\) \uc758 pdf\uc640 \uac19\ub2e4. 2. Binomical model \\(B(1,\\theta)\\) \uc5d0\uc11c \ubaa8\uc218 \\(\\Theta\\) \uc5d0 \ub300\ud55c \uc0ac\uc804\ubd84\ud3ec\ub85c \\(\\alpha=\\beta=1\\) \uc778 \\(Beta(\\alpha,\\beta)\\) \ubd84\ud3ec\ub97c \uac00\uc815\ud558\uba74, \uc0ac\uc804\ubd84\ud3ec\uac00 Bayes estimator\uc5d0 \uc544\ubb34\ub7f0 \uc601\ud5a5\uc744 \ubbf8\uce58\uc9c0 \uc54a\uc744 \uac83\uc774\ub77c\uace0 \uc608\uc0c1\ud558\ub294 \uc774\uc720\ub294 \ubb34\uc5c7\uc778\uac00? \uc77c\ubc18\uc801\uc73c\ub85c \uc6b0\ub9ac\uac00 \uc0ac\uc804\ubd84\ud3ec\uc5d0 \ub300\ud574 \uc544\ubb34\ub7f0 \uc9c0\uc2dd\uc774 \uc5c6\ub2e4\ub294 \uc0c1\ud669\uc744 \uac00\uc815\ud560 \ub54c \uc6b0\ub9ac\ub294 \\(Unif(0,1)\\) \ubd84\ud3ec\ub97c \uc0ac\uc6a9\ud558\uac8c \ub41c\ub2e4. \uc774\ub294 \ubaa8\ub4e0 \uad6c\uac04\uc5d0 \uade0\ub4f1\ud55c \ud655\ub960\uc744 \uac16\ub294 \uac00\uc7a5 \uae30\ubcf8\uc801\uc778 \ubd84\ud3ec\ub97c \uac00\uc815\ud55c \uac83\uc778\ub370, \uc774\ub294 \uc989 \uc0ac\uc804\ubd84\ud3ec\uc5d0 \ub300\ud574 \uc544\ubb34\ub7f0 \uc815\ubcf4\uac00 \uc5c6\uae30\uc5d0 \uc5b4\ub5a4 \uc120\ud0dd\uc744 \ud558\ub354\ub77c\ub3c4 \uade0\uc77c\ud558\uac8c \uc0ac\uac74\uc774 \ubc1c\uc0dd\ud560 \uac83\uc774\ub77c\ub294 \ubbff\uc74c (\uace7 \uc544\ubb34 \uc815\ubcf4\uac00 \uc5c6\uc5b4\uc11c \uc5b4\ub5a4 \uacb0\uacfc\ub098 \ub098\uc624\ub354\ub77c\ub3c4 \uacf5\ud3c9\ud55c \uacb0\uacfc\uac00 \ub098\uc62c \uac83\uc774\ub77c\ub294 \ubbff\uc74c) \uc989 \uc0ac\uc804 \ubbff\uc74c\uc774 \uc5c6\uc74c\uc744 \uc758\ubbf8\ud558\ub294 \uac83\uc774\ubbc0\ub85c, \\(Unif(0,1)\\) \uc744 \uc0ac\uc804\ubd84\ud3ec\ub85c \uc0ac\uc6a9\ud560 \ub54c \uc0ac\uc804\ubd84\ud3ec\uac00 Bayes estimator\uc5d0 \uc544\ubb34\ub7f0 \uc601\ud5a5\uc744 \ubbf8\uce58 \uc54a\uc744 \uac83\uc774\ub77c\uace0 \uc608\uc0c1\ud558\ub294 \uac83\uc774\ub2e4. 3. \uc9c8\ubb38 2\uc5d0 \ub300\ud55c \ub2f5\ubcc0\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0, \uc2e4\uc81c\ub85c\ub294 Bayes estimator\uac00 \uc0ac\uc804\ubd84\ud3ec\uc5d0 \uc601\ud5a5\uc744 \ubc1b\uc74c\uc744 \ubcf4\uc774\uc2dc\uc624. \ud558\uc9c0\ub9cc \ubaa8\uc218 \\(\\theta\\) \uc5d0 \ub300\ud55c Bayes estimator\ub294 \uad00\ucc30\ub41c \ud45c\ubcf8\uc73c\ub85c\ubd80\ud130 \uc5bb\uc740 \ucd5c\ub300\uc6b0\ub3c4\ucd94\uc815\ub7c9\uacfc \uc0ac\uc804\ubd84\ud3ec\uc758 \ud3c9\uade0\uc758 \uac00\uc911\ud3c9\uade0\uc73c\ub85c \uc5bb\uc5b4\uc9c4 \uacb0\uacfc\ub85c\uc368, \uc704\uc640 \uac19\uc740 Binomial case\uc5d0\uc11c Bayes estimator\ub97c \uad6c\ud560 \ub54c \\(\\alpha=\\beta=1\\) \ub85c \ub450\uc5c8\uc744 \ub54c Bayes esitmator\ub294 \uc6b0\ub9ac\uac00 \uc0ac\uc804\uc5d0 \ub300\ud55c \ubbff\uc74c\uc774 \uc544\uc608 \uc5c6\ub2e4\uace0 \uac00\uc815\ud588\uc744 \ub54c \uc608\uc0c1\ub418\ub294 \uacb0\uacfc(\uc0ac\uc804\ubbff\uc74c\uc774 \uc5c6\uc73c\ubbc0\ub85c \uc774\ub54c\uc758 Bayes estimator\ub294 MLE\uc640 \uac19\uac8c \ub098\uc640\uc57c \uc77c\ub9ac\uac00 \uc788\uc744 \uac83\uc774\ub2e4.)\uc640\ub294 \ub2e4\ub978 \uacb0\uacfc\uac00 \ub098\uc624\ubbc0\ub85c, \uc6b0\ub9ac\uc758 \uc608\uc0c1\uacfc\ub294 \ubc97\uc5b4\ub098\ub294 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc900\ub2e4. \uc9c8\ubb38 2\uc758 \ucf00\uc774\uc2a4\ub97c \uc0b4\ud3b4\ubcf4\uba74 \uc774 \ub54c\uc758 \\(\\theta\\) \uc5d0 \ub300\ud55c Bayes estimator\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ub41c\ub2e4. \\[(\\frac{n}{\\alpha+\\beta+n})\\frac{y}{n}+(\\frac{\\alpha+\\beta}{\\alpha+\\beta+n})\\frac{\\alpha}{\\alpha+\\beta}=(\\frac{n}{n+2})\\frac{y}{n}+(\\frac{2}{n+2})\\frac{1}{2}\\] \uc5ec\uae30\uc11c \\(y\\) \ub294 \\(\\theta\\) \uc5d0 \ub300\ud55c \ucda9\ubd84\ud1b5\uacc4\ub7c9\uc774\ub2e4. \uc6b0\ub9ac\uc758 \uc608\uc0c1\uacfc\ub294 \ub2e4\ub974\uac8c MLE\uac00 prior mean\ucabd\uc73c\ub85c \uc0b4\uc9dd \ub04c\uc5b4 \ub2f9\uaca8\uc9c0\uace0, \uc5ec\uc804\ud788 prior\uc5d0 \ub300\ud574 \uc601\ud5a5\uc744 \ubc1b\uace0 \uc788\uc74c\uc744 \uc54c \uc218 \uc788\ub2e4. \uc5c4\ubc00\ud788 \uc0ac\uc804\ubbff\uc74c\uc5d0 \uc544\ubb34\ub7f0 \uc601\ud5a5\uc744 \ubc1b\uc9c0 \uc54a\uc73c\ub824\uba74 \uc704\uc758 Bayes estimator\ub294 \uc815\ud655\ud788 \\(\\frac{y}{n}\\) \uc774 \ub098\uc640\uc57c\ud560 \uac83\uc774\ub2e4. 4. Bayes estimator\uac00 \uc0ac\uc804\ubd84\ud3ec\uc5d0 \uc601\ud5a5\uc744 \ubc1b\uc9c0 \uc54a\uac8c \ud558\ub294 \ubc29\ubc95 \uc911\uc758 \ud558\ub098\uac00 \\(\\alpha=\\beta=0\\) \uc73c\ub85c \ub193\uc740 \uac83\uc784\uc744 \ubcf4\uc774\uc2dc\uc624. \ub9cc\uc57d \uc6b0\ub9ac\uac00 \uc704\uc758 \uc608\uc2dc\uc5d0\uc11c \\(\\alpha=\\beta=0\\) \uc73c\ub85c \ub454\ub2e4\uba74, shirinkage estimate\ub294 MLE\uc778 \\(y/n\\) \uc73c\ub85c reduce \ub420 \uac83\uc774\ub2e4. \uc774\ub7ec\ud55c prior\ub97c \ub450\ub294 \uac83\uc740 \ucd94\ub860\uc5d0 \uc544\ubb34\ub7f0 \uc601\ud5a5\uc744 \uc8fc\uc9c0 \uc54a\ub294\ub2e4. \ud558\uc9c0\ub9cc \\(Beta(0,0)\\) \uc740 pdf\uc758 \uaf34\uc774 \uc544\ub2c8\ub2e4. 5. \uc9c8\ubb38 4\uc5d0\uc11c \\(\\alpha=\\beta=0\\) \uc778 beta(\\alpha,\\beta)\ubd84\ud3ec\ub294 pdf\uac00 \ub418\uc9c0 \uc54a\uc74c\uc744 \ubcf4\uc774\uc2dc\uc624. \uba3c\uc800 \uc6b0\ub9ac\uac00 \uc544\ub294 Beta \ubd84\ud3ec\uc5d0\uc11c \ubaa8\uc218 \\(\\alpha, \\beta\\) \uc758 \ubaa8\uc218\uacf5\uac04\uc740 0\ubcf4\ub2e4 \ud070 \uacf3\uc5d0\uc11c \uc874\uc7ac\ud558\ubbc0\ub85c pdf\uc758 \uaf34\uc774 \ub420 \uc218 \uc5c6\ub2e4. \ub610\ub294 \uac04\ub2e8\ud55c \uc218\uc2dd\uc744 \ud1b5\ud574\uc11c \uc801\ubd84\uac12\uc774 \uc0c1\uc218\uaf34\uc774 \ub098\uc624\uc9c0 \uc54a\uc74c\uc744, \ub610\ud55c 1\uc758 \uac12\uc774 \ub098\uc624\uc9c0 \uc54a\uc74c\uc744 \ubcf4\uc77c \uc218 \uc788\ub2e4. \\(\\int_0^1\\frac{1}{\\theta(1-\\theta)}d\\theta=\\int_0^1\\frac{1}{\\theta}d\\theta+ \\int_0^1\\frac{1}{1-\\theta}d\\theta\\) theta\uc758 \ubc94\uc704\ub294 0\uacfc 1 \uc0ac\uc774\uc774\ubbc0\ub85c, improper integral\uc5d0 \uc758\ud574 \uc704\uc758 \uc801\ubd84\uc744 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. $ \\(\\lim_{a\\to\\ 0+}\\int_a^1\\frac{1}{\\theta}d\\theta+\\lim_{b\\to\\ 1-}\\int_0^b\\frac{1}{1-\\theta}d\\theta\\) $ $ \\(\\lim_{a\\to\\ 0+}[log(\\theta)]_a^1-\\lim_{b\\to\\ 1-}[log(1-\\theta)]_0^b\\) $ $ \\(=\\infty\\) $ \uace0\ub85c \\(Beta(0,0)\\) \uc740 pdf\uac00 \uc544\ub2c8\ub2e4.","title":"Bayes Homework"},{"location":"03%20Theoretical%20Statistics/Bayes_Homework/#1-betaalphabeta-alphabeta1-beta-01-uniform-distribution","text":"Beta \ubd84\ud3ec\uc758 pdf\ub294 \uc544\ub798\uc640 \uac19\ub2e4. \\(Beta(\\alpha,\\beta) \\sim \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}{x}^{\\alpha-1}(1-x)^{\\beta-1},\\ 0<x<1\\) \uc5ec\uae30\uc11c \\(\\alpha, \\beta\\) \uc5d0 \uac01\uac01 1\uc744 \ub300\uc785\ud558\uba74 pdf\uc758 \uc0c1\uc218\ud56d \uaf34\uc778 \\(\\Gamma()\\) \ubd80\ubd84\ub4e4\uc774 1\ub85c \uacc4\uc0b0\ub418\uace0, \\(x^0*(1-x)^0=1\\) \uc758 \uaf34\ub85c \ubc14\ub00c\uc5b4 pdf\uac00 \\(0<x<1\\) \uc0ac\uc774\uc5d0\uc11c 1\uc778 \ud615\ud0dc\uac00 \ub418\ub294\ub370, \uc774\ub294 \\(Unif(0,1)\\) \uc758 pdf\uc640 \uac19\ub2e4.","title":"1. \\(Beta(\\alpha,\\beta)\\) \ubd84\ud3ec\uc758 \uacbd\uc6b0 \\(\\alpha=\\beta=1\\)\uc774\uba74, \uadf8 beta\ubd84\ud3ec\ub294 (0,1) \uc0ac\uc774\uc758 Uniform distribution\uc774 \ub428\uc744 \ubcf4\uc774\uc2dc\uc624."},{"location":"03%20Theoretical%20Statistics/Bayes_Homework/#2-binomical-model-b1theta-theta-alphabeta1-betaalphabeta-bayes-estimator","text":"\uc77c\ubc18\uc801\uc73c\ub85c \uc6b0\ub9ac\uac00 \uc0ac\uc804\ubd84\ud3ec\uc5d0 \ub300\ud574 \uc544\ubb34\ub7f0 \uc9c0\uc2dd\uc774 \uc5c6\ub2e4\ub294 \uc0c1\ud669\uc744 \uac00\uc815\ud560 \ub54c \uc6b0\ub9ac\ub294 \\(Unif(0,1)\\) \ubd84\ud3ec\ub97c \uc0ac\uc6a9\ud558\uac8c \ub41c\ub2e4. \uc774\ub294 \ubaa8\ub4e0 \uad6c\uac04\uc5d0 \uade0\ub4f1\ud55c \ud655\ub960\uc744 \uac16\ub294 \uac00\uc7a5 \uae30\ubcf8\uc801\uc778 \ubd84\ud3ec\ub97c \uac00\uc815\ud55c \uac83\uc778\ub370, \uc774\ub294 \uc989 \uc0ac\uc804\ubd84\ud3ec\uc5d0 \ub300\ud574 \uc544\ubb34\ub7f0 \uc815\ubcf4\uac00 \uc5c6\uae30\uc5d0 \uc5b4\ub5a4 \uc120\ud0dd\uc744 \ud558\ub354\ub77c\ub3c4 \uade0\uc77c\ud558\uac8c \uc0ac\uac74\uc774 \ubc1c\uc0dd\ud560 \uac83\uc774\ub77c\ub294 \ubbff\uc74c (\uace7 \uc544\ubb34 \uc815\ubcf4\uac00 \uc5c6\uc5b4\uc11c \uc5b4\ub5a4 \uacb0\uacfc\ub098 \ub098\uc624\ub354\ub77c\ub3c4 \uacf5\ud3c9\ud55c \uacb0\uacfc\uac00 \ub098\uc62c \uac83\uc774\ub77c\ub294 \ubbff\uc74c) \uc989 \uc0ac\uc804 \ubbff\uc74c\uc774 \uc5c6\uc74c\uc744 \uc758\ubbf8\ud558\ub294 \uac83\uc774\ubbc0\ub85c, \\(Unif(0,1)\\) \uc744 \uc0ac\uc804\ubd84\ud3ec\ub85c \uc0ac\uc6a9\ud560 \ub54c \uc0ac\uc804\ubd84\ud3ec\uac00 Bayes estimator\uc5d0 \uc544\ubb34\ub7f0 \uc601\ud5a5\uc744 \ubbf8\uce58 \uc54a\uc744 \uac83\uc774\ub77c\uace0 \uc608\uc0c1\ud558\ub294 \uac83\uc774\ub2e4.","title":"2. Binomical model \\(B(1,\\theta)\\) \uc5d0\uc11c \ubaa8\uc218 \\(\\Theta\\) \uc5d0 \ub300\ud55c \uc0ac\uc804\ubd84\ud3ec\ub85c \\(\\alpha=\\beta=1\\)\uc778 \\(Beta(\\alpha,\\beta)\\) \ubd84\ud3ec\ub97c \uac00\uc815\ud558\uba74, \uc0ac\uc804\ubd84\ud3ec\uac00 Bayes estimator\uc5d0 \uc544\ubb34\ub7f0 \uc601\ud5a5\uc744 \ubbf8\uce58\uc9c0 \uc54a\uc744 \uac83\uc774\ub77c\uace0 \uc608\uc0c1\ud558\ub294 \uc774\uc720\ub294 \ubb34\uc5c7\uc778\uac00?"},{"location":"03%20Theoretical%20Statistics/Bayes_Homework/#3-2-bayes-estimator","text":"\ud558\uc9c0\ub9cc \ubaa8\uc218 \\(\\theta\\) \uc5d0 \ub300\ud55c Bayes estimator\ub294 \uad00\ucc30\ub41c \ud45c\ubcf8\uc73c\ub85c\ubd80\ud130 \uc5bb\uc740 \ucd5c\ub300\uc6b0\ub3c4\ucd94\uc815\ub7c9\uacfc \uc0ac\uc804\ubd84\ud3ec\uc758 \ud3c9\uade0\uc758 \uac00\uc911\ud3c9\uade0\uc73c\ub85c \uc5bb\uc5b4\uc9c4 \uacb0\uacfc\ub85c\uc368, \uc704\uc640 \uac19\uc740 Binomial case\uc5d0\uc11c Bayes estimator\ub97c \uad6c\ud560 \ub54c \\(\\alpha=\\beta=1\\) \ub85c \ub450\uc5c8\uc744 \ub54c Bayes esitmator\ub294 \uc6b0\ub9ac\uac00 \uc0ac\uc804\uc5d0 \ub300\ud55c \ubbff\uc74c\uc774 \uc544\uc608 \uc5c6\ub2e4\uace0 \uac00\uc815\ud588\uc744 \ub54c \uc608\uc0c1\ub418\ub294 \uacb0\uacfc(\uc0ac\uc804\ubbff\uc74c\uc774 \uc5c6\uc73c\ubbc0\ub85c \uc774\ub54c\uc758 Bayes estimator\ub294 MLE\uc640 \uac19\uac8c \ub098\uc640\uc57c \uc77c\ub9ac\uac00 \uc788\uc744 \uac83\uc774\ub2e4.)\uc640\ub294 \ub2e4\ub978 \uacb0\uacfc\uac00 \ub098\uc624\ubbc0\ub85c, \uc6b0\ub9ac\uc758 \uc608\uc0c1\uacfc\ub294 \ubc97\uc5b4\ub098\ub294 \uacb0\uacfc\ub97c \ubcf4\uc5ec\uc900\ub2e4. \uc9c8\ubb38 2\uc758 \ucf00\uc774\uc2a4\ub97c \uc0b4\ud3b4\ubcf4\uba74 \uc774 \ub54c\uc758 \\(\\theta\\) \uc5d0 \ub300\ud55c Bayes estimator\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ub41c\ub2e4. \\[(\\frac{n}{\\alpha+\\beta+n})\\frac{y}{n}+(\\frac{\\alpha+\\beta}{\\alpha+\\beta+n})\\frac{\\alpha}{\\alpha+\\beta}=(\\frac{n}{n+2})\\frac{y}{n}+(\\frac{2}{n+2})\\frac{1}{2}\\] \uc5ec\uae30\uc11c \\(y\\) \ub294 \\(\\theta\\) \uc5d0 \ub300\ud55c \ucda9\ubd84\ud1b5\uacc4\ub7c9\uc774\ub2e4. \uc6b0\ub9ac\uc758 \uc608\uc0c1\uacfc\ub294 \ub2e4\ub974\uac8c MLE\uac00 prior mean\ucabd\uc73c\ub85c \uc0b4\uc9dd \ub04c\uc5b4 \ub2f9\uaca8\uc9c0\uace0, \uc5ec\uc804\ud788 prior\uc5d0 \ub300\ud574 \uc601\ud5a5\uc744 \ubc1b\uace0 \uc788\uc74c\uc744 \uc54c \uc218 \uc788\ub2e4. \uc5c4\ubc00\ud788 \uc0ac\uc804\ubbff\uc74c\uc5d0 \uc544\ubb34\ub7f0 \uc601\ud5a5\uc744 \ubc1b\uc9c0 \uc54a\uc73c\ub824\uba74 \uc704\uc758 Bayes estimator\ub294 \uc815\ud655\ud788 \\(\\frac{y}{n}\\) \uc774 \ub098\uc640\uc57c\ud560 \uac83\uc774\ub2e4.","title":"3. \uc9c8\ubb38 2\uc5d0 \ub300\ud55c \ub2f5\ubcc0\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0, \uc2e4\uc81c\ub85c\ub294 Bayes estimator\uac00 \uc0ac\uc804\ubd84\ud3ec\uc5d0 \uc601\ud5a5\uc744 \ubc1b\uc74c\uc744 \ubcf4\uc774\uc2dc\uc624."},{"location":"03%20Theoretical%20Statistics/Bayes_Homework/#4-bayes-estimator-alphabeta0","text":"\ub9cc\uc57d \uc6b0\ub9ac\uac00 \uc704\uc758 \uc608\uc2dc\uc5d0\uc11c \\(\\alpha=\\beta=0\\) \uc73c\ub85c \ub454\ub2e4\uba74, shirinkage estimate\ub294 MLE\uc778 \\(y/n\\) \uc73c\ub85c reduce \ub420 \uac83\uc774\ub2e4. \uc774\ub7ec\ud55c prior\ub97c \ub450\ub294 \uac83\uc740 \ucd94\ub860\uc5d0 \uc544\ubb34\ub7f0 \uc601\ud5a5\uc744 \uc8fc\uc9c0 \uc54a\ub294\ub2e4. \ud558\uc9c0\ub9cc \\(Beta(0,0)\\) \uc740 pdf\uc758 \uaf34\uc774 \uc544\ub2c8\ub2e4.","title":"4. Bayes estimator\uac00 \uc0ac\uc804\ubd84\ud3ec\uc5d0 \uc601\ud5a5\uc744 \ubc1b\uc9c0 \uc54a\uac8c \ud558\ub294 \ubc29\ubc95 \uc911\uc758 \ud558\ub098\uac00 \\(\\alpha=\\beta=0\\)\uc73c\ub85c \ub193\uc740 \uac83\uc784\uc744 \ubcf4\uc774\uc2dc\uc624."},{"location":"03%20Theoretical%20Statistics/Bayes_Homework/#5-4-alphabeta0-betaalphabeta-pdf","text":"\uba3c\uc800 \uc6b0\ub9ac\uac00 \uc544\ub294 Beta \ubd84\ud3ec\uc5d0\uc11c \ubaa8\uc218 \\(\\alpha, \\beta\\) \uc758 \ubaa8\uc218\uacf5\uac04\uc740 0\ubcf4\ub2e4 \ud070 \uacf3\uc5d0\uc11c \uc874\uc7ac\ud558\ubbc0\ub85c pdf\uc758 \uaf34\uc774 \ub420 \uc218 \uc5c6\ub2e4. \ub610\ub294 \uac04\ub2e8\ud55c \uc218\uc2dd\uc744 \ud1b5\ud574\uc11c \uc801\ubd84\uac12\uc774 \uc0c1\uc218\uaf34\uc774 \ub098\uc624\uc9c0 \uc54a\uc74c\uc744, \ub610\ud55c 1\uc758 \uac12\uc774 \ub098\uc624\uc9c0 \uc54a\uc74c\uc744 \ubcf4\uc77c \uc218 \uc788\ub2e4. \\(\\int_0^1\\frac{1}{\\theta(1-\\theta)}d\\theta=\\int_0^1\\frac{1}{\\theta}d\\theta+ \\int_0^1\\frac{1}{1-\\theta}d\\theta\\) theta\uc758 \ubc94\uc704\ub294 0\uacfc 1 \uc0ac\uc774\uc774\ubbc0\ub85c, improper integral\uc5d0 \uc758\ud574 \uc704\uc758 \uc801\ubd84\uc744 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. $ \\(\\lim_{a\\to\\ 0+}\\int_a^1\\frac{1}{\\theta}d\\theta+\\lim_{b\\to\\ 1-}\\int_0^b\\frac{1}{1-\\theta}d\\theta\\) $ $ \\(\\lim_{a\\to\\ 0+}[log(\\theta)]_a^1-\\lim_{b\\to\\ 1-}[log(1-\\theta)]_0^b\\) $ $ \\(=\\infty\\) $ \uace0\ub85c \\(Beta(0,0)\\) \uc740 pdf\uac00 \uc544\ub2c8\ub2e4.","title":"5. \uc9c8\ubb38 4\uc5d0\uc11c \\(\\alpha=\\beta=0\\)\uc778 beta(\\alpha,\\beta)\ubd84\ud3ec\ub294 pdf\uac00 \ub418\uc9c0 \uc54a\uc74c\uc744 \ubcf4\uc774\uc2dc\uc624."},{"location":"03%20Theoretical%20Statistics/Bayes_Homework2/","text":"1-1. \uac15\uc758\ub178\ud2b8 14\ud398\uc774\uc9c0 \ucc38\uace0, \uc65c \\(\\theta_3\\) \uc5d0 \ub300\ud55c conjugate prior\uac00 Gamma \\((\\alpha,\\beta)\\) \uc778\uc9c0 \uc124\uba85\ud558\uc2dc\uc624. \uc815\uaddc\ubd84\ud3ec\uc758 \ucf00\uc774\uc2a4\uc5d0\uc11c \ucda9\ubd84\ud1b5\uacc4\ub7c9\uc740 \\(\\bar{X}\\) \uc640 \\(S^2\\) \uc774\ub2e4. \uc774\ub4e4\uc744 \uc774\uc6a9\ud558\uc5ec likelihood\ub97c \uc0b4\uc9dd \ubcc0\ud615\ud558\uc5ec \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\(L(\\theta_1,\\theta_3|\\mathbf{x}) \\propto (\\frac{\\theta_3}{2\\pi})^{n/2}exp[-{\\frac{1}{2}} \\{(n-1)s^2+n(\\bar{x}-\\theta_1)^2\\}\\theta_3]\\) \uc774\ub294 \ub2e4\uc2dc \ubcf4\uba74 \uac10\ub9c8\ubd84\ud3ec\uc778 \\(\\frac{1}{\\Gamma{(\\alpha})\\beta^{\\alpha}}x^{\\alpha-1}exp(-\\frac{x}{\\beta})\\) \uc758 \uaf34\ub85c \ud45c\ud604\uc774 \uac00\ub2a5\ud568\uc744 \ud655\uc778\ud560 \uc218 \uc788\uace0, \ubaa8\uc218 \\(theta_3\\) \uc5d0 \ub300\ud574 likelihood\uac00 \uac10\ub9c8\ubd84\ud3ec\uc758 kernel\ub85c \uc774\ub8e8\uc5b4\uc9d0\uc744 \uc54c \uc218 \uc788\ub2e4. \uace0\ub85c \\(\\theta_3\\) \uc5d0 \ub300\ud55c conjugate prior\uac00 Gamma \\((\\alpha,\\beta)\\) \uc774\ub2e4. 1-2. \\(\\theta_3\\) \uac00 \uc8fc\uc5b4\uc9c4 \uacbd\uc6b0, \\(\\theta_1\\) \uc5d0 \ub300\ud55c prior\ub85c \\(N(\\theta_0,\\frac{1}{n_0\\theta_3})\\) \ub97c \uc0ac\uc6a9\ud558\uba74 \uc5b4\ub5a4 \uc810\uc774 \ud3b8\ub9ac\ud574\uc9c0\ub294\uc9c0 \uc124\uba85\ud558\uc2dc\uc624. \\(\\theta_3\\) \uc774 \uc8fc\uc5b4\uc9c4 \uacbd\uc6b0 \\(\\theta_1\\) \uc5d0 \ub300\ud55c prior\ub97c \uc704\uc640 \uac19\uc774 \uc815\ud558\uba74, posterior joint pdf\ub97c \uc544\ub798\uc640 \uac19\uc740 \uaf34\ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\(k(\\theta_1,\\theta_3|\\bar{x},s^2) \\propto L(\\theta_1,\\theta_3|\\mathbf{x})h(\\theta_3)h(\\theta_1|\\theta_3)\\) \ub610\ud55c \uc774\ub294 \uc774\uc5b4\uc11c \uc4f0\uba74 \\(\\propto \\theta_3^{\\alpha+\\frac{n}{2}+\\frac{1}{2}-1}exp[-\\frac{1}{2}Q(\\theta_1)\\theta_3]\\) \\(where\\ Q(\\theta_1)=\\frac{2}{\\beta}+n_0(\\theta_1-\\theta_0)^2+[(n-1)s^2+n(\\bar{x}-\\theta_1)^2]\\) posterior joint pdf\uac00 \uc704\ucc98\ub7fc \ub098\uc62c \uc2dc, \uc774\ub294 \\(\\theta_3\\) \uc5d0 \ub300\ud55c Gamma kernel \uaf34\uc774\ubbc0\ub85c, \\(\\theta_1\\) \uc5d0 \ub300\ud55c prior\ub97c \\(N(\\theta_0,\\frac{1}{n_0\\theta_3})\\) \ub85c \uc8fc\ub294 \uac83\uc774 \ud3b8\ub9ac\ud55c \uc774\uc720\ub294 \uc6b0\ub9ac\uac00 \uc54c\uace0\uc790\ud558\ub294 \ubaa8\uc218 \\(\\theta_1\\) \uc758 marginal pdf\ub97c \ub9e4\uc6b0 \ud3b8\ud558\uac8c \uad6c\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. 1-3. \\(\\theta_1\\) \uacfc \\(\\theta_3\\) \uc5d0 \ub300\ud55c posterior joint pdf\uac00 14\ud398\uc774\uc9c0 \ub9e8 \uc544\ub798\uc758 \uc2dd\ucc98\ub7fc \uc8fc\uc5b4\uc9d0\uc744 \ubcf4\uc774\uc2dc\uc624. \uc704\uc5d0\uc11c \uc6b0\ub9ac\ub294 \\(h(\\theta_1,\\theta_3)=h(\\theta_3)h_1(\\theta_1|\\theta_3)\\) \\(\\propto \\theta_3^{\\alpha-1}exp(-\\frac{1}{\\beta}\\theta_3)(n_0\\theta_3)^{1/2}exp(-\\frac{n_0\\theta_3}{2}(\\theta_1-\\theta_0)^2)\\) \ub85c \ud45c\ud604\ud558\uc600\uace0, \uc774\uc5d0 \\(g(\\bar{x},s^2|\\theta_1,\\theta_3)\\) \uc744 \uacf1\ud558\uc5ec joint posterior pdf\ub97c \ub2e4\uc74c\ucc98\ub7fc \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\(k(\\theta_1,\\theta_3|\\bar{x},s^2) \\propto g(\\bar{x},s^2|\\theta_1,\\theta_3)h(\\theta_1,\\theta_3)\\) \\(\\propto (\\theta_3)^{\\frac{n}{2}}exp[-\\frac{\\theta_3}{2}\\{(n-1)s^2+n(\\bar{x}-\\theta_1)^2\\}] \\theta_3^{\\alpha-1}exp(-\\frac{1}{\\beta}\\theta_3)(n_0\\theta_3)^{1/2}exp(-\\frac{n_0\\theta_3}{2}(\\theta_1-\\theta_0)^2)\\) \\(\\propto (\\theta_3)^{\\alpha+\\frac{n}{2}+\\frac{1}{2}-1}exp[-\\frac{\\theta_3}{2}\\{\\{(n-1)s^2+n(\\bar{x}-\\theta_1)^2\\}+n_0(\\theta_1-\\theta_0)^2+\\frac{2}{\\beta}\\}]\\) \\(\\propto (\\theta_3)^{\\alpha+\\frac{n}{2}+\\frac{1}{2}-1}exp[-\\frac{\\theta_3}{2}Q(\\theta_1)]\\) \\(where\\ Q(\\theta_1)=\\frac{2}{\\beta}+n_0(\\theta_1-\\theta_0)^2+[(n-1)s^2+n(\\bar{x}-\\theta_1)^2]\\) 2. Prior\ub85c logistic distribution\uc744 \uc0ac\uc6a9\ud558\uace0 \uc788\ub2e4. \ub9cc\uc77c squared-error loss function\uc744 \uc0ac\uc6a9\ud55c\ub2e4\uba74, \\(\\theta\\) \uc5d0 \ub300\ud55c Bayes estimator\ub97c \uad6c\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc801\ubd84\uc744 \ub450\ubc88\uc774\ub098 \ud574\uc57c\ud55c\ub2e4\uace0 \uac15\uc758\ub178\ud2b8\uc5d0 \uc801\ud600\uc788\ub2e4. \uc65c \uadf8\ub7f0\uc9c0 \uc124\uba85\ud558\uace0, \uadf8 \uc801\ubd84\uc774 closed form\uc73c\ub85c \uad6c\ud560 \uc218 \uc5c6\uc74c\ub3c4 \uc124\uba85\ud558\uc2dc\uc624. \ucc45\uc758 \uc608\uc81c\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\[X_1,...,X_n \\stackrel{iid}{\\sim} N(\\theta_0,\\sigma^2),\\ \\sigma^2\\ is\\ known\\] \\[Y=\\bar{X},\\ a\\ sufficient\\ statistic\\ for\\ \\theta\\] \\(Y|\\theta \\sim N(\\theta, \\frac{\\sigma^2}{n})\\) \\(\\Theta \\sim h(\\theta)=\\frac{1}{b}\\frac{exp\\{-(\\theta-a)/b\\}}{[1+exp\\{-(\\theta-a)/b\\}]^2}\\) where \\(-\\infty<\\theta<\\infty,\\ a\\ and\\ b >0\\ are\\ known\\) \uc774\ub294 prior\uc758 \ubd84\ud3ec\ub97c logistic\ubd84\ud3ec\ub85c \uac00\uc815\ud55c \uac83\uc778\ub370, \ubca0\uc774\uc988 \uc815\ub9ac\ub97c \ud1b5\ud574 \uc0ac\ud6c4\ubd84\ud3ec\ub97c \ub098\ud0c0\ub0b4\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\[k(\\theta|y)=\\frac{g(\\theta|y)h(\\theta)}{g_1(y)}=\\frac{\\frac{\\sqrt{n}}{\\sqrt{2\\pi}\\sigma}exp\\{-\\frac{1}{2}\\frac{(y-\\theta)^2}{\\sigma^2/n}\\}\\frac{b^{-1}exp\\{-(\\theta-a)/b\\}}{[1+exp\\{-(\\theta-a)/b\\}]^2}}{\\int_{-\\infty}^{\\infty}\\frac{\\sqrt{n}}{\\sqrt{2\\pi}\\sigma}exp\\{-\\frac{1}{2}\\frac{(y-\\theta)^2}{\\sigma^2/n}\\}\\frac{b^{-1}exp\\{-(\\theta-a)/b\\}}{[1+exp\\{-(\\theta-a)/b\\}]^2}d\\theta}\\] \uc989 posterior pdf\ub97c \uad6c\ud558\ub294 \uacfc\uc815\uc5d0\uc11c \ubd84\uc790\uc758 Y\uc640 \\(\\Theta\\) \uc758 joint pdf\uc5d0\uc11c Y\uc758 marginal\ub85c \ub098\ub220\uc8fc\uae30 \uc704\ud574 \\(\\theta\\) \ub97c marginalize\ud558\ub294 \uacfc\uc815\uc758 \uc801\ubd84\uc774 \ud3ec\ud568\ub41c\ub2e4. \uc774\ud6c4 squared-error loss function\uc73c\ub85c \uacc4\uc0b0\ub418\ub294 \\(\\theta\\) \uc5d0 \ub300\ud55c Bayes estimate\ub294 \uc0ac\ud6c4\ubd84\ud3ec\uc758 \ud3c9\uade0\uc774\uae30\uc5d0, \uc774 \ud3c9\uade0\uc744 \uacc4\uc0b0\ud558\ub294 \uacfc\uc815\uc5d0\uc11c \ud544\uc694\ud55c \uc801\ubd84\uacfc\uc815\uc774 \ud558\ub098 \ucd94\uac00 \ub418\uc5b4 \ucd5c\uc885\uc801\uc73c\ub85c \ub450\ubc88\uc758 \uc801\ubd84\uacfc\uc815\uc774 \ud3ec\ud568\ub418\uac8c \ub418\ub294 \uac83\uc774\ub2e4. \\(\\delta(y)=E[\\Theta|y]\\) \\(={\\int_{-\\infty}^\\infty}\\theta k(\\theta|y)d\\theta\\) \\(={\\int_{-\\infty}^\\infty}\\theta \\frac{\\frac{\\sqrt{n}}{\\sqrt{2\\pi}\\sigma}exp\\{-\\frac{1}{2}\\frac{(y-\\theta)^2}{\\sigma^2/n}\\}\\frac{b^{-1}exp\\{-(\\theta-a)/b\\}}{[1+exp\\{-(\\theta-a)/b\\}]^2}}{\\int_{-\\infty}^{\\infty}\\frac{\\sqrt{n}}{\\sqrt{2\\pi}\\sigma}exp\\{-\\frac{1}{2}\\frac{(y-\\theta)^2}{\\sigma^2/n}\\}\\frac{b^{-1}exp\\{-(\\theta-a)/b\\}}{[1+exp\\{-(\\theta-a)/b\\}]^2}d\\theta}\\) \uadf8\ub9ac\uace0 \ucf24\ub840\uc0ac\uc804\ubd84\ud3ec\ub294 \uc774\ub85c \uc778\ud574 \uc5f0\uc0b0\ub418\ub294 \uc0ac\ud6c4\ubd84\ud3ec\uac00 \uac19\uc740 family\uc5d0 \uc18d\ud558\uc5ec \uacc4\uc0b0\uc774 closed form\uc73c\ub85c \ud45c\ud604\ub420 \uc218 \uc788\ub294 \uac83\uc778\ub370, logistic distribution\uc758 pdf\ub294 normal\uacfc conjugate\ub97c \uc774\ub8e8\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 \uc801\ubd84\uacc4\uc0b0\uc774 closed form\uc73c\ub85c \uc5bb\uc5b4\uc9c0\uc9c0 \uc54a\ub294\ub2e4. 3. \uac15\uc758 \ub178\ud2b8 18\ud398\uc774\uc9c0\uc5d0\ub294 \"inverse of the logistic cdf\"\ub97c \uad6c\ud558\uc600\ub2e4. \uc65c \uadf8\uac83\uc744 \uad6c\ud574\uc57c \ud558\ub294\uc9c0 \uc124\uba85\ud558\uc2dc\uc624. inverse of the logistic cdf\ub97c \uad6c\ud55c \uc774\uc720\ub294 \\(Unif(0,1)\\) \ub97c \ub9cc\ub4e4\uc5b4 \uc0d8\ud50c\ub9c1\uc744 \ud558\uae30 \uc704\ud574\uc11c\uc774\ub2e4. \uc774\ub97c \uad6c\ud55c \uc774\uc720\ub294, \uc704\uc758 \ubb38\uc81c\uc5d0\uc11c Bayes estimate\ub97c \uacc4\uc0b0\ud558\ub294\ub370 \uc801\ubd84\uc744 closed form\uc73c\ub85c \uc5bb\uc744 \uc218 \uc5c6\uc73c\ubbc0\ub85c, \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 \\(w(\\theta)=f(y|\\theta)=\\frac{\\sqrt{n}}{\\sqrt{2\\pi}{\\sigma}}exp\\{-\\frac{1}{2}\\frac{(y-\\theta)^2}{\\sigma^2/n}\\}\\) \uc73c\ub85c \uc815\uc758\ud558\uc5ec Bayes estimate\ub97c \ub2e4\uc74c\uc640 \uac19\uc774 \ud45c\ud604\ud55c\ub2e4. \\(\\delta(y)=\\frac{E[\\Theta w(\\Theta)]}{E[w(\\Theta)]}\\) Monte Carlo \uae30\ubc95\uc73c\ub85c \uc704\uc758 \\(\\frac{E[\\Theta w(\\Theta)]}{E[w(\\Theta)]}\\) \ub85c \ud655\ub960\uc218\ub834\ud558\ub294 \\(\\frac{m^{-1}\\sum_{i=1}^m\\Theta w(\\Theta_i)}{m^{-1}\\sum_{i=1}^mw(\\Theta_i)}\\) \ub97c \uacc4\uc0b0\ud558\uae30 \uc704\ud574 \\(\\Theta_i\\) \ub4e4\uc744 \ub85c\uc9c0\uc2a4\ud2f1 \ubd84\ud3ec\uc5d0\uc11c \ucd94\ucd9c\ud574\uc57c\ud558\uba70, \uc774\ub7ec\ud55c \uacfc\uc815\uc744 \uc704\ud574 \uc544\ub798\uc758 \uae30\ubc95\uc774 \ud544\uc694\ud558\ub2e4. continuous random variable\uacfc \uadf8\uac83\uc758 cdf \ub610\ud55c \uc5f0\uc18d\uc778\uc778 \\(X\\) \ub97c \uc0dd\uac01\ud574\ubcfc \ub54c, \uc6b0\ub9ac\ub294 \uc774\uac83\uc758 cdf \\(F(X)\\) \uc758 \ubd84\ud3ec \uac00 \\([0,1]\\) \uc744 \ubc94\uc704\ub85c \uac16\ub294 Uniform Distribution\uc774\ub77c\ub294 \uac83\uc744 \uc54c \uc218\uc788\ub294\ub370, \uac04\ub2e8\ud788 \uc218\uc2dd\uc73c\ub85c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \uc704\uc5d0\uc11c \uc124\uba85\ud55c \ud655\ub960\ubcc0\uc218\uc758 \ub204\uc801\ud655\ub960\ubd84\ud3ec\ub97c \\(F(X)\\) \ub77c\uace0 \uce6d\ud558\uc790. \uadf8\ub807\ub2e4\uba74 \\(0<x<1\\) \uc744 \ub9cc\uc871\ud558\ub294 \uc784\uc758\uc758 \\(x\\) \uc5d0 \ub300\ud558\uc5ec \uc544\ub798\uc640 \uac19\uc740 \uc2dd\uc804\uac1c\uac00 \uac00\ub2a5\ud558\ub2e4. $ \\(P(F(X) \\leq x)\\) $ $ \\(=P(X \\leq F^{-1}(x))\\) $ $ \\(=F(F^{-1}(x))\\) $ $ \\(=x\\) $ \uc5ec\uae30\uc11c \\(X=F^{-1}(U)\\ has\\ distribution\\ function\\ F,\\ where U~ Unif(0,1)\\) \uc774\ub2e4. 4. WLLN\uac00 \ubb54\uc9c0 \uae30\uc220\ud558\uace0, WLLN\uac00 \uc5b4\ub5bb\uac8c Bayes estimator\ub97c \uc2dc\ubbac\ub808\uc774\uc158\uc744 \uc774\uc6a9\ud558\uc5ec \uad6c\ud558\ub294\ub370 \uc0ac\uc6a9\ub418\uc5c8\ub294\uc9c0 \uc124\uba85\ud558\uc2dc\uc624. Let \\({X_n}\\) be a sequence of iid random variables having common mean \\(\\mu\\) and variance \\(\\sigma^2<\\infty\\) . Let \\(\\bar{X_n}=\\frac{\\sum_{i=1}^nX_i}{n}\\) . Then \\[\\bar{X_n}\\xrightarrow{p} \\mu\\] WLLN\ub294 \ud45c\ubcf8\uc758 \ud06c\uae30\uac00 \ucee4\uc9d0\uc5d0 \ub530\ub77c \ud45c\ubcf8\ud3c9\uade0\uc774 \ubaa8\ud3c9\uade0\uc73c\ub85c \ud655\ub960\uc218\ub834\ud558\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc774\ub7ec\ud55c WLLN\uac00 Bayes estimator\ub97c \uc2dc\ubbac\ub808\uc774\uc158\uc744 \uc774\uc6a9\ud558\uc5ec \uad6c\ud558\ub294\ub370 \uc0ac\uc6a9\ub420 \uc218 \uc788\ub294 \uc774\uc720\ub294 squared error loss function \ud558\uc5d0\uc11c Bayes estimator\uac00 \uc0ac\ud6c4\ubd84\ud3ec\uc758 \uae30\ub313\uac12\uc774\uae30 \ub54c\ubb38\uc774\uace0, \uae30\ub313\uac12\uc740 \uc801\ubd84\uc73c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. \ub530\ub77c\uc11c \uc704\uc5d0\uc11c \uad6c\ud55c logistic prior\uc758 \uc608\uc2dc\uac19\uc740 \uc801\ubd84 \uacc4\uc0b0\uc758 \uc5b4\ub824\uc6c0\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574 \ucda9\ubd84\ud1b5\uacc4\ub7c9\uc758 pdf\ub97c \\(\\theta\\) \uc5d0 \ub300\ud55c \ud568\uc218 \\(w(\\theta)\\) \ub85c \ucde8\uae09\ud558\uc5ec \\(\\frac{E[\\Theta w(\\Theta)]}{E[w(\\Theta)]}\\) \uc73c\ub85c \\(\\delta(y)\\) \ub97c \ub098\ud0c0\ub0b4\uc5b4 \\(\\Theta_i\\) \ub9cc \uc2dc\ubbac\ub808\uc774\uc158 \ud560 \uc218 \uc788\uc73c\uba74, WLLN\uc5d0 \uc758\ud574 m\uc774 \ucee4\uc9d0\uc5d0 \ub530\ub77c \\(\\theta(y)\\) \uc5d0 \ub300\ud55c consistent estimator\ub85c\uc758 \\(\\frac{m^{-1}\\sum_{i=1}^m\\Theta w(\\Theta_i)}{m^{-1}\\sum_{i=1}^mw(\\Theta_i)}\\) \ub97c \uacc4\uc0b0\ud560 \uc218 \uc788\ub2e4. 5. HMC 11.4.5 Suppose (X,Y) has the mixed discrete-continuous pdf \\[f(x,y)= \\frac{1}{\\Gamma(\\alpha)}\\frac{1}{x!}y^{\\alpha+x-1}e^{-2y}, y>0,\\ x=0,1,2,..,\\ \\alpha >0\\] Prove Y ~ Gamma( \\(\\alpha\\) ,1) \\(f_Y(y)= \\sum_{x=0}^\\infty \\frac{1}{\\Gamma(\\alpha)}\\frac{1}{x!}y^{\\alpha+x-1}e^{-2y}= \\frac{1}{\\Gamma(\\alpha)}e^{-2y}y^{\\alpha-1}\\sum_{x=0}^{\\infty}\\frac{1}{x!}y^x\\) , \\(e^x=\\sum_{k=0}^{\\infty}\\frac{1}{k!}x^k\\) \uc774\ubbc0\ub85c, \\(f_Y(y)=\\frac{1}{\\Gamma(\\alpha)}e^{-y}y^{\\alpha-1},\\ y>0,\\ \\alpha>0\\) \uc774\uace0 \uc774\ub294 \\(Gamma(\\alpha,1)\\) \uc758 pdf\uc774\ub2e4. Prove X is negative Binomial \\(fX(x)= \\int_0^{\\infty}f(x,y)\\) \uc5ec\uae30\uc11c \\(y^{\\alpha+x-1}e^{-2y}\\) \ub294 \\(\\Gamma(\\alpha+x,\\frac{1}{2})\\) \uc758 kernel\uc774\ub2e4. \uace0\ub85c \\(f_X(x)= \\frac{1}{\\Gamma(\\alpha)}\\frac{1}{x!}\\Gamma{(\\alpha+x)2^{-(\\alpha+x)}}\\) \\(= \\frac{(\\alpha+x-1)!}{x!(\\alpha-1)!}2^{-(\\alpha+x)}\\) \uc774\ub2e4. \uc774\ub294 \uc131\uacf5\ud69f\uc218\uac00 x\uace0 \uc2e4\ud328\ud69f\uc218\uac00 \\(\\alpha\\) , \uc131\uacf5\ud655\ub960\uc774 \\(\\frac{1}{2}\\) \uc778 \uc74c\uc774\ud56d\ubd84\ud3ec\uc758 pdf\uc774\ub2e4. 6. \uac01\uc790 Hogg \ucc45 648p\uc5d0 \uc788\ub294 gibbser2.s\ub77c\ub294 \ud504\ub85c\uadf8\ub7a8\uc744 \uc2e4\ud589\ud558\uc5ec, \uc720\uc0ac\ud55c \uacb0\uacfc\uac00 \ub098\uc624\ub294\uc9c0 \ud655\uc778\ud558\uc2dc\uc624. gibbser2 = function(alpha,m,n){ x0 = 1 yc = rep(0,m+n) xc = c(x0,rep(0,m-1+n)) for(i in 2:(m+n)){yc[i] = rgamma(1,alpha+xc[i-1],2) xc[i] = rpois(1,yc[i])} y1=yc[1:m] y2=yc[(m+1):(m+n)] x1=xc[1:m] x2=xc[(m+1):(m+n)] list(y1 = y1,y2=y2,x1=x1,x2=x2) } set.seed(2013122059) result=gibbser2(10,3000,3000) yhat <- result[['y2']] xhat <- result[['x2']] yse <- 1.96*(sd(yhat)/sqrt(3000)) yCI <- paste0('(',round(mean(yhat)-yse,3),',', round(mean(yhat)+yse,3),')') xse <-1.96*(sd(yhat)/sqrt(3000)) xCI <- paste0('(',round(mean(xhat)-xse,3),',', round(mean(xhat)+xse,3),')') mean(xhat) ; mean(yhat) ## [1] 10.15433 ## [1] 10.10725 var(xhat) ; var(yhat) ## [1] 20.31862 ## [1] 10.46886 xCI ; yCI ## [1] \"(10.039,10.27)\" ## [1] \"(9.991,10.223)\" \ucc45\uc5d0 \uc788\ub294 gibbs sampler \ud504\ub85c\uadf8\ub7a8\uc744 \uc2e4\ud589\ud558\uc5ec \\(\\alpha=10,\\ m=3000,\\ n=6000\\) \ub85c \uc2dc\ubbac\ub808\uc774\uc158\uc744 \ud574\ubcf8 \uacb0\uacfc lecture note\uc758 \uac12\uacfc \ube44\uc2b7\ud558\uac8c \ub098\uc624\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.","title":"Bayes Homework2"},{"location":"03%20Theoretical%20Statistics/Bayes_Homework2/#1-1-14-theta_3-conjugate-prior-gammaalphabeta","text":"\uc815\uaddc\ubd84\ud3ec\uc758 \ucf00\uc774\uc2a4\uc5d0\uc11c \ucda9\ubd84\ud1b5\uacc4\ub7c9\uc740 \\(\\bar{X}\\) \uc640 \\(S^2\\) \uc774\ub2e4. \uc774\ub4e4\uc744 \uc774\uc6a9\ud558\uc5ec likelihood\ub97c \uc0b4\uc9dd \ubcc0\ud615\ud558\uc5ec \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\(L(\\theta_1,\\theta_3|\\mathbf{x}) \\propto (\\frac{\\theta_3}{2\\pi})^{n/2}exp[-{\\frac{1}{2}} \\{(n-1)s^2+n(\\bar{x}-\\theta_1)^2\\}\\theta_3]\\) \uc774\ub294 \ub2e4\uc2dc \ubcf4\uba74 \uac10\ub9c8\ubd84\ud3ec\uc778 \\(\\frac{1}{\\Gamma{(\\alpha})\\beta^{\\alpha}}x^{\\alpha-1}exp(-\\frac{x}{\\beta})\\) \uc758 \uaf34\ub85c \ud45c\ud604\uc774 \uac00\ub2a5\ud568\uc744 \ud655\uc778\ud560 \uc218 \uc788\uace0, \ubaa8\uc218 \\(theta_3\\) \uc5d0 \ub300\ud574 likelihood\uac00 \uac10\ub9c8\ubd84\ud3ec\uc758 kernel\ub85c \uc774\ub8e8\uc5b4\uc9d0\uc744 \uc54c \uc218 \uc788\ub2e4. \uace0\ub85c \\(\\theta_3\\) \uc5d0 \ub300\ud55c conjugate prior\uac00 Gamma \\((\\alpha,\\beta)\\) \uc774\ub2e4.","title":"1-1. \uac15\uc758\ub178\ud2b8 14\ud398\uc774\uc9c0 \ucc38\uace0, \uc65c \\(\\theta_3\\)\uc5d0 \ub300\ud55c conjugate prior\uac00 Gamma\\((\\alpha,\\beta)\\)\uc778\uc9c0 \uc124\uba85\ud558\uc2dc\uc624."},{"location":"03%20Theoretical%20Statistics/Bayes_Homework2/#1-2-theta_3-theta_1-prior-ntheta_0frac1n_0theta_3","text":"\\(\\theta_3\\) \uc774 \uc8fc\uc5b4\uc9c4 \uacbd\uc6b0 \\(\\theta_1\\) \uc5d0 \ub300\ud55c prior\ub97c \uc704\uc640 \uac19\uc774 \uc815\ud558\uba74, posterior joint pdf\ub97c \uc544\ub798\uc640 \uac19\uc740 \uaf34\ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\(k(\\theta_1,\\theta_3|\\bar{x},s^2) \\propto L(\\theta_1,\\theta_3|\\mathbf{x})h(\\theta_3)h(\\theta_1|\\theta_3)\\) \ub610\ud55c \uc774\ub294 \uc774\uc5b4\uc11c \uc4f0\uba74 \\(\\propto \\theta_3^{\\alpha+\\frac{n}{2}+\\frac{1}{2}-1}exp[-\\frac{1}{2}Q(\\theta_1)\\theta_3]\\) \\(where\\ Q(\\theta_1)=\\frac{2}{\\beta}+n_0(\\theta_1-\\theta_0)^2+[(n-1)s^2+n(\\bar{x}-\\theta_1)^2]\\) posterior joint pdf\uac00 \uc704\ucc98\ub7fc \ub098\uc62c \uc2dc, \uc774\ub294 \\(\\theta_3\\) \uc5d0 \ub300\ud55c Gamma kernel \uaf34\uc774\ubbc0\ub85c, \\(\\theta_1\\) \uc5d0 \ub300\ud55c prior\ub97c \\(N(\\theta_0,\\frac{1}{n_0\\theta_3})\\) \ub85c \uc8fc\ub294 \uac83\uc774 \ud3b8\ub9ac\ud55c \uc774\uc720\ub294 \uc6b0\ub9ac\uac00 \uc54c\uace0\uc790\ud558\ub294 \ubaa8\uc218 \\(\\theta_1\\) \uc758 marginal pdf\ub97c \ub9e4\uc6b0 \ud3b8\ud558\uac8c \uad6c\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4.","title":"1-2. \\(\\theta_3\\)\uac00 \uc8fc\uc5b4\uc9c4 \uacbd\uc6b0, \\(\\theta_1\\)\uc5d0 \ub300\ud55c prior\ub85c \\(N(\\theta_0,\\frac{1}{n_0\\theta_3})\\)\ub97c \uc0ac\uc6a9\ud558\uba74 \uc5b4\ub5a4 \uc810\uc774 \ud3b8\ub9ac\ud574\uc9c0\ub294\uc9c0 \uc124\uba85\ud558\uc2dc\uc624."},{"location":"03%20Theoretical%20Statistics/Bayes_Homework2/#1-3-theta_1-theta_3-posterior-joint-pdf-14","text":"\uc704\uc5d0\uc11c \uc6b0\ub9ac\ub294 \\(h(\\theta_1,\\theta_3)=h(\\theta_3)h_1(\\theta_1|\\theta_3)\\) \\(\\propto \\theta_3^{\\alpha-1}exp(-\\frac{1}{\\beta}\\theta_3)(n_0\\theta_3)^{1/2}exp(-\\frac{n_0\\theta_3}{2}(\\theta_1-\\theta_0)^2)\\) \ub85c \ud45c\ud604\ud558\uc600\uace0, \uc774\uc5d0 \\(g(\\bar{x},s^2|\\theta_1,\\theta_3)\\) \uc744 \uacf1\ud558\uc5ec joint posterior pdf\ub97c \ub2e4\uc74c\ucc98\ub7fc \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\(k(\\theta_1,\\theta_3|\\bar{x},s^2) \\propto g(\\bar{x},s^2|\\theta_1,\\theta_3)h(\\theta_1,\\theta_3)\\) \\(\\propto (\\theta_3)^{\\frac{n}{2}}exp[-\\frac{\\theta_3}{2}\\{(n-1)s^2+n(\\bar{x}-\\theta_1)^2\\}] \\theta_3^{\\alpha-1}exp(-\\frac{1}{\\beta}\\theta_3)(n_0\\theta_3)^{1/2}exp(-\\frac{n_0\\theta_3}{2}(\\theta_1-\\theta_0)^2)\\) \\(\\propto (\\theta_3)^{\\alpha+\\frac{n}{2}+\\frac{1}{2}-1}exp[-\\frac{\\theta_3}{2}\\{\\{(n-1)s^2+n(\\bar{x}-\\theta_1)^2\\}+n_0(\\theta_1-\\theta_0)^2+\\frac{2}{\\beta}\\}]\\) \\(\\propto (\\theta_3)^{\\alpha+\\frac{n}{2}+\\frac{1}{2}-1}exp[-\\frac{\\theta_3}{2}Q(\\theta_1)]\\) \\(where\\ Q(\\theta_1)=\\frac{2}{\\beta}+n_0(\\theta_1-\\theta_0)^2+[(n-1)s^2+n(\\bar{x}-\\theta_1)^2]\\)","title":"1-3. \\(\\theta_1\\)\uacfc \\(\\theta_3\\)\uc5d0 \ub300\ud55c posterior joint pdf\uac00 14\ud398\uc774\uc9c0 \ub9e8 \uc544\ub798\uc758 \uc2dd\ucc98\ub7fc \uc8fc\uc5b4\uc9d0\uc744 \ubcf4\uc774\uc2dc\uc624."},{"location":"03%20Theoretical%20Statistics/Bayes_Homework2/#2-prior-logistic-distribution-squared-error-loss-function-theta-bayes-estimator-closed-form","text":"\ucc45\uc758 \uc608\uc81c\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\[X_1,...,X_n \\stackrel{iid}{\\sim} N(\\theta_0,\\sigma^2),\\ \\sigma^2\\ is\\ known\\] \\[Y=\\bar{X},\\ a\\ sufficient\\ statistic\\ for\\ \\theta\\] \\(Y|\\theta \\sim N(\\theta, \\frac{\\sigma^2}{n})\\) \\(\\Theta \\sim h(\\theta)=\\frac{1}{b}\\frac{exp\\{-(\\theta-a)/b\\}}{[1+exp\\{-(\\theta-a)/b\\}]^2}\\) where \\(-\\infty<\\theta<\\infty,\\ a\\ and\\ b >0\\ are\\ known\\) \uc774\ub294 prior\uc758 \ubd84\ud3ec\ub97c logistic\ubd84\ud3ec\ub85c \uac00\uc815\ud55c \uac83\uc778\ub370, \ubca0\uc774\uc988 \uc815\ub9ac\ub97c \ud1b5\ud574 \uc0ac\ud6c4\ubd84\ud3ec\ub97c \ub098\ud0c0\ub0b4\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\[k(\\theta|y)=\\frac{g(\\theta|y)h(\\theta)}{g_1(y)}=\\frac{\\frac{\\sqrt{n}}{\\sqrt{2\\pi}\\sigma}exp\\{-\\frac{1}{2}\\frac{(y-\\theta)^2}{\\sigma^2/n}\\}\\frac{b^{-1}exp\\{-(\\theta-a)/b\\}}{[1+exp\\{-(\\theta-a)/b\\}]^2}}{\\int_{-\\infty}^{\\infty}\\frac{\\sqrt{n}}{\\sqrt{2\\pi}\\sigma}exp\\{-\\frac{1}{2}\\frac{(y-\\theta)^2}{\\sigma^2/n}\\}\\frac{b^{-1}exp\\{-(\\theta-a)/b\\}}{[1+exp\\{-(\\theta-a)/b\\}]^2}d\\theta}\\] \uc989 posterior pdf\ub97c \uad6c\ud558\ub294 \uacfc\uc815\uc5d0\uc11c \ubd84\uc790\uc758 Y\uc640 \\(\\Theta\\) \uc758 joint pdf\uc5d0\uc11c Y\uc758 marginal\ub85c \ub098\ub220\uc8fc\uae30 \uc704\ud574 \\(\\theta\\) \ub97c marginalize\ud558\ub294 \uacfc\uc815\uc758 \uc801\ubd84\uc774 \ud3ec\ud568\ub41c\ub2e4. \uc774\ud6c4 squared-error loss function\uc73c\ub85c \uacc4\uc0b0\ub418\ub294 \\(\\theta\\) \uc5d0 \ub300\ud55c Bayes estimate\ub294 \uc0ac\ud6c4\ubd84\ud3ec\uc758 \ud3c9\uade0\uc774\uae30\uc5d0, \uc774 \ud3c9\uade0\uc744 \uacc4\uc0b0\ud558\ub294 \uacfc\uc815\uc5d0\uc11c \ud544\uc694\ud55c \uc801\ubd84\uacfc\uc815\uc774 \ud558\ub098 \ucd94\uac00 \ub418\uc5b4 \ucd5c\uc885\uc801\uc73c\ub85c \ub450\ubc88\uc758 \uc801\ubd84\uacfc\uc815\uc774 \ud3ec\ud568\ub418\uac8c \ub418\ub294 \uac83\uc774\ub2e4. \\(\\delta(y)=E[\\Theta|y]\\) \\(={\\int_{-\\infty}^\\infty}\\theta k(\\theta|y)d\\theta\\) \\(={\\int_{-\\infty}^\\infty}\\theta \\frac{\\frac{\\sqrt{n}}{\\sqrt{2\\pi}\\sigma}exp\\{-\\frac{1}{2}\\frac{(y-\\theta)^2}{\\sigma^2/n}\\}\\frac{b^{-1}exp\\{-(\\theta-a)/b\\}}{[1+exp\\{-(\\theta-a)/b\\}]^2}}{\\int_{-\\infty}^{\\infty}\\frac{\\sqrt{n}}{\\sqrt{2\\pi}\\sigma}exp\\{-\\frac{1}{2}\\frac{(y-\\theta)^2}{\\sigma^2/n}\\}\\frac{b^{-1}exp\\{-(\\theta-a)/b\\}}{[1+exp\\{-(\\theta-a)/b\\}]^2}d\\theta}\\) \uadf8\ub9ac\uace0 \ucf24\ub840\uc0ac\uc804\ubd84\ud3ec\ub294 \uc774\ub85c \uc778\ud574 \uc5f0\uc0b0\ub418\ub294 \uc0ac\ud6c4\ubd84\ud3ec\uac00 \uac19\uc740 family\uc5d0 \uc18d\ud558\uc5ec \uacc4\uc0b0\uc774 closed form\uc73c\ub85c \ud45c\ud604\ub420 \uc218 \uc788\ub294 \uac83\uc778\ub370, logistic distribution\uc758 pdf\ub294 normal\uacfc conjugate\ub97c \uc774\ub8e8\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 \uc801\ubd84\uacc4\uc0b0\uc774 closed form\uc73c\ub85c \uc5bb\uc5b4\uc9c0\uc9c0 \uc54a\ub294\ub2e4.","title":"2. Prior\ub85c logistic distribution\uc744 \uc0ac\uc6a9\ud558\uace0 \uc788\ub2e4. \ub9cc\uc77c squared-error loss function\uc744 \uc0ac\uc6a9\ud55c\ub2e4\uba74, \\(\\theta\\)\uc5d0 \ub300\ud55c Bayes estimator\ub97c \uad6c\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc801\ubd84\uc744 \ub450\ubc88\uc774\ub098 \ud574\uc57c\ud55c\ub2e4\uace0 \uac15\uc758\ub178\ud2b8\uc5d0 \uc801\ud600\uc788\ub2e4. \uc65c \uadf8\ub7f0\uc9c0 \uc124\uba85\ud558\uace0, \uadf8 \uc801\ubd84\uc774 closed form\uc73c\ub85c \uad6c\ud560 \uc218 \uc5c6\uc74c\ub3c4 \uc124\uba85\ud558\uc2dc\uc624."},{"location":"03%20Theoretical%20Statistics/Bayes_Homework2/#3-18-inverse-of-the-logistic-cdf","text":"inverse of the logistic cdf\ub97c \uad6c\ud55c \uc774\uc720\ub294 \\(Unif(0,1)\\) \ub97c \ub9cc\ub4e4\uc5b4 \uc0d8\ud50c\ub9c1\uc744 \ud558\uae30 \uc704\ud574\uc11c\uc774\ub2e4. \uc774\ub97c \uad6c\ud55c \uc774\uc720\ub294, \uc704\uc758 \ubb38\uc81c\uc5d0\uc11c Bayes estimate\ub97c \uacc4\uc0b0\ud558\ub294\ub370 \uc801\ubd84\uc744 closed form\uc73c\ub85c \uc5bb\uc744 \uc218 \uc5c6\uc73c\ubbc0\ub85c, \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574 \\(w(\\theta)=f(y|\\theta)=\\frac{\\sqrt{n}}{\\sqrt{2\\pi}{\\sigma}}exp\\{-\\frac{1}{2}\\frac{(y-\\theta)^2}{\\sigma^2/n}\\}\\) \uc73c\ub85c \uc815\uc758\ud558\uc5ec Bayes estimate\ub97c \ub2e4\uc74c\uc640 \uac19\uc774 \ud45c\ud604\ud55c\ub2e4. \\(\\delta(y)=\\frac{E[\\Theta w(\\Theta)]}{E[w(\\Theta)]}\\) Monte Carlo \uae30\ubc95\uc73c\ub85c \uc704\uc758 \\(\\frac{E[\\Theta w(\\Theta)]}{E[w(\\Theta)]}\\) \ub85c \ud655\ub960\uc218\ub834\ud558\ub294 \\(\\frac{m^{-1}\\sum_{i=1}^m\\Theta w(\\Theta_i)}{m^{-1}\\sum_{i=1}^mw(\\Theta_i)}\\) \ub97c \uacc4\uc0b0\ud558\uae30 \uc704\ud574 \\(\\Theta_i\\) \ub4e4\uc744 \ub85c\uc9c0\uc2a4\ud2f1 \ubd84\ud3ec\uc5d0\uc11c \ucd94\ucd9c\ud574\uc57c\ud558\uba70, \uc774\ub7ec\ud55c \uacfc\uc815\uc744 \uc704\ud574 \uc544\ub798\uc758 \uae30\ubc95\uc774 \ud544\uc694\ud558\ub2e4. continuous random variable\uacfc \uadf8\uac83\uc758 cdf \ub610\ud55c \uc5f0\uc18d\uc778\uc778 \\(X\\) \ub97c \uc0dd\uac01\ud574\ubcfc \ub54c, \uc6b0\ub9ac\ub294 \uc774\uac83\uc758 cdf \\(F(X)\\) \uc758 \ubd84\ud3ec \uac00 \\([0,1]\\) \uc744 \ubc94\uc704\ub85c \uac16\ub294 Uniform Distribution\uc774\ub77c\ub294 \uac83\uc744 \uc54c \uc218\uc788\ub294\ub370, \uac04\ub2e8\ud788 \uc218\uc2dd\uc73c\ub85c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \uc704\uc5d0\uc11c \uc124\uba85\ud55c \ud655\ub960\ubcc0\uc218\uc758 \ub204\uc801\ud655\ub960\ubd84\ud3ec\ub97c \\(F(X)\\) \ub77c\uace0 \uce6d\ud558\uc790. \uadf8\ub807\ub2e4\uba74 \\(0<x<1\\) \uc744 \ub9cc\uc871\ud558\ub294 \uc784\uc758\uc758 \\(x\\) \uc5d0 \ub300\ud558\uc5ec \uc544\ub798\uc640 \uac19\uc740 \uc2dd\uc804\uac1c\uac00 \uac00\ub2a5\ud558\ub2e4. $ \\(P(F(X) \\leq x)\\) $ $ \\(=P(X \\leq F^{-1}(x))\\) $ $ \\(=F(F^{-1}(x))\\) $ $ \\(=x\\) $ \uc5ec\uae30\uc11c \\(X=F^{-1}(U)\\ has\\ distribution\\ function\\ F,\\ where U~ Unif(0,1)\\) \uc774\ub2e4.","title":"3. \uac15\uc758 \ub178\ud2b8 18\ud398\uc774\uc9c0\uc5d0\ub294 \"inverse of the logistic cdf\"\ub97c \uad6c\ud558\uc600\ub2e4. \uc65c \uadf8\uac83\uc744 \uad6c\ud574\uc57c \ud558\ub294\uc9c0 \uc124\uba85\ud558\uc2dc\uc624."},{"location":"03%20Theoretical%20Statistics/Bayes_Homework2/#4-wlln-wlln-bayes-estimator","text":"Let \\({X_n}\\) be a sequence of iid random variables having common mean \\(\\mu\\) and variance \\(\\sigma^2<\\infty\\) . Let \\(\\bar{X_n}=\\frac{\\sum_{i=1}^nX_i}{n}\\) . Then \\[\\bar{X_n}\\xrightarrow{p} \\mu\\] WLLN\ub294 \ud45c\ubcf8\uc758 \ud06c\uae30\uac00 \ucee4\uc9d0\uc5d0 \ub530\ub77c \ud45c\ubcf8\ud3c9\uade0\uc774 \ubaa8\ud3c9\uade0\uc73c\ub85c \ud655\ub960\uc218\ub834\ud558\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc774\ub7ec\ud55c WLLN\uac00 Bayes estimator\ub97c \uc2dc\ubbac\ub808\uc774\uc158\uc744 \uc774\uc6a9\ud558\uc5ec \uad6c\ud558\ub294\ub370 \uc0ac\uc6a9\ub420 \uc218 \uc788\ub294 \uc774\uc720\ub294 squared error loss function \ud558\uc5d0\uc11c Bayes estimator\uac00 \uc0ac\ud6c4\ubd84\ud3ec\uc758 \uae30\ub313\uac12\uc774\uae30 \ub54c\ubb38\uc774\uace0, \uae30\ub313\uac12\uc740 \uc801\ubd84\uc73c\ub85c \ud45c\ud604\ud560 \uc218 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. \ub530\ub77c\uc11c \uc704\uc5d0\uc11c \uad6c\ud55c logistic prior\uc758 \uc608\uc2dc\uac19\uc740 \uc801\ubd84 \uacc4\uc0b0\uc758 \uc5b4\ub824\uc6c0\uc744 \ud574\uacb0\ud558\uae30 \uc704\ud574 \ucda9\ubd84\ud1b5\uacc4\ub7c9\uc758 pdf\ub97c \\(\\theta\\) \uc5d0 \ub300\ud55c \ud568\uc218 \\(w(\\theta)\\) \ub85c \ucde8\uae09\ud558\uc5ec \\(\\frac{E[\\Theta w(\\Theta)]}{E[w(\\Theta)]}\\) \uc73c\ub85c \\(\\delta(y)\\) \ub97c \ub098\ud0c0\ub0b4\uc5b4 \\(\\Theta_i\\) \ub9cc \uc2dc\ubbac\ub808\uc774\uc158 \ud560 \uc218 \uc788\uc73c\uba74, WLLN\uc5d0 \uc758\ud574 m\uc774 \ucee4\uc9d0\uc5d0 \ub530\ub77c \\(\\theta(y)\\) \uc5d0 \ub300\ud55c consistent estimator\ub85c\uc758 \\(\\frac{m^{-1}\\sum_{i=1}^m\\Theta w(\\Theta_i)}{m^{-1}\\sum_{i=1}^mw(\\Theta_i)}\\) \ub97c \uacc4\uc0b0\ud560 \uc218 \uc788\ub2e4.","title":"4. WLLN\uac00 \ubb54\uc9c0 \uae30\uc220\ud558\uace0, WLLN\uac00 \uc5b4\ub5bb\uac8c Bayes estimator\ub97c \uc2dc\ubbac\ub808\uc774\uc158\uc744 \uc774\uc6a9\ud558\uc5ec \uad6c\ud558\ub294\ub370 \uc0ac\uc6a9\ub418\uc5c8\ub294\uc9c0 \uc124\uba85\ud558\uc2dc\uc624."},{"location":"03%20Theoretical%20Statistics/Bayes_Homework2/#5-hmc-1145","text":"Suppose (X,Y) has the mixed discrete-continuous pdf \\[f(x,y)= \\frac{1}{\\Gamma(\\alpha)}\\frac{1}{x!}y^{\\alpha+x-1}e^{-2y}, y>0,\\ x=0,1,2,..,\\ \\alpha >0\\] Prove Y ~ Gamma( \\(\\alpha\\) ,1) \\(f_Y(y)= \\sum_{x=0}^\\infty \\frac{1}{\\Gamma(\\alpha)}\\frac{1}{x!}y^{\\alpha+x-1}e^{-2y}= \\frac{1}{\\Gamma(\\alpha)}e^{-2y}y^{\\alpha-1}\\sum_{x=0}^{\\infty}\\frac{1}{x!}y^x\\) , \\(e^x=\\sum_{k=0}^{\\infty}\\frac{1}{k!}x^k\\) \uc774\ubbc0\ub85c, \\(f_Y(y)=\\frac{1}{\\Gamma(\\alpha)}e^{-y}y^{\\alpha-1},\\ y>0,\\ \\alpha>0\\) \uc774\uace0 \uc774\ub294 \\(Gamma(\\alpha,1)\\) \uc758 pdf\uc774\ub2e4. Prove X is negative Binomial \\(fX(x)= \\int_0^{\\infty}f(x,y)\\) \uc5ec\uae30\uc11c \\(y^{\\alpha+x-1}e^{-2y}\\) \ub294 \\(\\Gamma(\\alpha+x,\\frac{1}{2})\\) \uc758 kernel\uc774\ub2e4. \uace0\ub85c \\(f_X(x)= \\frac{1}{\\Gamma(\\alpha)}\\frac{1}{x!}\\Gamma{(\\alpha+x)2^{-(\\alpha+x)}}\\) \\(= \\frac{(\\alpha+x-1)!}{x!(\\alpha-1)!}2^{-(\\alpha+x)}\\) \uc774\ub2e4. \uc774\ub294 \uc131\uacf5\ud69f\uc218\uac00 x\uace0 \uc2e4\ud328\ud69f\uc218\uac00 \\(\\alpha\\) , \uc131\uacf5\ud655\ub960\uc774 \\(\\frac{1}{2}\\) \uc778 \uc74c\uc774\ud56d\ubd84\ud3ec\uc758 pdf\uc774\ub2e4.","title":"5. HMC 11.4.5"},{"location":"03%20Theoretical%20Statistics/Bayes_Homework2/#6-hogg-648p-gibbser2s","text":"gibbser2 = function(alpha,m,n){ x0 = 1 yc = rep(0,m+n) xc = c(x0,rep(0,m-1+n)) for(i in 2:(m+n)){yc[i] = rgamma(1,alpha+xc[i-1],2) xc[i] = rpois(1,yc[i])} y1=yc[1:m] y2=yc[(m+1):(m+n)] x1=xc[1:m] x2=xc[(m+1):(m+n)] list(y1 = y1,y2=y2,x1=x1,x2=x2) } set.seed(2013122059) result=gibbser2(10,3000,3000) yhat <- result[['y2']] xhat <- result[['x2']] yse <- 1.96*(sd(yhat)/sqrt(3000)) yCI <- paste0('(',round(mean(yhat)-yse,3),',', round(mean(yhat)+yse,3),')') xse <-1.96*(sd(yhat)/sqrt(3000)) xCI <- paste0('(',round(mean(xhat)-xse,3),',', round(mean(xhat)+xse,3),')') mean(xhat) ; mean(yhat) ## [1] 10.15433 ## [1] 10.10725 var(xhat) ; var(yhat) ## [1] 20.31862 ## [1] 10.46886 xCI ; yCI ## [1] \"(10.039,10.27)\" ## [1] \"(9.991,10.223)\" \ucc45\uc5d0 \uc788\ub294 gibbs sampler \ud504\ub85c\uadf8\ub7a8\uc744 \uc2e4\ud589\ud558\uc5ec \\(\\alpha=10,\\ m=3000,\\ n=6000\\) \ub85c \uc2dc\ubbac\ub808\uc774\uc158\uc744 \ud574\ubcf8 \uacb0\uacfc lecture note\uc758 \uac12\uacfc \ube44\uc2b7\ud558\uac8c \ub098\uc624\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.","title":"6. \uac01\uc790 Hogg \ucc45 648p\uc5d0 \uc788\ub294 gibbser2.s\ub77c\ub294 \ud504\ub85c\uadf8\ub7a8\uc744 \uc2e4\ud589\ud558\uc5ec, \uc720\uc0ac\ud55c \uacb0\uacfc\uac00 \ub098\uc624\ub294\uc9c0 \ud655\uc778\ud558\uc2dc\uc624."},{"location":"04%20Multivariate%20Statistics/Image_compression_using_SVD/","text":"Mutivariate Statistics","title":"R Notebook"},{"location":"04%20Multivariate%20Statistics/Image_compression_using_SVD/#mutivariate-statistics","text":"","title":"Mutivariate Statistics"},{"location":"05%20Design%20of%20Experiments/DOE_HW1/","text":"Design of Experiments HW1 2.25 (a) \ub4f1\ubd84\uc0b0 \uac00\uc124\uac80\uc815\uc744 \ud558\ub77c. \\(\\alpha=0.05\\) \ub85c \uc218\ud589\ud558\ub77c. (b) (a)\uc758 \uacb0\uacfc\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc9d1\ub2e8 \uac04 \ud3c9\uade0\uc5f0\uc18c\uc2dc\uac04\uc774 \uac19\ub2e4\ub294 \uac00\uc124\uac80\uc815\uc744 \uc218\ud589\ud558\ub77c. \\(\\alpha=0.05\\) \ub97c \uc0ac\uc639\ud558\uace0, p-value \ub610\ud55c \uad6c\ud558\uc5ec\ub77c. (c) R\uc744 \uc774\uc6a9\ud558\uc5ec \uc794\ucc28\uc5d0 \ub300\ud55c normal qqplot\uc744 \uc218\ud589\ud558\ub77c. 2.46 (a)\uc758 \ubb38\uc81c\ub97c \ud480\uae30 \uc704\ud574\uc11c\ub294 \uba3c\uc800 \ubaa8\uc9d1\ub2e8\uc5d0\uc11c iid\ud558\uac8c \ucd94\ucd9c\ub41c \uc0d8\ud50c\uc5d0 \ub300\ud574 \uc815\uaddc\uc131 \uac00\uc815\uc774 \ub9cc\uc871\ud558\ub294\uc9c0\uc5d0 \ub300\ud574 \ub17c\uc758\ud558\uc5ec\uc57c\ud55c\ub2e4. \ud558\uc9c0\ub9cc (c)\uc5d0\uc11c \uadf8 \uc815\uaddc\uc131 \ub17c\uc758\ub97c \ub2e4\ub8f0 \uc608\uc815\uc774\ub2c8, 1.\ubaa8\uc9d1\ub2e8\uc774 \uc815\uaddc\ubd84\ud3ec\uc778 \uacbd\uc6b0\uc640, 2. \ubaa8\uc9d1\ub2e8\uc774 \uc815\uaddc\ubd84\ud3ec\uac00 \uc544\ub2cc \ub2e4\ub978\ubd84\ud3ec\uc778 \uacbd\uc6b0\ub97c \uace0\ub824\ud574\uc57c\ud55c\ub2e4. \uba3c\uc800 \ubaa8\uc9d1\ub2e8\uc774 \uc815\uaddc\ubd84\ud3ec\uac00 \uc544\ub2c8\ub77c\ub294 \uac00\uc815\ud558\uc5d0\uc11c \ud560 \uc218 \uc788\ub294 Levene test\ub97c \uc2dc\ud589\ud574\ubcf4\uaca0\ub2e4. library(lawstat) sample_1 <- c(65,81,57,66,82,82,67,59,75,70) sample_2 <- c(64,71,83,59,65,56,69,74,82,79) sample <- c(sample_1,sample_2) group <- c(rep(1,10),rep(2,10)) dat <- data.frame(group,sample) head(dat) ## group sample ## 1 1 65 ## 2 1 81 ## 3 1 57 ## 4 1 66 ## 5 1 82 ## 6 1 82 levene.test(sample,group,location='mean') ## ## Classical Levene's test based on the absolute deviations from the ## mean ( none not applied because the location is not set to median ## ) ## ## data: sample ## Test Statistic = 0.0014598, p-value = 0.9699 \ubaa8\uc9d1\ub2e8\uc774 \uc815\uaddc\ubd84\ud3ec\uac00 \uc544\ub2c8\ub77c\uace0 \uac00\uc815\uc744 \ud558\uc600\uc744 \uc2dc, \uc774\ub584\uc758 p-value \uac12\uc740 0.9699\uac00 \ub098\uc624\uba70, \uc774\ub294 \uc6d0\ub798 \uadc0\ubb34\uac00\uc124\uc778 '\uadf8\ub8f9\uac04\uc758 \ubd84\uc0b0\uc774 \ub4f1\ubd84\uc0b0\uc774\ub2e4'\ub97c \uae30\uac01\ud560 \uc218 \uc5c6\uc74c\uc744 \uc758\ubbf8\ud55c\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub294 \ubaa8\uc9d1\ub2e8\uc774 \uc815\uaddc\ubd84\ud3ec\ub77c\ub294 \uac00\uc815\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc558\uae30 \ub54c\ubb38\uc5d0, F\ud1b5\uacc4\ub7c9\uc744 \uc774\uc6a9\ud55c \uac80\uc815\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc5c6\ub2e4. \ub2e4\uc74c\uc73c\ub85c \ubaa8\uc9d1\ub2e8\uc5d0\uc11c \ucd94\ucd9c\ud55c \uc0d8\ud50c\uc774 \uc815\uaddc\ubd84\ud3ec\uc5d0\uc11c \ucd94\ucd9c\ub418\uc5c8\ub2e4\uba74 , R\uc5d0 \ub0b4\uc7a5\ub418\uc5b4\uc788\ub294 \ud568\uc218\ub97c \ud1b5\ud574 Bartlett's test\ub97c \uc2dc\ud589\ud574\ubcfc \uc218 \uc788\ub2e4. library(stats) bartlett.test(sample~group,dat) ## ## Bartlett test of homogeneity of variances ## ## data: sample by group ## Bartlett's K-squared = 0.0010339, df = 1, p-value = 0.9743 \ub4f1\ubd84\uc0b0\uc131 \uac80\uc815\uc5d0\uc11c\uc758 \uadc0\ubb34\uac00\uc124\uacfc \ub300\ub9bd\uac00\uc124\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \\(H_0:\\sigma^2_1=\\sigma^2_2\\) \\(H_1:{\\sigma^2_1}\\ne{\\sigma^2_2}\\) \uc704\uc758 \uccad\ud06c\ucc3d\uc744 \ud655\uc778\ud55c \uacb0\uacfc p-value\uac00 0.9743\uc73c\ub85c \ub9e4\uc6b0 \ub192\uc740 \uac12\uc73c\ub85c \ub098\uc640 \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud560 \uc218 \uc5c6\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\ubbc0\ub85c, \uc704\uc758 \ub450 \uadf8\ub8f9\uc2e4\ud5d8\uc740 \uc774\ubd84\uc0b0\uc131\uc744 \uac16\uc9c0 \uc54a\uc74c(\uc989 \uadc0\ubb34\uac00\uc124\uc778 \ub4f1\ubd84\uc0b0\uc131\uc774\ub2e4\ub97c \uae30\uac01\ud560 \uc218 \uc5c6\uc74c)\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc774\ubc88\uc5d0\ub294 \uc9c1\uc811 \uacc4\uc0b0\uae30\ub97c \ud1b5\ud574 \uc5bb\uc740 \uac12\uacfc R\uc5d0\uc11c \uc81c\uc2dc\ud55c p-value\uac12\uc774 \uac19\uc740\uc9c0 \ud655\uc778\ud574\ubcf4\uaca0\ub2e4. \ub4f1\ubd84\uc0b0\uc131 \uac80\uc815\uc5d0\uc11c \uadc0\ubb34\uac00\uc124\uc774 \ucc38\uc774\ub77c\ub294 \uc804\uc81c\ud558\uc5d0 \uc0ac\uc6a9\ub418\ub294 \uac80\uc815\ud1b5\uacc4\ub7c9 F\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\[F=\\frac{s_1^2}{s_2^2}\\] #sample\uc758 \ubd84\uc0b0\uc774 \ud45c\ubcf8\ubd84\uc0b0\uc774\ubbc0\ub85c, \ub450 \uc9d1\ub2e8\uc758 \ud45c\ubcf8\ubd84\uc0b0\uc740 \uc544\ub798\uc640 \uac19\uc774 \uad6c\ud574\uc9c4\ub2e4. var(sample_1) ## [1] 85.82222 var(sample_2) ## [1] 87.73333 F <- var(sample_1)/var(sample_2) F ## [1] 0.9782168 upper <- qf(0.975,9,9) lower <- qf(0.025,9,9) upper ## [1] 4.025994 lower ## [1] 0.2483859 \ub450 \uc9d1\ub2e8\uc758 \uc790\uc720\ub3c4\ub294 \ubaa8\ub450 9\ub85c \uac19\uc73c\uba70, \uae30\uac01\uc5ed\uc744 \ud655\uc778\ud558\uba74 upper\uacfc lower\uac00 \uac01\uac01 4.025\uc640 0.248\ub85c \ub098\uc628\ub2e4. \uc5ec\uae30\uc11c\uc758 \uac80\uc815\ud1b5\uacc4\ub7c9 F\ub294 \uc57d 0.978\ub85c\uc368, p-value\ub97c \uad6c\ud558\uae30 \uc704\ud574\uc11c\ub294 pf()\ud568\uc218\ub97c \ud1b5\ud574 lower.tail\uc744 True\ub85c \uc124\uc815\ud558\uc5ec \uacc4\uc0b0\ud558\uba74 \uacc4\uc0b0\uae30\ub97c \ud1b5\ud574 \uc5bb\ub294 \uacb0\uacfc\uac12\uacfc \uac19\ub2e4. \uc5ec\uae30\uc11c lower.tail\uc744 T\ub85c \uc124\uc815\ud558\ub294 \uc774\uc720\ub294 F\ub85c \uc124\uc815\ud588\uc744 \uc2dc p-value \uac12\uc774 1\uc744 \ucd08\uacfc\ud558\uae30 \ub54c\ubb38\uc5d0, \uc67c\ucabd\uaf2c\ub9ac\uba74\uc801\uc744 \uae30\uc900\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. pvalue <- 2*pf(F,9,9,lower.tail=T) pvalue ## [1] 0.9743665 \ucd94\uac00\ub85c \uc5d1\uc140\uc744 \uc774\uc6a9\ud558\uc5ec pvalue\ub97c \uc9c1\uc811 \uacc4\uc0b0\ud574\ubcf4\uc790. p-value\ub294 0.9743\uc73c\ub85c Bartlett \uac80\uc815\uc744 \ud1b5\ud574 \uc5bb\uc740 R\uacb0\uacfc\uac12\uacfc \uc9c1\uc811 \uacc4\uc0b0\uae30\ub97c \ud1b5\ud574 \uad6c\ud55c \uacb0\uacfc\uac12\uacfc \uac19\ub2e4\ub294 \uac83\uc774 \ud655\uc778\ub418\uc5c8\ub2e4. (b) (a)\uc758 \uacb0\uacfc\ub97c \ud1b5\ud574 \uc6b0\ub9ac\ub294 \ub450 \uc9d1\ub2e8\uc758 \ubd84\uc0b0\uc774 \ub4f1\ubd84\uc0b0\uc784\uc744 \uae30\uac01\ud560 \uc218 \uc5c6\ub2e4\ub294 \uacb0\ub860\uc744 \ub0b4\ub838\ub2e4. \uace0\ub85c \ub450 \uc9d1\ub2e8\uc774 \ub4f1\ubd84\uc0b0\uc774\ub77c\ub294 \uc5f0\uc7a5\uc120 \ud558\uc5d0 \ub450 \uc9d1\ub2e8\uc758 \ud3c9\uade0\uc774 \uac19\uc740\uc9c0 \uc544\ub2cc\uc9c0\ub97c \ube44\uad50\ud558\uace0\uc790 \ud55c\ub2e4. \ub9cc\uc57d \uc6b0\ub9ac\uac00 \ubaa8\ubd84\uc0b0 \uc744 \uc548\ub2e4\uba74 \uc6b0\ub9ac\ub294 Z\ud1b5\uacc4\ub7c9\uc744 \ud1b5\ud574 \uc815\uaddc\ubd84\ud3ec\ub97c \uc774\uc6a9\ud55c \uac80\uc815\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc \uc6b0\ub9ac\ub294 \ubaa8\ubd84\uc0b0\uc744 \uc2e4\uc81c\ub85c \ubaa8\ub97c\ubfd0\ub354\ub7ec, \uac00\uc9c0\uace0 \uc788\ub294 \ub2e8\uc11c\ub77c\uace0\ub294 \ub450 \ubaa8\uc9d1\ub2e8\uc758 \ubd84\uc0b0\uc774 \uac19\ub2e4\ub294 \uac83\ub9cc \uc54c\uc544\ub0b4\uc5c8\ub2e4. \uace0\ub85c \uc6b0\ub9ac\ub294 pooled variance\ub97c \uc774\uc6a9\ud558\uc5ec t\ud1b5\uacc4\ub7c9\uc744 \ud1b5\ud55c t\uac80\uc815\uc744 \uc218\ud589\ud558\uc5ec\uc57c\ud55c\ub2e4. \uba3c\uc800 \uadc0\ubb34\uac00\uc124\uacfc \ub300\ub9bd\uac00\uc124\uc740 \uc544\ub798\uc640 \uac19\uace0, \\(H_0: {\\mu_1} - {\\mu_2} =0\\) \\(H_1: {\\mu_1} -{\\mu_2} \\ne 0\\) \uc0ac\uc6a9 \ub420 T \ud1b5\uacc4\ub7c9\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \\[T= \\frac{\\bar{X_1}-\\bar{X_2}}{\\sqrt{s_p^2}(\\frac{1}{n_1}+\\frac{1}{n_2})},\\ *s_p^2= \\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}\\] \ub450 \uc9d1\ub2e8\uc758 \uc790\uc720\ub3c4\ub294 9\ub85c \ub3d9\uc77c\ud558\uba70 pooled variance\ub97c \uacc4\uc0b0\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. sp <- (9*var(sample_1)+9*var(sample_2))/10+10-2 sp ## [1] 164.2 t <- (mean(sample_1)-mean(sample_2))/sqrt(sp*(1/20)) t ## [1] 0.06980048 p <- 2*pt(t,18,lower.tail=F) p ## [1] 0.9451221 \uc5d1\uc140\uc758 \uacc4\uc0b0\uae30\ub85c \uc9c1\uc811\uacc4\uc0b0\ud574\ubcf8 \uacb0\uacfc R\uc5d0\uc11c \uc5bb\uc740 p-value\uac12\uacfc \ub3d9\uc77c\ud558\uac8c \ub098\uc634\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. p-value\uac12\uc740 \uc57d 0.945 \ub85c \ub9e4\uc6b0 \ud06c\uac8c \ub098\uc624\uba70, \uad00\ucc30\ub41c \ud1b5\uacc4\ub7c9\ubcf4\ub2e4 \ub354 \uadf9\ub2e8\uc801\uc778 \uac12\uc774 \ub098\uc62c \ud655\ub960\uc774 0.945\ub77c\ub294 \ub73b\uc73c\ub85c \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud558\uae30\uc5d0 \uc544\uc8fc \ud798\ub4e0 \uac12\uc784\uc744 \uc758\ubbf8\ud55c\ub2e4. \uace0\ub85c \uc6b0\ub9ac\ub294 \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud560 \ud1b5\uacc4\uc801 \uc720\uc758\uc131\uc744 \uc5bb\uc744 \uc218 \uc5c6\uc73c\uba70, \uc774\ub294 \ub450 \uc9d1\ub2e8\uc758 \uc5f0\uc18c\uc2dc\uac04 \uac04\uc758 \ud3c9\uade0\ucc28\uc774\uac00 \uc874\uc7ac\ud558\uc9c0 \uc54a\ub294\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4 . (c) \uc774 \ubb38\uc81c\uc5d0\uc11c \uc9d1\ub2e8\uc758 \uc794\ucc28\uc5d0 \ub300\ud574 \uc815\uaddc\uc131 \uac00\uc815\uc744 \ub17c\ud558\ub77c \uc6b0\ub9ac\ub294 \uc794\ucc28\uc5d0 \ub300\ud574\uc11c qqnorm \uadf8\ub798\ud504\ub97c \uadf8\ub824\ubcf4\uc544 \uc794\ucc28\uac00 \uc815\uaddc\ubd84\ud3ec\ub97c \ub744\ub294\uc9c0\ub97c \ud655\uc778\ud574 \ubcfc \uc218 \uc788\ub2e4. \uc794\ucc28\ub77c\ub294 \uac83\uc740 \uc911\uc694\ud55c \uc815\ubcf4\ub97c \uc81c\uc678\ud55c \ub098\uba38\uc9c0 \ucc0c\uaebc\uae30\ub77c\uace0 \ud574\uc11d\ud560 \uc218 \uc788\ub294\ub370, \uc774 \uc794\ucc28\ub294 \uc5b4\ub5a0\ud55c \uc758\ubbf8\ub3c4 \uac00\uc838\uc11c\ub294 \uc548\ub418\uba70 \uc815\uaddc\ubd84\ud3ec\ub97c \ub744\uc5b4\uc57c\ud55c\ub2e4. \uc794\ucc28\ub780 \uad00\uce21\uce58\uc640 fitted value \uac12\uc758 \ucc28\uc774\ub85c \ubcfc \uc218 \uc788\uc73c\uba70, \uc774 \uc9d1\ub2e8 \ub370\uc774\ud130\uc5d0\uc11c fitted value\ub294 \ud45c\ubcf8\ud3c9\uade0\uc774 \ub41c\ub2e4. \uace0\ub85c \uc794\ucc28\uc640 \uc774\uc5d0 \ub300\ud55c normal qqplot\uc744 \uadf8\ub824\ubcf8 \uacb0\uacfc\ub294 \uc544\ub798\uc640 \uac19\ub2e4. residual1 <- sample_1-mean(sample_1) residual2 <- sample_2-mean(sample_2) par(mfrow=c(1,2)) qqnorm(residual1) qqnorm(residual2) \uc721\uc548\uc73c\ub85c \ud655\uc778\ud574\ubcf4\uc558\uc744 \ub54c, \uadf8\ub798\ud504\uac00 45\ub3c4 \uac01\ub3c4\ub85c \uc798 \ub298\uc5ec\uc788\ub294 \uac78 \ubcf4\uc544 \uc794\ucc28\uac00 \uc815\uaddc\uc131\uc744 \ub744\ub294 \uac83\uc744 \ubcf4\uc778\ub2e4. \uc774\ub97c fitdistr\ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uba74 \uc544\ub798\uc640 \uac19\uc740 \ubaa8\uc218\ub97c \uac16\ub294 \uc815\uaddc\ubd84\ud3ec\uc5d0 \ud761\uc0ac\ud558\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. library(MASS) fitdistr(residual1,'normal') ## mean sd ## -5.684117e-15 8.788629e+00 ## ( 2.779209e+00) ( 1.965197e+00) fitdistr(residual2,'normal') ## mean sd ## -2.842171e-15 8.885944e+00 ## ( 2.809982e+00) ( 1.986957e+00) 2.46 2.25\uc5d0\uc11c \uc18c\uac1c\ud55c \ubb38\uc81c\uc5d0\uc11c \ud3c9\uade0 \uc18c\uac01 \uc2dc\uac04\uc758 \ucc28\uc774\uac00 2\ubd84 \uc77c\ub54c\uc758 \uac80\uc815\ub825\uc744 \ucc3e\uc544\ub77c. \ud3c9\uade0 \uc2dc\uac04\uc758 \ucc28\uc774\uac00 1\ubd84\uc774\uace0 \uac80\uc815\ub825\uc774 \ucd5c\uc18c 0.9\uac00 \ub418\uae30 \uc704\ud55c \ud45c\ubcf8\ud06c\uae30\ub97c \uad6c\ud558\ub77c. \ub4f1\ubd84\uc0b0\uacfc equal sample size, \\(\\sigma=9\\) \ub97c \uac00\uc815\ud558\uc5ec \ubb38\uc81c\ub97c \ud480\uc5b4\ubcf4\uc790. Case1) \ub450 \uc9d1\ub2e8\uac04 \ud3c9\uade0\uc774 2\uc77c \ub54c \uac80\uc815\ub825 \ucc3e\uae30 \uc704\uc758 \ucf00\uc774\uc2a4\ub294 \ub4f1\ubd84\uc0b0\uacfc \ub3d9\uc77c \uc0d8\ud50c\uc0ac\uc774\uc988\ub97c \uac16\uace0 \ubaa8\ubd84\uc0b0\uc744 \uc54c\uae30 \ub54c\ubb38\uc5d0 Z\ud1b5\uacc4\ub7c9\uc744 \ud1b5\ud574 \uc811\uadfc\ud574\ubcfc \uc218 \uc788\ub2e4. \uc774 \ub54c\uc758 Z\ud1b5\uacc4\ub7c9\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \\[Z=\\frac{\\bar{X_1}-{\\bar{X_2}}-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{2\\sigma^2}{n}}}\\] \uac80\uc815\ub825\uc740 \uadc0\ubb34\uac00\uc124\uc774 \uac70\uc9d3\uc77c \ub54c \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud560 \ud655\ub960\uc774\uba70 \uc774\ub294 $P(\\bar{X_1}-\\bar{X_2}>c|under\\ H_1)\\ or\\ P(\\bar{X_1}-\\bar{X_2}<-c|under\\ H_1) $\uc640 \uac19\ub2e4. \uc774\ub97c \uc704\ud574\uc11c \uba3c\uc800 c\ub97c \uad6c\ud574\uc57c \ud558\ubbc0\ub85c, \uc6b0\ub9ac\ub294 \uc720\uc758\uc218\uc900 0.05\ud558\uc5d0\uc11c c\ub97c \uad6c\ud560 \uc218 \uc788\ub2e4. \uba3c\uc800 \uc720\uc758\uc218\uc900 \\(\\alpha\\) \uc758 \uc808\ubc18\uac12\uc778 0.025\ub294(\uc591\uce21\uac80\uc815\uc774\ubbc0\ub85c) \uadc0\ubb34\uac00\uc124 \ud558\uc5d0\uc11c \uacc4\uc0b0\ub418\ub294 \uac83\uc774\ubbc0\ub85c \uc544\ub798\uc640 \uac19\uc740 \uc2dd\uc744 \ud1b5\ud574\uc11c c\ub97c \uad6c\ud560 \uc218 \uc788\ub2e4. \\[P(\\frac{\\bar{X_1}-{\\bar{X_2}}-0}{\\sqrt{2\\sigma^2/n}}>\\frac{c-0}{\\sqrt{2\\sigma^2/n}})=0.025\\] \uc6b0\ub9ac\ub294 Z\uac80\uc815\ud558\uc5d0\uc11c \uc591\uce21\uac80\uc815\ud558\uc5d0 \\(P(Z>1.96)=0.025\\) \ub77c\ub294 \uc0ac\uc2e4\uc744 \uc54c\uace0 \uc788\ub2e4. \uace0\ub85c \uc774\ub97c \ud1b5\ud574\uc11c \uc5ed\uc73c\ub85c c\uac12\uc744 \uad6c\ud574\ub0bc \uc218 \uc788\ub2e4. \\(\\frac{c}{\\sqrt{(2*81)/10}}= 1.96\\) \uc774\ubbc0\ub85c, \uc5ed\uc0b0\ud558\uba74 \\(c = 1.96*\\sqrt{16.2}\\) \uc774\ubbc0\ub85c c<- sqrt(16.2)*1.96 c ## [1] 7.888848 critical value\ub294 \uc57d 7.88\uc774 \ub098\uc628\ub2e4. \uc774\ub97c \ud1b5\ud574\uc11c \uac80\uc815\ub825\uc744 \uad6c\ud574\ubcf4\uba74, \\(P(\\frac{\\bar{X_1}-\\bar{X_2}-2}{\\sqrt{16.2}}>\\frac{c-2}{\\sqrt{16.2}})+P(\\frac{\\bar{X_1}-\\bar{X_2}-2}{\\sqrt{16.2}}<\\frac{-c-2}{\\sqrt{16.2}})\\) \ub97c \uad6c\ud558\uba74 \ub41c\ub2e4. \uc774\ub294 \\(P(Z>\\frac{c-2}{\\sqrt{16.2}})+P(Z<\\frac{-c-2}{\\sqrt{16.2}})\\) \uc640 \uac19\uc73c\uba70 \uc774 \ud655\ub960\uac12\uc774 \uac80\uc815\ub825\uc774 \ub418\ub294 \uac83\uc774\ub2e4. \uc6b0\ub9ac\ub294 \uc591\uce21\uac80\uc815\uc73c\ub85c \uc2dc\ud589\ud558\uc600\uae30 \ub54c\ubb38\uc5d0 \ub300\ub9bd\uac00\uc124 \ud558\uc5d0\uc11c \ub300\uce6d\ub418\ub294 \uae30\uac01\uc5ed\uc758 \uc808\ub300\uac12\uc744 \ub118\uc5b4\uc11c\ub294 \uba74\uc801\ub4e4\uc774 \uac80\uc815\ub825\uc774 \ub41c\ub2e4. \uacc4\uc0b0\ud55c \uacb0\uacfc\ub294 \uc544\ub798\uc640 \uac19\ub2e4. powervalue <- (c-2)/sqrt(16.2) powervalue ## [1] 1.463096 powervalue2 <- (-c-2)/sqrt(16.2) powervalue2 ## [1] -2.456904 power1 <- pnorm(powervalue,lower.tail=F) power2 <- pnorm(powervalue2,lower.tail=T) power1 ## [1] 0.07172056 power2 ## [1] 0.007007007 beta <- power1+power2 beta ## [1] 0.07872756 \uc5d1\uc140\ub85c \uacc4\uc0b0\ud55c \uacb0\uacfc, r\uc5d0\uc11c \uad6c\ud55c power\uac12\uacfc \uac19\uac8c \ub098\uc634\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\ub2e4. \uccad\ud06c\ucc3d\uc758 beta\uac00 \uc5ec\uae30\uc11c\uc758 \uac80\uc815\ub825\uc774\uba70, \ubcf4\ud1b5 \uc591\uce21\uac80\uc815\uc5d0\uc11c\ub294 \ud55c\ucabd \uaf2c\ub9ac\uc758 \uba74\uc801\uc774 \ub9e4\uc6b0 \uc791\uc544\uc11c \uadf8 \ud6a8\uacfc\ub97c \ubb34\uc2dc\ud574\ub3c4 \uad1c\ucc2e\uc73c\uba70 \ud55c\ucabd\uc758 \uba74\uc801\ub9cc \uacc4\uc0b0\ud558\uc5ec \uac80\uc815\ub825\uc744 \uc815\ud558\uace0\ub294 \ud558\ub294\ub370, \uc704\uc758 \ucf00\uc774\uc2a4\ub294 sample size\uac00 \ub9e4\uc6b0 \uc791\uc73c\uba70 \uc774\ub294 \ud544\uc790\uc758 \uc0dd\uac01\uc774\uc9c0\ub9cc, \uac80\uc815\uc5d0 \uc0ac\uc6a9\ub418\ub294 effect size( \\(\\mu_1-\\mu_2\\) )\uac00 \ub9e4\uc6b0 \uc791\uae30 \ub54c\ubb38\uc5d0 \uac80\uc815\ub825\uc774 \uc544\uc8fc\ud615\ud3b8\uc5c6\uac8c \ub098\uc628\ub2e4. \uace0\ub85c \ud544\uc790\ub294 \ud55c\ucabd \uc791\uc740 \uba74\uc801\uc758 \ub113\uc774\ub97c \ubb34\uc2dc\ud558\uc9c0\uc54a\uace0(\uc5b4\ucc28\ud53c \uc591\uce21 \ubaa8\ub450 \ub9e4\uc6b0 \uc791\uc740 \uba74\uc801\uc774\uae30\uc5d0) \ub450 \uba74\uc801\uc744 \ud569\uc0b0\ud558\uc5ec \uacc4\uc0b0\ud558\uc600\ub2e4. \uadf8 \uacb0\uacfc\ub294 \uc57d 0.078 \uc774 \ub098\uc654\ub2e4. \ucc98\uc74c\uc5d0\ub294 \uac80\uc815\ub825\uc774 \ub108\ubb34 \ub0ae\uc740\uac83\uc744 \ubcf4\uace0 \ubbff\uc9c0 \ubabb\ud574 3\ubc88\uc774\ub098 \uacc4\uc0b0\uc744 \ub2e4\uc2dc\ud574\ubcf4\uc558\uc9c0\ub9cc, sample size\uc640 effect size\ub97c \uace0\ub824\ud574\ubcf4\uba74 \ucda9\ubd84\ud788 \ub098\uc62c \uc218 \uc788\ub2e4\ub294 \uacb0\ub860\uc744 \ub0b4\ub838\ub2e4. (\uc624\ub2f5\uc774\ub77c\uba74 \uc804\uc801\uc73c\ub85c \uc791\uc131\uc790\uc758 \ubb34\uc9c0\ub54c\ubb38) Case2) \uc9d1\ub2e8\uac04 \uc5f0\uc18c\uc2dc\uac04\uc758 \ucc28\uac00 1\ubd84\uc774\uace0 \uac80\uc815\ub825\uc774 \ucd5c\uc18c 0.9\uac00 \ub418\uae30 \uc704\ud55c sample size \ucc3e\uae30 \uc591\uce21\uac80\uc815\uc5d0\uc11c \uac80\uc815\ub825\uc740 \uc704\uc5d0\uc11c \uc5b8\uae09\ud588\ub358 \uc2dd\uc744 \uc778\uc6a9\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \uc801\uc744 \uc218 \uc788\ub2e4. $ \\(P(\\frac{\\bar{X_1}-\\bar{X_2}-1}{\\sqrt{2*81/n}}>\\frac{c-1}{\\sqrt{2*81/n}})+P(\\frac{\\bar{X_1}-\\bar{X_2}-1}{\\sqrt{2*81/n}}<\\frac{-c-1}{\\sqrt{2*81/n}}) \\geq 0.9\\) $ \uc774\ub97c \uc694\uc57d\ud558\uba74 \\(P(Z>(\\frac{c-1}{\\sqrt{2*81/n}})+P(Z<\\frac{-c-1}{\\sqrt{2*81/n}}))\\) \ub85c \uc801\uc744 \uc218 \uc788\ub2e4. \uc774\ubc88 \ucf00\uc774\uc2a4\uc5d0\uc11c\ub294 \\(P(\\bar{X_1}-\\bar{X_2}<-c|\\ under\\ H_1)\\) \uc758 \uac12\uc774 \uba74\uc801\uac12\uc774 \ub9e4\uc6b0 \uc791\uc544 \uad6c\ud574\ub3c4 \ud070 \uc758\ubbf8\uac00 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \uc774 \ud55c\ucabd \uaf2c\ub9ac\uba74\uc801 \uac12\uc758 \uc601\ud5a5\uc774 \ubbf8\ubbf8\ud560 \uac83\uc774\ub77c\uace0 \uac00\uc815\ud558\uace0 \ubc18\ub300 \ucabd \uaf2c\ub9ac\uc758 \uba74\uc801\ub9cc\uc73c\ub85c \uac80\uc815\ub825\uc744 \uacc4\uc0b0\ud558\uaca0\ub2e4. \uace0\ub85c \\(P(Z> \\frac{c-1}{\\sqrt{162/n}}) \\geq 0.9\\) \ub97c \ub9cc\uc871\ud558\ub294 \ud45c\ubcf8\ud06c\uae30\ub97c \ucc3e\uc73c\uba74 \ub41c\ub2e4. qnorm(0.9,lower.tail=F) ## [1] -1.281552 \uc704\uc5d0\uc11c \uc5bb\uc740 \uac12\uc740 1.28\uc774\uc9c0\ub9cc \uc815\uaddc\ubd84\ud3ec\ub294 \ub300\uce6d\uc744 \uc774\ub8e8\ubbc0\ub85c, \ubc18\ub300\ucabd \uac12\uc5d0\uc11c -1.281552\ub97c \uac16\uac8c \ub418\ubbc0\ub85c, r\uc5d0\uc11c \uc5bb\uc740 qnorm\uac12\uacfc \uac19\uac8c \ub098\uc628\ub2e4. \uc704\uc758 \uac12\uc744 \ud1b5\ud574 \uc5bb\uc740 qnorm\uac12 \uc57d -1.2815\uac00 \uac80\uc815\ub825\uc744 0.9\ub85c \ub9cc\ub4e4\uc5b4\uc8fc\ub294 value\uac00 \ub41c\ub2e4. \uc774\ub294 \\(\\frac{c}{\\sqrt{162/n}}=1.96\\) \uacfc \\(\\frac{c-1}{\\sqrt{162/n}}=-1.281552\\) \ub450 \uc2dd\uc744 \uc5f0\ub9bd\ubc29\uc815\uc2dd\uc744 \ud480\uc5b4\uc11c n\uac12\uc744 \uad6c\ud558\uba74 \ub41c\ub2e4. \uc704 \uc2dd\uc744 c\ub85c \uc815\ub9ac\ud558\uc5ec n\uc744 \uad6c\ud558\uba74 n\uc740 \\(n=2\\times(1.96+1.281552)^2\\) \uac00 \ub418\uba70 \uc774\ub294 \uc57d 21.01532\uac00 \ub098\uc624\ubbc0\ub85c, \uac80\uc815\ub825\uc774 \ucd5c\uc18c 0.9\uac00 \ub418\uae30 \uc704\ud55c \ud45c\ubcf8\ud06c\uae30\ub294 22\uac00 \ub41c\ub2e4.","title":"Design of Experiments HW1"},{"location":"05%20Design%20of%20Experiments/DOE_HW1/#design-of-experiments-hw1","text":"","title":"Design of Experiments HW1"},{"location":"05%20Design%20of%20Experiments/DOE_HW1/#225","text":"","title":"2.25"},{"location":"05%20Design%20of%20Experiments/DOE_HW1/#a-alpha005","text":"","title":"(a) \ub4f1\ubd84\uc0b0 \uac00\uc124\uac80\uc815\uc744 \ud558\ub77c. \\(\\alpha=0.05\\)\ub85c \uc218\ud589\ud558\ub77c."},{"location":"05%20Design%20of%20Experiments/DOE_HW1/#b-a-alpha005-p-value","text":"","title":"(b) (a)\uc758 \uacb0\uacfc\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc9d1\ub2e8 \uac04 \ud3c9\uade0\uc5f0\uc18c\uc2dc\uac04\uc774 \uac19\ub2e4\ub294 \uac00\uc124\uac80\uc815\uc744 \uc218\ud589\ud558\ub77c. \\(\\alpha=0.05\\)\ub97c \uc0ac\uc639\ud558\uace0, p-value \ub610\ud55c \uad6c\ud558\uc5ec\ub77c."},{"location":"05%20Design%20of%20Experiments/DOE_HW1/#c-r-normal-qqplot","text":"","title":"(c) R\uc744 \uc774\uc6a9\ud558\uc5ec \uc794\ucc28\uc5d0 \ub300\ud55c normal qqplot\uc744 \uc218\ud589\ud558\ub77c."},{"location":"05%20Design%20of%20Experiments/DOE_HW1/#246","text":"(a)\uc758 \ubb38\uc81c\ub97c \ud480\uae30 \uc704\ud574\uc11c\ub294 \uba3c\uc800 \ubaa8\uc9d1\ub2e8\uc5d0\uc11c iid\ud558\uac8c \ucd94\ucd9c\ub41c \uc0d8\ud50c\uc5d0 \ub300\ud574 \uc815\uaddc\uc131 \uac00\uc815\uc774 \ub9cc\uc871\ud558\ub294\uc9c0\uc5d0 \ub300\ud574 \ub17c\uc758\ud558\uc5ec\uc57c\ud55c\ub2e4. \ud558\uc9c0\ub9cc (c)\uc5d0\uc11c \uadf8 \uc815\uaddc\uc131 \ub17c\uc758\ub97c \ub2e4\ub8f0 \uc608\uc815\uc774\ub2c8, 1.\ubaa8\uc9d1\ub2e8\uc774 \uc815\uaddc\ubd84\ud3ec\uc778 \uacbd\uc6b0\uc640, 2. \ubaa8\uc9d1\ub2e8\uc774 \uc815\uaddc\ubd84\ud3ec\uac00 \uc544\ub2cc \ub2e4\ub978\ubd84\ud3ec\uc778 \uacbd\uc6b0\ub97c \uace0\ub824\ud574\uc57c\ud55c\ub2e4. \uba3c\uc800 \ubaa8\uc9d1\ub2e8\uc774 \uc815\uaddc\ubd84\ud3ec\uac00 \uc544\ub2c8\ub77c\ub294 \uac00\uc815\ud558\uc5d0\uc11c \ud560 \uc218 \uc788\ub294 Levene test\ub97c \uc2dc\ud589\ud574\ubcf4\uaca0\ub2e4. library(lawstat) sample_1 <- c(65,81,57,66,82,82,67,59,75,70) sample_2 <- c(64,71,83,59,65,56,69,74,82,79) sample <- c(sample_1,sample_2) group <- c(rep(1,10),rep(2,10)) dat <- data.frame(group,sample) head(dat) ## group sample ## 1 1 65 ## 2 1 81 ## 3 1 57 ## 4 1 66 ## 5 1 82 ## 6 1 82 levene.test(sample,group,location='mean') ## ## Classical Levene's test based on the absolute deviations from the ## mean ( none not applied because the location is not set to median ## ) ## ## data: sample ## Test Statistic = 0.0014598, p-value = 0.9699 \ubaa8\uc9d1\ub2e8\uc774 \uc815\uaddc\ubd84\ud3ec\uac00 \uc544\ub2c8\ub77c\uace0 \uac00\uc815\uc744 \ud558\uc600\uc744 \uc2dc, \uc774\ub584\uc758 p-value \uac12\uc740 0.9699\uac00 \ub098\uc624\uba70, \uc774\ub294 \uc6d0\ub798 \uadc0\ubb34\uac00\uc124\uc778 '\uadf8\ub8f9\uac04\uc758 \ubd84\uc0b0\uc774 \ub4f1\ubd84\uc0b0\uc774\ub2e4'\ub97c \uae30\uac01\ud560 \uc218 \uc5c6\uc74c\uc744 \uc758\ubbf8\ud55c\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub294 \ubaa8\uc9d1\ub2e8\uc774 \uc815\uaddc\ubd84\ud3ec\ub77c\ub294 \uac00\uc815\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc558\uae30 \ub54c\ubb38\uc5d0, F\ud1b5\uacc4\ub7c9\uc744 \uc774\uc6a9\ud55c \uac80\uc815\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc5c6\ub2e4. \ub2e4\uc74c\uc73c\ub85c \ubaa8\uc9d1\ub2e8\uc5d0\uc11c \ucd94\ucd9c\ud55c \uc0d8\ud50c\uc774 \uc815\uaddc\ubd84\ud3ec\uc5d0\uc11c \ucd94\ucd9c\ub418\uc5c8\ub2e4\uba74 , R\uc5d0 \ub0b4\uc7a5\ub418\uc5b4\uc788\ub294 \ud568\uc218\ub97c \ud1b5\ud574 Bartlett's test\ub97c \uc2dc\ud589\ud574\ubcfc \uc218 \uc788\ub2e4. library(stats) bartlett.test(sample~group,dat) ## ## Bartlett test of homogeneity of variances ## ## data: sample by group ## Bartlett's K-squared = 0.0010339, df = 1, p-value = 0.9743 \ub4f1\ubd84\uc0b0\uc131 \uac80\uc815\uc5d0\uc11c\uc758 \uadc0\ubb34\uac00\uc124\uacfc \ub300\ub9bd\uac00\uc124\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \\(H_0:\\sigma^2_1=\\sigma^2_2\\) \\(H_1:{\\sigma^2_1}\\ne{\\sigma^2_2}\\) \uc704\uc758 \uccad\ud06c\ucc3d\uc744 \ud655\uc778\ud55c \uacb0\uacfc p-value\uac00 0.9743\uc73c\ub85c \ub9e4\uc6b0 \ub192\uc740 \uac12\uc73c\ub85c \ub098\uc640 \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud560 \uc218 \uc5c6\ub2e4\ub294 \uac83\uc744 \ubcf4\uc5ec\uc8fc\ubbc0\ub85c, \uc704\uc758 \ub450 \uadf8\ub8f9\uc2e4\ud5d8\uc740 \uc774\ubd84\uc0b0\uc131\uc744 \uac16\uc9c0 \uc54a\uc74c(\uc989 \uadc0\ubb34\uac00\uc124\uc778 \ub4f1\ubd84\uc0b0\uc131\uc774\ub2e4\ub97c \uae30\uac01\ud560 \uc218 \uc5c6\uc74c)\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc774\ubc88\uc5d0\ub294 \uc9c1\uc811 \uacc4\uc0b0\uae30\ub97c \ud1b5\ud574 \uc5bb\uc740 \uac12\uacfc R\uc5d0\uc11c \uc81c\uc2dc\ud55c p-value\uac12\uc774 \uac19\uc740\uc9c0 \ud655\uc778\ud574\ubcf4\uaca0\ub2e4. \ub4f1\ubd84\uc0b0\uc131 \uac80\uc815\uc5d0\uc11c \uadc0\ubb34\uac00\uc124\uc774 \ucc38\uc774\ub77c\ub294 \uc804\uc81c\ud558\uc5d0 \uc0ac\uc6a9\ub418\ub294 \uac80\uc815\ud1b5\uacc4\ub7c9 F\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\[F=\\frac{s_1^2}{s_2^2}\\] #sample\uc758 \ubd84\uc0b0\uc774 \ud45c\ubcf8\ubd84\uc0b0\uc774\ubbc0\ub85c, \ub450 \uc9d1\ub2e8\uc758 \ud45c\ubcf8\ubd84\uc0b0\uc740 \uc544\ub798\uc640 \uac19\uc774 \uad6c\ud574\uc9c4\ub2e4. var(sample_1) ## [1] 85.82222 var(sample_2) ## [1] 87.73333 F <- var(sample_1)/var(sample_2) F ## [1] 0.9782168 upper <- qf(0.975,9,9) lower <- qf(0.025,9,9) upper ## [1] 4.025994 lower ## [1] 0.2483859 \ub450 \uc9d1\ub2e8\uc758 \uc790\uc720\ub3c4\ub294 \ubaa8\ub450 9\ub85c \uac19\uc73c\uba70, \uae30\uac01\uc5ed\uc744 \ud655\uc778\ud558\uba74 upper\uacfc lower\uac00 \uac01\uac01 4.025\uc640 0.248\ub85c \ub098\uc628\ub2e4. \uc5ec\uae30\uc11c\uc758 \uac80\uc815\ud1b5\uacc4\ub7c9 F\ub294 \uc57d 0.978\ub85c\uc368, p-value\ub97c \uad6c\ud558\uae30 \uc704\ud574\uc11c\ub294 pf()\ud568\uc218\ub97c \ud1b5\ud574 lower.tail\uc744 True\ub85c \uc124\uc815\ud558\uc5ec \uacc4\uc0b0\ud558\uba74 \uacc4\uc0b0\uae30\ub97c \ud1b5\ud574 \uc5bb\ub294 \uacb0\uacfc\uac12\uacfc \uac19\ub2e4. \uc5ec\uae30\uc11c lower.tail\uc744 T\ub85c \uc124\uc815\ud558\ub294 \uc774\uc720\ub294 F\ub85c \uc124\uc815\ud588\uc744 \uc2dc p-value \uac12\uc774 1\uc744 \ucd08\uacfc\ud558\uae30 \ub54c\ubb38\uc5d0, \uc67c\ucabd\uaf2c\ub9ac\uba74\uc801\uc744 \uae30\uc900\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. pvalue <- 2*pf(F,9,9,lower.tail=T) pvalue ## [1] 0.9743665 \ucd94\uac00\ub85c \uc5d1\uc140\uc744 \uc774\uc6a9\ud558\uc5ec pvalue\ub97c \uc9c1\uc811 \uacc4\uc0b0\ud574\ubcf4\uc790. p-value\ub294 0.9743\uc73c\ub85c Bartlett \uac80\uc815\uc744 \ud1b5\ud574 \uc5bb\uc740 R\uacb0\uacfc\uac12\uacfc \uc9c1\uc811 \uacc4\uc0b0\uae30\ub97c \ud1b5\ud574 \uad6c\ud55c \uacb0\uacfc\uac12\uacfc \uac19\ub2e4\ub294 \uac83\uc774 \ud655\uc778\ub418\uc5c8\ub2e4.","title":"2.46"},{"location":"05%20Design%20of%20Experiments/DOE_HW1/#b","text":"(a)\uc758 \uacb0\uacfc\ub97c \ud1b5\ud574 \uc6b0\ub9ac\ub294 \ub450 \uc9d1\ub2e8\uc758 \ubd84\uc0b0\uc774 \ub4f1\ubd84\uc0b0\uc784\uc744 \uae30\uac01\ud560 \uc218 \uc5c6\ub2e4\ub294 \uacb0\ub860\uc744 \ub0b4\ub838\ub2e4. \uace0\ub85c \ub450 \uc9d1\ub2e8\uc774 \ub4f1\ubd84\uc0b0\uc774\ub77c\ub294 \uc5f0\uc7a5\uc120 \ud558\uc5d0 \ub450 \uc9d1\ub2e8\uc758 \ud3c9\uade0\uc774 \uac19\uc740\uc9c0 \uc544\ub2cc\uc9c0\ub97c \ube44\uad50\ud558\uace0\uc790 \ud55c\ub2e4. \ub9cc\uc57d \uc6b0\ub9ac\uac00 \ubaa8\ubd84\uc0b0 \uc744 \uc548\ub2e4\uba74 \uc6b0\ub9ac\ub294 Z\ud1b5\uacc4\ub7c9\uc744 \ud1b5\ud574 \uc815\uaddc\ubd84\ud3ec\ub97c \uc774\uc6a9\ud55c \uac80\uc815\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc \uc6b0\ub9ac\ub294 \ubaa8\ubd84\uc0b0\uc744 \uc2e4\uc81c\ub85c \ubaa8\ub97c\ubfd0\ub354\ub7ec, \uac00\uc9c0\uace0 \uc788\ub294 \ub2e8\uc11c\ub77c\uace0\ub294 \ub450 \ubaa8\uc9d1\ub2e8\uc758 \ubd84\uc0b0\uc774 \uac19\ub2e4\ub294 \uac83\ub9cc \uc54c\uc544\ub0b4\uc5c8\ub2e4. \uace0\ub85c \uc6b0\ub9ac\ub294 pooled variance\ub97c \uc774\uc6a9\ud558\uc5ec t\ud1b5\uacc4\ub7c9\uc744 \ud1b5\ud55c t\uac80\uc815\uc744 \uc218\ud589\ud558\uc5ec\uc57c\ud55c\ub2e4. \uba3c\uc800 \uadc0\ubb34\uac00\uc124\uacfc \ub300\ub9bd\uac00\uc124\uc740 \uc544\ub798\uc640 \uac19\uace0, \\(H_0: {\\mu_1} - {\\mu_2} =0\\) \\(H_1: {\\mu_1} -{\\mu_2} \\ne 0\\) \uc0ac\uc6a9 \ub420 T \ud1b5\uacc4\ub7c9\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \\[T= \\frac{\\bar{X_1}-\\bar{X_2}}{\\sqrt{s_p^2}(\\frac{1}{n_1}+\\frac{1}{n_2})},\\ *s_p^2= \\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}\\] \ub450 \uc9d1\ub2e8\uc758 \uc790\uc720\ub3c4\ub294 9\ub85c \ub3d9\uc77c\ud558\uba70 pooled variance\ub97c \uacc4\uc0b0\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. sp <- (9*var(sample_1)+9*var(sample_2))/10+10-2 sp ## [1] 164.2 t <- (mean(sample_1)-mean(sample_2))/sqrt(sp*(1/20)) t ## [1] 0.06980048 p <- 2*pt(t,18,lower.tail=F) p ## [1] 0.9451221 \uc5d1\uc140\uc758 \uacc4\uc0b0\uae30\ub85c \uc9c1\uc811\uacc4\uc0b0\ud574\ubcf8 \uacb0\uacfc R\uc5d0\uc11c \uc5bb\uc740 p-value\uac12\uacfc \ub3d9\uc77c\ud558\uac8c \ub098\uc634\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. p-value\uac12\uc740 \uc57d 0.945 \ub85c \ub9e4\uc6b0 \ud06c\uac8c \ub098\uc624\uba70, \uad00\ucc30\ub41c \ud1b5\uacc4\ub7c9\ubcf4\ub2e4 \ub354 \uadf9\ub2e8\uc801\uc778 \uac12\uc774 \ub098\uc62c \ud655\ub960\uc774 0.945\ub77c\ub294 \ub73b\uc73c\ub85c \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud558\uae30\uc5d0 \uc544\uc8fc \ud798\ub4e0 \uac12\uc784\uc744 \uc758\ubbf8\ud55c\ub2e4. \uace0\ub85c \uc6b0\ub9ac\ub294 \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud560 \ud1b5\uacc4\uc801 \uc720\uc758\uc131\uc744 \uc5bb\uc744 \uc218 \uc5c6\uc73c\uba70, \uc774\ub294 \ub450 \uc9d1\ub2e8\uc758 \uc5f0\uc18c\uc2dc\uac04 \uac04\uc758 \ud3c9\uade0\ucc28\uc774\uac00 \uc874\uc7ac\ud558\uc9c0 \uc54a\ub294\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4 .","title":"(b)"},{"location":"05%20Design%20of%20Experiments/DOE_HW1/#c","text":"\uc6b0\ub9ac\ub294 \uc794\ucc28\uc5d0 \ub300\ud574\uc11c qqnorm \uadf8\ub798\ud504\ub97c \uadf8\ub824\ubcf4\uc544 \uc794\ucc28\uac00 \uc815\uaddc\ubd84\ud3ec\ub97c \ub744\ub294\uc9c0\ub97c \ud655\uc778\ud574 \ubcfc \uc218 \uc788\ub2e4. \uc794\ucc28\ub77c\ub294 \uac83\uc740 \uc911\uc694\ud55c \uc815\ubcf4\ub97c \uc81c\uc678\ud55c \ub098\uba38\uc9c0 \ucc0c\uaebc\uae30\ub77c\uace0 \ud574\uc11d\ud560 \uc218 \uc788\ub294\ub370, \uc774 \uc794\ucc28\ub294 \uc5b4\ub5a0\ud55c \uc758\ubbf8\ub3c4 \uac00\uc838\uc11c\ub294 \uc548\ub418\uba70 \uc815\uaddc\ubd84\ud3ec\ub97c \ub744\uc5b4\uc57c\ud55c\ub2e4. \uc794\ucc28\ub780 \uad00\uce21\uce58\uc640 fitted value \uac12\uc758 \ucc28\uc774\ub85c \ubcfc \uc218 \uc788\uc73c\uba70, \uc774 \uc9d1\ub2e8 \ub370\uc774\ud130\uc5d0\uc11c fitted value\ub294 \ud45c\ubcf8\ud3c9\uade0\uc774 \ub41c\ub2e4. \uace0\ub85c \uc794\ucc28\uc640 \uc774\uc5d0 \ub300\ud55c normal qqplot\uc744 \uadf8\ub824\ubcf8 \uacb0\uacfc\ub294 \uc544\ub798\uc640 \uac19\ub2e4. residual1 <- sample_1-mean(sample_1) residual2 <- sample_2-mean(sample_2) par(mfrow=c(1,2)) qqnorm(residual1) qqnorm(residual2) \uc721\uc548\uc73c\ub85c \ud655\uc778\ud574\ubcf4\uc558\uc744 \ub54c, \uadf8\ub798\ud504\uac00 45\ub3c4 \uac01\ub3c4\ub85c \uc798 \ub298\uc5ec\uc788\ub294 \uac78 \ubcf4\uc544 \uc794\ucc28\uac00 \uc815\uaddc\uc131\uc744 \ub744\ub294 \uac83\uc744 \ubcf4\uc778\ub2e4. \uc774\ub97c fitdistr\ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uba74 \uc544\ub798\uc640 \uac19\uc740 \ubaa8\uc218\ub97c \uac16\ub294 \uc815\uaddc\ubd84\ud3ec\uc5d0 \ud761\uc0ac\ud558\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. library(MASS) fitdistr(residual1,'normal') ## mean sd ## -5.684117e-15 8.788629e+00 ## ( 2.779209e+00) ( 1.965197e+00) fitdistr(residual2,'normal') ## mean sd ## -2.842171e-15 8.885944e+00 ## ( 2.809982e+00) ( 1.986957e+00)","title":"(c) \uc774 \ubb38\uc81c\uc5d0\uc11c \uc9d1\ub2e8\uc758 \uc794\ucc28\uc5d0 \ub300\ud574 \uc815\uaddc\uc131 \uac00\uc815\uc744 \ub17c\ud558\ub77c"},{"location":"05%20Design%20of%20Experiments/DOE_HW1/#246-225-2-1-09","text":"\ub4f1\ubd84\uc0b0\uacfc equal sample size, \\(\\sigma=9\\) \ub97c \uac00\uc815\ud558\uc5ec \ubb38\uc81c\ub97c \ud480\uc5b4\ubcf4\uc790. Case1) \ub450 \uc9d1\ub2e8\uac04 \ud3c9\uade0\uc774 2\uc77c \ub54c \uac80\uc815\ub825 \ucc3e\uae30 \uc704\uc758 \ucf00\uc774\uc2a4\ub294 \ub4f1\ubd84\uc0b0\uacfc \ub3d9\uc77c \uc0d8\ud50c\uc0ac\uc774\uc988\ub97c \uac16\uace0 \ubaa8\ubd84\uc0b0\uc744 \uc54c\uae30 \ub54c\ubb38\uc5d0 Z\ud1b5\uacc4\ub7c9\uc744 \ud1b5\ud574 \uc811\uadfc\ud574\ubcfc \uc218 \uc788\ub2e4. \uc774 \ub54c\uc758 Z\ud1b5\uacc4\ub7c9\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \\[Z=\\frac{\\bar{X_1}-{\\bar{X_2}}-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{2\\sigma^2}{n}}}\\] \uac80\uc815\ub825\uc740 \uadc0\ubb34\uac00\uc124\uc774 \uac70\uc9d3\uc77c \ub54c \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud560 \ud655\ub960\uc774\uba70 \uc774\ub294 $P(\\bar{X_1}-\\bar{X_2}>c|under\\ H_1)\\ or\\ P(\\bar{X_1}-\\bar{X_2}<-c|under\\ H_1) $\uc640 \uac19\ub2e4. \uc774\ub97c \uc704\ud574\uc11c \uba3c\uc800 c\ub97c \uad6c\ud574\uc57c \ud558\ubbc0\ub85c, \uc6b0\ub9ac\ub294 \uc720\uc758\uc218\uc900 0.05\ud558\uc5d0\uc11c c\ub97c \uad6c\ud560 \uc218 \uc788\ub2e4. \uba3c\uc800 \uc720\uc758\uc218\uc900 \\(\\alpha\\) \uc758 \uc808\ubc18\uac12\uc778 0.025\ub294(\uc591\uce21\uac80\uc815\uc774\ubbc0\ub85c) \uadc0\ubb34\uac00\uc124 \ud558\uc5d0\uc11c \uacc4\uc0b0\ub418\ub294 \uac83\uc774\ubbc0\ub85c \uc544\ub798\uc640 \uac19\uc740 \uc2dd\uc744 \ud1b5\ud574\uc11c c\ub97c \uad6c\ud560 \uc218 \uc788\ub2e4. \\[P(\\frac{\\bar{X_1}-{\\bar{X_2}}-0}{\\sqrt{2\\sigma^2/n}}>\\frac{c-0}{\\sqrt{2\\sigma^2/n}})=0.025\\] \uc6b0\ub9ac\ub294 Z\uac80\uc815\ud558\uc5d0\uc11c \uc591\uce21\uac80\uc815\ud558\uc5d0 \\(P(Z>1.96)=0.025\\) \ub77c\ub294 \uc0ac\uc2e4\uc744 \uc54c\uace0 \uc788\ub2e4. \uace0\ub85c \uc774\ub97c \ud1b5\ud574\uc11c \uc5ed\uc73c\ub85c c\uac12\uc744 \uad6c\ud574\ub0bc \uc218 \uc788\ub2e4. \\(\\frac{c}{\\sqrt{(2*81)/10}}= 1.96\\) \uc774\ubbc0\ub85c, \uc5ed\uc0b0\ud558\uba74 \\(c = 1.96*\\sqrt{16.2}\\) \uc774\ubbc0\ub85c c<- sqrt(16.2)*1.96 c ## [1] 7.888848 critical value\ub294 \uc57d 7.88\uc774 \ub098\uc628\ub2e4. \uc774\ub97c \ud1b5\ud574\uc11c \uac80\uc815\ub825\uc744 \uad6c\ud574\ubcf4\uba74, \\(P(\\frac{\\bar{X_1}-\\bar{X_2}-2}{\\sqrt{16.2}}>\\frac{c-2}{\\sqrt{16.2}})+P(\\frac{\\bar{X_1}-\\bar{X_2}-2}{\\sqrt{16.2}}<\\frac{-c-2}{\\sqrt{16.2}})\\) \ub97c \uad6c\ud558\uba74 \ub41c\ub2e4. \uc774\ub294 \\(P(Z>\\frac{c-2}{\\sqrt{16.2}})+P(Z<\\frac{-c-2}{\\sqrt{16.2}})\\) \uc640 \uac19\uc73c\uba70 \uc774 \ud655\ub960\uac12\uc774 \uac80\uc815\ub825\uc774 \ub418\ub294 \uac83\uc774\ub2e4. \uc6b0\ub9ac\ub294 \uc591\uce21\uac80\uc815\uc73c\ub85c \uc2dc\ud589\ud558\uc600\uae30 \ub54c\ubb38\uc5d0 \ub300\ub9bd\uac00\uc124 \ud558\uc5d0\uc11c \ub300\uce6d\ub418\ub294 \uae30\uac01\uc5ed\uc758 \uc808\ub300\uac12\uc744 \ub118\uc5b4\uc11c\ub294 \uba74\uc801\ub4e4\uc774 \uac80\uc815\ub825\uc774 \ub41c\ub2e4. \uacc4\uc0b0\ud55c \uacb0\uacfc\ub294 \uc544\ub798\uc640 \uac19\ub2e4. powervalue <- (c-2)/sqrt(16.2) powervalue ## [1] 1.463096 powervalue2 <- (-c-2)/sqrt(16.2) powervalue2 ## [1] -2.456904 power1 <- pnorm(powervalue,lower.tail=F) power2 <- pnorm(powervalue2,lower.tail=T) power1 ## [1] 0.07172056 power2 ## [1] 0.007007007 beta <- power1+power2 beta ## [1] 0.07872756 \uc5d1\uc140\ub85c \uacc4\uc0b0\ud55c \uacb0\uacfc, r\uc5d0\uc11c \uad6c\ud55c power\uac12\uacfc \uac19\uac8c \ub098\uc634\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\ub2e4. \uccad\ud06c\ucc3d\uc758 beta\uac00 \uc5ec\uae30\uc11c\uc758 \uac80\uc815\ub825\uc774\uba70, \ubcf4\ud1b5 \uc591\uce21\uac80\uc815\uc5d0\uc11c\ub294 \ud55c\ucabd \uaf2c\ub9ac\uc758 \uba74\uc801\uc774 \ub9e4\uc6b0 \uc791\uc544\uc11c \uadf8 \ud6a8\uacfc\ub97c \ubb34\uc2dc\ud574\ub3c4 \uad1c\ucc2e\uc73c\uba70 \ud55c\ucabd\uc758 \uba74\uc801\ub9cc \uacc4\uc0b0\ud558\uc5ec \uac80\uc815\ub825\uc744 \uc815\ud558\uace0\ub294 \ud558\ub294\ub370, \uc704\uc758 \ucf00\uc774\uc2a4\ub294 sample size\uac00 \ub9e4\uc6b0 \uc791\uc73c\uba70 \uc774\ub294 \ud544\uc790\uc758 \uc0dd\uac01\uc774\uc9c0\ub9cc, \uac80\uc815\uc5d0 \uc0ac\uc6a9\ub418\ub294 effect size( \\(\\mu_1-\\mu_2\\) )\uac00 \ub9e4\uc6b0 \uc791\uae30 \ub54c\ubb38\uc5d0 \uac80\uc815\ub825\uc774 \uc544\uc8fc\ud615\ud3b8\uc5c6\uac8c \ub098\uc628\ub2e4. \uace0\ub85c \ud544\uc790\ub294 \ud55c\ucabd \uc791\uc740 \uba74\uc801\uc758 \ub113\uc774\ub97c \ubb34\uc2dc\ud558\uc9c0\uc54a\uace0(\uc5b4\ucc28\ud53c \uc591\uce21 \ubaa8\ub450 \ub9e4\uc6b0 \uc791\uc740 \uba74\uc801\uc774\uae30\uc5d0) \ub450 \uba74\uc801\uc744 \ud569\uc0b0\ud558\uc5ec \uacc4\uc0b0\ud558\uc600\ub2e4. \uadf8 \uacb0\uacfc\ub294 \uc57d 0.078 \uc774 \ub098\uc654\ub2e4. \ucc98\uc74c\uc5d0\ub294 \uac80\uc815\ub825\uc774 \ub108\ubb34 \ub0ae\uc740\uac83\uc744 \ubcf4\uace0 \ubbff\uc9c0 \ubabb\ud574 3\ubc88\uc774\ub098 \uacc4\uc0b0\uc744 \ub2e4\uc2dc\ud574\ubcf4\uc558\uc9c0\ub9cc, sample size\uc640 effect size\ub97c \uace0\ub824\ud574\ubcf4\uba74 \ucda9\ubd84\ud788 \ub098\uc62c \uc218 \uc788\ub2e4\ub294 \uacb0\ub860\uc744 \ub0b4\ub838\ub2e4. (\uc624\ub2f5\uc774\ub77c\uba74 \uc804\uc801\uc73c\ub85c \uc791\uc131\uc790\uc758 \ubb34\uc9c0\ub54c\ubb38) Case2) \uc9d1\ub2e8\uac04 \uc5f0\uc18c\uc2dc\uac04\uc758 \ucc28\uac00 1\ubd84\uc774\uace0 \uac80\uc815\ub825\uc774 \ucd5c\uc18c 0.9\uac00 \ub418\uae30 \uc704\ud55c sample size \ucc3e\uae30 \uc591\uce21\uac80\uc815\uc5d0\uc11c \uac80\uc815\ub825\uc740 \uc704\uc5d0\uc11c \uc5b8\uae09\ud588\ub358 \uc2dd\uc744 \uc778\uc6a9\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \uc801\uc744 \uc218 \uc788\ub2e4. $ \\(P(\\frac{\\bar{X_1}-\\bar{X_2}-1}{\\sqrt{2*81/n}}>\\frac{c-1}{\\sqrt{2*81/n}})+P(\\frac{\\bar{X_1}-\\bar{X_2}-1}{\\sqrt{2*81/n}}<\\frac{-c-1}{\\sqrt{2*81/n}}) \\geq 0.9\\) $ \uc774\ub97c \uc694\uc57d\ud558\uba74 \\(P(Z>(\\frac{c-1}{\\sqrt{2*81/n}})+P(Z<\\frac{-c-1}{\\sqrt{2*81/n}}))\\) \ub85c \uc801\uc744 \uc218 \uc788\ub2e4. \uc774\ubc88 \ucf00\uc774\uc2a4\uc5d0\uc11c\ub294 \\(P(\\bar{X_1}-\\bar{X_2}<-c|\\ under\\ H_1)\\) \uc758 \uac12\uc774 \uba74\uc801\uac12\uc774 \ub9e4\uc6b0 \uc791\uc544 \uad6c\ud574\ub3c4 \ud070 \uc758\ubbf8\uac00 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \uc774 \ud55c\ucabd \uaf2c\ub9ac\uba74\uc801 \uac12\uc758 \uc601\ud5a5\uc774 \ubbf8\ubbf8\ud560 \uac83\uc774\ub77c\uace0 \uac00\uc815\ud558\uace0 \ubc18\ub300 \ucabd \uaf2c\ub9ac\uc758 \uba74\uc801\ub9cc\uc73c\ub85c \uac80\uc815\ub825\uc744 \uacc4\uc0b0\ud558\uaca0\ub2e4. \uace0\ub85c \\(P(Z> \\frac{c-1}{\\sqrt{162/n}}) \\geq 0.9\\) \ub97c \ub9cc\uc871\ud558\ub294 \ud45c\ubcf8\ud06c\uae30\ub97c \ucc3e\uc73c\uba74 \ub41c\ub2e4. qnorm(0.9,lower.tail=F) ## [1] -1.281552 \uc704\uc5d0\uc11c \uc5bb\uc740 \uac12\uc740 1.28\uc774\uc9c0\ub9cc \uc815\uaddc\ubd84\ud3ec\ub294 \ub300\uce6d\uc744 \uc774\ub8e8\ubbc0\ub85c, \ubc18\ub300\ucabd \uac12\uc5d0\uc11c -1.281552\ub97c \uac16\uac8c \ub418\ubbc0\ub85c, r\uc5d0\uc11c \uc5bb\uc740 qnorm\uac12\uacfc \uac19\uac8c \ub098\uc628\ub2e4. \uc704\uc758 \uac12\uc744 \ud1b5\ud574 \uc5bb\uc740 qnorm\uac12 \uc57d -1.2815\uac00 \uac80\uc815\ub825\uc744 0.9\ub85c \ub9cc\ub4e4\uc5b4\uc8fc\ub294 value\uac00 \ub41c\ub2e4. \uc774\ub294 \\(\\frac{c}{\\sqrt{162/n}}=1.96\\) \uacfc \\(\\frac{c-1}{\\sqrt{162/n}}=-1.281552\\) \ub450 \uc2dd\uc744 \uc5f0\ub9bd\ubc29\uc815\uc2dd\uc744 \ud480\uc5b4\uc11c n\uac12\uc744 \uad6c\ud558\uba74 \ub41c\ub2e4. \uc704 \uc2dd\uc744 c\ub85c \uc815\ub9ac\ud558\uc5ec n\uc744 \uad6c\ud558\uba74 n\uc740 \\(n=2\\times(1.96+1.281552)^2\\) \uac00 \ub418\uba70 \uc774\ub294 \uc57d 21.01532\uac00 \ub098\uc624\ubbc0\ub85c, \uac80\uc815\ub825\uc774 \ucd5c\uc18c 0.9\uac00 \ub418\uae30 \uc704\ud55c \ud45c\ubcf8\ud06c\uae30\ub294 22\uac00 \ub41c\ub2e4.","title":"2.46 2.25\uc5d0\uc11c \uc18c\uac1c\ud55c \ubb38\uc81c\uc5d0\uc11c \ud3c9\uade0 \uc18c\uac01 \uc2dc\uac04\uc758 \ucc28\uc774\uac00 2\ubd84 \uc77c\ub54c\uc758 \uac80\uc815\ub825\uc744 \ucc3e\uc544\ub77c. \ud3c9\uade0 \uc2dc\uac04\uc758 \ucc28\uc774\uac00 1\ubd84\uc774\uace0 \uac80\uc815\ub825\uc774 \ucd5c\uc18c 0.9\uac00 \ub418\uae30 \uc704\ud55c \ud45c\ubcf8\ud06c\uae30\ub97c \uad6c\ud558\ub77c."},{"location":"05%20Design%20of%20Experiments/DOE_HW2/","text":"Design of Experiments HW2 1.Example 4.3\ub97c \ud480\uace0 R\uacb0\uacfc\ubb3c\uacfc \uc9c1\uc811 \uacc4\uc0b0\ud55c \uacb0\uacfc\ubb3c\uc744 \ube44\uad50\ud558\ub77c. \uc784\uc758\ud654 \uc644\uc804\ube14\ub85d\uc124\uacc4\ubaa8\ud615\uc744 \uc704\ud574\uc11c \uad00\uce21\uac12\uacfc Treatment\uc5d0 \ud574\ub2f9\ud558\ub294 \uc138\ud0c1\uc6a9\uc218\uac04\uc758 \uc544\ub178\ubc14\uac80\uc815\uc5d0 Block\uc73c\ub85c \uc791\uc6a9\ud558\ub294 \uc77c\uc790\ubcc4 factor\ub97c \ucd94\uac00\ud558\uc5ec RCBD \uc544\ub178\ubc14\uac80\uc815\uc744 \ud558\uba74 \uc544\ub798\uc640 \uac19\uc740 \uacb0\uacfc\ubb3c\uc774 \ub098\uc628\ub2e4. a <- c(1,2,3,4,1,2,3,4,1,2,3,4) b <- c(1,1,1,1,2,2,2,2,3,3,3,3) c <- c(13,22,18,39,16,25,16,44,5,4,2,22) data <- cbind(a,b,c) colnames(data) <- c('Day','Treatment','observations') data <- as.data.frame(data) data ## Day Treatment observations ## 1 1 1 13 ## 2 2 1 22 ## 3 3 1 18 ## 4 4 1 39 ## 5 1 2 16 ## 6 2 2 25 ## 7 3 2 16 ## 8 4 2 44 ## 9 1 3 5 ## 10 2 3 4 ## 11 3 3 2 ## 12 4 3 22 data$Treatment <- as.factor(data$Treatment) data$Day <- as.factor(data$Day) rcbd.aov <- aov(observations~Treatment+Day,data=data) summary(rcbd.aov) ## Df Sum Sq Mean Sq F value Pr(>F) ## Treatment 2 682.2 341.1 34.98 0.000493 *** ## Day 3 1103.0 367.7 37.71 0.000274 *** ## Residuals 6 58.5 9.7 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ANOVA\uac80\uc815\uc744 \uc704\ud574\uc11c random error\uac00 homoskedasticity \uc640 homogenous \ud558\ub2e4\ub294 \uac00\uc815\uc774 \uc788\uc5b4\uc57c F-test\ub97c \ud1b5\ud55c \uac80\uc815\uc774 \uac00\ub2a5\ud55c\ub370, \uc544\ub798\uc758 \uac04\ub2e8\ud55c \uc2dc\uac01\ud654\ub97c \ud1b5\ud574\uc11c \ub4f1\ubd84\uc0b0\uc131 \uac80\uc815 \uacfc \ub3d9\uc9c8\uc131 \uac80\uc815 \ub610\ud55c \ud574\ubcf4\uc558\ub2e4. \ub370\uc774\ud130\uc758 \uad00\uce21\uce58\uac00 \uc6cc\ub099\uc801\uc5b4(12\uac1c) \ub69c\ub837\ud558\uac8c \ub4f1\ubd84\uc0b0\uc131\uc784\uc744 \ubc14\ub85c \ud655\uc778\ud560 \uc218\ub294 \uc5c6\uc9c0\ub9cc, \ud070 \ud328\ud134\uc774 \ubcf4\uc774\uc9c0 \uc54a\ub294 \uac83\uc73c\ub85c \ubcf4\uc544 \ub4f1\ubd84\uc0b0\uc131 \uac00\uc815\uc744 \ud1b5\ud574 F\uac80\uc815\uc744 \uc0ac\uc6a9\ud558\ub294\ub370 \ubb34\ub9ac\uac00 \uc5c6\uc74c\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\ub2e4. \ub610\ud55c \uc794\ucc28\uc5d0 \ub300\ud574 qqplot\uc744 \uadf8\ub824\ubcf4\uc558\uc744 \ub54c, \uc794\ucc28\uac00 \uc815\uaddc\ubd84\ud3ec\uc5d0 \uc798 fitting\ub418\uace0 \uc788\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc8fc\uace0\uc788\ub2e4. \uc774\ub294 error\uc758 \ucd94\uc815\uce58\uc778 \uc794\ucc28\uac00 \uc815\uaddc\ubd84\ud3ec\uc5d0 \uc798 \ub4e4\uc5b4\ub9de\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. ANOVA \ubaa8\ub378\uc5d0\uc11c \uad00\uce21\uce58 y\uac12\uc774 \ud655\ub960\ubcc0\uc218 error\uc640 \uc0c1\uc218\ub4e4\uc758 \ud569\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c4 \ud655\ub960\ubcc0\uc218\uc774\ubbc0\ub85c, error\uac00 \uc815\uaddc\ubd84\ud3ec\ub97c \uac16\ub294\ub2e4\uba74 \uad00\uce21\uce58 y\ub3c4 \uc815\uaddc\ubd84\ud3ec\ub97c \ub744\uac8c\ub418\ub294\ub370, error\uc758 \ucd94\uc815\uce58\uc778 \uc794\ucc28\uac00 \uc815\uaddc\ubd84\ud3ec\uc5d0 \uc798 \ub4e4\uc5b4\ub9de\ub294 \uac83\uc73c\ub85c \ubcf4\uc544, \uad00\uce21\uce58\ub4e4 \uac04\uc5d0 \ub3d9\uc9c8\uc131(homogenous) \uc774 \ub9cc\uc871\ud55c\ub2e4\uace0 \ubcfc \uc218 \uc788\uc744 \uac83\uc774\ub2e4. residuals <- resid(rcbd.aov) Fitted <- fitted(rcbd.aov) plot(Fitted, residuals, xlab=\"Fitted Value\", ylab= \"Residuals\", pch=20, main=\"Residuals vs Fitted values\") abline(0,0,lty=2) abline(2,0,lty=2,col='red') abline(-2,0,lty=2,col='red') qqnorm(residuals,datax=T,ylab=\"Normal Scores\", xlab=\"Residuals\", main=\"Normal Prob Plot of the Residuals\") qqline(residuals,datax=T) \uc774\uc81c \uc704\uc758 R\ucf54\ub4dc\ub97c \ud1b5\ud574 \uad6c\ud604\ud55c SSTrt, SSB, SSE, MSTrt, MSB, MSE\uc640 F\ud1b5\uacc4\ub7c9, p-value\uac00 \uac19\uc740\uc9c0 \uc9c1\uc811 \uacc4\uc0b0\uc744 \ud1b5\ud574 \ud655\uc778\ud574\ubcf4\uaca0\ub2e4. \uc704\uc5d0\uc11c \uad6c\ud558\uace0\uc790\ud558\ub294 \uc5ec\ub7ec \ud1b5\uacc4\uac12\ub4e4\uc758 \uc2dd\uc740 \uc544\ub798\uc640 \uac19\uc73c\uba70, \uacc4\uc0b0\uc744 \uc704\ud574\uc11c\ub294 R\ud504\ub85c\uadf8\ub7a8\uc744 \ud1b5\ud574 \uacc4\uc0b0\uae30\uc6a9\ub3c4\ub85c \uc0ac\uc6a9\ud558\uc600\ub2e4. \\[SS_T = \\sum_{i=1}^a\\sum_{j=1}^by_{ij}^2-\\frac{y_{ii}^2}{N},\\ a=3,\\ b=4, N=12\\] \\[SS_{trt}=\\frac{1}{b}\\sum_{i=1}^ay_{i.}^2-\\frac{y_{..}^2}{N}\\] \\[SS_{Block} = \\frac{1}{a}\\sum_{j=1}^by_{.j}^2-\\frac{y_{..}^2}{N}\\] \\[SSE = SS_T -SS_{trt}-SS_{Block}\\] \\[MS_{trt}=\\frac{SS_{trt}}{a-1},\\ MS_{Block} = \\frac{SS_{Block}}{b-1},\\ MSE = \\frac{SSE}{(a-1)(b-1)}\\] \ub610\ud55c p-value\ub294 treatment\uc640 block\uc758 \uac80\uc815\uc744 \ud1b5\ud574 \uc5bb\uc740 \uac01\uac01\uc758 F-statistics\uc774 \uc21c\uc11c\ub300\ub85c \\(F_{(a-1),(a-1)(b-1)}\\) \uacfc \\(F_{(b-1),(a-1)(b-1)}\\) \ubcf4\ub2e4 \ud070 \uba74\uc801\uc758 \uac12\uc744 \uad6c\ud55c \uacb0\uacfc\uc640 \ub3d9\uc77c\ud558\ub2e4. ss_y_dotdot <- 13^2 + 22^2 + 18^2 + 39^2 +16^2+ 25^2 +16^2 + 44^2 +5^2 +4^2 +2^2 + 22^2 N <- 12 a <- 3 b <- 4 ss_y_i_dot <- 92^2+ 101^2 + 33^2 ss_y_j_dot <- 34^2 + 51^2 + 36^2 + 105^2 y_dotdot <- 13+22+18+39+16+25+16+44+5+4+2+22 SST <- ss_y_dotdot - (y_dotdot^2/N) SStrt <- ss_y_i_dot/b - (y_dotdot^2/N) SSB <- ss_y_j_dot/a - (y_dotdot^2/N) SSE <- SST - SStrt - SSB SST ; SStrt ; SSB ; SSE ## [1] 1843.667 ## [1] 682.1667 ## [1] 1103 ## [1] 58.5 MStrt <- SStrt/(a-1) MSB <- SSB/(b-1) MSE <- SSE/((a-1)*(b-1)) MStrt ; MSB ; MSE ## [1] 341.0833 ## [1] 367.6667 ## [1] 9.75 Ftest_about_treatment <- MStrt/MSE Ftest_about_block <- MSB/MSE Ftest_about_treatment ; Ftest_about_block ## [1] 34.98291 ## [1] 37.7094 pvalue_for_trt <- pf(Ftest_about_treatment,a-1,(a-1)*(b-1),lower.tail=F) pvalue_for_block <- pf(Ftest_about_block,b-1,(a-1)*(b-1),lower.tail=F) pvalue_for_trt ; pvalue_for_block ## [1] 0.0004927189 ## [1] 0.0002741517 \uc704\uc758 \ucf54\ub4dc\ucc3d\uc774 \uc9c1\uc811 \uc190\uacc4\uc0b0\uc744 \ud558\uc5ec \ud480\uc740 \uacb0\uacfc\uac12\uc774\uba70, R\ucf54\ub4dc\ucc3d\uacfc \ube44\uad50\ud574\ubcf4\uc558\uc744 \ub54c \uc18c\uc22b\uc810\uc5d0\uc11c \ubbf8\ubbf8\ud55c \ucc28\uc774\ub9cc\uc744 \uac16\uae30\uc5d0 \uc774\ub860\uc801\uc73c\ub85c \uac19\uc740 \uacb0\uacfc\ub97c \uac00\uc9c4\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \ub610\ud55c Treatment\uc640 Block\uc5d0 \ub300\ud574\uc11c \ub458\ub2e4 F-test\ub97c \uc2dc\ud589\ud558\uc600\uc744 \uc2dc p-value\uac00 \\(\\alpha=0.05\\) \ubcf4\ub2e4 \ub9e4\uc6b0 \uc791\uc740 0\uc5d0 \uac00\uae4c\uc6b4 \uac12\uc774 \ub098\uc624\ubbc0\ub85c, \uac01 \uac80\uc815\ub4e4\uc758 \uadc0\ubb34\uac00\uc124\uc778 \\(H_0: \\tau_1=..=\\tau_3=0\\) \uacfc, \\(H_0: \\beta_1=..=\\beta_4=0\\) \uc774\ub77c\ub294 \uadc0\ubb34\uac00\uc124\uc744 \ubaa8\ub450 \uae30\uac01\ud558\uace0, Treatment\uc758 \ud6a8\uacfc\uc640 Block\uc758 \ud6a8\uacfc\uac00 \uc801\uc5b4\ub3c4 \ud558\ub098\ub294 \uc720\uc758\ubbf8\ud55c \ud6a8\uacfc\ub97c \uac16\ub294\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \uace0\ub85c \ubc15\ud14c\ub9ac\uc544 \uc131\uc7a5 \uc5b5\uc81c \ud6a8\uacfc\uc758 \ube44\uad50\uc2e4\ud5d8\uc744 \ud568\uc5d0 \uc788\uc5b4, \uc138\ud0c1\uc6a9\uc218 \uc885\ub958\ubcc4 \uac04, \uc2e4\ud5d8\uc774 \uc774\ub8e8\uc5b4\uc9c4 \ub0a0 \uac04\uc5d0 \uc720\uc758\ubbf8\ud55c \ucc28\uc774\uac00 \uc788\ub2e4 \ub77c\uace0 \uacb0\ub860\uc744 \ub0b4\ub9b4 \uc218 \uc788\ub2e4. 2. Example 4.40\ub97c \ud480\uace0 R\uacb0\uacfc\ubb3c\uacfc \uc9c1\uc811 \uacc4\uc0b0\ud55c \uacb0\uacfc\ubb3c\uc744 \ube44\uad50\ud558\ub77c. \ubd88\uc644\uc804 \uade0\ud615 \ube14\ub85d\uc124\uacc4\ub97c \uc704\ud574\uc11c Treatment\uc5d0 \ud574\ub2f9\ud558\ub294 \ucca8\uac00\uc81c\uc640 \uad00\uce21\uac12\uc778 \uc8fc\ud589\uac70\ub9ac\uac04\uc758 \uc544\ub178\uac80\uc815\uc5d0 Block\uc73c\ub85c \uc791\uc6a9\ub418\ub294 \uc790\ub3d9\ucc28 \uc885\ub958\ub97c factor\ub85c \ucd94\uac00\ud558\uc5ec BIBD \ubaa8\ud615\uc744 R\uc5d0 \ub3cc\ub824\ubcf8\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc740 \uacb0\uacfc\ub97c \uac16\ub294\ub2e4. \uc774\ub54c BIBD\ub97c \uc801\uc6a9\ud558\ub294 \uc774\uc720\ub294 \uc2dc\uac04 \uc81c\uc57d\uc758 \uc870\uac74\uc73c\ub85c \uc778\ud574\uc11c \ud55c treatment\ub97c \ubaa8\ub4e0 \ube14\ub85d\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc5c6\uae30 \ub54c\ubb38\uc774\ub2e4.(Incomplete) a <- c(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5) b <- c(2,3,4,5,1,2,4,5,1,3,4,5,1,2,3,4,1,2,3,5) c <- c(17,14,13,12,14,14,13,10,12,13,12,9,13,11,11,12,11,12,10,8) data2 <- cbind(a,b,c) colnames(data2) <- c(\"Trt\",\"Block\",\"Observations\") data2 <- as.data.frame(data2) head(data2) ## Trt Block Observations ## 1 1 2 17 ## 2 1 3 14 ## 3 1 4 13 ## 4 1 5 12 ## 5 2 1 14 ## 6 2 2 14 data2$Trt <- as.factor(data2$Trt) data2$Block <- as.factor(data2$Block) BIBD\uac19\uc740 \uacbd\uc6b0\uc5d0\ub294 \uc81c\uc57d\uc5d0 \uc758\ud55c \uacb0\uce21\uac12\uc774 \uc874\uc7ac\ud558\uae30\ub54c\ubb38\uc5d0, Treatment\uc640 block\uc758 \ud6a8\uacfc \uc911 \ud558\ub098\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc815\ud574\uc918\uc57c\ub9cc \ud55c\ub2e4. \uac01\uac01\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc815\ud558\uc5ec \ubd84\uc11d\uc744 \uc2dc\ud589\ud55c \uac83\uc744 \uc21c\uc11c\ub300\ub85c Intrablock analysis, Interblock analysis\ub77c\uace0 \ud55c\ub2e4. \ubd84\uc11d\uc744 \uc2dc\ud589\ud558\uae30 \uc804\uc5d0 \uc55e\uc11c \ub4f1\ubd84\uc0b0\uc131, \ub3d9\uc9c8\uc131 \uac80\uc815\uc744 \ucc28\ub840\ub300\ub85c \uc218\ud589\ud574\ubcf4\uaca0\ub2e4. \ub4f1\ubd84\uc0b0\uc131\uacfc \ub3d9\uc9c8\uc131 \uac00\uc815 \uac80\uc815\uc740 treatment \ud639\uc740 block \uc911 \uc5b4\ub290 \ubd80\ubd84\uc744 \ubcf4\uc815\ud574\uc8fc\ub294 \uac83\uacfc \uc0c1\uad00\uc5c6\uc774 \uc6d0 \uad00\uce21\uce58 \ub370\uc774\ud130\ub294 \uadf8\ub300\ub85c\uc774\ubbc0\ub85c, case\ub97c \ub098\ub220\uc5b4\uc11c \ud655\uc778\ud574\ubcfc \ud544\uc694\uac00 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \ud55c\ubc88\ub9cc \uc218\ud589\ud558\uc600\ub2e4. residuals2 <- resid(lm(Observations ~ Block + Trt, data2)) Fitted2 <- fitted(lm(Observations ~ Block + Trt, data2)) plot(Fitted2, residuals2, xlab=\"Fitted Value\", ylab= \"Residuals\", pch=20, main=\"Residuals vs Fitted values about Intrablock\") abline(0,0,lty=2) abline(1.1,0,lty=2,col='red') abline(-1.1,0,lty=2,col='red') qqnorm(residuals2,datax=T,ylab=\"Normal Scores\", xlab=\"Residuals\", main=\"Normal Prob Plot of the Residuals\") qqline(residuals2,datax=T) \uc721\uc548\uc73c\ub85c \ud655\uc778\ud574\ubcf4\uc558\uc744 \ub54c, \ub4f1\ubd84\uc0b0\uc131\uacfc \ub3d9\uc9c8\uc131 \uac00\uc815\uc5d0 \ud06c\uac8c \ubc97\uc5b4\ub098\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc774\uc9c0 \uc54a\ub294 \uac83\uc73c\ub85c \ud310\ub2e8\ub41c\ub2e4. \uc774\uc81c Intrablock case\ub85c \uba3c\uc800 BIBD\ub97c \uc2dc\ud589\ud574\ubcf4\uaca0\ub2e4. BIBD_Intrablock <- anova(lm(Observations ~ Block + Trt, data2)) BIBD_Intrablock ## Analysis of Variance Table ## ## Response: Observations ## Df Sum Sq Mean Sq F value Pr(>F) ## Block 4 31.200 7.8000 8.5657 0.002158 ** ## Trt 4 35.733 8.9333 9.8103 0.001247 ** ## Residuals 11 10.017 0.9106 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \uc774\ub294 Treatment\ub97c \ubcf4\uc815\ud558\uc600\uc73c\ubbc0\ub85c \uacb0\uacfc\ucc3d\uc758 Trt \ubd80\ubd84\uc758 P-value\ub97c \ud655\uc778\ud558\uc5ec\uc57c\ud55c\ub2e4. 0.001\ub85c \uc720\uc758\uc218\uc900 0.05\ubcf4\ub2e4 \uc791\uae30\uc5d0, \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud560 \uc218 \uc788\ub294 Treatment\uac04\uc5d0 \uc720\uc758\ubbf8\ud55c \ucc28\uc774\uac00 \uc788\ub2e4\uace0 \ubcfc \uc218 \uc788\ub2e4. \uc774 \uacb0\uacfc\ucc3d\uc758 \ud1b5\uacc4\uac12\ub4e4\uacfc \uc9c1\uc811 \uc190\uc73c\ub85c \uacc4\uc0b0\ud55c \uac12\uc774 \ub3d9\uc77c\ud55c\uc9c0\ub97c \ud655\uc778\ud574\ubcf4\uaca0\ub2e4. \uba3c\uc800 \uacf5\uc2dd\uc5d0 \ub530\ub974\uba74, $ \\(SS_T = \\sum_{i=1}^a\\sum_{j=1}^by_{ij}^2-\\frac{y_{ii}^2}{N},\\ a=5,\\ b=5, N=20\\) $ $ \\(SS_{Block} = \\frac{1}{k}\\sum_{j=1}^by_{.j}^2-\\frac{y_{..}^2}{N},\\ k=4,\\ r=4\\) $ $ \\(SS_{Trt(adj)}= \\frac{k\\sum_{i=1}^aQ_i^2}{{\\lambda}a},\\ Q_i=(y_{i.}-\\frac{1}{k}\\sum{n_{ij}}y_{.j}),\\ \\lambda=3\\) $ $ \\(SST= SS_{trt(adj)}+SSB+SSE\\) $ \\[MS_{trt(adj)}=\\frac{SS_{trt(adj)}}{a-1},\\ MSB= \\frac{SSB}{b-1},\\ MSE=\\frac{SSE}{N-a-b+1}\\] \\[F_0 = \\frac{MSTrt(adj)}{MSE} \\sim F_{a-1,N-a-b+1}\\] $ \\(F_0 = \\frac{MSB}{MSE} \\sim F_{b-1,N-a-b+1}\\) $ \uc758 \uc2dd\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4. \uc9c1\uc811 \uacc4\uc0b0\uc744 \uc774\uc6a9\ud558\uc5ec SST\uc640 SSB, SStrt(adj)\ub97c \uad6c\ud574\ubcf4\uba74 \uc544\ub798\uc640 \uac19\ub2e4. SST <- (17^2+ 14^2 + 13^2 + 12^2 + 14^2 + 14^2 + 13^2 + 10^2 + 12^2 +13^2+ 12^2+ 9^2 + 13^2+ 11^2 + 11^2 + 12^2 + 11^2 +12^2 + 10^2 + 8^2) - ((241)^2/20) SST ## [1] 76.95 a=5 b=5 N=20 SS_trtadj <- (4*(8.25^2+2.75^2+(-0.75)^2+(-3.5)^2 + (-6.75)^2))/(3*5) SS_trtadj ## [1] 35.73333 SSB <- (50^2+54^2+48^2+50^2+39^2)/4 - ((241^2)/20) MSB <- SSB/(b-1) MSTrtadj <- SS_trtadj/(a-1) SSE <- SST - SSB - SS_trtadj MSE <- SSE/(N-a-b+1) MSB ; MSTrtadj ## [1] 7.8 ## [1] 8.933333 SSB; SSE ; MSE ## [1] 31.2 ## [1] 10.01667 ## [1] 0.9106061 F_trtadj <- MSTrtadj/MSE F_block <- MSB/MSE F_trtadj; F_block ## [1] 9.810316 ## [1] 8.565724 pvalue_for_trtadj <- pf(F_trtadj,a-1,N-a-b+1,lower.tail=F) pvalue_for_blo <- pf(F_block,b-1,N-a-b+1,lower.tail=F) pvalue_for_trtadj ## [1] 0.001246692 pvalue_for_blo ## [1] 0.002157793 ( \ubcf4\uc815\ud55c Treatment \uc81c\uacf1\ud569\uc5d0 Q\uc5d0 \ub300\ud55c \ubd80\ubd84\uc740 \uc190\uacc4\uc0b0\uc744 \ucd94\uac00\ud558\uc5ec \uc774\ubbf8\uc9c0\ub85c \ucca8\ubd80\ud558\uc600\uc2b5\ub2c8\ub2e4.) \uc704\uc758 \uacc4\uc0b0\uae30\ub85c \uacc4\uc0b0\ud55c \uac12\uacfc R\uacb0\uacfc\ucc3d\uc5d0\uc11c \uc5bb\uc740 \uac12\ub4e4\uc774 \ub3d9\uc77c\ud568\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uac80\uc815\uc744 \uc704\ud55c p-value\ud655\uc778\uc740 treatment \ubd80\ubd84\ub9cc \ud574\ub3c4 \ub418\uc9c0\ub9cc, \ube14\ub85d\uacfc treatment\uc5d0 \ub300\ud55c p\uac12\uc774 \ub458\ub2e4 \ubaa8\ub450 \ub3d9\uc77c\ud558\uac8c \ub098\uc624\uace0 0.05\ubcf4\ub2e4 \uc791\uc740 \uac12\uc74b \uac00\uc9c0\ubbc0\ub85c, \uae30\uc874 treatment\uac04\uc758 \ud6a8\uacfc\uac00 \ucc28\uc774\uac00 \uc5c6\ub2e4\ub294 \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud558\uace0 treatment\uac04\uc5d0 \uc720\uc758\ubbf8\ud55c \ucc28\uc774\uac00 \uc788\ub2e4\ub294 \uacb0\ub860\uc744 \ub0b4\ub9b4 \uc218 \uc788\ub2e4. \uc774\ubc88\uc5d0\ub294 Interblock case\uc5d0 \ub300\ud574\uc11c\ub3c4 \uacb0\uacfc\ub97c \ud655\uc778\ud574\ubcf4\uaca0\ub2e4. BIBD_Interblock <- anova(lm(Observations ~ Trt + Block, data2)) BIBD_Interblock ## Analysis of Variance Table ## ## Response: Observations ## Df Sum Sq Mean Sq F value Pr(>F) ## Trt 4 31.700 7.9250 8.703 0.002026 ** ## Block 4 35.233 8.8083 9.673 0.001321 ** ## Residuals 11 10.017 0.9106 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \uacb0\uacfc\ub97c \ud655\uc778\ud574\ubcf4\uba74 Intrablock case\uc77c\ub54c\uc640 \uc544\uc8fc \ubbf8\ubbf8\ud55c SS\uac12\ub4e4\uc744 \ubcf4\uc774\ub294\ub370, \uc5b4\ub290 case\uac74\uac04\uc5d0 \ubaa8\ub450 \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud560\ub9cc\ud55c \uc720\uc758\ud55c \ucc28\uc774\ub97c \uac16\ub294 \uac83\uc73c\ub85c \ubcf4\uc778\ub2e4. InterBlock case\uc77c\ub54c\ub294 SSB\uc640 SSTrt\uc758 \uc2dd\uc774 \ubc14\ub00c\ub294\ub370 \uc774\ub294 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(SS_{Trt} = \\frac{1}{r}\\sum_{i=1}^ay_{i.}^2-\\frac{y_{..}^2}{N}\\) $ $ \\(SS_{B(adj)}= \\frac{r\\sum_{j=1}^bQ_i^2}{{\\lambda}b},\\ Q_i=(y_{.j}-\\frac{1}{r}\\sum{n_{ij}}y_{i.}),\\ \\lambda=3\\) $ \ub610 \uc9c1\uc811 \ud1b5\uacc4\uac12\ub4e4\uc744 \uad6c\ud574\ubcf4\uaca0\ub2e4. N=20 a=5 b=5 r=4 k=4 lambda=3 SSB_adj <- (4*(3.75^2+5.25^2+0.5^2+(-9.5)^2))/(3*5) SSB_adj ## [1] 35.23333 SStrt <- (56^2+51^2+46^2+47^2+41^2)/4 - ((241^2)/N) SStrt ## [1] 31.7 SST ## [1] 76.95 MStrt <- SStrt/(b-1) MSBadj <- SSB_adj/(a-1) MStrt ; MSBadj ; MSE ## [1] 7.925 ## [1] 8.808333 ## [1] 0.9106061 F_blockadj <- MStrt/MSE F_trt <- MSBadj/MSE F_blockadj; F_trt ## [1] 8.702995 ## [1] 9.673045 pvalue_for_trt <- pf(F_trt,a-1,N-a-b+1,lower.tail=F) pvalue_for_bloadj <- pf(F_blockadj,b-1,N-a-b+1,lower.tail=F) pvalue_for_trt ## [1] 0.001321038 pvalue_for_bloadj ## [1] 0.002025597 Q\uc5d0 \ub300\ud55c \uc2dd\uc740 \uc704\uc758 \uc774\ubbf8\uc9c0\ub97c \ucca8\ubd80\ud558\uc600\ub2e4. \uacc4\uc0b0\uac12\uc744 \ud655\uc778\ud574\ubcf4\uba74 R\uacb0\uacfc\ucc3d\uacfc \uc5ed\uc2dc \uc218\uce58\uac00 \ub3d9\uc77c\ud558\uac8c \ub098\uc634\uc744 \uc54c \uc218 \uc788\ub2e4. Block\ubd80\ubd84\uc758 p-value\ub97c \ud655\uc778\ud574\ubcf4\uba74 0.05\ubcf4\ub2e4 \ub9e4\uc6b0 \uc791\uc740 0\uc5d0 \uac00\uae4c\uc6b4 \uac12\uc774 \ub098\uc624\ubbc0\ub85c, \uae30\uc874 \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud558\uace0, Block\uac04\uc5d0 \uc720\uc758\ubbf8\ud55c \ucc28\uc774\uac00 \uc788\ub2e4\uace0 \uacb0\ub860\ub0b4\ub9b4 \uc218 \uc788\ub2e4.","title":"R Notebook"},{"location":"05%20Design%20of%20Experiments/DOE_HW2/#design-of-experiments-hw2","text":"","title":"Design of Experiments HW2"},{"location":"05%20Design%20of%20Experiments/DOE_HW2/#1example-43-r","text":"\uc784\uc758\ud654 \uc644\uc804\ube14\ub85d\uc124\uacc4\ubaa8\ud615\uc744 \uc704\ud574\uc11c \uad00\uce21\uac12\uacfc Treatment\uc5d0 \ud574\ub2f9\ud558\ub294 \uc138\ud0c1\uc6a9\uc218\uac04\uc758 \uc544\ub178\ubc14\uac80\uc815\uc5d0 Block\uc73c\ub85c \uc791\uc6a9\ud558\ub294 \uc77c\uc790\ubcc4 factor\ub97c \ucd94\uac00\ud558\uc5ec RCBD \uc544\ub178\ubc14\uac80\uc815\uc744 \ud558\uba74 \uc544\ub798\uc640 \uac19\uc740 \uacb0\uacfc\ubb3c\uc774 \ub098\uc628\ub2e4. a <- c(1,2,3,4,1,2,3,4,1,2,3,4) b <- c(1,1,1,1,2,2,2,2,3,3,3,3) c <- c(13,22,18,39,16,25,16,44,5,4,2,22) data <- cbind(a,b,c) colnames(data) <- c('Day','Treatment','observations') data <- as.data.frame(data) data ## Day Treatment observations ## 1 1 1 13 ## 2 2 1 22 ## 3 3 1 18 ## 4 4 1 39 ## 5 1 2 16 ## 6 2 2 25 ## 7 3 2 16 ## 8 4 2 44 ## 9 1 3 5 ## 10 2 3 4 ## 11 3 3 2 ## 12 4 3 22 data$Treatment <- as.factor(data$Treatment) data$Day <- as.factor(data$Day) rcbd.aov <- aov(observations~Treatment+Day,data=data) summary(rcbd.aov) ## Df Sum Sq Mean Sq F value Pr(>F) ## Treatment 2 682.2 341.1 34.98 0.000493 *** ## Day 3 1103.0 367.7 37.71 0.000274 *** ## Residuals 6 58.5 9.7 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ANOVA\uac80\uc815\uc744 \uc704\ud574\uc11c random error\uac00 homoskedasticity \uc640 homogenous \ud558\ub2e4\ub294 \uac00\uc815\uc774 \uc788\uc5b4\uc57c F-test\ub97c \ud1b5\ud55c \uac80\uc815\uc774 \uac00\ub2a5\ud55c\ub370, \uc544\ub798\uc758 \uac04\ub2e8\ud55c \uc2dc\uac01\ud654\ub97c \ud1b5\ud574\uc11c \ub4f1\ubd84\uc0b0\uc131 \uac80\uc815 \uacfc \ub3d9\uc9c8\uc131 \uac80\uc815 \ub610\ud55c \ud574\ubcf4\uc558\ub2e4. \ub370\uc774\ud130\uc758 \uad00\uce21\uce58\uac00 \uc6cc\ub099\uc801\uc5b4(12\uac1c) \ub69c\ub837\ud558\uac8c \ub4f1\ubd84\uc0b0\uc131\uc784\uc744 \ubc14\ub85c \ud655\uc778\ud560 \uc218\ub294 \uc5c6\uc9c0\ub9cc, \ud070 \ud328\ud134\uc774 \ubcf4\uc774\uc9c0 \uc54a\ub294 \uac83\uc73c\ub85c \ubcf4\uc544 \ub4f1\ubd84\uc0b0\uc131 \uac00\uc815\uc744 \ud1b5\ud574 F\uac80\uc815\uc744 \uc0ac\uc6a9\ud558\ub294\ub370 \ubb34\ub9ac\uac00 \uc5c6\uc74c\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\ub2e4. \ub610\ud55c \uc794\ucc28\uc5d0 \ub300\ud574 qqplot\uc744 \uadf8\ub824\ubcf4\uc558\uc744 \ub54c, \uc794\ucc28\uac00 \uc815\uaddc\ubd84\ud3ec\uc5d0 \uc798 fitting\ub418\uace0 \uc788\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc8fc\uace0\uc788\ub2e4. \uc774\ub294 error\uc758 \ucd94\uc815\uce58\uc778 \uc794\ucc28\uac00 \uc815\uaddc\ubd84\ud3ec\uc5d0 \uc798 \ub4e4\uc5b4\ub9de\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. ANOVA \ubaa8\ub378\uc5d0\uc11c \uad00\uce21\uce58 y\uac12\uc774 \ud655\ub960\ubcc0\uc218 error\uc640 \uc0c1\uc218\ub4e4\uc758 \ud569\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c4 \ud655\ub960\ubcc0\uc218\uc774\ubbc0\ub85c, error\uac00 \uc815\uaddc\ubd84\ud3ec\ub97c \uac16\ub294\ub2e4\uba74 \uad00\uce21\uce58 y\ub3c4 \uc815\uaddc\ubd84\ud3ec\ub97c \ub744\uac8c\ub418\ub294\ub370, error\uc758 \ucd94\uc815\uce58\uc778 \uc794\ucc28\uac00 \uc815\uaddc\ubd84\ud3ec\uc5d0 \uc798 \ub4e4\uc5b4\ub9de\ub294 \uac83\uc73c\ub85c \ubcf4\uc544, \uad00\uce21\uce58\ub4e4 \uac04\uc5d0 \ub3d9\uc9c8\uc131(homogenous) \uc774 \ub9cc\uc871\ud55c\ub2e4\uace0 \ubcfc \uc218 \uc788\uc744 \uac83\uc774\ub2e4. residuals <- resid(rcbd.aov) Fitted <- fitted(rcbd.aov) plot(Fitted, residuals, xlab=\"Fitted Value\", ylab= \"Residuals\", pch=20, main=\"Residuals vs Fitted values\") abline(0,0,lty=2) abline(2,0,lty=2,col='red') abline(-2,0,lty=2,col='red') qqnorm(residuals,datax=T,ylab=\"Normal Scores\", xlab=\"Residuals\", main=\"Normal Prob Plot of the Residuals\") qqline(residuals,datax=T) \uc774\uc81c \uc704\uc758 R\ucf54\ub4dc\ub97c \ud1b5\ud574 \uad6c\ud604\ud55c SSTrt, SSB, SSE, MSTrt, MSB, MSE\uc640 F\ud1b5\uacc4\ub7c9, p-value\uac00 \uac19\uc740\uc9c0 \uc9c1\uc811 \uacc4\uc0b0\uc744 \ud1b5\ud574 \ud655\uc778\ud574\ubcf4\uaca0\ub2e4. \uc704\uc5d0\uc11c \uad6c\ud558\uace0\uc790\ud558\ub294 \uc5ec\ub7ec \ud1b5\uacc4\uac12\ub4e4\uc758 \uc2dd\uc740 \uc544\ub798\uc640 \uac19\uc73c\uba70, \uacc4\uc0b0\uc744 \uc704\ud574\uc11c\ub294 R\ud504\ub85c\uadf8\ub7a8\uc744 \ud1b5\ud574 \uacc4\uc0b0\uae30\uc6a9\ub3c4\ub85c \uc0ac\uc6a9\ud558\uc600\ub2e4. \\[SS_T = \\sum_{i=1}^a\\sum_{j=1}^by_{ij}^2-\\frac{y_{ii}^2}{N},\\ a=3,\\ b=4, N=12\\] \\[SS_{trt}=\\frac{1}{b}\\sum_{i=1}^ay_{i.}^2-\\frac{y_{..}^2}{N}\\] \\[SS_{Block} = \\frac{1}{a}\\sum_{j=1}^by_{.j}^2-\\frac{y_{..}^2}{N}\\] \\[SSE = SS_T -SS_{trt}-SS_{Block}\\] \\[MS_{trt}=\\frac{SS_{trt}}{a-1},\\ MS_{Block} = \\frac{SS_{Block}}{b-1},\\ MSE = \\frac{SSE}{(a-1)(b-1)}\\] \ub610\ud55c p-value\ub294 treatment\uc640 block\uc758 \uac80\uc815\uc744 \ud1b5\ud574 \uc5bb\uc740 \uac01\uac01\uc758 F-statistics\uc774 \uc21c\uc11c\ub300\ub85c \\(F_{(a-1),(a-1)(b-1)}\\) \uacfc \\(F_{(b-1),(a-1)(b-1)}\\) \ubcf4\ub2e4 \ud070 \uba74\uc801\uc758 \uac12\uc744 \uad6c\ud55c \uacb0\uacfc\uc640 \ub3d9\uc77c\ud558\ub2e4. ss_y_dotdot <- 13^2 + 22^2 + 18^2 + 39^2 +16^2+ 25^2 +16^2 + 44^2 +5^2 +4^2 +2^2 + 22^2 N <- 12 a <- 3 b <- 4 ss_y_i_dot <- 92^2+ 101^2 + 33^2 ss_y_j_dot <- 34^2 + 51^2 + 36^2 + 105^2 y_dotdot <- 13+22+18+39+16+25+16+44+5+4+2+22 SST <- ss_y_dotdot - (y_dotdot^2/N) SStrt <- ss_y_i_dot/b - (y_dotdot^2/N) SSB <- ss_y_j_dot/a - (y_dotdot^2/N) SSE <- SST - SStrt - SSB SST ; SStrt ; SSB ; SSE ## [1] 1843.667 ## [1] 682.1667 ## [1] 1103 ## [1] 58.5 MStrt <- SStrt/(a-1) MSB <- SSB/(b-1) MSE <- SSE/((a-1)*(b-1)) MStrt ; MSB ; MSE ## [1] 341.0833 ## [1] 367.6667 ## [1] 9.75 Ftest_about_treatment <- MStrt/MSE Ftest_about_block <- MSB/MSE Ftest_about_treatment ; Ftest_about_block ## [1] 34.98291 ## [1] 37.7094 pvalue_for_trt <- pf(Ftest_about_treatment,a-1,(a-1)*(b-1),lower.tail=F) pvalue_for_block <- pf(Ftest_about_block,b-1,(a-1)*(b-1),lower.tail=F) pvalue_for_trt ; pvalue_for_block ## [1] 0.0004927189 ## [1] 0.0002741517 \uc704\uc758 \ucf54\ub4dc\ucc3d\uc774 \uc9c1\uc811 \uc190\uacc4\uc0b0\uc744 \ud558\uc5ec \ud480\uc740 \uacb0\uacfc\uac12\uc774\uba70, R\ucf54\ub4dc\ucc3d\uacfc \ube44\uad50\ud574\ubcf4\uc558\uc744 \ub54c \uc18c\uc22b\uc810\uc5d0\uc11c \ubbf8\ubbf8\ud55c \ucc28\uc774\ub9cc\uc744 \uac16\uae30\uc5d0 \uc774\ub860\uc801\uc73c\ub85c \uac19\uc740 \uacb0\uacfc\ub97c \uac00\uc9c4\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \ub610\ud55c Treatment\uc640 Block\uc5d0 \ub300\ud574\uc11c \ub458\ub2e4 F-test\ub97c \uc2dc\ud589\ud558\uc600\uc744 \uc2dc p-value\uac00 \\(\\alpha=0.05\\) \ubcf4\ub2e4 \ub9e4\uc6b0 \uc791\uc740 0\uc5d0 \uac00\uae4c\uc6b4 \uac12\uc774 \ub098\uc624\ubbc0\ub85c, \uac01 \uac80\uc815\ub4e4\uc758 \uadc0\ubb34\uac00\uc124\uc778 \\(H_0: \\tau_1=..=\\tau_3=0\\) \uacfc, \\(H_0: \\beta_1=..=\\beta_4=0\\) \uc774\ub77c\ub294 \uadc0\ubb34\uac00\uc124\uc744 \ubaa8\ub450 \uae30\uac01\ud558\uace0, Treatment\uc758 \ud6a8\uacfc\uc640 Block\uc758 \ud6a8\uacfc\uac00 \uc801\uc5b4\ub3c4 \ud558\ub098\ub294 \uc720\uc758\ubbf8\ud55c \ud6a8\uacfc\ub97c \uac16\ub294\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \uace0\ub85c \ubc15\ud14c\ub9ac\uc544 \uc131\uc7a5 \uc5b5\uc81c \ud6a8\uacfc\uc758 \ube44\uad50\uc2e4\ud5d8\uc744 \ud568\uc5d0 \uc788\uc5b4, \uc138\ud0c1\uc6a9\uc218 \uc885\ub958\ubcc4 \uac04, \uc2e4\ud5d8\uc774 \uc774\ub8e8\uc5b4\uc9c4 \ub0a0 \uac04\uc5d0 \uc720\uc758\ubbf8\ud55c \ucc28\uc774\uac00 \uc788\ub2e4 \ub77c\uace0 \uacb0\ub860\uc744 \ub0b4\ub9b4 \uc218 \uc788\ub2e4.","title":"1.Example 4.3\ub97c \ud480\uace0 R\uacb0\uacfc\ubb3c\uacfc \uc9c1\uc811 \uacc4\uc0b0\ud55c \uacb0\uacfc\ubb3c\uc744 \ube44\uad50\ud558\ub77c."},{"location":"05%20Design%20of%20Experiments/DOE_HW2/#2-example-440-r","text":"\ubd88\uc644\uc804 \uade0\ud615 \ube14\ub85d\uc124\uacc4\ub97c \uc704\ud574\uc11c Treatment\uc5d0 \ud574\ub2f9\ud558\ub294 \ucca8\uac00\uc81c\uc640 \uad00\uce21\uac12\uc778 \uc8fc\ud589\uac70\ub9ac\uac04\uc758 \uc544\ub178\uac80\uc815\uc5d0 Block\uc73c\ub85c \uc791\uc6a9\ub418\ub294 \uc790\ub3d9\ucc28 \uc885\ub958\ub97c factor\ub85c \ucd94\uac00\ud558\uc5ec BIBD \ubaa8\ud615\uc744 R\uc5d0 \ub3cc\ub824\ubcf8\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc740 \uacb0\uacfc\ub97c \uac16\ub294\ub2e4. \uc774\ub54c BIBD\ub97c \uc801\uc6a9\ud558\ub294 \uc774\uc720\ub294 \uc2dc\uac04 \uc81c\uc57d\uc758 \uc870\uac74\uc73c\ub85c \uc778\ud574\uc11c \ud55c treatment\ub97c \ubaa8\ub4e0 \ube14\ub85d\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc5c6\uae30 \ub54c\ubb38\uc774\ub2e4.(Incomplete) a <- c(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5) b <- c(2,3,4,5,1,2,4,5,1,3,4,5,1,2,3,4,1,2,3,5) c <- c(17,14,13,12,14,14,13,10,12,13,12,9,13,11,11,12,11,12,10,8) data2 <- cbind(a,b,c) colnames(data2) <- c(\"Trt\",\"Block\",\"Observations\") data2 <- as.data.frame(data2) head(data2) ## Trt Block Observations ## 1 1 2 17 ## 2 1 3 14 ## 3 1 4 13 ## 4 1 5 12 ## 5 2 1 14 ## 6 2 2 14 data2$Trt <- as.factor(data2$Trt) data2$Block <- as.factor(data2$Block) BIBD\uac19\uc740 \uacbd\uc6b0\uc5d0\ub294 \uc81c\uc57d\uc5d0 \uc758\ud55c \uacb0\uce21\uac12\uc774 \uc874\uc7ac\ud558\uae30\ub54c\ubb38\uc5d0, Treatment\uc640 block\uc758 \ud6a8\uacfc \uc911 \ud558\ub098\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc815\ud574\uc918\uc57c\ub9cc \ud55c\ub2e4. \uac01\uac01\uc758 \ud6a8\uacfc\ub97c \ubcf4\uc815\ud558\uc5ec \ubd84\uc11d\uc744 \uc2dc\ud589\ud55c \uac83\uc744 \uc21c\uc11c\ub300\ub85c Intrablock analysis, Interblock analysis\ub77c\uace0 \ud55c\ub2e4. \ubd84\uc11d\uc744 \uc2dc\ud589\ud558\uae30 \uc804\uc5d0 \uc55e\uc11c \ub4f1\ubd84\uc0b0\uc131, \ub3d9\uc9c8\uc131 \uac80\uc815\uc744 \ucc28\ub840\ub300\ub85c \uc218\ud589\ud574\ubcf4\uaca0\ub2e4. \ub4f1\ubd84\uc0b0\uc131\uacfc \ub3d9\uc9c8\uc131 \uac00\uc815 \uac80\uc815\uc740 treatment \ud639\uc740 block \uc911 \uc5b4\ub290 \ubd80\ubd84\uc744 \ubcf4\uc815\ud574\uc8fc\ub294 \uac83\uacfc \uc0c1\uad00\uc5c6\uc774 \uc6d0 \uad00\uce21\uce58 \ub370\uc774\ud130\ub294 \uadf8\ub300\ub85c\uc774\ubbc0\ub85c, case\ub97c \ub098\ub220\uc5b4\uc11c \ud655\uc778\ud574\ubcfc \ud544\uc694\uac00 \uc5c6\uae30 \ub54c\ubb38\uc5d0 \ud55c\ubc88\ub9cc \uc218\ud589\ud558\uc600\ub2e4. residuals2 <- resid(lm(Observations ~ Block + Trt, data2)) Fitted2 <- fitted(lm(Observations ~ Block + Trt, data2)) plot(Fitted2, residuals2, xlab=\"Fitted Value\", ylab= \"Residuals\", pch=20, main=\"Residuals vs Fitted values about Intrablock\") abline(0,0,lty=2) abline(1.1,0,lty=2,col='red') abline(-1.1,0,lty=2,col='red') qqnorm(residuals2,datax=T,ylab=\"Normal Scores\", xlab=\"Residuals\", main=\"Normal Prob Plot of the Residuals\") qqline(residuals2,datax=T) \uc721\uc548\uc73c\ub85c \ud655\uc778\ud574\ubcf4\uc558\uc744 \ub54c, \ub4f1\ubd84\uc0b0\uc131\uacfc \ub3d9\uc9c8\uc131 \uac00\uc815\uc5d0 \ud06c\uac8c \ubc97\uc5b4\ub098\ub294 \ubaa8\uc2b5\uc744 \ubcf4\uc774\uc9c0 \uc54a\ub294 \uac83\uc73c\ub85c \ud310\ub2e8\ub41c\ub2e4. \uc774\uc81c Intrablock case\ub85c \uba3c\uc800 BIBD\ub97c \uc2dc\ud589\ud574\ubcf4\uaca0\ub2e4. BIBD_Intrablock <- anova(lm(Observations ~ Block + Trt, data2)) BIBD_Intrablock ## Analysis of Variance Table ## ## Response: Observations ## Df Sum Sq Mean Sq F value Pr(>F) ## Block 4 31.200 7.8000 8.5657 0.002158 ** ## Trt 4 35.733 8.9333 9.8103 0.001247 ** ## Residuals 11 10.017 0.9106 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \uc774\ub294 Treatment\ub97c \ubcf4\uc815\ud558\uc600\uc73c\ubbc0\ub85c \uacb0\uacfc\ucc3d\uc758 Trt \ubd80\ubd84\uc758 P-value\ub97c \ud655\uc778\ud558\uc5ec\uc57c\ud55c\ub2e4. 0.001\ub85c \uc720\uc758\uc218\uc900 0.05\ubcf4\ub2e4 \uc791\uae30\uc5d0, \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud560 \uc218 \uc788\ub294 Treatment\uac04\uc5d0 \uc720\uc758\ubbf8\ud55c \ucc28\uc774\uac00 \uc788\ub2e4\uace0 \ubcfc \uc218 \uc788\ub2e4. \uc774 \uacb0\uacfc\ucc3d\uc758 \ud1b5\uacc4\uac12\ub4e4\uacfc \uc9c1\uc811 \uc190\uc73c\ub85c \uacc4\uc0b0\ud55c \uac12\uc774 \ub3d9\uc77c\ud55c\uc9c0\ub97c \ud655\uc778\ud574\ubcf4\uaca0\ub2e4. \uba3c\uc800 \uacf5\uc2dd\uc5d0 \ub530\ub974\uba74, $ \\(SS_T = \\sum_{i=1}^a\\sum_{j=1}^by_{ij}^2-\\frac{y_{ii}^2}{N},\\ a=5,\\ b=5, N=20\\) $ $ \\(SS_{Block} = \\frac{1}{k}\\sum_{j=1}^by_{.j}^2-\\frac{y_{..}^2}{N},\\ k=4,\\ r=4\\) $ $ \\(SS_{Trt(adj)}= \\frac{k\\sum_{i=1}^aQ_i^2}{{\\lambda}a},\\ Q_i=(y_{i.}-\\frac{1}{k}\\sum{n_{ij}}y_{.j}),\\ \\lambda=3\\) $ $ \\(SST= SS_{trt(adj)}+SSB+SSE\\) $ \\[MS_{trt(adj)}=\\frac{SS_{trt(adj)}}{a-1},\\ MSB= \\frac{SSB}{b-1},\\ MSE=\\frac{SSE}{N-a-b+1}\\] \\[F_0 = \\frac{MSTrt(adj)}{MSE} \\sim F_{a-1,N-a-b+1}\\] $ \\(F_0 = \\frac{MSB}{MSE} \\sim F_{b-1,N-a-b+1}\\) $ \uc758 \uc2dd\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4. \uc9c1\uc811 \uacc4\uc0b0\uc744 \uc774\uc6a9\ud558\uc5ec SST\uc640 SSB, SStrt(adj)\ub97c \uad6c\ud574\ubcf4\uba74 \uc544\ub798\uc640 \uac19\ub2e4. SST <- (17^2+ 14^2 + 13^2 + 12^2 + 14^2 + 14^2 + 13^2 + 10^2 + 12^2 +13^2+ 12^2+ 9^2 + 13^2+ 11^2 + 11^2 + 12^2 + 11^2 +12^2 + 10^2 + 8^2) - ((241)^2/20) SST ## [1] 76.95 a=5 b=5 N=20 SS_trtadj <- (4*(8.25^2+2.75^2+(-0.75)^2+(-3.5)^2 + (-6.75)^2))/(3*5) SS_trtadj ## [1] 35.73333 SSB <- (50^2+54^2+48^2+50^2+39^2)/4 - ((241^2)/20) MSB <- SSB/(b-1) MSTrtadj <- SS_trtadj/(a-1) SSE <- SST - SSB - SS_trtadj MSE <- SSE/(N-a-b+1) MSB ; MSTrtadj ## [1] 7.8 ## [1] 8.933333 SSB; SSE ; MSE ## [1] 31.2 ## [1] 10.01667 ## [1] 0.9106061 F_trtadj <- MSTrtadj/MSE F_block <- MSB/MSE F_trtadj; F_block ## [1] 9.810316 ## [1] 8.565724 pvalue_for_trtadj <- pf(F_trtadj,a-1,N-a-b+1,lower.tail=F) pvalue_for_blo <- pf(F_block,b-1,N-a-b+1,lower.tail=F) pvalue_for_trtadj ## [1] 0.001246692 pvalue_for_blo ## [1] 0.002157793 ( \ubcf4\uc815\ud55c Treatment \uc81c\uacf1\ud569\uc5d0 Q\uc5d0 \ub300\ud55c \ubd80\ubd84\uc740 \uc190\uacc4\uc0b0\uc744 \ucd94\uac00\ud558\uc5ec \uc774\ubbf8\uc9c0\ub85c \ucca8\ubd80\ud558\uc600\uc2b5\ub2c8\ub2e4.) \uc704\uc758 \uacc4\uc0b0\uae30\ub85c \uacc4\uc0b0\ud55c \uac12\uacfc R\uacb0\uacfc\ucc3d\uc5d0\uc11c \uc5bb\uc740 \uac12\ub4e4\uc774 \ub3d9\uc77c\ud568\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uac80\uc815\uc744 \uc704\ud55c p-value\ud655\uc778\uc740 treatment \ubd80\ubd84\ub9cc \ud574\ub3c4 \ub418\uc9c0\ub9cc, \ube14\ub85d\uacfc treatment\uc5d0 \ub300\ud55c p\uac12\uc774 \ub458\ub2e4 \ubaa8\ub450 \ub3d9\uc77c\ud558\uac8c \ub098\uc624\uace0 0.05\ubcf4\ub2e4 \uc791\uc740 \uac12\uc74b \uac00\uc9c0\ubbc0\ub85c, \uae30\uc874 treatment\uac04\uc758 \ud6a8\uacfc\uac00 \ucc28\uc774\uac00 \uc5c6\ub2e4\ub294 \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud558\uace0 treatment\uac04\uc5d0 \uc720\uc758\ubbf8\ud55c \ucc28\uc774\uac00 \uc788\ub2e4\ub294 \uacb0\ub860\uc744 \ub0b4\ub9b4 \uc218 \uc788\ub2e4. \uc774\ubc88\uc5d0\ub294 Interblock case\uc5d0 \ub300\ud574\uc11c\ub3c4 \uacb0\uacfc\ub97c \ud655\uc778\ud574\ubcf4\uaca0\ub2e4. BIBD_Interblock <- anova(lm(Observations ~ Trt + Block, data2)) BIBD_Interblock ## Analysis of Variance Table ## ## Response: Observations ## Df Sum Sq Mean Sq F value Pr(>F) ## Trt 4 31.700 7.9250 8.703 0.002026 ** ## Block 4 35.233 8.8083 9.673 0.001321 ** ## Residuals 11 10.017 0.9106 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \uacb0\uacfc\ub97c \ud655\uc778\ud574\ubcf4\uba74 Intrablock case\uc77c\ub54c\uc640 \uc544\uc8fc \ubbf8\ubbf8\ud55c SS\uac12\ub4e4\uc744 \ubcf4\uc774\ub294\ub370, \uc5b4\ub290 case\uac74\uac04\uc5d0 \ubaa8\ub450 \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud560\ub9cc\ud55c \uc720\uc758\ud55c \ucc28\uc774\ub97c \uac16\ub294 \uac83\uc73c\ub85c \ubcf4\uc778\ub2e4. InterBlock case\uc77c\ub54c\ub294 SSB\uc640 SSTrt\uc758 \uc2dd\uc774 \ubc14\ub00c\ub294\ub370 \uc774\ub294 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(SS_{Trt} = \\frac{1}{r}\\sum_{i=1}^ay_{i.}^2-\\frac{y_{..}^2}{N}\\) $ $ \\(SS_{B(adj)}= \\frac{r\\sum_{j=1}^bQ_i^2}{{\\lambda}b},\\ Q_i=(y_{.j}-\\frac{1}{r}\\sum{n_{ij}}y_{i.}),\\ \\lambda=3\\) $ \ub610 \uc9c1\uc811 \ud1b5\uacc4\uac12\ub4e4\uc744 \uad6c\ud574\ubcf4\uaca0\ub2e4. N=20 a=5 b=5 r=4 k=4 lambda=3 SSB_adj <- (4*(3.75^2+5.25^2+0.5^2+(-9.5)^2))/(3*5) SSB_adj ## [1] 35.23333 SStrt <- (56^2+51^2+46^2+47^2+41^2)/4 - ((241^2)/N) SStrt ## [1] 31.7 SST ## [1] 76.95 MStrt <- SStrt/(b-1) MSBadj <- SSB_adj/(a-1) MStrt ; MSBadj ; MSE ## [1] 7.925 ## [1] 8.808333 ## [1] 0.9106061 F_blockadj <- MStrt/MSE F_trt <- MSBadj/MSE F_blockadj; F_trt ## [1] 8.702995 ## [1] 9.673045 pvalue_for_trt <- pf(F_trt,a-1,N-a-b+1,lower.tail=F) pvalue_for_bloadj <- pf(F_blockadj,b-1,N-a-b+1,lower.tail=F) pvalue_for_trt ## [1] 0.001321038 pvalue_for_bloadj ## [1] 0.002025597 Q\uc5d0 \ub300\ud55c \uc2dd\uc740 \uc704\uc758 \uc774\ubbf8\uc9c0\ub97c \ucca8\ubd80\ud558\uc600\ub2e4. \uacc4\uc0b0\uac12\uc744 \ud655\uc778\ud574\ubcf4\uba74 R\uacb0\uacfc\ucc3d\uacfc \uc5ed\uc2dc \uc218\uce58\uac00 \ub3d9\uc77c\ud558\uac8c \ub098\uc634\uc744 \uc54c \uc218 \uc788\ub2e4. Block\ubd80\ubd84\uc758 p-value\ub97c \ud655\uc778\ud574\ubcf4\uba74 0.05\ubcf4\ub2e4 \ub9e4\uc6b0 \uc791\uc740 0\uc5d0 \uac00\uae4c\uc6b4 \uac12\uc774 \ub098\uc624\ubbc0\ub85c, \uae30\uc874 \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud558\uace0, Block\uac04\uc5d0 \uc720\uc758\ubbf8\ud55c \ucc28\uc774\uac00 \uc788\ub2e4\uace0 \uacb0\ub860\ub0b4\ub9b4 \uc218 \uc788\ub2e4.","title":"2. Example 4.40\ub97c \ud480\uace0 R\uacb0\uacfc\ubb3c\uacfc \uc9c1\uc811 \uacc4\uc0b0\ud55c \uacb0\uacfc\ubb3c\uc744 \ube44\uad50\ud558\ub77c."},{"location":"06%20Simulation%20Practice%20and%20Theory/01_Monte_Carlo___Bootstrap/","text":"Introduction of Monte Carlo & Bootstrap Procedures 1. Method of Monte Carlo 1-a. Concept and Generating Random Numbers \uc694\uc998 \ud604\ub300 \ud1b5\uacc4\ud559\uc5d0\uc11c\ub294 Asymptotic techniques\uac00 \uad11\ubc94\uc704\ud558\uac8c \uc801\uc6a9\ub418\uace0 \uc788\ub2e4. \ud5c8\ub098 \uc774 \uadfc\uc0ac\uc801 \uc811\uadfc\ubc29\ubc95\uc740 sample size\uac00 \ubb34\ud55c\ud560 \ub54c\ub9cc \uadf9\ud55c\ubd84\ud3ec\ub97c \uc5bb\uc5b4\ub0bc \uc218 \uc788\ub2e4. \uc2e4\uc81c\ub85c \uc774\ub7ec\ud55c size\uc5d0 \ub300\ud55c \uc81c\uc57d\uc73c\ub85c \uc778\ud574 Asymptotic techniques\ub294 \uc5ec\ub7ec\uac00\uc9c0 \ubb38\uc81c\uc810\uc744 \uac16\uace0 \uc788\ub2e4. \ud604\uc2e4\uc5d0\uc11c\ub294 sample size\uac00 \uc720\ud55c\ud558\ub2e4\ub294 \uac83. \uadf8\ub807\uae30\uc5d0 \uc720\ud55c\uc0d8\ud50c\uc744 \uc774\uc6a9\ud558\uc5ec \ud1b5\uacc4\ubc29\ubc95\ub860\uc801 empirical check\ub85c \uc811\uadfc\uc744 \ud55c\ub2e4. \uac00\ub054 \uc5f0\uc0b0\uc774 \ub108\ubb34 \ubcf5\uc7a1\ud558\uc5ec limiting distribution\uc744 \uad6c\ud560 \uc218\uac00 \uc5c6\ub2e4. \uc704\uc640 \uac19\uc740 \ubb38\uc81c\uc810\uc73c\ub85c \uc778\ud574 \uadfc\uc0ac\uc801 \uc811\uadfc\uc774 \uc5b4\ub824\uc6b8 \ub54c \uc6b0\ub9ac\ub294 \uadf8 \ub300\uc548\uc73c\ub85c Monte Carlo \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud55c\ub2e4. \uc774\ub294 \ud2b9\uc815\ud55c \ubd84\ud3ec \ud639\uc740 \uc0d8\ud50c\ub85c\ubd80\ud130 \uad00\uce21\uce58\ub4e4\uc744 \uc0dd\uc131\ud558\ub294 \ubc29\ubc95\uc744 \ub9d0\ud55c\ub2e4. \uc774 \ubc29\ubc95\uc744 \uc704\ud574 \uc6b0\ub9ac\uac00 \ud544\uc694\ud55c \uac83\uc740 \\(Unif(0,1)\\) \uc758 \ubd84\ud3ec\uc5d0\uc11c \ub09c\uc218\ub97c \uc0dd\uc131\ud558\ub294 \uac83\uc774\ub2e4. \uacfc\uac70\uc5d0 \ucef4\ud4e8\ud130\uac00 \ubc1c\uc804\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\ub294 \uc774\ub7ec\ud55c \ub09c\uc218\ub97c \uc0dd\uc131\ud558\ub294 \uac83\uc774 \ub2e4\uc18c \ubcf5\uc7a1\ud558\uc600\uc9c0\ub9cc, \uc694\uc998\uc740 statistical software pacakages\ub97c \ud1b5\ud574 \uc27d\uac8c \\(Unif(0,1)\\) \uc744 \ub530\ub974\ub294 \ub09c\uc218\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\ub2e4. \uc790 \uadf8\ub7ec\uba74 \uac04\ub2e8\ud558\uac8c \\(Unif(0,1)\\) \ubd84\ud3ec\ub97c \uc774\uc6a9\ud558\uc5ec \uc774\uc0b0\ud615\ubd84\ud3ec\uc5d0\uc11c \ubb34\uc791\uc704 \ub09c\uc218\ub97c \uc0dd\uc131\ud574\ubcf4\uc790! \uc774\ub97c \ud480\uc5b4\uc11c \uc124\uba85\ud55c\ub2e4\uba74, \uade0\ub4f1\ubd84\ud3ec\uc5d0 \ud2b9\uc815\ud55c \uaddc\uce59\uc744 \uc801\uc6a9\ud558\uc5ec \ub09c\uc218\ub97c \uc0dd\uc131\ud55c \ub4a4, \uc774 sample\ub4e4\uc744 \uc6b0\ub9ac\uc758 \ubaa9\ud45c \uc774\uc0b0\ud615\ubd84\ud3ec\uc5d0\uc11c \ucd94\ucd9c\ud55c \ub09c\uc218\ucc98\ub7fc \ub9cc\ub4dc\ub294 \uac83\uc774\ub77c\uace0 \ubcfc \uc218 \uc788\uc744 \uac83\uc774\ub2e4. \uadf8 \uc608\ub85c \\(B(10,1/3)\\) \uc758 \ubd84\ud3ec\ub97c \ub530\ub974\ub294 \ub09c\uc218\ub97c \uc0dd\uc131\ud574\ubcf4\uc790. \ub9cc\uc57d \\(U\\) \uac00 \\(Unif(0,1)\\) \uc744 \ub530\ub974\ub294 \ud655\ub960\ubcc0\uc218\ub77c \ud558\uba74, \ud655\ub960\ubcc0\uc218 \\(X\\) \ub294 \uc544\ub798\uc640 \uac19\uc774 \ub9cc\ub4e4\uc5b4 \ubcfc \uc218\uac00 \uc788\ub2e4. $ \\(X= \\begin{cases} 1\\ if\\ 0< U \\leq 1/3\\\\ 0\\ if\\ 1/3 \\leq U < 1 \\end{cases}\\) $ set.seed(2013122059) n <- 10 U <- runif(n,0,1) X <- rep(0,n) for (i in 1:10){ if (U[i] <=1/3){X[i]=1} } U ## [1] 0.56813976 0.35981586 0.49809107 0.29650199 0.50092175 0.16731997 ## [7] 0.52250479 0.84050584 0.06741438 0.27603992 X ## [1] 0 0 0 1 0 1 0 0 1 1 \uc704\uc758 \uaddc\uce59\uc744 \ud1b5\ud574 \uac04\ub2e8\ud558\uac8c cutoff\ub97c 1/3\ub85c \uc815\ud558\uc5ec 1/3\uc744 \ub118\uc9c0 \ubabb\ud558\uba74 X\ub294 1\ub85c, \uadf8\ub807\uc9c0 \uc54a\ub2e4\uba74 0\uc73c\ub85c \ubcc0\ud658\ud574\uc8fc\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud588\ub2e4. \uadf8\ub9ac\ud558\uc5ec U\uc640 X\uc758 \uac12\uc744 \uc0b4\ud3b4\ubcf4\uba74, cutoff\ub97c \ub118\uc9c0 \ubabb\ud558\ub294 U\uc5d0 \ub300\ud574\uc11c\ub294 X\uac00 1\ub85c \ubc18\ud658\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc774\ub97c \uc870\uae08 \uc9c1\uad00\uc801\uc73c\ub85c \uc124\uba85\ud558\uba74 \uc544\ub798\ucc98\ub7fc \ud480\uc5b4 \uc4f8 \uc218 \uc788\ub2e4. \\(Bin(10,1/3)\\) \uc744 \ub530\ub974\ub294 \ubd84\ud3ec\ub77c\ub294 \uac83\uc740, 1/3\uc758 \ud655\ub960\ub85c event\uac00 \uc131\uacf5\uc744\ud558\uace0, 2/3\uc758 \ud655\ub960\ub85c \uc2e4\ud328\ub97c \ud55c\ub2e4\ub294 \uac83\uc774\uace0, \uc774\ub97c \ub192\uc774\uc640 \ubc11\ubcc0\uc758 \uae38\uc774\uac00 \uac01\uac01 1\uc778 \uc815\uc0ac\uac01\ud615\uc758 \uba74\uc801\uc5d0\uc11c( \\(Unif(0,1)\\) ) 1/3\uc5d0 \ud574\ub2f9\ud558\ub294 \uba74\uc801\uc744 \uce60\ud558\uace0, \uce60\ud55c \ubd80\ubd84\uc740 1(\uc989 \uc774\ud56d\ubd84\ud3ec\uc5d0\uc11c \uc131\uacf5\uc744 \uc758\ubbf8)\ub85c \ubc18\ud658\ud558\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud55c \uac83\uc774\ub77c\uace0 \ubcfc \uc218 \uc788\ub2e4. \uc5ec\uae30\uc5d0 \uc870\uae08 \ub354 \ub098\uc544\uac04 \uc608\uc81c\ub85c, \uc11c\ub85c iid\uc778 \ub450\uac1c\uc758 Uniform \ubd84\ud3ec\ub97c \uc774\uc6a9\ud558\uc5ec \\(\\pi\\) \uc758 \uac12\ub3c4 \ucd94\uc815\uc744 \ud574\ubcfc \uc218 \uc788\ub2e4. \\(\\pi\\) \ub97c \ucd94\uc815\ud558\ub294 \ubc29\ubc95\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \\[X= \\begin{cases} 1\\ \\ \\ if\\ U_1^2+U_2^2 <1 \\\\ 0\\ \\ \\ o.w \\end{cases}\\] \uc5ec\uae30\uc11c \\(U_1\\) \uacfc \\(U_2\\) \ub294 \uac01\uac01 \\(Unif(0,1)\\) \uc758 \ubd84\ud3ec\ub97c \ub530\ub978\ub2e4. \\(X\\) \uc758 \ud3c9\uade0\uc744 \uc5ec\uae30\uc11c \uad6c\ud574\ubcf8\ub2e4\uba74, $E(X)= \\sum x\\cdot p(x)= 1\\cdot p(x=1) + 0\\cdot p(x=0) $ \uc774\ubbc0\ub85c \\(1\\cdot\\frac{\\pi}{4}+0\\cdot (1-\\frac{\\pi}{4})= \\frac{\\pi}{4}\\) \uac00 \ub41c\ub2e4. \uace0\ub85c \\(\\mu= E(X) = \\pi/4\\) \uc774\uba70, Weak Law of Large Numbers\uc5d0 \uc758\ud574\uc11c \\(4E(\\bar{X})\\) \ub294 \\(\\pi\\) \uc758 consistent estimator\uac00 \ub41c\ub2e4. \uc2e4\uc81c\ub85c n\uc758 \uc0ac\uc774\uc988\ub97c \uc810\uc810 \ud0a4\uc6b0\uac8c \ub418\uba74, \\(4E(\\bar{X})\\) \ub294 \uc810\uc810 \\(\\pi\\) \uac12\uc73c\ub85c \uc218\ub834\ud558\uac8c \ub428\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc704\uc5d0\uc11c \uc5b8\uae09\ud55c \ub0b4\uc6a9\uc744 \uc791\uc131\ud55c \ucf54\ub4dc\ub294 \uc544\ub798\uc640 \uac19\ub2e4. set.seed(2013122059) iteration <- c(30,100,1000,5000,1000000) mean_convergence<- rep(0,5) CI_length_convergence <- rep(0,5) for (i in 1:length(iteration)){ U1 <- runif(iteration[i],0,1) U2 <- runif(iteration[i],0,1) X <- rep(0,iteration[i]) for ( j in 1:iteration[i]) if (U1[j]^2+U2[j]^2 <=1){X[j]=1} mean_convergence[i] <- 4*mean(X) CI_length_convergence[i] <- ((1.96*4*mean(X)*(1-mean(X)))/iteration[i]) } mean_convergence ## [1] 2.666667 2.800000 3.152000 3.138400 3.142284 CI_length_convergence ## [1] 5.807407e-02 1.646400e-02 1.309719e-03 2.649965e-04 1.320642e-06 Sample size\uac00 \uc810\uc810 \ucee4\uc9c8\uc218\ub85d, \uc2e0\ub8b0\uad6c\uac04\uc758 \ub108\ube44\ub294 \uc9e7\uc544\uc9c0\uace0 \ud3c9\uade0 \ub610\ud55c \\(\\pi\\) \uac12\uc73c\ub85c \uc810\uc810 \uac00\uae4c\uc6cc\uc9d0\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\ub2e4. Theorem 4.8.1 \uc704\uc5d0\uc11c \uc6b0\ub9ac\ub294 \ubaac\ud14c\uce74\ub97c\ub85c \ubc29\ubc95\uc758 \uac04\ub2e8\ud55c \uc608\uc81c\uc640 \ub79c\ub364\ub09c\uc218\uc0dd\uc131\uc5d0 \ub300\ud574\uc11c \ubc30\uc6e0\ub2e4. \uc2dc\ubbac\ub808\uc774\uc158\uc744 \ud589\ud560 \ub54c, \uc6b0\ub9ac\ub294 \ud2b9\uc815\ud55c \ubd84\ud3ec\ub97c \uac00\uc9c4 \uc0d8\ud50c\ub4e4\uc744 \ucd94\ucd9c\ud558\ub294 \uacbd\uc6b0\uac00 \ub9ce\ub2e4. \ud558\uc9c0\ub9cc \ub2e8\uc21c\ud788 rexp(), rnorm(), rpois() \ub4f1\uc758 \ub09c\uc218\uc0dd\uc131 \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc544\ub2c8\ub77c \\(Unif(0,1)\\) \ubd84\ud3ec\ub97c \ud1b5\ud574 \ud2b9\uc815 \ubd84\ud3ec\uc5d0\uc11c \uc0d8\ud50c\ub9c1\ud558\ub294 \ud6a8\uacfc\ub97c \ub0b4\ub294 \uac83\uc744 \ubc30\uc6e0\ub2e4. \uc774\ub807\uac8c \uade0\ub4f1\ubd84\ud3ec\ub97c \ud1b5\ud574 \ud0c0\ubd84\ud3ec\uc758 \ub09c\uc218 \uc0dd\uc131\uc774 \uac00\ub2a5\ud55c \uac83\uc740 \ubc14\ub85c \uc774 Theorem 4.8.1 \uc5d0 \ub2f4\uaca8\uc788\ub2e4. Theorem\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(U \\sim Unif(0,1)\\) , Let F be a continuous distribution function., Then \\(X= F^{-1}(U)\\) has distribution function F \ub300\ucda9 \ud574\uc11d\uc744 \ud574\ubcf4\uba74 U\uac00 \uade0\ub4f1\ubd84\ud3ec\ub97c \ub530\ub974\uace0, \ud568\uc218 F\ub97c \uc5b4\ub5a0\ud55c \ub204\uc801\ubd84\ud3ec\ud568\uc218\ub77c\uace0 \ud558\uc600\uc744 \ub54c, \ud655\ub960\ubcc0\uc218 X\ub294 \ud568\uc218 F\ub97c \ub204\uc801\ud655\ub960\ubd84\ud3ec\ub85c \uac16\ub294\ub2e4 \ub77c\ub294 \ub73b\uc774\ub2e4. \ud574\uc11d\uc744 \ud574\ub3c4 \uc804\ud600 \uac10\uc774 \uc624\uc9c0 \uc54a\uc9c0\ub9cc, \ub2e4\uc2dc \uc774\ud574\ub97c \ud574\ubcf8\ub2e4\uba74 \ud655\ub960\ubcc0\uc218 X\uc758 cdf\ub294 Unif(0,1)\uc744 \ub530\ub978 \ub2e4\ub294 \ub73b\uc774\ub2e4. \ud568\uc218 F\uac00 strictly increasing function\uc774\ub77c\uace0 \uac00\uc815\uc744 \ud55c \ud6c4, \ud655\ub960\ubcc0\uc218 X\uac00 \ud568\uc218 F\ub97c cdf\ub85c \uac16\ub294\uc9c0 \ud655\uc778\ud558\uc5ec\ubcf4\uc790. \\[Note\\ that\\ the\\ distribution\\ function\\ of\\ U\\ is\\ F_U(u)=u, u \\in (0,1)\\] \\[We\\ assume\\ that\\ F(x)\\ is\\ strictly\\ increasing, \\] \\[P[X \\leq x] = P[F^{-1}(U) \\leq x] = P[U \\leq F(x)] = F(x)\\] \uc790 \uc774\ub807\uac8c \ud655\ub960\ubcc0\uc218 X\uac00 \ub204\uc801\ud655\ub960\ubd84\ud3ec\ud568\uc218\ub85c F\ub97c \uac16\ub294\ub2e4\ub294 \uc99d\uba85\uc774 \ub05d\ub0ac\ub2e4. (?!) \ub9c8\uc9c0\ub9c9 \ub4f1\ud638\uac00 \uc131\ub9bd\ud558\ub294 \uc774\uc720\ub294 \ud655\ub960\ubcc0\uc218 U\uac00 \\(Unif(0,1)\\) \uc744 \ub530\ub974\uae30 \ub54c\ubb38\uc774\ub2e4. \\(Unif(0,1)\\) \uc774\ub77c\ub294 \ud655\ub960\ubcc0\uc218\ub294 \uba74\uc801\uc774 1\uc778 \uc815\uc0ac\uac01\ud615\uc548\uc5d0\uc11c \uc790\uc720\ub86d\uac8c \ub3cc\uc544\ub2e4\ub2c8\ub294 \ub188\uc774\ubbc0\ub85c, \uadf8 \uc0ac\uac01\ud615 \uc548\uc5d0\uc11c \\(F(x)\\) \ubcf4\ub2e4 \uc791\uc744 \ud655\ub960\uc774\ub77c \ud568\uc740 \uade0\ub4f1\ubd84\ud3ec\uc758 \ud2b9\uc9d5\uc73c\ub85c \uc778\ud574 x\ucd95\uacfc y\ucd95\uc774 \uac01\uac01 \\((F(x),F(x))\\) \uc778 \uc9c0\uc810\uc73c\ub85c \ub458\ub7ec\uc313\uc778 \uc601\uc5ed\uc774\uae30 \ub54c\ubb38\uc5d0 \\(P[U \\leq F(x)] = F(x)\\) \uac00 \ub418\ub294 \uac83\uc774\ub2e4. \uc704\uc758 \uc2dd\uc744 \uc0b4\uc9dd \ud2c0\uc5b4\uc11c \uc0dd\uac01\ud574\ubcf4\uba74 \\(U = F(X)\\) \uc774\uba70, \uc774\ub294 \ud655\ub960\ubcc0\uc218 X\uac00 \ub204\uc801\ubd84\ud3ec\ud568\uc218\ub85c F\ub97c \uac16\ub294\ub2e4\uace0 \ud558\uc600\ub294\ub370 \uadf8 \ub204\uc801\ubd84\ud3ec\ud568\uc218\uac00 U\ub77c\ub294 \uac83\uc774\ub2e4. \uc989 \uc5b4\ub5a4 \ud655\ub960\ubcc0\uc218 X\uc758 cdf\uac00 \\(Unif(0,1)\\) \uc744 \ub530\ub978\ub2e4\ub294 \uac83\uc774\ub2e4. \uc2e4\uc81c\ub85c \uc9c1\uc811 \uac04\ub2e8\ud55c \uacc4\uc0b0\uc744 \ud574\ubcf4\uba74 \\(P[F(X) \\leq x]= x\\) \ub97c \ud1b5\ud574 F(X)\uc758 cdf\uac00 x\uc774\ubbc0\ub85c \uc774\ub97c x\uc5d0 \ub300\ud574\uc11c \ubbf8\ubd84\ud55c 1\uc740 \\(Unif(0,1)\\) \uc758 pdf\ub77c\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \\(function\\ F\\) \ub294 strictly increasing\ud558\ub294 \ud568\uc218\ub85c \uac00\uc815\ud558\uc600\uae30\uc5d0, F\ub294 \ud56d\uc0c1 Inverse function\uc774 \uc874\uc7ac\ud55c\ub2e4. \uace0\ub85c \uc6b0\ub9ac\ub294 \uc5b4\ub5a0\ud55c \ud655\ub960\ubcc0\uc218 X\uc758 \ub09c\uc218\ub97c \uc0dd\uc131\ud558\uace0\uc790 \ud560\ub54c, \uadf8\uc800 \\(F(\\cdot)\\) \uc744 \uc54c\uace0, \uc774\uc758 \uc5ed\ud568\uc218\ub97c \uad6c\ud560 \uc218 \uc788\ub2e4\uba74, Uniform(0,1)\uc744 \uc774\uc6a9\ud558\uc5ec \uc0dd\uc131\uc774 \uac00\ub2a5\ud558\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc77c\ub840\ub85c Uniform(0,1)\ubd84\ud3ec\ub97c \ud1b5\ud574 \uc9c0\uc218\ubd84\ud3ec \ub09c\uc218 X\ub97c \uc0dd\uc131\ud558\uace0 \uc2f6\ub2e4\uba74, \\(X = -log(1-u)\\) \ub97c \ud1b5\ud574 \uad6c\ud558\ub294 \uac83\uc774 \uac00\ub2a5\ud558\ub2e4. 1-b. Monte Carlo Integration \ubaac\ud14c\uce74\ub97c\ub85c \ubc29\ubc95\uc740 \uc801\ubd84\uacfc\uc815\uc5d0\uc11c\ub3c4 \uc801\uc6a9\ub420 \uc218 \uc788\ub2e4. \uc6b0\ub9ac\uac00 \\(\\int_a^bg(x)dx\\) \ub97c \uad6c\ud558\ub824 \ud560 \ub54c, \ud568\uc218 g\uc758 anti-derivative\uac00 \uc874\uc7ac\ud558\uc9c0 \uc54a\uac70\ub098 \uc774\ub97c \uc54c \uc218 \uc5c6\ub294 \uacbd\uc6b0\uac00 \uc774\uc5d0 \ud574\ub2f9\ud55c\ub2e4. Monte Carlo Integration\uc744 \uc0ac\uc6a9\ud558\uba74 \uc704\uc758 \uc801\ubd84\uc744 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\[\\int_a^bg(x)dx = (b-a)\\int_a^bg(x)\\cdot \\frac{1}{b-a}dx= (b-a)E[g(x)],\\ where\\ X\\sim Unif(a,b)\\] \uc6b0\ub9ac\ub294 \\(X_1, X_2,..,X_n\\) \ub4e4\uc758 \ub79c\ub364\uc0d8\ud50c\uc744 uniform(a,b)\uc758 \ubd84\ud3ec\uc5d0\uc11c \ucd94\ucd9c\ud558\uc5ec \uc0dd\uc131\ud560 \uc218 \uc788\uc73c\uba70, \\(Y_i = (b-a)g(X_i)\\) \ub97c \uacc4\uc0b0\ud558\uba74 \ub41c\ub2e4. \uadf8\ub807\ub2e4\uba74, \\(\\bar{Y}\\) \ub294 \\(\\int_a^bg(x)dx\\) \uc758 consistent estimator\uac00 \ub41c\ub2e4. $* \\frac{\\sum_{i=1}^ng(x_i)}{n} \\xrightarrow{P} E[g(X_1)]\\ by\\ WLLN $ 2. Bootstrap Procedures Bootstrap\uae30\ubc95\uc744 \ub2e4\ub8e8\uae30 \uc55e\uc11c\uc11c \uc6b0\ub9ac\uac00 \uae30\ubcf8\uc801\uc73c\ub85c \ub2e4\ub8e8\ub294 \ud1b5\uacc4\uc801\uc778 \ucd94\uc815\uc758 \ub2e8\uacc4\ub97c \uac04\ub2e8\ud558\uac8c \uc77d\uac08\ubb34\ub9ac\ub97c \ud574\ubcf4\uc790. \\(\\mathbf{X}'=(X_1,...,X_n)\\) \ub97c cdf \\(F(x;\\theta)\\) \uc5d0\uc11c \uc5bb\uc740 random sample\uc774\ub77c\uace0 \uac00\uc815\ud574\ubcf4\uc790. \uadf8\ub807\ub2e4\uba74 \\(\\mathbf{x}'=(x_1,...,x_n)\\) \uc740 random sample \\(\\mathbf{X}'\\) \uc5d0\uc11c \uad00\uce21\ub41c \uac12 observed values\uac00 \ub41c\ub2e4. \uc5ec\uae30\uc11c \ud1b5\uacc4\uc801\uc778 \ucd94\uc815\uc744 \ud568\uc5d0 \uc788\uc5b4 \uc6b0\ub9ac\ub294 \ubaa8\uc218 \\(\\theta\\) \uc5d0 \ub300\ud55c estimationd\uc744 \ud558\ub294 \uac83\uc5d0 \uc8fc\ubaa9\ud55c\ub2e4. \uadf8\ub9ac\uace0 \uc6b0\ub9ac\ub294 \uc774 \ubaa8\uc218 \\(\\theta\\) \ub97c \ud5a5\ud574 \uad6c\uac04\ucd94\uc815\uacfc \uc810\ucd94\uc815\uc744 \ud1b5\ud574 \uc804\uc9c4\ud55c\ub2e4. \\(\\theta\\) \uc5d0 \ub300\ud55c \ucd94\uc815\ub7c9\uc744 \\(\\hat\\theta_n\\) \uc774\ub77c \ud558\uc790. \uc77c\ubc18\uc801\uc778 \ubc29\ubc95\uc73c\ub85c\ub294, \uc6b0\ub9ac\ub294 sample size n\uc744 \ubb34\ud55c\ud788 \ub298\ub824 \uadfc\uc0ac\ubd84\ud3ec\ub97c \ucd94\uc815\ud558\uace0, \uc774 \uadfc\uc0ac\ubd84\ud3ec\uc758 \ubaa8\uc218\uac12\uc73c\ub85c \uae30\uc874 \ubaa8\uc218\uc778 \\(\\theta\\) \ub97c \ucd94\uc815\ud55c\ub2e4. \ub9cc\uc57d \uc6b0\ub9ac\uac00 \uc544\ub798\uc640 \uac19\uc740 \uad00\uacc4\uc2dd\uc744 \uc720\ub3c4\ud560 \uc218 \uc788\ub2e4\uba74, \\[\\sqrt{n}(\\hat\\theta_n-\\theta) \\xrightarrow{D} N(0, \\sigma^2),\\ and\\ \\hat\\sigma^2 \\xrightarrow{P} \\sigma^2\\] $$(\\hat\\theta_n-1.96\\frac{\\hat\\sigma}{\\sqrt{n}},\\ \\hat\\theta_n+1.96\\frac{\\hat\\sigma}{\\sqrt{n}}) $$ \uc704\uc640 \uac19\uc740 \uc2e0\ub8b0\uad6c\uac04\uc774 \ubaa8\uc218 \\(\\theta\\) \uc5d0 \ub300\ud55c \\(100(1-\\alpha)\\) % \uadfc\uc0ac\uc2e0\ub8b0\uad6c\uac04\uc774 \ub41c\ub2e4. \ud558\uc9c0\ub9cc, \uc2e4\uc81c\ub85c\ub294 \uc704\uc758 \uadfc\uc0ac\uc801\uc778 \uc811\uadfc\ubc29\uc2dd\uc744 \uc801\uc6a9\ud558\uae30\uc5d0 \ub9e4\uc6b0 \uc5b4\ub824\uc6b4 \uacbd\uc6b0\uac00 \ub9ce\ub2e4. sample size\ub97c \ubb34\ud55c\ud788 \ub298\ub9ac\ub294 \uc77c\uc740 \uc774\ub860\uc801\uc73c\ub85c\ub294 \uac00\ub2a5\ud558\uc9c0\ub9cc, \ud604\uc2e4\uc5d0\uc11c \uc801\uc6a9\ud558\uae30\uc5d0\ub294 \uc5b4\ub824\uc6b4 \uacbd\uc6b0\uac00 \ub9ce\uc73c\uba70, \uadfc\uc0ac\ubd84\ud3ec\ub97c \ucc3e\uae30 \ub9e4\uc6b0\uc5b4\ub824\uc6b4 \uacbd\uc6b0\ub294 \uc801\uc6a9\ud560 \uc218\uac00 \uc5c6\ub2e4. \uc6b0\ub9ac\uac00 \ucd94\uc815\ub7c9 \\(\\hat\\theta_n\\) \uc5d0 \ub300\ud55c \ubd84\ud3ec\ub97c \uc54c\uc9c0 \ubabb\ud558\uc9c0\ub9cc, \uc544\ub798\uc640 \uac19\uc774 \ub9cc\uc57d \ub3d9\uc77c\ud55c \uc0ac\uc774\uc988\uc758 \ubb34\ud55c\ud55c random sample\uc744 \uac16\uace0 \uc788\ub2e4\uace0\ud55c\ub2e4\uba74, $ \\(\\mathbf{x_1}'=(x_{11},x_{12},...,x_{1n})\\) $ $ \\(\\mathbf{x_1}'=(x_{21},x_{22},...,x_{2n})\\) $ $ \\(.\\) $ $ \\(.\\) $ $ \\(.\\) $ $ \\(from\\ a\\ cdf\\ F(x;\\theta)\\) $ \uc6b0\ub9ac\ub294 \\(\\hat\\theta_1 = \\hat\\theta(\\mathbf{x_1}'),..,\\) \ub97c \uacc4\uc0b0\ud560 \uc218 \uc788\uc73c\uba70, \uc774\ub4e4\uc758 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \ub9cc\ub4e4 \uc218\uac00 \uc788\ub2e4. \uadf8\ub7ec\uba74 \uc989 \uc774 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc740 \\(\\hat\\theta\\) \uc758 \ubd84\ud3ec\uc640 \uac19\uac8c \ub41c\ub2e4. \ud558\uc9c0\ub9cc \ubb34\ud55c\ud55c random sample\uc774 \uc544\ub2c8\ub77c \ub2e8 \ud55c\uac1c\uc758 sample\ub9cc \uc874\uc7ac\ud560 \ub54c \ubb38\uc81c\uc810\uc774 \uc0dd\uae34\ub2e4. \uc774\ub7f4 \ub54c \uc801\uc6a9\ud558\ub294 \uac83\uc774 Bootstrap Procedure \uc774\ub2e4. \uc774\ub294 \uac04\ub2e8\ud558\uac8c original sample\uc744 \uacc4\uc18d \ubcf5\uc6d0\ucd94\ucd9c\ud558\uc5ec sample\uc744 \uc5ec\ub7ec\uac1c \ubcf5\uc81c\ud558\ub294 \uac83\uacfc \uac19\ub2e4. resampling\ub41c \uc0c8\ub85c\uc6b4 \uc0d8\ud50c\ub4e4\uc744 \\(\\mathbf{x^{*}_1}'=(x_{11}^*,...,x_{1n}^*), \\mathbf{x^{*}_2}',...,\\mathbf{x^{*}_B}'\\) \uc774\ub77c Notating\ud55c\ub2e4\uba74, ( \uc5ec\uae30\uc11c B\ub294 \ubcf5\uc6d0\ucd94\ucd9c\ud558\uc5ec \ub9cc\ub4e0 \uc0d8\ud50c\uc758 \uac2f\uc218\ub97c \uc758\ubbf8 ) \uc6b0\ub9ac\ub294 \\(\\hat\\theta_1^*,\\hat\\theta_2^*,..,\\hat\\theta_B^*\\) \uc758 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \uadf8\ub9b4 \uc218 \uc788\uc73c\uba70, \uc774\ub860\uc801\uc73c\ub85c \uc774 \ubcf5\uc81c\ub41c \uc0d8\ud50c\uc758 \ucd94\uc815\ub7c9\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c4 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc740 \uadfc\uc0ac\uc801\uc73c\ub85c \\(\\hat\\theta\\) \uc758 distribution\uacfc \ub3d9\uc77c\ud558\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c \ucef4\ud4e8\ud130 \ud504\ub85c\uadf8\ub798\ubc0d\uc758 \uc2dc\ubbac\ub808\uc774\uc158\uc744 \ud1b5\ud574\uc11c\ub294 \uc27d\uac8c sample\uc758 \uc0ac\uc774\uc988\ub97c \ub298\ub9b4 \uc218 \uc788\uace0, n\uc774 \uc5b4\ub290\uc815\ub3c4\ub9cc \ucee4\uc9c4\ub2e4\uba74 CLT\ub97c \ud1b5\ud574\uc11c \ud45c\ubcf8\ud3c9\uade0\uc758 \uadfc\uc0ac\ubd84\ud3ec\ub97c \uc27d\uac8c \uad6c\ud560 \uc218 \uc788\uc9c0\ub9cc, Bootstrap \uae30\ubc95\uc744 \uc774\uc6a9\ud558\uc5ec \uc5bb\uc740 \uadfc\uc0ac\uc2e0\ub8b0\uad6c\uac04\uacfc CLT\ub97c \ud1b5\ud574\uc11c \ub3c4\ucd9c\ud560 \uc218 \uc788\ub294 \uadfc\uc0ac\uc2e0\ub8b0\uad6c\uac04\uc744 \ube44\uad50\ud574\ubcf4\ub294 \ubc29\ubc95\uc744 \ud1b5\ud574 Bootstrap Procedure \uc2dc\ubbac\ub808\uc774\uc158\uc744 \uc9c4\ud589\ud574\ubcf4\uaca0\ub2e4. \ub610\ud55c rough\ud55c \ubc29\ubc95\uc774\uae34 \ud558\uc9c0\ub9cc, MASS package\uc5d0 \ub0b4\uc7a5\ub418\uc5b4\uc788\ub294 fitdistr() \ud568\uc218\ub97c \ud1b5\ud558\uc5ec, Bootstrap sampling\uc73c\ub85c \uc5bb\uc740 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \ud45c\uc900\ud654\uc2dc\ud0a8 \uacb0\uacfc\uac00 \ud45c\uc900\uc815\uaddc\ubd84\ud3ec\uc640 \uadfc\uc0ac\ud55c \ud615\ud0dc\ub97c \ub744\ub294\uc9c0\ub97c \ud655\uc778\ud558\ub294 \uac83\uc73c\ub85c \uc2dc\ubbac\ub808\uc774\uc158\uc744 \uc9c4\ud589\ud558\uaca0\ub2e4. Resampling\uacfc\uc815\uc740 3000\ubc88\uc744 \uc9c4\ud589\ud558\uc600\ub2e4.(B=3000) Bootstrap with One sample from \\(\\Gamma(1,100)\\) #Lets do the Bootstrap! sample <- rgamma(30,1,1/100) B <- 3000 alpha <- 0.05 m <- (alpha/2)*B m ## [1] 75 bootstrap_samplemean <- rep(0,B) for (i in 1:B){ bootstrap_samplemean[i] <- mean(sample(sample,replace=T)) } head(sort(bootstrap_samplemean),50) ## [1] 69.34995 69.38044 70.32409 71.08792 71.68891 71.95984 72.12971 ## [8] 72.62477 72.71791 72.80969 72.95041 73.15467 74.10887 74.16133 ## [15] 74.52411 74.56133 75.18926 75.27353 75.30119 75.33049 75.68648 ## [22] 75.76593 75.76702 75.78427 76.04546 76.26449 76.33295 76.66625 ## [29] 76.76444 76.78385 76.84236 76.86567 77.12012 77.12724 77.26601 ## [36] 77.36715 77.41874 77.48135 77.59661 77.61867 77.67783 77.87053 ## [43] 77.94403 78.09464 78.10425 78.16664 78.25914 78.55849 78.66119 ## [50] 78.77573 hist(sort(bootstrap_samplemean), xlab='sample mean from bootstrap',main='') Lower_Bound <- sort(bootstrap_samplemean)[order(sort(bootstrap_samplemean))==m] Upper_Bound <- sort(bootstrap_samplemean)[order(sort(bootstrap_samplemean))==B+1-m] Lower_Bound ## [1] 81.12866 Upper_Bound ## [1] 142.9677 percentile_bootstrap_CI <- paste('(','81.12866',',','142.9677',')') Theoretical_Lower_Bound <- mean(sample)-1.96*(sqrt(var(sample))/sqrt(30)) Theoretical_Upper_Bound <- mean(sample)+1.96*(sqrt(var(sample))/sqrt(30)) Theoretical_Lower_Bound ## [1] 79.38917 Theoretical_Upper_Bound ## [1] 142.0627 Theoretical_CI <- paste('(','79.38917',',','142.0627',')') percentile_bootstrap_CI ## [1] \"( 81.12866 , 142.9677 )\" Theoretical_CI ## [1] \"( 79.38917 , 142.0627 )\" library(MASS) x <- sort(bootstrap_samplemean) fitdistr( ((x-mean(x))/sqrt(var(x))), 'Normal') ## mean sd ## 2.274520e-16 9.998333e-01 ## (1.825438e-02) (1.290779e-02) \uc704\uc758 \ucf54\ub4dc\ub97c \ud1b5\ud574, 3000\ubc88\uc758 resampling\uc744 \uc218\ud589\ud558\uc5ec \uc5bb\uc740 Bootstrap \ud45c\ubcf8\ud3c9\uade0\ub4e4\uc758 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \ubcf4\uba74 \uc774\ub860\uc801\uc73c\ub85c \ub098\uc640\uc57c\ud558\ub294 \ud45c\ubcf8\ud3c9\uade0\uc758 \uadfc\uc0ac\ubd84\ud3ec\uc778 \uc815\uaddc\ubd84\ud3ec\ub97c \uc798 \ub530\ub974\ub294\uac83\uc73c\ub85c \ubcf4\uc778\ub2e4. \uc0ac\uc2e4 \uc704\uc758 \uc2dc\ubbac\ub808\uc774\uc158\uc740 \ub108\ubb34\ub098\ub3c4 \uc26c\uc6b4 \uc2dc\ubbac\ub808\uc774\uc158\uc5d0 \uc18d\ud558\uace0, \uc6b0\ub9ac\uac00 \uc774\ubbf8 \uc774\ub860\uc801\uc778 \uacb0\uacfc\uac12\uc744 \ub2e4 \uc54c\uace0 \uc788\uae30 \ub54c\ubb38\uc5d0 \uac00\ubccd\uac8c Bootstrap\uc774 \uc81c\ub300\ub85c \uae30\ub2a5\uc744 \ud558\ub294\uc9c0 \ud655\uc778\uc6a9\uc73c\ub85c\ub9cc \ucc38\uace0\ud558\uba74 \uc88b\uc744 \uac83 \uac19\ub2e4. Bootstrap percentile\uc5d0 \ub530\ub978 \uadfc\uc0ac\uc2e0\ub8b0\uad6c\uac04\uc744 \uad6c\ud55c\uacb0\uacfc \ub610\ud55c CLT\ub97c \ud1b5\ud574 \uc5bb\uc740 \uadfc\uc0ac\uc2e0\ub8b0\uad6c\uac04\uacfc \ub9e4\uc6b0 \uc720\uc0ac\ud55c \ubc94\uc704\ub97c \ub098\ud0c0\ub0b4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.","title":"R Notebook"},{"location":"06%20Simulation%20Practice%20and%20Theory/01_Monte_Carlo___Bootstrap/#introduction-of-monte-carlo-bootstrap-procedures","text":"","title":"Introduction of Monte Carlo &amp; Bootstrap Procedures"},{"location":"06%20Simulation%20Practice%20and%20Theory/01_Monte_Carlo___Bootstrap/#1-method-of-monte-carlo","text":"","title":"1. Method of Monte Carlo"},{"location":"06%20Simulation%20Practice%20and%20Theory/01_Monte_Carlo___Bootstrap/#1-a-concept-and-generating-random-numbers","text":"\uc694\uc998 \ud604\ub300 \ud1b5\uacc4\ud559\uc5d0\uc11c\ub294 Asymptotic techniques\uac00 \uad11\ubc94\uc704\ud558\uac8c \uc801\uc6a9\ub418\uace0 \uc788\ub2e4. \ud5c8\ub098 \uc774 \uadfc\uc0ac\uc801 \uc811\uadfc\ubc29\ubc95\uc740 sample size\uac00 \ubb34\ud55c\ud560 \ub54c\ub9cc \uadf9\ud55c\ubd84\ud3ec\ub97c \uc5bb\uc5b4\ub0bc \uc218 \uc788\ub2e4. \uc2e4\uc81c\ub85c \uc774\ub7ec\ud55c size\uc5d0 \ub300\ud55c \uc81c\uc57d\uc73c\ub85c \uc778\ud574 Asymptotic techniques\ub294 \uc5ec\ub7ec\uac00\uc9c0 \ubb38\uc81c\uc810\uc744 \uac16\uace0 \uc788\ub2e4. \ud604\uc2e4\uc5d0\uc11c\ub294 sample size\uac00 \uc720\ud55c\ud558\ub2e4\ub294 \uac83. \uadf8\ub807\uae30\uc5d0 \uc720\ud55c\uc0d8\ud50c\uc744 \uc774\uc6a9\ud558\uc5ec \ud1b5\uacc4\ubc29\ubc95\ub860\uc801 empirical check\ub85c \uc811\uadfc\uc744 \ud55c\ub2e4. \uac00\ub054 \uc5f0\uc0b0\uc774 \ub108\ubb34 \ubcf5\uc7a1\ud558\uc5ec limiting distribution\uc744 \uad6c\ud560 \uc218\uac00 \uc5c6\ub2e4. \uc704\uc640 \uac19\uc740 \ubb38\uc81c\uc810\uc73c\ub85c \uc778\ud574 \uadfc\uc0ac\uc801 \uc811\uadfc\uc774 \uc5b4\ub824\uc6b8 \ub54c \uc6b0\ub9ac\ub294 \uadf8 \ub300\uc548\uc73c\ub85c Monte Carlo \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud55c\ub2e4. \uc774\ub294 \ud2b9\uc815\ud55c \ubd84\ud3ec \ud639\uc740 \uc0d8\ud50c\ub85c\ubd80\ud130 \uad00\uce21\uce58\ub4e4\uc744 \uc0dd\uc131\ud558\ub294 \ubc29\ubc95\uc744 \ub9d0\ud55c\ub2e4. \uc774 \ubc29\ubc95\uc744 \uc704\ud574 \uc6b0\ub9ac\uac00 \ud544\uc694\ud55c \uac83\uc740 \\(Unif(0,1)\\) \uc758 \ubd84\ud3ec\uc5d0\uc11c \ub09c\uc218\ub97c \uc0dd\uc131\ud558\ub294 \uac83\uc774\ub2e4. \uacfc\uac70\uc5d0 \ucef4\ud4e8\ud130\uac00 \ubc1c\uc804\ud558\uc9c0 \uc54a\uc558\uc744 \ub54c\ub294 \uc774\ub7ec\ud55c \ub09c\uc218\ub97c \uc0dd\uc131\ud558\ub294 \uac83\uc774 \ub2e4\uc18c \ubcf5\uc7a1\ud558\uc600\uc9c0\ub9cc, \uc694\uc998\uc740 statistical software pacakages\ub97c \ud1b5\ud574 \uc27d\uac8c \\(Unif(0,1)\\) \uc744 \ub530\ub974\ub294 \ub09c\uc218\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\ub2e4.","title":"1-a. Concept and Generating Random Numbers"},{"location":"06%20Simulation%20Practice%20and%20Theory/01_Monte_Carlo___Bootstrap/#unif01","text":"\uc774\ub97c \ud480\uc5b4\uc11c \uc124\uba85\ud55c\ub2e4\uba74, \uade0\ub4f1\ubd84\ud3ec\uc5d0 \ud2b9\uc815\ud55c \uaddc\uce59\uc744 \uc801\uc6a9\ud558\uc5ec \ub09c\uc218\ub97c \uc0dd\uc131\ud55c \ub4a4, \uc774 sample\ub4e4\uc744 \uc6b0\ub9ac\uc758 \ubaa9\ud45c \uc774\uc0b0\ud615\ubd84\ud3ec\uc5d0\uc11c \ucd94\ucd9c\ud55c \ub09c\uc218\ucc98\ub7fc \ub9cc\ub4dc\ub294 \uac83\uc774\ub77c\uace0 \ubcfc \uc218 \uc788\uc744 \uac83\uc774\ub2e4. \uadf8 \uc608\ub85c \\(B(10,1/3)\\) \uc758 \ubd84\ud3ec\ub97c \ub530\ub974\ub294 \ub09c\uc218\ub97c \uc0dd\uc131\ud574\ubcf4\uc790. \ub9cc\uc57d \\(U\\) \uac00 \\(Unif(0,1)\\) \uc744 \ub530\ub974\ub294 \ud655\ub960\ubcc0\uc218\ub77c \ud558\uba74, \ud655\ub960\ubcc0\uc218 \\(X\\) \ub294 \uc544\ub798\uc640 \uac19\uc774 \ub9cc\ub4e4\uc5b4 \ubcfc \uc218\uac00 \uc788\ub2e4. $ \\(X= \\begin{cases} 1\\ if\\ 0< U \\leq 1/3\\\\ 0\\ if\\ 1/3 \\leq U < 1 \\end{cases}\\) $ set.seed(2013122059) n <- 10 U <- runif(n,0,1) X <- rep(0,n) for (i in 1:10){ if (U[i] <=1/3){X[i]=1} } U ## [1] 0.56813976 0.35981586 0.49809107 0.29650199 0.50092175 0.16731997 ## [7] 0.52250479 0.84050584 0.06741438 0.27603992 X ## [1] 0 0 0 1 0 1 0 0 1 1 \uc704\uc758 \uaddc\uce59\uc744 \ud1b5\ud574 \uac04\ub2e8\ud558\uac8c cutoff\ub97c 1/3\ub85c \uc815\ud558\uc5ec 1/3\uc744 \ub118\uc9c0 \ubabb\ud558\uba74 X\ub294 1\ub85c, \uadf8\ub807\uc9c0 \uc54a\ub2e4\uba74 0\uc73c\ub85c \ubcc0\ud658\ud574\uc8fc\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud588\ub2e4. \uadf8\ub9ac\ud558\uc5ec U\uc640 X\uc758 \uac12\uc744 \uc0b4\ud3b4\ubcf4\uba74, cutoff\ub97c \ub118\uc9c0 \ubabb\ud558\ub294 U\uc5d0 \ub300\ud574\uc11c\ub294 X\uac00 1\ub85c \ubc18\ud658\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc774\ub97c \uc870\uae08 \uc9c1\uad00\uc801\uc73c\ub85c \uc124\uba85\ud558\uba74 \uc544\ub798\ucc98\ub7fc \ud480\uc5b4 \uc4f8 \uc218 \uc788\ub2e4. \\(Bin(10,1/3)\\) \uc744 \ub530\ub974\ub294 \ubd84\ud3ec\ub77c\ub294 \uac83\uc740, 1/3\uc758 \ud655\ub960\ub85c event\uac00 \uc131\uacf5\uc744\ud558\uace0, 2/3\uc758 \ud655\ub960\ub85c \uc2e4\ud328\ub97c \ud55c\ub2e4\ub294 \uac83\uc774\uace0, \uc774\ub97c \ub192\uc774\uc640 \ubc11\ubcc0\uc758 \uae38\uc774\uac00 \uac01\uac01 1\uc778 \uc815\uc0ac\uac01\ud615\uc758 \uba74\uc801\uc5d0\uc11c( \\(Unif(0,1)\\) ) 1/3\uc5d0 \ud574\ub2f9\ud558\ub294 \uba74\uc801\uc744 \uce60\ud558\uace0, \uce60\ud55c \ubd80\ubd84\uc740 1(\uc989 \uc774\ud56d\ubd84\ud3ec\uc5d0\uc11c \uc131\uacf5\uc744 \uc758\ubbf8)\ub85c \ubc18\ud658\ud558\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud55c \uac83\uc774\ub77c\uace0 \ubcfc \uc218 \uc788\ub2e4. \uc5ec\uae30\uc5d0 \uc870\uae08 \ub354 \ub098\uc544\uac04 \uc608\uc81c\ub85c, \uc11c\ub85c iid\uc778 \ub450\uac1c\uc758 Uniform \ubd84\ud3ec\ub97c \uc774\uc6a9\ud558\uc5ec \\(\\pi\\) \uc758 \uac12\ub3c4 \ucd94\uc815\uc744 \ud574\ubcfc \uc218 \uc788\ub2e4. \\(\\pi\\) \ub97c \ucd94\uc815\ud558\ub294 \ubc29\ubc95\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \\[X= \\begin{cases} 1\\ \\ \\ if\\ U_1^2+U_2^2 <1 \\\\ 0\\ \\ \\ o.w \\end{cases}\\] \uc5ec\uae30\uc11c \\(U_1\\) \uacfc \\(U_2\\) \ub294 \uac01\uac01 \\(Unif(0,1)\\) \uc758 \ubd84\ud3ec\ub97c \ub530\ub978\ub2e4. \\(X\\) \uc758 \ud3c9\uade0\uc744 \uc5ec\uae30\uc11c \uad6c\ud574\ubcf8\ub2e4\uba74, $E(X)= \\sum x\\cdot p(x)= 1\\cdot p(x=1) + 0\\cdot p(x=0) $ \uc774\ubbc0\ub85c \\(1\\cdot\\frac{\\pi}{4}+0\\cdot (1-\\frac{\\pi}{4})= \\frac{\\pi}{4}\\) \uac00 \ub41c\ub2e4. \uace0\ub85c \\(\\mu= E(X) = \\pi/4\\) \uc774\uba70, Weak Law of Large Numbers\uc5d0 \uc758\ud574\uc11c \\(4E(\\bar{X})\\) \ub294 \\(\\pi\\) \uc758 consistent estimator\uac00 \ub41c\ub2e4. \uc2e4\uc81c\ub85c n\uc758 \uc0ac\uc774\uc988\ub97c \uc810\uc810 \ud0a4\uc6b0\uac8c \ub418\uba74, \\(4E(\\bar{X})\\) \ub294 \uc810\uc810 \\(\\pi\\) \uac12\uc73c\ub85c \uc218\ub834\ud558\uac8c \ub428\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc704\uc5d0\uc11c \uc5b8\uae09\ud55c \ub0b4\uc6a9\uc744 \uc791\uc131\ud55c \ucf54\ub4dc\ub294 \uc544\ub798\uc640 \uac19\ub2e4. set.seed(2013122059) iteration <- c(30,100,1000,5000,1000000) mean_convergence<- rep(0,5) CI_length_convergence <- rep(0,5) for (i in 1:length(iteration)){ U1 <- runif(iteration[i],0,1) U2 <- runif(iteration[i],0,1) X <- rep(0,iteration[i]) for ( j in 1:iteration[i]) if (U1[j]^2+U2[j]^2 <=1){X[j]=1} mean_convergence[i] <- 4*mean(X) CI_length_convergence[i] <- ((1.96*4*mean(X)*(1-mean(X)))/iteration[i]) } mean_convergence ## [1] 2.666667 2.800000 3.152000 3.138400 3.142284 CI_length_convergence ## [1] 5.807407e-02 1.646400e-02 1.309719e-03 2.649965e-04 1.320642e-06 Sample size\uac00 \uc810\uc810 \ucee4\uc9c8\uc218\ub85d, \uc2e0\ub8b0\uad6c\uac04\uc758 \ub108\ube44\ub294 \uc9e7\uc544\uc9c0\uace0 \ud3c9\uade0 \ub610\ud55c \\(\\pi\\) \uac12\uc73c\ub85c \uc810\uc810 \uac00\uae4c\uc6cc\uc9d0\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\ub2e4. Theorem 4.8.1 \uc704\uc5d0\uc11c \uc6b0\ub9ac\ub294 \ubaac\ud14c\uce74\ub97c\ub85c \ubc29\ubc95\uc758 \uac04\ub2e8\ud55c \uc608\uc81c\uc640 \ub79c\ub364\ub09c\uc218\uc0dd\uc131\uc5d0 \ub300\ud574\uc11c \ubc30\uc6e0\ub2e4. \uc2dc\ubbac\ub808\uc774\uc158\uc744 \ud589\ud560 \ub54c, \uc6b0\ub9ac\ub294 \ud2b9\uc815\ud55c \ubd84\ud3ec\ub97c \uac00\uc9c4 \uc0d8\ud50c\ub4e4\uc744 \ucd94\ucd9c\ud558\ub294 \uacbd\uc6b0\uac00 \ub9ce\ub2e4. \ud558\uc9c0\ub9cc \ub2e8\uc21c\ud788 rexp(), rnorm(), rpois() \ub4f1\uc758 \ub09c\uc218\uc0dd\uc131 \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc544\ub2c8\ub77c \\(Unif(0,1)\\) \ubd84\ud3ec\ub97c \ud1b5\ud574 \ud2b9\uc815 \ubd84\ud3ec\uc5d0\uc11c \uc0d8\ud50c\ub9c1\ud558\ub294 \ud6a8\uacfc\ub97c \ub0b4\ub294 \uac83\uc744 \ubc30\uc6e0\ub2e4. \uc774\ub807\uac8c \uade0\ub4f1\ubd84\ud3ec\ub97c \ud1b5\ud574 \ud0c0\ubd84\ud3ec\uc758 \ub09c\uc218 \uc0dd\uc131\uc774 \uac00\ub2a5\ud55c \uac83\uc740 \ubc14\ub85c \uc774 Theorem 4.8.1 \uc5d0 \ub2f4\uaca8\uc788\ub2e4. Theorem\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(U \\sim Unif(0,1)\\) , Let F be a continuous distribution function., Then \\(X= F^{-1}(U)\\) has distribution function F \ub300\ucda9 \ud574\uc11d\uc744 \ud574\ubcf4\uba74 U\uac00 \uade0\ub4f1\ubd84\ud3ec\ub97c \ub530\ub974\uace0, \ud568\uc218 F\ub97c \uc5b4\ub5a0\ud55c \ub204\uc801\ubd84\ud3ec\ud568\uc218\ub77c\uace0 \ud558\uc600\uc744 \ub54c, \ud655\ub960\ubcc0\uc218 X\ub294 \ud568\uc218 F\ub97c \ub204\uc801\ud655\ub960\ubd84\ud3ec\ub85c \uac16\ub294\ub2e4 \ub77c\ub294 \ub73b\uc774\ub2e4. \ud574\uc11d\uc744 \ud574\ub3c4 \uc804\ud600 \uac10\uc774 \uc624\uc9c0 \uc54a\uc9c0\ub9cc, \ub2e4\uc2dc \uc774\ud574\ub97c \ud574\ubcf8\ub2e4\uba74 \ud655\ub960\ubcc0\uc218 X\uc758 cdf\ub294 Unif(0,1)\uc744 \ub530\ub978 \ub2e4\ub294 \ub73b\uc774\ub2e4. \ud568\uc218 F\uac00 strictly increasing function\uc774\ub77c\uace0 \uac00\uc815\uc744 \ud55c \ud6c4, \ud655\ub960\ubcc0\uc218 X\uac00 \ud568\uc218 F\ub97c cdf\ub85c \uac16\ub294\uc9c0 \ud655\uc778\ud558\uc5ec\ubcf4\uc790. \\[Note\\ that\\ the\\ distribution\\ function\\ of\\ U\\ is\\ F_U(u)=u, u \\in (0,1)\\] \\[We\\ assume\\ that\\ F(x)\\ is\\ strictly\\ increasing, \\] \\[P[X \\leq x] = P[F^{-1}(U) \\leq x] = P[U \\leq F(x)] = F(x)\\] \uc790 \uc774\ub807\uac8c \ud655\ub960\ubcc0\uc218 X\uac00 \ub204\uc801\ud655\ub960\ubd84\ud3ec\ud568\uc218\ub85c F\ub97c \uac16\ub294\ub2e4\ub294 \uc99d\uba85\uc774 \ub05d\ub0ac\ub2e4. (?!) \ub9c8\uc9c0\ub9c9 \ub4f1\ud638\uac00 \uc131\ub9bd\ud558\ub294 \uc774\uc720\ub294 \ud655\ub960\ubcc0\uc218 U\uac00 \\(Unif(0,1)\\) \uc744 \ub530\ub974\uae30 \ub54c\ubb38\uc774\ub2e4. \\(Unif(0,1)\\) \uc774\ub77c\ub294 \ud655\ub960\ubcc0\uc218\ub294 \uba74\uc801\uc774 1\uc778 \uc815\uc0ac\uac01\ud615\uc548\uc5d0\uc11c \uc790\uc720\ub86d\uac8c \ub3cc\uc544\ub2e4\ub2c8\ub294 \ub188\uc774\ubbc0\ub85c, \uadf8 \uc0ac\uac01\ud615 \uc548\uc5d0\uc11c \\(F(x)\\) \ubcf4\ub2e4 \uc791\uc744 \ud655\ub960\uc774\ub77c \ud568\uc740 \uade0\ub4f1\ubd84\ud3ec\uc758 \ud2b9\uc9d5\uc73c\ub85c \uc778\ud574 x\ucd95\uacfc y\ucd95\uc774 \uac01\uac01 \\((F(x),F(x))\\) \uc778 \uc9c0\uc810\uc73c\ub85c \ub458\ub7ec\uc313\uc778 \uc601\uc5ed\uc774\uae30 \ub54c\ubb38\uc5d0 \\(P[U \\leq F(x)] = F(x)\\) \uac00 \ub418\ub294 \uac83\uc774\ub2e4. \uc704\uc758 \uc2dd\uc744 \uc0b4\uc9dd \ud2c0\uc5b4\uc11c \uc0dd\uac01\ud574\ubcf4\uba74 \\(U = F(X)\\) \uc774\uba70, \uc774\ub294 \ud655\ub960\ubcc0\uc218 X\uac00 \ub204\uc801\ubd84\ud3ec\ud568\uc218\ub85c F\ub97c \uac16\ub294\ub2e4\uace0 \ud558\uc600\ub294\ub370 \uadf8 \ub204\uc801\ubd84\ud3ec\ud568\uc218\uac00 U\ub77c\ub294 \uac83\uc774\ub2e4. \uc989 \uc5b4\ub5a4 \ud655\ub960\ubcc0\uc218 X\uc758 cdf\uac00 \\(Unif(0,1)\\) \uc744 \ub530\ub978\ub2e4\ub294 \uac83\uc774\ub2e4. \uc2e4\uc81c\ub85c \uc9c1\uc811 \uac04\ub2e8\ud55c \uacc4\uc0b0\uc744 \ud574\ubcf4\uba74 \\(P[F(X) \\leq x]= x\\) \ub97c \ud1b5\ud574 F(X)\uc758 cdf\uac00 x\uc774\ubbc0\ub85c \uc774\ub97c x\uc5d0 \ub300\ud574\uc11c \ubbf8\ubd84\ud55c 1\uc740 \\(Unif(0,1)\\) \uc758 pdf\ub77c\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \\(function\\ F\\) \ub294 strictly increasing\ud558\ub294 \ud568\uc218\ub85c \uac00\uc815\ud558\uc600\uae30\uc5d0, F\ub294 \ud56d\uc0c1 Inverse function\uc774 \uc874\uc7ac\ud55c\ub2e4. \uace0\ub85c \uc6b0\ub9ac\ub294 \uc5b4\ub5a0\ud55c \ud655\ub960\ubcc0\uc218 X\uc758 \ub09c\uc218\ub97c \uc0dd\uc131\ud558\uace0\uc790 \ud560\ub54c, \uadf8\uc800 \\(F(\\cdot)\\) \uc744 \uc54c\uace0, \uc774\uc758 \uc5ed\ud568\uc218\ub97c \uad6c\ud560 \uc218 \uc788\ub2e4\uba74, Uniform(0,1)\uc744 \uc774\uc6a9\ud558\uc5ec \uc0dd\uc131\uc774 \uac00\ub2a5\ud558\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc77c\ub840\ub85c Uniform(0,1)\ubd84\ud3ec\ub97c \ud1b5\ud574 \uc9c0\uc218\ubd84\ud3ec \ub09c\uc218 X\ub97c \uc0dd\uc131\ud558\uace0 \uc2f6\ub2e4\uba74, \\(X = -log(1-u)\\) \ub97c \ud1b5\ud574 \uad6c\ud558\ub294 \uac83\uc774 \uac00\ub2a5\ud558\ub2e4.","title":"\uc790 \uadf8\ub7ec\uba74 \uac04\ub2e8\ud558\uac8c \\(Unif(0,1)\\)\ubd84\ud3ec\ub97c \uc774\uc6a9\ud558\uc5ec \uc774\uc0b0\ud615\ubd84\ud3ec\uc5d0\uc11c \ubb34\uc791\uc704 \ub09c\uc218\ub97c \uc0dd\uc131\ud574\ubcf4\uc790!"},{"location":"06%20Simulation%20Practice%20and%20Theory/01_Monte_Carlo___Bootstrap/#1-b-monte-carlo-integration","text":"\ubaac\ud14c\uce74\ub97c\ub85c \ubc29\ubc95\uc740 \uc801\ubd84\uacfc\uc815\uc5d0\uc11c\ub3c4 \uc801\uc6a9\ub420 \uc218 \uc788\ub2e4. \uc6b0\ub9ac\uac00 \\(\\int_a^bg(x)dx\\) \ub97c \uad6c\ud558\ub824 \ud560 \ub54c, \ud568\uc218 g\uc758 anti-derivative\uac00 \uc874\uc7ac\ud558\uc9c0 \uc54a\uac70\ub098 \uc774\ub97c \uc54c \uc218 \uc5c6\ub294 \uacbd\uc6b0\uac00 \uc774\uc5d0 \ud574\ub2f9\ud55c\ub2e4. Monte Carlo Integration\uc744 \uc0ac\uc6a9\ud558\uba74 \uc704\uc758 \uc801\ubd84\uc744 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\[\\int_a^bg(x)dx = (b-a)\\int_a^bg(x)\\cdot \\frac{1}{b-a}dx= (b-a)E[g(x)],\\ where\\ X\\sim Unif(a,b)\\] \uc6b0\ub9ac\ub294 \\(X_1, X_2,..,X_n\\) \ub4e4\uc758 \ub79c\ub364\uc0d8\ud50c\uc744 uniform(a,b)\uc758 \ubd84\ud3ec\uc5d0\uc11c \ucd94\ucd9c\ud558\uc5ec \uc0dd\uc131\ud560 \uc218 \uc788\uc73c\uba70, \\(Y_i = (b-a)g(X_i)\\) \ub97c \uacc4\uc0b0\ud558\uba74 \ub41c\ub2e4. \uadf8\ub807\ub2e4\uba74, \\(\\bar{Y}\\) \ub294 \\(\\int_a^bg(x)dx\\) \uc758 consistent estimator\uac00 \ub41c\ub2e4. $* \\frac{\\sum_{i=1}^ng(x_i)}{n} \\xrightarrow{P} E[g(X_1)]\\ by\\ WLLN $","title":"1-b. Monte Carlo Integration"},{"location":"06%20Simulation%20Practice%20and%20Theory/01_Monte_Carlo___Bootstrap/#2-bootstrap-procedures","text":"Bootstrap\uae30\ubc95\uc744 \ub2e4\ub8e8\uae30 \uc55e\uc11c\uc11c \uc6b0\ub9ac\uac00 \uae30\ubcf8\uc801\uc73c\ub85c \ub2e4\ub8e8\ub294 \ud1b5\uacc4\uc801\uc778 \ucd94\uc815\uc758 \ub2e8\uacc4\ub97c \uac04\ub2e8\ud558\uac8c \uc77d\uac08\ubb34\ub9ac\ub97c \ud574\ubcf4\uc790. \\(\\mathbf{X}'=(X_1,...,X_n)\\) \ub97c cdf \\(F(x;\\theta)\\) \uc5d0\uc11c \uc5bb\uc740 random sample\uc774\ub77c\uace0 \uac00\uc815\ud574\ubcf4\uc790. \uadf8\ub807\ub2e4\uba74 \\(\\mathbf{x}'=(x_1,...,x_n)\\) \uc740 random sample \\(\\mathbf{X}'\\) \uc5d0\uc11c \uad00\uce21\ub41c \uac12 observed values\uac00 \ub41c\ub2e4. \uc5ec\uae30\uc11c \ud1b5\uacc4\uc801\uc778 \ucd94\uc815\uc744 \ud568\uc5d0 \uc788\uc5b4 \uc6b0\ub9ac\ub294 \ubaa8\uc218 \\(\\theta\\) \uc5d0 \ub300\ud55c estimationd\uc744 \ud558\ub294 \uac83\uc5d0 \uc8fc\ubaa9\ud55c\ub2e4. \uadf8\ub9ac\uace0 \uc6b0\ub9ac\ub294 \uc774 \ubaa8\uc218 \\(\\theta\\) \ub97c \ud5a5\ud574 \uad6c\uac04\ucd94\uc815\uacfc \uc810\ucd94\uc815\uc744 \ud1b5\ud574 \uc804\uc9c4\ud55c\ub2e4. \\(\\theta\\) \uc5d0 \ub300\ud55c \ucd94\uc815\ub7c9\uc744 \\(\\hat\\theta_n\\) \uc774\ub77c \ud558\uc790. \uc77c\ubc18\uc801\uc778 \ubc29\ubc95\uc73c\ub85c\ub294, \uc6b0\ub9ac\ub294 sample size n\uc744 \ubb34\ud55c\ud788 \ub298\ub824 \uadfc\uc0ac\ubd84\ud3ec\ub97c \ucd94\uc815\ud558\uace0, \uc774 \uadfc\uc0ac\ubd84\ud3ec\uc758 \ubaa8\uc218\uac12\uc73c\ub85c \uae30\uc874 \ubaa8\uc218\uc778 \\(\\theta\\) \ub97c \ucd94\uc815\ud55c\ub2e4. \ub9cc\uc57d \uc6b0\ub9ac\uac00 \uc544\ub798\uc640 \uac19\uc740 \uad00\uacc4\uc2dd\uc744 \uc720\ub3c4\ud560 \uc218 \uc788\ub2e4\uba74, \\[\\sqrt{n}(\\hat\\theta_n-\\theta) \\xrightarrow{D} N(0, \\sigma^2),\\ and\\ \\hat\\sigma^2 \\xrightarrow{P} \\sigma^2\\] $$(\\hat\\theta_n-1.96\\frac{\\hat\\sigma}{\\sqrt{n}},\\ \\hat\\theta_n+1.96\\frac{\\hat\\sigma}{\\sqrt{n}}) $$ \uc704\uc640 \uac19\uc740 \uc2e0\ub8b0\uad6c\uac04\uc774 \ubaa8\uc218 \\(\\theta\\) \uc5d0 \ub300\ud55c \\(100(1-\\alpha)\\) % \uadfc\uc0ac\uc2e0\ub8b0\uad6c\uac04\uc774 \ub41c\ub2e4. \ud558\uc9c0\ub9cc, \uc2e4\uc81c\ub85c\ub294 \uc704\uc758 \uadfc\uc0ac\uc801\uc778 \uc811\uadfc\ubc29\uc2dd\uc744 \uc801\uc6a9\ud558\uae30\uc5d0 \ub9e4\uc6b0 \uc5b4\ub824\uc6b4 \uacbd\uc6b0\uac00 \ub9ce\ub2e4. sample size\ub97c \ubb34\ud55c\ud788 \ub298\ub9ac\ub294 \uc77c\uc740 \uc774\ub860\uc801\uc73c\ub85c\ub294 \uac00\ub2a5\ud558\uc9c0\ub9cc, \ud604\uc2e4\uc5d0\uc11c \uc801\uc6a9\ud558\uae30\uc5d0\ub294 \uc5b4\ub824\uc6b4 \uacbd\uc6b0\uac00 \ub9ce\uc73c\uba70, \uadfc\uc0ac\ubd84\ud3ec\ub97c \ucc3e\uae30 \ub9e4\uc6b0\uc5b4\ub824\uc6b4 \uacbd\uc6b0\ub294 \uc801\uc6a9\ud560 \uc218\uac00 \uc5c6\ub2e4. \uc6b0\ub9ac\uac00 \ucd94\uc815\ub7c9 \\(\\hat\\theta_n\\) \uc5d0 \ub300\ud55c \ubd84\ud3ec\ub97c \uc54c\uc9c0 \ubabb\ud558\uc9c0\ub9cc, \uc544\ub798\uc640 \uac19\uc774 \ub9cc\uc57d \ub3d9\uc77c\ud55c \uc0ac\uc774\uc988\uc758 \ubb34\ud55c\ud55c random sample\uc744 \uac16\uace0 \uc788\ub2e4\uace0\ud55c\ub2e4\uba74, $ \\(\\mathbf{x_1}'=(x_{11},x_{12},...,x_{1n})\\) $ $ \\(\\mathbf{x_1}'=(x_{21},x_{22},...,x_{2n})\\) $ $ \\(.\\) $ $ \\(.\\) $ $ \\(.\\) $ $ \\(from\\ a\\ cdf\\ F(x;\\theta)\\) $ \uc6b0\ub9ac\ub294 \\(\\hat\\theta_1 = \\hat\\theta(\\mathbf{x_1}'),..,\\) \ub97c \uacc4\uc0b0\ud560 \uc218 \uc788\uc73c\uba70, \uc774\ub4e4\uc758 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \ub9cc\ub4e4 \uc218\uac00 \uc788\ub2e4. \uadf8\ub7ec\uba74 \uc989 \uc774 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc740 \\(\\hat\\theta\\) \uc758 \ubd84\ud3ec\uc640 \uac19\uac8c \ub41c\ub2e4. \ud558\uc9c0\ub9cc \ubb34\ud55c\ud55c random sample\uc774 \uc544\ub2c8\ub77c \ub2e8 \ud55c\uac1c\uc758 sample\ub9cc \uc874\uc7ac\ud560 \ub54c \ubb38\uc81c\uc810\uc774 \uc0dd\uae34\ub2e4. \uc774\ub7f4 \ub54c \uc801\uc6a9\ud558\ub294 \uac83\uc774 Bootstrap Procedure \uc774\ub2e4. \uc774\ub294 \uac04\ub2e8\ud558\uac8c original sample\uc744 \uacc4\uc18d \ubcf5\uc6d0\ucd94\ucd9c\ud558\uc5ec sample\uc744 \uc5ec\ub7ec\uac1c \ubcf5\uc81c\ud558\ub294 \uac83\uacfc \uac19\ub2e4. resampling\ub41c \uc0c8\ub85c\uc6b4 \uc0d8\ud50c\ub4e4\uc744 \\(\\mathbf{x^{*}_1}'=(x_{11}^*,...,x_{1n}^*), \\mathbf{x^{*}_2}',...,\\mathbf{x^{*}_B}'\\) \uc774\ub77c Notating\ud55c\ub2e4\uba74, ( \uc5ec\uae30\uc11c B\ub294 \ubcf5\uc6d0\ucd94\ucd9c\ud558\uc5ec \ub9cc\ub4e0 \uc0d8\ud50c\uc758 \uac2f\uc218\ub97c \uc758\ubbf8 ) \uc6b0\ub9ac\ub294 \\(\\hat\\theta_1^*,\\hat\\theta_2^*,..,\\hat\\theta_B^*\\) \uc758 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \uadf8\ub9b4 \uc218 \uc788\uc73c\uba70, \uc774\ub860\uc801\uc73c\ub85c \uc774 \ubcf5\uc81c\ub41c \uc0d8\ud50c\uc758 \ucd94\uc815\ub7c9\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c4 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc740 \uadfc\uc0ac\uc801\uc73c\ub85c \\(\\hat\\theta\\) \uc758 distribution\uacfc \ub3d9\uc77c\ud558\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c \ucef4\ud4e8\ud130 \ud504\ub85c\uadf8\ub798\ubc0d\uc758 \uc2dc\ubbac\ub808\uc774\uc158\uc744 \ud1b5\ud574\uc11c\ub294 \uc27d\uac8c sample\uc758 \uc0ac\uc774\uc988\ub97c \ub298\ub9b4 \uc218 \uc788\uace0, n\uc774 \uc5b4\ub290\uc815\ub3c4\ub9cc \ucee4\uc9c4\ub2e4\uba74 CLT\ub97c \ud1b5\ud574\uc11c \ud45c\ubcf8\ud3c9\uade0\uc758 \uadfc\uc0ac\ubd84\ud3ec\ub97c \uc27d\uac8c \uad6c\ud560 \uc218 \uc788\uc9c0\ub9cc, Bootstrap \uae30\ubc95\uc744 \uc774\uc6a9\ud558\uc5ec \uc5bb\uc740 \uadfc\uc0ac\uc2e0\ub8b0\uad6c\uac04\uacfc CLT\ub97c \ud1b5\ud574\uc11c \ub3c4\ucd9c\ud560 \uc218 \uc788\ub294 \uadfc\uc0ac\uc2e0\ub8b0\uad6c\uac04\uc744 \ube44\uad50\ud574\ubcf4\ub294 \ubc29\ubc95\uc744 \ud1b5\ud574 Bootstrap Procedure \uc2dc\ubbac\ub808\uc774\uc158\uc744 \uc9c4\ud589\ud574\ubcf4\uaca0\ub2e4. \ub610\ud55c rough\ud55c \ubc29\ubc95\uc774\uae34 \ud558\uc9c0\ub9cc, MASS package\uc5d0 \ub0b4\uc7a5\ub418\uc5b4\uc788\ub294 fitdistr() \ud568\uc218\ub97c \ud1b5\ud558\uc5ec, Bootstrap sampling\uc73c\ub85c \uc5bb\uc740 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \ud45c\uc900\ud654\uc2dc\ud0a8 \uacb0\uacfc\uac00 \ud45c\uc900\uc815\uaddc\ubd84\ud3ec\uc640 \uadfc\uc0ac\ud55c \ud615\ud0dc\ub97c \ub744\ub294\uc9c0\ub97c \ud655\uc778\ud558\ub294 \uac83\uc73c\ub85c \uc2dc\ubbac\ub808\uc774\uc158\uc744 \uc9c4\ud589\ud558\uaca0\ub2e4. Resampling\uacfc\uc815\uc740 3000\ubc88\uc744 \uc9c4\ud589\ud558\uc600\ub2e4.(B=3000)","title":"2. Bootstrap Procedures"},{"location":"06%20Simulation%20Practice%20and%20Theory/01_Monte_Carlo___Bootstrap/#bootstrap-with-one-sample-from-gamma1100","text":"#Lets do the Bootstrap! sample <- rgamma(30,1,1/100) B <- 3000 alpha <- 0.05 m <- (alpha/2)*B m ## [1] 75 bootstrap_samplemean <- rep(0,B) for (i in 1:B){ bootstrap_samplemean[i] <- mean(sample(sample,replace=T)) } head(sort(bootstrap_samplemean),50) ## [1] 69.34995 69.38044 70.32409 71.08792 71.68891 71.95984 72.12971 ## [8] 72.62477 72.71791 72.80969 72.95041 73.15467 74.10887 74.16133 ## [15] 74.52411 74.56133 75.18926 75.27353 75.30119 75.33049 75.68648 ## [22] 75.76593 75.76702 75.78427 76.04546 76.26449 76.33295 76.66625 ## [29] 76.76444 76.78385 76.84236 76.86567 77.12012 77.12724 77.26601 ## [36] 77.36715 77.41874 77.48135 77.59661 77.61867 77.67783 77.87053 ## [43] 77.94403 78.09464 78.10425 78.16664 78.25914 78.55849 78.66119 ## [50] 78.77573 hist(sort(bootstrap_samplemean), xlab='sample mean from bootstrap',main='') Lower_Bound <- sort(bootstrap_samplemean)[order(sort(bootstrap_samplemean))==m] Upper_Bound <- sort(bootstrap_samplemean)[order(sort(bootstrap_samplemean))==B+1-m] Lower_Bound ## [1] 81.12866 Upper_Bound ## [1] 142.9677 percentile_bootstrap_CI <- paste('(','81.12866',',','142.9677',')') Theoretical_Lower_Bound <- mean(sample)-1.96*(sqrt(var(sample))/sqrt(30)) Theoretical_Upper_Bound <- mean(sample)+1.96*(sqrt(var(sample))/sqrt(30)) Theoretical_Lower_Bound ## [1] 79.38917 Theoretical_Upper_Bound ## [1] 142.0627 Theoretical_CI <- paste('(','79.38917',',','142.0627',')') percentile_bootstrap_CI ## [1] \"( 81.12866 , 142.9677 )\" Theoretical_CI ## [1] \"( 79.38917 , 142.0627 )\" library(MASS) x <- sort(bootstrap_samplemean) fitdistr( ((x-mean(x))/sqrt(var(x))), 'Normal') ## mean sd ## 2.274520e-16 9.998333e-01 ## (1.825438e-02) (1.290779e-02) \uc704\uc758 \ucf54\ub4dc\ub97c \ud1b5\ud574, 3000\ubc88\uc758 resampling\uc744 \uc218\ud589\ud558\uc5ec \uc5bb\uc740 Bootstrap \ud45c\ubcf8\ud3c9\uade0\ub4e4\uc758 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \ubcf4\uba74 \uc774\ub860\uc801\uc73c\ub85c \ub098\uc640\uc57c\ud558\ub294 \ud45c\ubcf8\ud3c9\uade0\uc758 \uadfc\uc0ac\ubd84\ud3ec\uc778 \uc815\uaddc\ubd84\ud3ec\ub97c \uc798 \ub530\ub974\ub294\uac83\uc73c\ub85c \ubcf4\uc778\ub2e4. \uc0ac\uc2e4 \uc704\uc758 \uc2dc\ubbac\ub808\uc774\uc158\uc740 \ub108\ubb34\ub098\ub3c4 \uc26c\uc6b4 \uc2dc\ubbac\ub808\uc774\uc158\uc5d0 \uc18d\ud558\uace0, \uc6b0\ub9ac\uac00 \uc774\ubbf8 \uc774\ub860\uc801\uc778 \uacb0\uacfc\uac12\uc744 \ub2e4 \uc54c\uace0 \uc788\uae30 \ub54c\ubb38\uc5d0 \uac00\ubccd\uac8c Bootstrap\uc774 \uc81c\ub300\ub85c \uae30\ub2a5\uc744 \ud558\ub294\uc9c0 \ud655\uc778\uc6a9\uc73c\ub85c\ub9cc \ucc38\uace0\ud558\uba74 \uc88b\uc744 \uac83 \uac19\ub2e4. Bootstrap percentile\uc5d0 \ub530\ub978 \uadfc\uc0ac\uc2e0\ub8b0\uad6c\uac04\uc744 \uad6c\ud55c\uacb0\uacfc \ub610\ud55c CLT\ub97c \ud1b5\ud574 \uc5bb\uc740 \uadfc\uc0ac\uc2e0\ub8b0\uad6c\uac04\uacfc \ub9e4\uc6b0 \uc720\uc0ac\ud55c \ubc94\uc704\ub97c \ub098\ud0c0\ub0b4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.","title":"Bootstrap with One sample from \\(\\Gamma(1,100)\\)"},{"location":"07%20Intermediate%20Applied%20Statistics/2page/","text":"\\(Suppose\\ X_1,X_2,..,X_n \\sim N(\\mu, \\sigma^2)\\) \\(pivot\\ random\\ variable\\ for\\ a\\ CI\\ is\\ t=\\frac{ \\bar{X} - \\mu }{S/\\sqrt{n}}\\) Theoratical CI is \\((\\bar{x}-t^{(1-\\alpha/2)}\\frac{s}{\\sqrt{n}},\\bar{x}-t^{(\\alpha/2)}\\frac{s}{\\sqrt{n}})\\) . Pivot Bootstrap CI is \\((\\bar{x}-t^{*(1-\\alpha/2)}\\frac{s}{\\sqrt{n}},\\bar{x}-t^{*(\\alpha/2)}\\frac{s}{\\sqrt{n}})\\) . \\(t^* = \\frac{\\bar{x^*}-\\bar{x}}{s^*/\\sqrt{n}},\\ s^{*2} = (n-1)^{-1}\\sum_{i=1}^n(x_i^*-\\bar{x}^*)^2\\)","title":"R Notebook"},{"location":"07%20Intermediate%20Applied%20Statistics/Final/","text":"Intermediate Applied Statistics Final Takehome Exam 1. 1-1. Show that we can also check whether \\(X\\) follows \\(F_0(x)\\) by plotting \\(-log(1-F_0(X))\\) versus \\(-log(1-i/(n+1))'s.\\) \uc77c\ubc18\uc801\uc73c\ub85c Normal\ubd84\ud3ec\uc5d0 fitting\ud560 \ub54c \uc0ac\uc6a9\ud558\ub294 qqplot\ucc98\ub7fc, \\(-log(1-F_0(X))\\) \uc640 \\(-log(1-i/(n+1))\\) \uc758 \ube44\uad50\ub97c \ud574\ubcf4\uaca0\ub2e4. $ X \\sim F_0(X)$ \ub97c \ub530\ub978\ub2e4\uace0 \ud560 \ub54c, \\(F_0(X)\\) \ub294 \ub204\uc801\ubc00\ub3c4\ud568\uc218(cdf)\uc774\uba70, \uc774 cdf\uc758 \ubd84\ud3ec\ub294 Uniform\ubd84\ud3ec\uac00 \ub41c\ub2e4. \uc704\uc758 \uc2dd\uc744 \ub2e4\uc2dc \ud480\uc5b4\uc11c \uc4f0\uc790\uba74, \uc774\ub294 \uc8fc\uc5b4\uc9c4 \uc0d8\ud50c\ub4e4\uc744 \ud06c\uae30 \uc21c\uc11c\ub300\ub85c \ub098\uc5f4\ud558\uc600\uc744 \ub54c, \\(x_{(i)} \\approx F^{-1}(\\frac{i}{n+1})\\) \uc744 \ube44\uad50\ud558\ub294 \uac83\uc774 \uadf8\ub9bc\uc744 \uadf8\ub9ac\ub294 \uac83\uacfc \ud568\uaed8 \ubb38\uc81c\uc758 \ub2f5\uc744 \ubcf4\uc774\ub294 \uac83\uc774 \ub41c\ub2e4. \uc774\ub97c \uc774\uc6a9\ud558\uc5ec \uc9c1\uc811 plot\uc744 \uadf8\ub824 qqplot\uacfc \ube44\uc2b7\ud55c form\uc744 \ub9cc\ub4e4\uc5b4\ubcf4\uace0, \\(x_{(i)} \\approx F^{-1}(\\frac{i}{n+1})\\) \uc774 \ub458\uc758 \uad00\uacc4 \ub610\ud55c \ub450 \ud56d\uc758 \ucc28\uc774\uac00 0\uacfc \ube44\uc2b7\ud55c\uc9c0\ub97c \uccb4\ud06c\ud558\ub294 \uacfc\uc815\uc744 \ud1b5\ud574 \ubcf4\uc600\ub2e4. set.seed(2020311194) # 100 sample form uniform a <- runif(100) x <- sort(a) #check plot i <- 1:100 plot(-log(1-x),-log(1-i/(length(x)+1))) #check the difference of two values c <- rep(0,length(x)) for (i in 1:length(x)){ c[i] <- x[i] - qunif(i/(length(x)+1)) } c ## [1] -0.0020358902 0.0032247549 0.0044838281 -0.0037792539 -0.0090086398 ## [6] -0.0178934427 -0.0260260130 -0.0272017346 -0.0303446013 -0.0296651059 ## [11] -0.0255229942 -0.0327193141 -0.0380216380 -0.0427320365 -0.0269755046 ## [16] -0.0350967352 -0.0414608830 -0.0448804717 -0.0537392683 -0.0428289944 ## [21] -0.0196609758 0.0039456506 -0.0020984610 -0.0006890500 -0.0061154514 ## [26] -0.0068897217 -0.0157445365 -0.0090605863 -0.0155620024 -0.0239936088 ## [31] -0.0317101204 -0.0399383068 -0.0026076430 0.0214324310 0.0242884377 ## [36] 0.0179033486 0.0320410312 0.0245256318 0.0191693473 0.0114126480 ## [41] 0.0147882477 0.0071808881 0.0376980454 0.0344877337 0.0414879340 ## [46] 0.0346011515 0.0567058272 0.0498212035 0.0416583978 0.0508939199 ## [51] 0.0422886903 0.0531330912 0.0500616136 0.0405467347 0.0401381934 ## [56] 0.0407216133 0.0322004060 0.0305376502 0.0290926693 0.0304862390 ## [61] 0.0310735891 0.0658745717 0.0578853876 0.0615256389 0.0540617281 ## [66] 0.0452655280 0.0404460451 0.0447246316 0.0353560281 0.0433776455 ## [71] 0.0438249843 0.0442056699 0.0396245396 0.0448213800 0.0573250927 ## [76] 0.0510092447 0.0492991438 0.0394761377 0.0366026018 0.0368163020 ## [81] 0.0370831460 0.0307857112 0.0387236285 0.0680595882 0.0612288510 ## [86] 0.0586341999 0.0559009177 0.0504839753 0.0427199190 0.0378402503 ## [91] 0.0315178029 0.0330100886 0.0246411754 0.0161714742 0.0081281122 ## [96] 0.0010923949 0.0031889546 0.0009318988 0.0129626816 0.0050487876 sum(c) ## [1] 1.757704 Plot\uc744 \uc0b4\ud3b4\ubcf4\uc558\uc744 \ub54c \ub300\ub7b5\uc801\uc73c\ub85c 45\ub3c4\uc758 \uac01\ub3c4\ub85c \ub450 \uac12\ub4e4\uc774 \uc798 fitting\uc774 \ub418\ub294 \uac83\uc744 \ud655\uc778 \ud560 \uc218 \uc788\uc73c\uba70, 100\uac1c\uc758 \uc0d8\ud50c\uc744 \ubf51\uc544\uc11c \uc9c4\ud589\ud558\uc5ec\uc11c \ubaa8\ub4e0 \ud56d\ub4e4\uc758 \ucc28\uc774\uac00 0\uc5d0 \uac00\uae4c\uc6b4 \uac12\uc744 \uac16\ub294 \uac83\uc744 \ud655\uc778\ud558\uc600\ub2e4. 1-2. Suppose that \\(F_0\\) is the normal distribution. Take a random sample from \\(F_0\\) and draw two plots in 1). Then discuss the relation between two plots. set.seed(2020311194) a <- rnorm(100,0,1) x <- sort(a) i <- 1:length(a) #Q-Q plot plot(x,qnorm(i/(length(x)+1))) #-log(1-F(X)) vs -log(1-i/(n+1)) plot(-log(1-pnorm(x)), -log(1-i/(length(x)+1))) plot\uc744 \uadf8\ub824\uc11c \ud655\uc778\ud574\ubcf4\uc558\uc744 \ub54c, X\ub97c normal\ubd84\ud3ec\uc5d0\uc11c \uc0d8\ud50c\ub9c1\uc744 \ud558\uc5ec\ub3c4, \uc704\uc758 \ubb38\uc81c\uc5d0\uc11c \uad6c\ud55c \uac83\ucc98\ub7fc fiitting\uc774 \uc798 \ub418\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\ub2e4. 1-3. Suppose that the data includes some \\(x_{i}+'s\\) which are actually the censored observations at \\(x_{i}+'s\\) . Then describe how we can check whether X follows \\(F_0(x)\\) ? \uc0dd\uc874\ubd84\uc11d\uc5d0 \uc788\uc5b4\uc11c \uac1c\uac1c\uc778\ub4e4\uc758 \uc0dd\uc874\uc2dc\uac04\uc5d0 \ub300\ud558\uc5ec \ubd84\uc11d\uc744 \uc2dc\ud589\ud560 \ub54c, \uba87\uba85\uc758 \uc0ac\ub78c\ub4e4\uc758 \uc815\ubcf4\uac00 censored \ub418\uc5b4\ub3c4, \uc0dd\uc874\uc2dc\uac04\uc5d0 \ub300\ud55c observation value\uc640 estimated value\uac04\uc758 \ubd84\uc11d\uc774 \uac00\ub2a5\ud558\uae30\ub3c4 \ud558\ub2e4, \uc774\ub294 Cox - Snell Residual\ub97c \uadf8\ub824\ubd04\uc73c\ub85c\uc368 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \ud754\ud788 survivorship function\uc5d0 \ub300\ud558\uc5ec Uniform \ubd84\ud3ec\ub97c \uac00\uc815\ud558\uc5ec \uc0dd\uc874\ubd84\uc11d\uc744 \uc9c4\ud589\ud558\ub294\ub370, \uc0dd\uc874\ud568\uc218 \\(S(x)\\) \ub294 \\(1-F_0(x)\\) \uc640 \uac19\ub2e4. 1-1\uc758 \ubb38\uc81c\uc5d0\uc11c \uc0ac\uc6a9\ud558\uc600\ub358, Uniform\ubd84\ud3ec\uc758 log transformation\uc744 \ud1b5\ud558\uc5ec Exponential \ubd84\ud3ec\ub97c \ub9cc\ub4dc\ub294 \uac83\uc744 \ub5a0\uc62c\ub824 \ubcf8\ub2e4\uba74, \uc704\uc758 \uacbd\uc6b0\uc758 quantile \uac04\uc758 \ube44\uad50\uac00 cox-snell residual method\ub97c \ud1b5\ud574 \ud480 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud55c\ub2e4. Cox - snell residual\uc740, \ub450 \uac00\uc9c0\uc758 \ubc29\ubc95\uc744 \ud1b5\ud574 censored\ub41c \uac1c\ubcc4 \uc0dd\uc874\uc2dc\uac04\uc5d0 \ub300\ud558\uc5ec \uc0c8\ub85c\uc6b4 \uac12\uc73c\ub85c \ub300\uccb4\ud558\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\ub294\ub370, \uc774\ub294 \ud3c9\uade0 \ud639\uc740 \uc911\uc704\uc218\ub97c censored\ub41c time\uc5d0 \ub354\ud558\ub294 \ubc29\ubc95\uc774\ub2e4. \uc5ec\uae30\uc11c \\(-log(1-F_0(x))\\) \ub294 cumulative hazard function\uc774\uba70, \uc774\ub294 \uc704\uc5d0\uc11c \ud655\uc778\ud588\ub358 \ubc14\uc640 \uac19\uc774 Exponential \ubd84\ud3ec\ub97c \uac16\uac8c\ub41c\ub2e4. censored\ub41c point \\(x_{i}+'s\\) \ub294 \\(x_{i}+E(-log(1-F_0(X_1)))\\) \ud639\uc740 \\(x_{i}+Median(-log(1-F_0(X_1)))\\) \uc758 \uac12\uc73c\ub85c \ub300\uccb4\ub41c\ub2e4. cumulative hazard function\uc774 \\(Exp(1)\\) \uc744 \ub530\ub974\ubbc0\ub85c, \ub300\uccb4\ub420 \ub450 \uac12 \uc5ed\uc2dc \\(Exp(1)\\) \ub97c \ub530\ub978\ub2e4. \uc774\ub97c \ud1b5\ud574\uc11c censored\ub41c \uac12\uc744 \uc0c8\ub85c\uc6b4 \uac12\uc73c\ub85c \ub300\uccb4\ud558\ub354\ub77c\ub3c4, quantile\uc744 \ube44\uad50\ud558\uace0\uc790\ud558\ub294 \uac12\ub4e4\uc758 \ubd84\ud3ec\ub294 \uc5ec\uc804\ud788 \uadf8\ub300\ub85c\uba70, \uc218\uce58\uc640 \ubc94\uc8fc\uac00 \ud63c\ud569\ub41c \uc790\ub8cc\ub4e4\uc744 numerical\ud558\uac8c \ub300\uccb4\ud568\uc73c\ub85c\uc368 X\uac00 \\(F_0(x)\\) \uc758 \ubd84\ud3ec\ub97c \ub530\ub974\ub294\uc9c0\uc758 \uc5ec\ubd80\ub97c check\ud560 \uc218 \uc788\ub2e4. 2. 2-1. boy <- c(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8,8,9,9,9,9,10,10,10,10, 11,11,11,11,12,12,12,12,13,13,13,13,14,14,14,14,15,15,15,15,16,16,16,16,17,17,17,17,18,18,18,18, 19,19,19,19,20,20,20,20) age <- rep(c(8,8.5,9,9.5),4) height <- c(47.8,48.8,49,49.7,46.4,47.3,47.7,48.4, 46.3,48.8,47.8,48.5,45.1,45.3,46.1,47.2, 47.6,48.5,48.9,49.3,52.5,53.2,53.3,53.7, 51.2,53,54.3,54.5,49.8,50,50.3,52.7, 48.1,50.8,52.3,54.5,45,47,47.3,48.3, 51.2,51.4,51.6,51.9,48.5,49.2,53,55.5, 52.1,52.8,53.7,55,48.2,48.9,49.3,49.8, 49.6,50.4,51.2,51.8,50.7,51.8,52.7,53.3, 47.2,47.7,48.4,49.5,53.3,54.6,55.1,55.3, 46.2,47.5,48.1,48.4,46.3,47.6,51.3,51.8) data <- cbind(boy,age,height) colnames(data) <- c('boy','age','height') data <- as.data.frame(data) #CRBD data$age <- as.factor(data$age) data.crbd.aov <- aov(height~age+boy,data=data) summary(data.crbd.aov) ## Df Sum Sq Mean Sq F value Pr(>F) ## age 3 85.6 28.55 4.486 0.00597 ** ## boy 1 32.3 32.30 5.075 0.02720 * ## Residuals 75 477.3 6.36 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Randomized Completed Block Design\uc744 \ud1b5\ud574 \ub124 \uc9d1\ub2e8\uac04\uc758 \ud3c9\uade0\ucc28\uc774\ub97c \ube44\uad50\ud574\ubcf4\uc558\uc744 \ub54c, age\uadf8\ub8f9\uc5d0 \ub300\ud55c p-value\ub294 0.00597\uc778 \uac83\uc73c\ub85c \ubcf4\uc544, \uadf8\ub8f9\uac04\uc758 \ucc28\uc774\uac00 \uc720\uc758\ubbf8\ud558\uac8c \uc874\uc7ac\ud55c\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. 2-2. #Contrast Method age8 <- data[data$age==8,3] age8.5 <- data[data$age==8.5,3] age9 <- data[data$age==9,3] age9.5 <- data[data$age==9.5,3] m <- cbind(age8,age8.5,age9,age9.5) #sample covariance matrix mean <- apply(m,2,mean) cov <- cov(m) mean <- matrix(mean,4,1) c1 <- rbind(c(1,-1,0,0),c(0,0,1,-1),c(1,1,-1,-1)) c2 <- rbind(c(-3,-1,1,3),c(1,-1,-1,1),c(-1,3,-3,1)) test_statistics1 <- 20*t(c1%*%mean)%*%solve(c1%*%cov%*%t(c1))%*%(c1%*%mean) test_statistics1 ## [,1] ## [1,] 81.95183 qf(0.975,1,76) ## [1] 5.22889 test_statistics2 <- 20*t(c2%*%mean)%*%solve(c2%*%cov%*%t(c2))%*%(c2%*%mean) test_statistics2 ## [,1] ## [1,] 81.95183 qf(0.975,1,76) ## [1] 5.22889 Contrast\ub97c \uc774\uc6a9\ud558\uc5ec \ub124 \uc9d1\ub2e8 \uac04\uc758 \ud3c9\uade0\ucc28\uc774\ub97c \uac80\uc815\ud55c \uacb0\uacfc\ub294 \uc704\uc640 \uac19\ub2e4. Orthogonal contrasts(\uc9c1\uad50\ub300\ube44)\ub97c \uc0ac\uc6a9\ud558\uc600\uc744 \ub54c Test Statistics\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(T^2\\ =\\ n(C{\\bar(y)})'(CSC')^{-1}(C\\bar(y))\\) \uc5ec\uae30\uc11c S\ub294 sample covariance matrix\uc774\uba70, \\(\\bar{y}\\) \ub294 \uadf8\ub8f9(age)\ubcc4 \ud3c9\uade0, C\ub294 \ub300\ube44\ud589\ub82c\uc774\ub2e4. \uac80\uc815\ud1b5\uacc4\ub7c9\uc758 \uac12\uc774 \uc8fc\uc5b4\uc9c4 \ub450 \ub300\ube44\ud589\ub82c\uc5d0 \ub300\ud558\uc5ec \uac19\uac8c \ub098\uc624\ub294 \uac83\uc744 \ud655\uc778\ud588\uc73c\uba70, \\(F_{\\alpha,1,N-a}\\) \uc640 \ube44\uad50\ud558\uc600\uc744 \ub54c, \\(F_{0.975,1,76} \\approx\\ 5.23\\) \uc774\ubbc0\ub85c, \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud558\uc5ec \\(\\mu_{age8} \\neq \\mu_{age8.5}\\) , \\(\\mu_{age8.5} \\neq \\mu_{age9}\\) , \\(\\mu_{age9} \\neq \\mu_{age9.5}\\) \ub77c\uace0 \uacb0\ub860 \uc9c0\uc744 \uc218 \uc788\ub2e4. 2-3. library(gee) gee <- gee(height~age+boy, family='gaussian',id=boy,corstr='exchangeable', data=data) ## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27 ## running glm to get initial regression estimate ## (Intercept) age8.5 age9 age9.5 boy ## 47.498026 1.075000 1.915000 2.800000 0.110188 summary(gee) ## ## GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA ## gee S-function, version 4.13 modified 98/01/27 (1998) ## ## Model: ## Link: Identity ## Variance to Mean Relation: Gaussian ## Correlation Structure: Exchangeable ## ## Call: ## gee(formula = height ~ age + boy, id = boy, data = data, family = \"gaussian\", ## corstr = \"exchangeable\") ## ## Summary of Residuals: ## Min 1Q Median 3Q Max ## -3.9915977 -2.0600940 -0.2338158 1.9876410 4.3408459 ## ## ## Coefficients: ## Estimate Naive S.E. Naive z Robust S.E. Robust z ## (Intercept) 47.498026 1.12410540 42.254068 1.00232618 47.387794 ## age8.5 1.075000 0.29693589 3.620310 0.15441422 6.961794 ## age9 1.915000 0.29693589 6.449203 0.28366133 6.751008 ## age9.5 2.800000 0.29693589 9.429645 0.36048578 7.767297 ## boy 0.110188 0.09260266 1.189901 0.09118725 1.208370 ## ## Estimated Scale Parameter: 6.363825 ## Number of Iterations: 1 ## ## Working Correlation ## [,1] [,2] [,3] [,4] ## [1,] 1.0000000 0.8614498 0.8614498 0.8614498 ## [2,] 0.8614498 1.0000000 0.8614498 0.8614498 ## [3,] 0.8614498 0.8614498 1.0000000 0.8614498 ## [4,] 0.8614498 0.8614498 0.8614498 1.0000000 GEE Method\ub97c \ud1b5\ud558\uc5ec \uc5bb\uc740\uacb0\uacfc\ub294 \uc704\uc640 \uac19\ub2e4. Summary\ub97c \ud1b5\ud574 input\uac12\uc774 \ud55c\ub2e8\uc704 \uc99d\uac00\ud560 \ub54c\uc758 age\uc5d0 \ub530\ub978 Y\uac12(\uc5ec\uae30\uc120 Bone\uc758 Height)\uc758 \ubcc0\ud654\ub97c \ud655\uc778\ud560 \uc218 \uc788\ub2e4. Exchangeable\ub85c covariance structure\ub97c \uc9f0\uc73c\uba70, Y\uac12 jaw's bone height\uac00 age\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ub354 \ud070 \uc601\ud5a5\uc744 \ubc1b\uc544 \ub354 \ud06c\uac8c \uc99d\uac00\ud55c\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218\uc788\ub2e4. 2-4. library(nlme) mm <- lme(height~age,random=~age|boy,data=data) anova(mm) ## numDF denDF F-value p-value ## (Intercept) 1 57 8412.129 <.0001 ## age 3 57 27.317 <.0001 summary(mm) ## Linear mixed-effects model fit by REML ## Data: data ## AIC BIC logLik ## 266.9375 301.8985 -118.4688 ## ## Random effects: ## Formula: ~age | boy ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 2.4915026 (Intr) age8.5 age9 ## age8.5 0.5071428 -0.202 ## age9 1.2038249 -0.144 0.576 ## age9.5 1.5782937 -0.168 0.423 0.969 ## Residual 0.3498422 ## ## Fixed effects: height ~ age ## Value Std.Error DF t-value p-value ## (Intercept) 48.655 0.5625822 57 86.48514 0 ## age8.5 1.075 0.1584255 57 6.78552 0 ## age9 1.915 0.2910304 57 6.58007 0 ## age9.5 2.800 0.3698507 57 7.57062 0 ## Correlation: ## (Intr) age8.5 age9 ## age8.5 -0.212 ## age9 -0.169 0.514 ## age9.5 -0.189 0.393 0.912 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.77709165 -0.27034918 0.01023047 0.26877890 1.86032622 ## ## Number of Observations: 80 ## Number of Groups: 20 Mixed model\uc744 \uc774\uc6a9\ud558\uace0, \uac19\uc740 \uac1c\uc778\uc5d0 \ub300\ud574\uc11c \uc2dc\uac04\uc5d0 \ub530\ub77c \ubc18\ubcf5\uce21\uc815\ud55c \uc790\ub8cc\uc5d0 \ub300\ud55c \ubd84\uc11d\uc744 \uc2dc\ud589\ud558\uae30 \ub54c\ubb38\uc5d0 time\uc5d0 \ub530\ub77c covariance\uac00 \uc77c\uc815\ud558\uc9c0 \uc54a\uc744 \uac83\uc774\ub77c\ub294 \uae30\ubcf8 \uac00\uc815\uc744 \ud55c \ud6c4 \ucd94\uc815\ud558\uace0\uc790\ud558\ub294 \ubaa8\ub378\uc2dd\uc5d0\uc11c \uc808\ud3b8\uacfc \uae30\uc6b8\uae30\uc5d0 \ub300\ud574 \ub458\ub2e4 random effect\ub97c \uc8fc\ub3c4\ub85d \ubaa8\ub378\uc744 \uad6c\uc131\ud558\uc600\ub2e4. \uc774\ub294 \uc218\uc2dd\ud654 \ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\[y_{ij} = \\beta_0+\\beta_1t_{ij}+b_{0i}+b_{1i}t_{ij}+e_{ij}\\] \uacb0\uacfc\ub97c \ubcf4\uc790\uba74, \uac01 age\uc5d0 \ub300\ud558\uc5ec p-value\uac00 \uc720\uc758\ud558\uac8c \ub098\uc624\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc73c\uba70, age\uc758 \uc99d\uac10\uc5d0 \ub530\ub77c Y\uac12 bone height \ub610\ud55c \uc99d\uac00\ub7c9\uc774 \uc720\uc758\ubbf8\ud558\uac8c \uc99d\uac00\ud558\ub294 \ucd94\uc138 \ub610\ud55c \ud655\uc778 \ud560 \uc218 \uc788\ub2e4. 2-5. \uc704\uc758 \ub124\uac00\uc9c0 \ubc29\ubc95\uc5d0\uc11c \ucd94\uc815\ud55c \ubaa8\uc218\uc758 \uac1c\uc218\ub294 \uccab\uc9f8\ub85c CRBD\ubc29\ubc95\uc758 \uacbd\uc6b0\ub294 4\uac1c\uac00 \ub41c\ub2e4. Boy\ub97c block\uc73c\ub85c \uc124\uc815\ud558\uace0, age \uc9d1\ub2e8\uac04\uc758 \ud3c9\uade0\ucc28\uc774\ub97c \ube44\uad50\ud558\uace0\uc790 \ud558\ub294 \uac83\uc73c\ub85c, \uac01 age group\ubcc4 \ud3c9\uade0\uc744 \ucd94\uc815\ud558\uc5ec \uac80\uc815\uc744 \uc2dc\ud589\ud558\uae30\uc5d0 \ucd94\uc815\ub41c \ubaa8\uc218\uc758 \uac1c\uc218\ub294 4\uac1c\uc774\ub2e4. contrast\ubc29\ubc95\uc5d0\uc11c \ucd94\uc815\ud55c \ubaa8\uc218\uc758 \uac1c\uc218 \ub610\ud55c 4\uac1c\uc77c \uac83\uc774\ub2e4. 4\uac1c\uc758 \uc9d1\ub2e8 \uac04\uc758 \ud3c9\uade0\ucc28\uc774\ub97c \ube44\uad50\ud558\ub294 \uac83\uc740 \uc704\uc640 \uac19\ub418, \ub300\ube44\ub97c \uc774\uc6a9\ud558\uc5ec \ub2e4\uc911\ube44\uad50\ub97c \uc2dc\ud589\ud55c \ubc29\ubc95\uc774\uae30 \ub54c\ubb38\uc774\ub2e4. GEE-exchangeable\uc744 \uc0ac\uc6a9\ud558\uc600\uc744 \ub54c \ucd94\uc815\ud558\ub294 \ubaa8\uc218\uc758 \uac1c\uc218\ub294 5\uac1c\uc774\ub2e4. age group\uac04\uc758 covariance matrix\ub97c \ucd94\uc815\ud558\uc5ec \uadf8\ub8f9\uac04\uc758 \ucc28\uc774\ub97c \ud655\uc778\ud558\ub294 \uacfc\uc815\uc5d0\uc11c age\uc758 \uacf5\ubd84\uc0b0\ud589\ub82c\uc740 \\(4\\times4\\) \ud589\ub82c\uc774\uba70, exchangeable\ub85c \ubd84\uc0b0\uc744 \uc81c\uc678\ud55c \uacf5\ubd84\uc0b0\uc6d0\uc18c\ub97c \uac19\uc740 \uac12\uc73c\ub85c \ucd94\uc815\ud558\uc600\uae30 \ub54c\ubb38\uc5d0, \ucd94\uc815\uc5d0 \uc0ac\uc6a9\ub41c \ubaa8\uc218\uc758 \uac1c\uc218\ub294 5\uac1c\ub2e4. Mixed model\uc5d0\uc11c \ucd94\uc815\ub41c \ubaa8\uc218\uc758 \uac1c\uc218\ub294 2\uac1c\uc774\ub2e4. \uc784\uc758\ud6a8\uacfc\ub97c \uacc1\ub4e4\uc778 \ud63c\ud569\ubaa8\ud615\uc5d0\uc11c \uc8fc\ub41c \uad00\uc2ec\uc0ac\ub294 \uc784\uc758\ub85c \uad00\uce21\uce58\ub4e4\uc744 \ucd94\ucd9c\ud558\uc5ec \uadf8\ub4e4\uc758 \ubd84\uc0b0\uc5d0 \ub300\ud574\uc11c \ud655\uc778\ud558\ub294 \uac83\uc774\ub2e4. \ubb38\uc81c\uc5d0\uc11c \uc808\ud3b8\uacfc \uae30\uc6b8\uae30 \ubaa8\ub450\uc5d0\uac8c \uc784\uc758\ud6a8\uacfc\ub97c \ucd94\uac00\ud558\uc5ec \ubaa8\ub378\uc2dd\uc744 \ucd94\uc815\ud558\uc600\uae30\uc5d0, \uc774 \ub450 \ubaa8\uc218\uc5d0 \ub300\ud55c \ucd94\uc815\uac12\uc740 2\uac1c\uc774\ub2e4. \uc704\uc758 4\uac00\uc9c0 \ubc29\ubc95\uc758 \uac00\uc7a5 \ud070 \ucc28\uc774\ub85c\ub294 \uc8fc\ub41c \uad00\uc2ec\uc0ac\uac00 \ubb34\uc5c7\uc774\ub0d0\ub85c \uc811\uadfc\ud560 \uc218 \uc788\ub2e4. CRBD\uc640 contrast \ubc29\ubc95\uc740 \uc9d1\ub2e8 \uac04\uc758 \ud3c9\uade0\ucc28\uc774\uac00 \uc788\ub294\uc9c0\uc758 \uc5ec\ubd80\ub97c \ud655\uc778\ud558\ub294 \uac83\uc73c\ub85c, age\uac00 \uc2dc\uac04\uc5d0 \uc9c0\ub0a8\uc5d0 \ub530\ub77c \uc5b4\ub5a4 \uacbd\ud5a5\uc131\uc744 \ub744\ub294\uc9c0\ub294 \ud655\uc778\ud560 \uc218 \uc5c6\uace0 \uc624\ub85c\uc9c0 \uadf8 \uac12\uc758 \ucc28\uc774\uac00 \uc720\uc758\ubbf8\ud55c\uc9c0\ub9cc\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc GEE\uc640 Mixed model\uc740 \uc2dc\uac04\uc5d0 \ub530\ub978 covariance\ub97c \uace0\ub824\ud558\uae30 \ub54c\ubb38\uc5d0 \ucc28\uc774 \ubfd0\uc544\ub2c8\ub77c, age\uc5d0 \ub530\ub978 Y\uac12\uc758 \uc99d\uac10 \ub4f1\uc758 pattern\uc744 \uccb4\ud06c\ud560 \uc218 \uc788\ub2e4. 3. 3-1. #gee no <- c(283,30,140,21,274,26,134,18,266,32,134,14) yes <- c(17,20,12,14,24,26,14,21,28,24,22,17) previous <- c(0,1,0,1,0,1,0,1,0,1,0,1) s <- c(0,0,1,1,0,0,1,1,0,0,1,1) t <- c(10,10,10,10,9,9,9,9,8,8,8,8) data <- cbind(no,yes,previous,s,t) data <- as.data.frame(data) library(gee) gee <- gee(cbind(yes,no)~previous+s+t,id=1:nrow(data),family=binomial,corstr='exchangeable',data=data) ## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27 ## running glm to get initial regression estimate ## (Intercept) previous s t ## -0.2925632 2.2110745 0.2959619 -0.2428077 summary(gee) ## ## GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA ## gee S-function, version 4.13 modified 98/01/27 (1998) ## ## Model: ## Link: Logit ## Variance to Mean Relation: Binomial ## Correlation Structure: Exchangeable ## ## Call: ## gee(formula = cbind(yes, no) ~ previous + s + t, id = 1:nrow(data), ## data = data, family = binomial, corstr = \"exchangeable\") ## ## Summary of Residuals: ## Min 1Q Median 3Q Max ## 11.91869 15.79896 20.05870 23.61013 27.90335 ## ## ## Coefficients: ## Estimate Naive S.E. Naive z Robust S.E. Robust z ## (Intercept) -0.2925632 0.52898439 -0.5530658 0.40667174 -0.7194086 ## previous 2.2110745 0.09891071 22.3542483 0.09359790 23.6231200 ## s 0.2959619 0.09775188 3.0276852 0.07310746 4.0483131 ## t -0.2428077 0.05918439 -4.1025632 0.04489908 -5.4078543 ## ## Estimated Scale Parameter: 0.3909426 ## Number of Iterations: 1 ## ## Working Correlation ## [,1] ## [1,] 1 data\ub97c \ub9cc\ub4e4\uace0 GEE method\ub97c \uc0ac\uc6a9\ud55c \uacb0\uacfc\ub294 \uc704\uc640 \uac19\ub2e4. \ubd84\uc11d\ud55c\ub2e4\uba74, \uc2dc\uac04\uc774 \uc9c0\ub0a8\uc5d0 \ub530\ub77c illness\uc5d0 \uac78\ub9b4 \ud655\ub960\uc740 \ub0ae\uc544\uc9d0\uc744 \ud655\uc778\ud560 \uc218 \uc788\uace0, \uc0b0\ubaa8\uc758 \ud761\uc5f0\ub825\uc740 illness\uc5d0 positive\ud55c \uc601\ud5a5\uc744, \uadf8\ub9ac\uace0 \ubc18\ubcf5\uce21\uc815\uc5d0 \uc788\uc5b4\uc11c \uc0ac\uc804\ubcd1\ub825\uc774 \uc774\ud6c4\uc758 age\uc758 illness\ud655\ub960\uc744 \uc99d\uac00\uc2dc\ud0a4\ub294 \uac83\uc73c\ub85c \ud655\uc778\ub41c\ub2e4. 3-2. #regressive logistic model fit1 <- glm(cbind(yes,no)~previous+s+t,family=binomial, data=data) summary(fit1) ## ## Call: ## glm(formula = cbind(yes, no) ~ previous + s + t, family = binomial, ## data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.98143 -0.30047 -0.09456 0.36684 0.96039 ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -0.29256 0.84603 -0.346 0.7295 ## previous 2.21107 0.15819 13.977 <2e-16 *** ## s 0.29596 0.15634 1.893 0.0583 . ## t -0.24281 0.09466 -2.565 0.0103 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 207.2212 on 11 degrees of freedom ## Residual deviance: 3.1186 on 8 degrees of freedom ## AIC: 64.392 ## ## Number of Fisher Scoring iterations: 3 regressive logistic model\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5bb\uc740 \uacb0\uacfc\ub294 \uc704\uc640 \uac19\ub2e4. \ubaa8\ub378\uc5d0 \ub300\ud574 \ucd94\uc815\ud55c \uacc4\uc218\uac00 \uc704\uc758 gee method\uc640 \uac19\uac8c \ub098\uc624\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc73c\uba70, \ud574\uc11d\uc740 \uc704\uc758 \ubc29\ubc95\uacfc \ub3d9\uc77c\ud558\uac8c \ubc1b\uc544\ub4e4\uc77c \uc218 \uc788\ub2e4. \ucd94\uac00\uc801\uc73c\ub85c \ud655\uc778\ud560 \uc218 \uc788\ub294 \uac83\uc740, p-value\uac00 \ud1b5\uacc4\uc801\uc73c\ub85c \uc720\uc758\ud558\uac8c \ub098\uc624\ub294 \uac83\uc73c\ub85c \ubcf4\uc544 \uac01 \ubcc0\uc218\ub4e4\uc774 illness\uc5d0 \ub300\ud574 \uc720\uc758\ubbf8\ud558\uac8c \uc791\uc6a9\ud558\uace0 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. 3-3. 4. 4-1. Draw the histogram geyser <- c(80,71,57,80,75,77,60,86,77,56,81,50,89,54,90,73,60,83,65,82,84, 54,85,58,79,57,88,68,76,78,74,85,75,65,76,58,91,50,87,48,93,54, 86,53,78,52,83,60,87,49,80,60,92,43,89,60,84,69,74,71,108,50,77, 57,80,61,82,48,81,73,62,79,54,80,73,81,62,81,71,79,81,74,59,81, 66,87,53,80,50,87,51,82,58,81,49,92,50,88,62,93,56,89,51,79,58, 82,52,88,52,78,69,75,77,53,80,55,87,53,85,61,93,54,76,80,81,59, 86,78,71,77,76,94,75,50,83,82,72,77,75,65,79,72,78,77,79,75,78, 64,80,49,88,54,86,51,96,50,80,78,81,72,75,78,87,69,55,83,49,82, 57,84,57,84,73,78,57,79,57,90,62,87,78,52,98,48,78,79,65,84,50, 83,60,80,50,88,50,84,74,76,65,89,49,88,51,78,85,65,75,77,69,92, 68,87,61,81,55,93,53,84,70,73,93,50,87,77,74,72,82,74,80,49,91, 53,86,49,79,89,87,76,59,80,89,45,93,72,71,54,79,74,65,78,57,87, 72,84,47,84,57,87,68,86,75,73,53,82,93,77,54,96,48,89,63,84,76, 62,83,50,85,78,78,81,78,76,74,81,66,84,48,93,47,87,51,78,54,87, 52,85,58,88,79) hist(geyser,main='Histogram of Old faithful geyser Eruption', xlab='# of Eruption',prob=T,breaks=30) 4-2. hist(geyser,main='Histogram of Old faithful geyser Eruption', xlab='# of Eruption',prob=T,col='grey',breaks=30) lines(density(geyser)) lines(density(geyser,bw=1.5,kernel=\"epanechnikov\"),col='red') legend('topright',legend='non-parametric',col='red',lty=1:2,cex=0.8) 4-3. Assume a normal distribution, and esimate the mean and standarad deviation. Fitdistr\ud568\uc218\ub97c \ud1b5\ud574 \ub178\ub9d0\ubd84\ud3ec\uc5d0 \ud53c\ud305\ud558\uc5ec \ubaa8\uc218\uac00 \uc5b4\ub5bb\uac8c \ub098\uc624\ub294\uc9c0 \ud655\uc778\ud558\uace0 \uc774\ub97c \uadf8\ub824\ubcf4\uc558\ub2e4. hist(geyser,main='Histogram of Old faithful geyser Eruption', xlab='# of Eruption',prob=T,col='grey',breaks=30) curve(dnorm(x, mean=mean(geyser), sd=sd(geyser)), col=\"blue\", lwd=2, add=T, yaxt=\"n\") legend('topright',legend='Normal Dist',col='blue',lty=1:2,cex=0.8) 4-4. EM-Algorithm\uc744 \uc2dc\ud589\ud558\uae30 \uc55e\uc11c, \uc774\ub294 geyser \ub370\uc774\ud130\uac00 \ub450\uac1c\uc758 \ubd84\ud3ec\uac00 \uc11e\uc5ec\uc788\ub294 \uac83\uc73c\ub85c \uac00\uc815\ud558\uace0 \ubb38\uc81c\ub97c \uc2dc\uc791\ud574\uc57c\ud558\ubbc0\ub85c, rough\ud55c \ubc29\ubc95\uc774\uc9c0\ub9cc, \ub370\uc774\ud130\ub97c \uc808\ubc18\uc73c\ub85c \ub098\ub208 \ud6c4 (299\uac1c\ub97c 149\uac1c\uc640 150\uac1c) \uc774\ub97c sorting\ud558\uc5ec \uac01\uac01\uc5d0 fitdistr\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\uc218\ub97c \ucd94\uc815\ud55c \ud6c4, \uc774\ub97c \ucd08\uae30\uac12\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec EM-algorithm\uc744 \uc9c4\ud589\ud574\ubcf4\uae30\ub85c \ud558\uc600\ub2e4. library(MASS) ## ## Attaching package: 'MASS' ## The following object is masked _by_ '.GlobalEnv': ## ## geyser x <- sort(geyser) a <-x[1:149] b <- x[150:299] fitdistr(a,'normal') ## mean sd ## 60.7986577 9.5713357 ## ( 0.7841144) ( 0.5544526) fitdistr(b,'normal') ## mean sd ## 83.7600000 5.4548816 ## ( 0.4453892) ( 0.3149377) set.seed(2020311194) x1 <- rnorm(149,60,9) x2 <- rnorm(150,83,5) p <- 0.5 x1_mean <- 60 x1_sd <- 8 x2_mean <- 83 x2_sd <-6 x <- c(x1,x2) for (i in 1:length(geyser)){ den_x <- p*dnorm(x, x1_mean, x1_sd) + (1-p)*dnorm(x, x2_mean, x2_sd) p1 <- p*dnorm(x, x1_mean, x1_sd)/den_x p2 <- 1 - p1 pi <- sum(p1)/length(geyser) x1_mean <- sum(p1*x)/sum(p1) x2_mean <- sum(p2*x)/sum(p2) newx1_sd <- sum(p1 *(x-x1_mean)^2)/ sum(p1) x1_sd<- sqrt(newx1_sd) newx2_sd <- sum(p2 *(x-x2_mean)^2)/ sum(p2) x2_sd<- sqrt(newx2_sd) } x<-seq(min(x),max(x),len=299) density_em2 <- p*dnorm(x,x1_mean,x1_sd) + (1-p)*dnorm(x,x2_mean,x2_sd) hist(geyser,main='Histogram & EM-Algorithm using two dist', xlab='# of Eruption',prob=T,col='grey',breaks=30) lines(x,density_em2,col='deeppink') legend('topright',legend='EM-Algorithm',col='deeppink',lty=1:2,cex=0.8) pi ; x1_mean; x1_sd; x2_mean ; x2_sd ## [1] 0.520283 ## [1] 61.23908 ## [1] 9.351038 ## [1] 83.3176 ## [1] 4.99325 \ucd94\uc815\ud55c 5\uac1c\uc758 \ubaa8\uc218 \\(\\pi_1,\\ \\mu_1,\\ \\sigma_1,\\ \\mu_2,\\ \\sigma_2\\) \ub294 \uac01\uac01 0.52, 61.24, 9.35, 83.3, 4.99\uac00 \ub098\uc628\ub2e4. ####4-5. \uc774\ubc88\uc5d0\ub294 3\uac00\uc9c0\uc758 \ub178\ub9d0\ubd84\ud3ec\ub97c \uac00\uc815\ud558\uace0 \ubb38\uc81c\ub97c \uc811\uadfc\ud574\uc57c\ud558\ubbc0\ub85c, \uc704\uc758 \ubb38\uc81c\uc640 \ub611\uac19\uc740 \ubc29\uc2dd\uc5d0\uc11c \uac1c\uc218\ub9cc \ub298\ub824\uc8fc\ub294 \ud615\uc2dd\uc73c\ub85c, 3\uac1c\ub85c \uad6c\uac04\uc744 \ub098\ub204\uc5b4\uc11c \ub178\ub9d0\ubd84\ud3ec\uc5d0 fitting \uc2dc\ud0a4\uace0 \ubb38\uc81c\ub97c \uc9c4\ud589\ud558\uc600\ub2e4. x <- sort(geyser) a <-x[1:99] b <- x[100:199] c <- x[200:299] fitdistr(a,'normal') ## mean sd ## 54.8383838 5.3251966 ## ( 0.5352024) ( 0.3784452) fitdistr(b,'normal') ## mean sd ## 75.4700000 3.5339921 ## ( 0.3533992) ( 0.2498910) fitdistr(c,'normal') ## mean sd ## 86.4700000 4.6871207 ## ( 0.4687121) ( 0.3314295) set.seed(2020311194) x1 <- rnorm(99,55,5) x2 <- rnorm(100,75,3.5) x3 <- rnorm(100,86,4.7) pi1 <- 0.3 pi2 <- 0.4 x1_mean <- 55 x1_sd <- 6 x2_mean <- 75 x2_sd <-4 x3_mean <- 86 x3_sd <- 5 x <- c(x1,x2,x3) for (i in 1:length(geyser)){ den_x <- pi1*dnorm(x, x1_mean, x1_sd) + pi2*dnorm(x, x2_mean, x2_sd)+ (1-pi1-pi2)*dnorm(x, x3_mean, x3_sd) p1 <- pi1*dnorm(x, x1_mean, x1_sd)/den_x p2 <- pi2*dnorm(x, x2_mean, x2_sd)/den_x p3 <- 1-p1-p2 new_p1 <- sum(p1)/length(geyser) new_p2 <- sum(p2)/length(geyser) x1_mean <- sum(p1*x)/sum(p1) x2_mean <- sum(p2*x)/sum(p2) x3_mean <- sum(p3*x)/sum(p3) newx1_sd <- sum(p1 *(x-x1_mean)^2)/ sum(p1) x1_sd<- sqrt(newx1_sd) newx2_sd <- sum(p2 *(x-x2_mean)^2)/ sum(p2) x2_sd<- sqrt(newx2_sd) newx3_sd <- sum(p3 *(x-x3_mean)^2)/ sum(p3) x3_sd<- sqrt(newx3_sd) } x<-seq(min(x),max(x),len=299) density_em3 <- pi1*dnorm(x,x1_mean,x1_sd) + pi2*dnorm(x,x2_mean,x2_sd)+ (1-pi1-pi2)*dnorm(x,x3_mean,x3_sd) hist(geyser,main='Histogram & EM-Algorithm using three dist', xlab='# of Eruption',prob=T,col='grey',breaks=30) lines(x,density_em3,col='orange') legend('topright',legend='EM-Algorithm2',col='orange',lty=1:2,cex=0.8) new_p1 ; new_p2 ; x1_mean; x1_sd; x2_mean ; x2_sd ; x3_mean ; x3_sd ## [1] 0.3307159 ## [1] 0.3771631 ## [1] 55.10935 ## [1] 4.865457 ## [1] 75.35787 ## [1] 3.730488 ## [1] 87.44965 ## [1] 3.88705 \ucd94\uc815\ud55c 5\uac1c\uc758 \ubaa8\uc218 \\(\\pi_1,\\ \\pi_2,\\ \\mu_1,\\ \\sigma_1,\\ \\mu_2,\\ \\sigma_2,\\ \\mu_3,\\ \\sigma_3\\) \ub294 \uac01\uac01 0.33, 0.377, 55.1, 4.8, 75.35, 3.7, 87.45, 3.88\ub85c \ub098\uc654\ub2e4. \uc774\ub97c EDA\uc801 \uad00\uc810\uc5d0\uc11c \uc0b4\ud3b4\ubcf4\uba74 \uc704\uc758 \ub450\uac00\uc9c0 \ubd84\ud3ec\ub97c \uc0ac\uc6a9\ud558\uc600\ub358 EM-Algorithm\ubcf4\ub2e4\ub294 \ubaa9\ud45c\ud588\ub358 \ub178\ub9d0\ubd84\ud3ec\uc758 \ubaa8\uc218\uc640 \ub2e4\uc18c \ucc28\uc774\uac00 \uc788\ub294\ub370, \uc774\ub294 \ub450\ubc88\uc9f8 \ubd09\uc6b0\ub9ac\uc758 \uc624\ub978\ucabd \ub05d\uc774 \uc57d\uac04 \uc19f\uc544\ub098\uc788\uc9c0\ub9cc \ub450\uac1c\uc758 \ubd84\ud3ec\ub85c \ubcf4\uae30\uc5d0\ub294 \uc57d\uac04 \ubb34\ub9ac\uac00 \uc788\uae30\ub54c\ubb38\uc73c\ub85c \uc0ac\ub8cc\ub41c\ub2e4. 4-6. which one is the best among 1,2,3,4,5 ? \uc721\uc548\uc73c\ub85c \uc0b4\ud3b4\ubcf8\ub2e4\uba74 \uba3c\uc800 \ub450\uac1c\uc758 \ubd09\uc6b0\ub9ac\ub97c check\ud55c \uac83\ub4e4 \uc911\uc5d0\uc11c best\ub97c \ubf51\uc73c\ub824 \ud55c\ub2e4. \ucd94\uac00\ub85c non-parametric method\ub294 fitting\uc758 \uad00\uc810\uc5d0\uc11c\ub294 \uc5bc\ucd94 \uad1c\ucc2e\uc544 \ubcf4\uc774\uc9c0\ub9cc, smoothness\ub97c \uace0\ub824\ud574\ubcf4\uc558\uc744 \ub54c, \ucd5c\uc801\uc758 \uc120\ud0dd\uc73c\ub85c \ucc44\ud0dd\ud558\uae30\uc5d0\ub294 \uadfc\uac70\uac00 \uc870\uae08 \ubd80\uc871\ud55c \uac83\uc73c\ub85c \ud310\ub2e8\ud574 best select\uc5d0\uc11c \uc81c\uc678\ud558\uae30\ub85c \ud558\uc600\ub2e4. \ub298 \uc774\ub7f0 \ud310\ub2e8\uc5d0\uc11c\ub294 bias-variance\uac04\uc758 trade off\uac00 \ubc1c\uc0dd\ud558\uc9c0\ub9cc, \ub108\ubb34 underfitting\ud558\uc9c0 \uc54a\uc740 \uac83\uc744 \ucc44\ud0dd\ud558\uae30 \uc704\ud574 \uc774\ubd09\ubd84\ud3ec\ub97c detect\ud558\uc9c0 \ubabb\ud55c model\uc740 \ubc84\ub824\ub0b4\uba74\uc11c Bias\ub97c \uc870\uae08 \ub35c\uc5b4\ub0b4\uace0, \ub0a8\uc740 \ubaa8\ub378 \uc911\uc5d0 data\uc758 variance\ub3c4 \uace0\ub824\ud558\uace0\uc790 \ud55c\ub2e4. \uba3c\uc800 \uc0ac\uc6a9\ud588\ub358 \ubaa8\ub4e0 \ubaa8\ub378\ub4e4\uc758 density line\ub4e4\uc744 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc5d0 \ub367\ubd99\ud600 \uadf8\ub9ac\uba74 \uc544\ub798\uc640 \uac19\ub2e4. hist(geyser,main='Histogram of Old faithful geyser Eruption', xlab='# of Eruption',prob=T,breaks=30) lines(density(geyser,bw=1.5,kernel=\"epanechnikov\"),col='red') curve(dnorm(x, mean=mean(geyser), sd=sd(geyser)), col=\"blue\", lwd=2, add=T, yaxt=\"n\") lines(x,density_em2,col='deeppink') lines(x,density_em3,col='orange') legend('topright',legend=c('non-parametric','Normal','EM-ALgorithm','EM-Algorithm2'), col=c('red','blue','deeppink','orange'),lty=1:4,cex=0.8) \ub9c9\uc0c1 \uc801\uc5c8\uc9c0\ub9cc clear criterion\ub97c \ub17c\ud558\ub294 \uac83\uc744 \uc0dd\uac01\ud558\uae30\uac00 \uc5b4\ub824\uc6cc \uc5ec\uae30\uae4c\uc9c0\ub9cc \ubb38\uc81c\ub97c \ud480\uc5c8\uc2b5\ub2c8\ub2e4.. \uac10\uc0ac\ud569\ub2c8\ub2e4","title":"R Notebook"},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#intermediate-applied-statistics-final-takehome-exam","text":"","title":"Intermediate Applied Statistics Final Takehome Exam"},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#1","text":"","title":"1."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#1-1-show-that-we-can-also-check-whether-x-follows-f_0x-by-plotting-log1-f_0x-versus-log1-in1s","text":"\uc77c\ubc18\uc801\uc73c\ub85c Normal\ubd84\ud3ec\uc5d0 fitting\ud560 \ub54c \uc0ac\uc6a9\ud558\ub294 qqplot\ucc98\ub7fc, \\(-log(1-F_0(X))\\) \uc640 \\(-log(1-i/(n+1))\\) \uc758 \ube44\uad50\ub97c \ud574\ubcf4\uaca0\ub2e4. $ X \\sim F_0(X)$ \ub97c \ub530\ub978\ub2e4\uace0 \ud560 \ub54c, \\(F_0(X)\\) \ub294 \ub204\uc801\ubc00\ub3c4\ud568\uc218(cdf)\uc774\uba70, \uc774 cdf\uc758 \ubd84\ud3ec\ub294 Uniform\ubd84\ud3ec\uac00 \ub41c\ub2e4. \uc704\uc758 \uc2dd\uc744 \ub2e4\uc2dc \ud480\uc5b4\uc11c \uc4f0\uc790\uba74, \uc774\ub294 \uc8fc\uc5b4\uc9c4 \uc0d8\ud50c\ub4e4\uc744 \ud06c\uae30 \uc21c\uc11c\ub300\ub85c \ub098\uc5f4\ud558\uc600\uc744 \ub54c, \\(x_{(i)} \\approx F^{-1}(\\frac{i}{n+1})\\) \uc744 \ube44\uad50\ud558\ub294 \uac83\uc774 \uadf8\ub9bc\uc744 \uadf8\ub9ac\ub294 \uac83\uacfc \ud568\uaed8 \ubb38\uc81c\uc758 \ub2f5\uc744 \ubcf4\uc774\ub294 \uac83\uc774 \ub41c\ub2e4. \uc774\ub97c \uc774\uc6a9\ud558\uc5ec \uc9c1\uc811 plot\uc744 \uadf8\ub824 qqplot\uacfc \ube44\uc2b7\ud55c form\uc744 \ub9cc\ub4e4\uc5b4\ubcf4\uace0, \\(x_{(i)} \\approx F^{-1}(\\frac{i}{n+1})\\) \uc774 \ub458\uc758 \uad00\uacc4 \ub610\ud55c \ub450 \ud56d\uc758 \ucc28\uc774\uac00 0\uacfc \ube44\uc2b7\ud55c\uc9c0\ub97c \uccb4\ud06c\ud558\ub294 \uacfc\uc815\uc744 \ud1b5\ud574 \ubcf4\uc600\ub2e4. set.seed(2020311194) # 100 sample form uniform a <- runif(100) x <- sort(a) #check plot i <- 1:100 plot(-log(1-x),-log(1-i/(length(x)+1))) #check the difference of two values c <- rep(0,length(x)) for (i in 1:length(x)){ c[i] <- x[i] - qunif(i/(length(x)+1)) } c ## [1] -0.0020358902 0.0032247549 0.0044838281 -0.0037792539 -0.0090086398 ## [6] -0.0178934427 -0.0260260130 -0.0272017346 -0.0303446013 -0.0296651059 ## [11] -0.0255229942 -0.0327193141 -0.0380216380 -0.0427320365 -0.0269755046 ## [16] -0.0350967352 -0.0414608830 -0.0448804717 -0.0537392683 -0.0428289944 ## [21] -0.0196609758 0.0039456506 -0.0020984610 -0.0006890500 -0.0061154514 ## [26] -0.0068897217 -0.0157445365 -0.0090605863 -0.0155620024 -0.0239936088 ## [31] -0.0317101204 -0.0399383068 -0.0026076430 0.0214324310 0.0242884377 ## [36] 0.0179033486 0.0320410312 0.0245256318 0.0191693473 0.0114126480 ## [41] 0.0147882477 0.0071808881 0.0376980454 0.0344877337 0.0414879340 ## [46] 0.0346011515 0.0567058272 0.0498212035 0.0416583978 0.0508939199 ## [51] 0.0422886903 0.0531330912 0.0500616136 0.0405467347 0.0401381934 ## [56] 0.0407216133 0.0322004060 0.0305376502 0.0290926693 0.0304862390 ## [61] 0.0310735891 0.0658745717 0.0578853876 0.0615256389 0.0540617281 ## [66] 0.0452655280 0.0404460451 0.0447246316 0.0353560281 0.0433776455 ## [71] 0.0438249843 0.0442056699 0.0396245396 0.0448213800 0.0573250927 ## [76] 0.0510092447 0.0492991438 0.0394761377 0.0366026018 0.0368163020 ## [81] 0.0370831460 0.0307857112 0.0387236285 0.0680595882 0.0612288510 ## [86] 0.0586341999 0.0559009177 0.0504839753 0.0427199190 0.0378402503 ## [91] 0.0315178029 0.0330100886 0.0246411754 0.0161714742 0.0081281122 ## [96] 0.0010923949 0.0031889546 0.0009318988 0.0129626816 0.0050487876 sum(c) ## [1] 1.757704 Plot\uc744 \uc0b4\ud3b4\ubcf4\uc558\uc744 \ub54c \ub300\ub7b5\uc801\uc73c\ub85c 45\ub3c4\uc758 \uac01\ub3c4\ub85c \ub450 \uac12\ub4e4\uc774 \uc798 fitting\uc774 \ub418\ub294 \uac83\uc744 \ud655\uc778 \ud560 \uc218 \uc788\uc73c\uba70, 100\uac1c\uc758 \uc0d8\ud50c\uc744 \ubf51\uc544\uc11c \uc9c4\ud589\ud558\uc5ec\uc11c \ubaa8\ub4e0 \ud56d\ub4e4\uc758 \ucc28\uc774\uac00 0\uc5d0 \uac00\uae4c\uc6b4 \uac12\uc744 \uac16\ub294 \uac83\uc744 \ud655\uc778\ud558\uc600\ub2e4.","title":"1-1. Show that we can also check whether \\(X\\) follows \\(F_0(x)\\) by plotting \\(-log(1-F_0(X))\\) versus \\(-log(1-i/(n+1))'s.\\)"},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#1-2-suppose-that-f_0-is-the-normal-distribution-take-a-random-sample-from-f_0-and-draw-two-plots-in-1-then-discuss-the-relation-between-two-plots","text":"set.seed(2020311194) a <- rnorm(100,0,1) x <- sort(a) i <- 1:length(a) #Q-Q plot plot(x,qnorm(i/(length(x)+1))) #-log(1-F(X)) vs -log(1-i/(n+1)) plot(-log(1-pnorm(x)), -log(1-i/(length(x)+1))) plot\uc744 \uadf8\ub824\uc11c \ud655\uc778\ud574\ubcf4\uc558\uc744 \ub54c, X\ub97c normal\ubd84\ud3ec\uc5d0\uc11c \uc0d8\ud50c\ub9c1\uc744 \ud558\uc5ec\ub3c4, \uc704\uc758 \ubb38\uc81c\uc5d0\uc11c \uad6c\ud55c \uac83\ucc98\ub7fc fiitting\uc774 \uc798 \ub418\ub294 \uac83\uc744 \ubcfc \uc218 \uc788\ub2e4.","title":"1-2. Suppose that \\(F_0\\) is the normal distribution. Take a random sample from \\(F_0\\) and draw two plots in 1). Then discuss the relation between two plots."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#1-3-suppose-that-the-data-includes-some-x_is-which-are-actually-the-censored-observations-at-x_is-then-describe-how-we-can-check-whether-x-follows-f_0x","text":"\uc0dd\uc874\ubd84\uc11d\uc5d0 \uc788\uc5b4\uc11c \uac1c\uac1c\uc778\ub4e4\uc758 \uc0dd\uc874\uc2dc\uac04\uc5d0 \ub300\ud558\uc5ec \ubd84\uc11d\uc744 \uc2dc\ud589\ud560 \ub54c, \uba87\uba85\uc758 \uc0ac\ub78c\ub4e4\uc758 \uc815\ubcf4\uac00 censored \ub418\uc5b4\ub3c4, \uc0dd\uc874\uc2dc\uac04\uc5d0 \ub300\ud55c observation value\uc640 estimated value\uac04\uc758 \ubd84\uc11d\uc774 \uac00\ub2a5\ud558\uae30\ub3c4 \ud558\ub2e4, \uc774\ub294 Cox - Snell Residual\ub97c \uadf8\ub824\ubd04\uc73c\ub85c\uc368 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \ud754\ud788 survivorship function\uc5d0 \ub300\ud558\uc5ec Uniform \ubd84\ud3ec\ub97c \uac00\uc815\ud558\uc5ec \uc0dd\uc874\ubd84\uc11d\uc744 \uc9c4\ud589\ud558\ub294\ub370, \uc0dd\uc874\ud568\uc218 \\(S(x)\\) \ub294 \\(1-F_0(x)\\) \uc640 \uac19\ub2e4. 1-1\uc758 \ubb38\uc81c\uc5d0\uc11c \uc0ac\uc6a9\ud558\uc600\ub358, Uniform\ubd84\ud3ec\uc758 log transformation\uc744 \ud1b5\ud558\uc5ec Exponential \ubd84\ud3ec\ub97c \ub9cc\ub4dc\ub294 \uac83\uc744 \ub5a0\uc62c\ub824 \ubcf8\ub2e4\uba74, \uc704\uc758 \uacbd\uc6b0\uc758 quantile \uac04\uc758 \ube44\uad50\uac00 cox-snell residual method\ub97c \ud1b5\ud574 \ud480 \uc218 \uc788\uc74c\uc744 \uc2dc\uc0ac\ud55c\ub2e4. Cox - snell residual\uc740, \ub450 \uac00\uc9c0\uc758 \ubc29\ubc95\uc744 \ud1b5\ud574 censored\ub41c \uac1c\ubcc4 \uc0dd\uc874\uc2dc\uac04\uc5d0 \ub300\ud558\uc5ec \uc0c8\ub85c\uc6b4 \uac12\uc73c\ub85c \ub300\uccb4\ud558\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\ub294\ub370, \uc774\ub294 \ud3c9\uade0 \ud639\uc740 \uc911\uc704\uc218\ub97c censored\ub41c time\uc5d0 \ub354\ud558\ub294 \ubc29\ubc95\uc774\ub2e4. \uc5ec\uae30\uc11c \\(-log(1-F_0(x))\\) \ub294 cumulative hazard function\uc774\uba70, \uc774\ub294 \uc704\uc5d0\uc11c \ud655\uc778\ud588\ub358 \ubc14\uc640 \uac19\uc774 Exponential \ubd84\ud3ec\ub97c \uac16\uac8c\ub41c\ub2e4. censored\ub41c point \\(x_{i}+'s\\) \ub294 \\(x_{i}+E(-log(1-F_0(X_1)))\\) \ud639\uc740 \\(x_{i}+Median(-log(1-F_0(X_1)))\\) \uc758 \uac12\uc73c\ub85c \ub300\uccb4\ub41c\ub2e4. cumulative hazard function\uc774 \\(Exp(1)\\) \uc744 \ub530\ub974\ubbc0\ub85c, \ub300\uccb4\ub420 \ub450 \uac12 \uc5ed\uc2dc \\(Exp(1)\\) \ub97c \ub530\ub978\ub2e4. \uc774\ub97c \ud1b5\ud574\uc11c censored\ub41c \uac12\uc744 \uc0c8\ub85c\uc6b4 \uac12\uc73c\ub85c \ub300\uccb4\ud558\ub354\ub77c\ub3c4, quantile\uc744 \ube44\uad50\ud558\uace0\uc790\ud558\ub294 \uac12\ub4e4\uc758 \ubd84\ud3ec\ub294 \uc5ec\uc804\ud788 \uadf8\ub300\ub85c\uba70, \uc218\uce58\uc640 \ubc94\uc8fc\uac00 \ud63c\ud569\ub41c \uc790\ub8cc\ub4e4\uc744 numerical\ud558\uac8c \ub300\uccb4\ud568\uc73c\ub85c\uc368 X\uac00 \\(F_0(x)\\) \uc758 \ubd84\ud3ec\ub97c \ub530\ub974\ub294\uc9c0\uc758 \uc5ec\ubd80\ub97c check\ud560 \uc218 \uc788\ub2e4.","title":"1-3. Suppose that the data includes some \\(x_{i}+'s\\) which are actually the censored observations at \\(x_{i}+'s\\). Then describe how we can check whether X follows \\(F_0(x)\\)?"},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#2","text":"","title":"2."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#2-1","text":"boy <- c(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8,8,8,8,9,9,9,9,10,10,10,10, 11,11,11,11,12,12,12,12,13,13,13,13,14,14,14,14,15,15,15,15,16,16,16,16,17,17,17,17,18,18,18,18, 19,19,19,19,20,20,20,20) age <- rep(c(8,8.5,9,9.5),4) height <- c(47.8,48.8,49,49.7,46.4,47.3,47.7,48.4, 46.3,48.8,47.8,48.5,45.1,45.3,46.1,47.2, 47.6,48.5,48.9,49.3,52.5,53.2,53.3,53.7, 51.2,53,54.3,54.5,49.8,50,50.3,52.7, 48.1,50.8,52.3,54.5,45,47,47.3,48.3, 51.2,51.4,51.6,51.9,48.5,49.2,53,55.5, 52.1,52.8,53.7,55,48.2,48.9,49.3,49.8, 49.6,50.4,51.2,51.8,50.7,51.8,52.7,53.3, 47.2,47.7,48.4,49.5,53.3,54.6,55.1,55.3, 46.2,47.5,48.1,48.4,46.3,47.6,51.3,51.8) data <- cbind(boy,age,height) colnames(data) <- c('boy','age','height') data <- as.data.frame(data) #CRBD data$age <- as.factor(data$age) data.crbd.aov <- aov(height~age+boy,data=data) summary(data.crbd.aov) ## Df Sum Sq Mean Sq F value Pr(>F) ## age 3 85.6 28.55 4.486 0.00597 ** ## boy 1 32.3 32.30 5.075 0.02720 * ## Residuals 75 477.3 6.36 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Randomized Completed Block Design\uc744 \ud1b5\ud574 \ub124 \uc9d1\ub2e8\uac04\uc758 \ud3c9\uade0\ucc28\uc774\ub97c \ube44\uad50\ud574\ubcf4\uc558\uc744 \ub54c, age\uadf8\ub8f9\uc5d0 \ub300\ud55c p-value\ub294 0.00597\uc778 \uac83\uc73c\ub85c \ubcf4\uc544, \uadf8\ub8f9\uac04\uc758 \ucc28\uc774\uac00 \uc720\uc758\ubbf8\ud558\uac8c \uc874\uc7ac\ud55c\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.","title":"2-1."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#2-2","text":"#Contrast Method age8 <- data[data$age==8,3] age8.5 <- data[data$age==8.5,3] age9 <- data[data$age==9,3] age9.5 <- data[data$age==9.5,3] m <- cbind(age8,age8.5,age9,age9.5) #sample covariance matrix mean <- apply(m,2,mean) cov <- cov(m) mean <- matrix(mean,4,1) c1 <- rbind(c(1,-1,0,0),c(0,0,1,-1),c(1,1,-1,-1)) c2 <- rbind(c(-3,-1,1,3),c(1,-1,-1,1),c(-1,3,-3,1)) test_statistics1 <- 20*t(c1%*%mean)%*%solve(c1%*%cov%*%t(c1))%*%(c1%*%mean) test_statistics1 ## [,1] ## [1,] 81.95183 qf(0.975,1,76) ## [1] 5.22889 test_statistics2 <- 20*t(c2%*%mean)%*%solve(c2%*%cov%*%t(c2))%*%(c2%*%mean) test_statistics2 ## [,1] ## [1,] 81.95183 qf(0.975,1,76) ## [1] 5.22889 Contrast\ub97c \uc774\uc6a9\ud558\uc5ec \ub124 \uc9d1\ub2e8 \uac04\uc758 \ud3c9\uade0\ucc28\uc774\ub97c \uac80\uc815\ud55c \uacb0\uacfc\ub294 \uc704\uc640 \uac19\ub2e4. Orthogonal contrasts(\uc9c1\uad50\ub300\ube44)\ub97c \uc0ac\uc6a9\ud558\uc600\uc744 \ub54c Test Statistics\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(T^2\\ =\\ n(C{\\bar(y)})'(CSC')^{-1}(C\\bar(y))\\) \uc5ec\uae30\uc11c S\ub294 sample covariance matrix\uc774\uba70, \\(\\bar{y}\\) \ub294 \uadf8\ub8f9(age)\ubcc4 \ud3c9\uade0, C\ub294 \ub300\ube44\ud589\ub82c\uc774\ub2e4. \uac80\uc815\ud1b5\uacc4\ub7c9\uc758 \uac12\uc774 \uc8fc\uc5b4\uc9c4 \ub450 \ub300\ube44\ud589\ub82c\uc5d0 \ub300\ud558\uc5ec \uac19\uac8c \ub098\uc624\ub294 \uac83\uc744 \ud655\uc778\ud588\uc73c\uba70, \\(F_{\\alpha,1,N-a}\\) \uc640 \ube44\uad50\ud558\uc600\uc744 \ub54c, \\(F_{0.975,1,76} \\approx\\ 5.23\\) \uc774\ubbc0\ub85c, \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud558\uc5ec \\(\\mu_{age8} \\neq \\mu_{age8.5}\\) , \\(\\mu_{age8.5} \\neq \\mu_{age9}\\) , \\(\\mu_{age9} \\neq \\mu_{age9.5}\\) \ub77c\uace0 \uacb0\ub860 \uc9c0\uc744 \uc218 \uc788\ub2e4.","title":"2-2."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#2-3","text":"library(gee) gee <- gee(height~age+boy, family='gaussian',id=boy,corstr='exchangeable', data=data) ## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27 ## running glm to get initial regression estimate ## (Intercept) age8.5 age9 age9.5 boy ## 47.498026 1.075000 1.915000 2.800000 0.110188 summary(gee) ## ## GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA ## gee S-function, version 4.13 modified 98/01/27 (1998) ## ## Model: ## Link: Identity ## Variance to Mean Relation: Gaussian ## Correlation Structure: Exchangeable ## ## Call: ## gee(formula = height ~ age + boy, id = boy, data = data, family = \"gaussian\", ## corstr = \"exchangeable\") ## ## Summary of Residuals: ## Min 1Q Median 3Q Max ## -3.9915977 -2.0600940 -0.2338158 1.9876410 4.3408459 ## ## ## Coefficients: ## Estimate Naive S.E. Naive z Robust S.E. Robust z ## (Intercept) 47.498026 1.12410540 42.254068 1.00232618 47.387794 ## age8.5 1.075000 0.29693589 3.620310 0.15441422 6.961794 ## age9 1.915000 0.29693589 6.449203 0.28366133 6.751008 ## age9.5 2.800000 0.29693589 9.429645 0.36048578 7.767297 ## boy 0.110188 0.09260266 1.189901 0.09118725 1.208370 ## ## Estimated Scale Parameter: 6.363825 ## Number of Iterations: 1 ## ## Working Correlation ## [,1] [,2] [,3] [,4] ## [1,] 1.0000000 0.8614498 0.8614498 0.8614498 ## [2,] 0.8614498 1.0000000 0.8614498 0.8614498 ## [3,] 0.8614498 0.8614498 1.0000000 0.8614498 ## [4,] 0.8614498 0.8614498 0.8614498 1.0000000 GEE Method\ub97c \ud1b5\ud558\uc5ec \uc5bb\uc740\uacb0\uacfc\ub294 \uc704\uc640 \uac19\ub2e4. Summary\ub97c \ud1b5\ud574 input\uac12\uc774 \ud55c\ub2e8\uc704 \uc99d\uac00\ud560 \ub54c\uc758 age\uc5d0 \ub530\ub978 Y\uac12(\uc5ec\uae30\uc120 Bone\uc758 Height)\uc758 \ubcc0\ud654\ub97c \ud655\uc778\ud560 \uc218 \uc788\ub2e4. Exchangeable\ub85c covariance structure\ub97c \uc9f0\uc73c\uba70, Y\uac12 jaw's bone height\uac00 age\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ub354 \ud070 \uc601\ud5a5\uc744 \ubc1b\uc544 \ub354 \ud06c\uac8c \uc99d\uac00\ud55c\ub2e4\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218\uc788\ub2e4.","title":"2-3."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#2-4","text":"library(nlme) mm <- lme(height~age,random=~age|boy,data=data) anova(mm) ## numDF denDF F-value p-value ## (Intercept) 1 57 8412.129 <.0001 ## age 3 57 27.317 <.0001 summary(mm) ## Linear mixed-effects model fit by REML ## Data: data ## AIC BIC logLik ## 266.9375 301.8985 -118.4688 ## ## Random effects: ## Formula: ~age | boy ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 2.4915026 (Intr) age8.5 age9 ## age8.5 0.5071428 -0.202 ## age9 1.2038249 -0.144 0.576 ## age9.5 1.5782937 -0.168 0.423 0.969 ## Residual 0.3498422 ## ## Fixed effects: height ~ age ## Value Std.Error DF t-value p-value ## (Intercept) 48.655 0.5625822 57 86.48514 0 ## age8.5 1.075 0.1584255 57 6.78552 0 ## age9 1.915 0.2910304 57 6.58007 0 ## age9.5 2.800 0.3698507 57 7.57062 0 ## Correlation: ## (Intr) age8.5 age9 ## age8.5 -0.212 ## age9 -0.169 0.514 ## age9.5 -0.189 0.393 0.912 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.77709165 -0.27034918 0.01023047 0.26877890 1.86032622 ## ## Number of Observations: 80 ## Number of Groups: 20 Mixed model\uc744 \uc774\uc6a9\ud558\uace0, \uac19\uc740 \uac1c\uc778\uc5d0 \ub300\ud574\uc11c \uc2dc\uac04\uc5d0 \ub530\ub77c \ubc18\ubcf5\uce21\uc815\ud55c \uc790\ub8cc\uc5d0 \ub300\ud55c \ubd84\uc11d\uc744 \uc2dc\ud589\ud558\uae30 \ub54c\ubb38\uc5d0 time\uc5d0 \ub530\ub77c covariance\uac00 \uc77c\uc815\ud558\uc9c0 \uc54a\uc744 \uac83\uc774\ub77c\ub294 \uae30\ubcf8 \uac00\uc815\uc744 \ud55c \ud6c4 \ucd94\uc815\ud558\uace0\uc790\ud558\ub294 \ubaa8\ub378\uc2dd\uc5d0\uc11c \uc808\ud3b8\uacfc \uae30\uc6b8\uae30\uc5d0 \ub300\ud574 \ub458\ub2e4 random effect\ub97c \uc8fc\ub3c4\ub85d \ubaa8\ub378\uc744 \uad6c\uc131\ud558\uc600\ub2e4. \uc774\ub294 \uc218\uc2dd\ud654 \ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\[y_{ij} = \\beta_0+\\beta_1t_{ij}+b_{0i}+b_{1i}t_{ij}+e_{ij}\\] \uacb0\uacfc\ub97c \ubcf4\uc790\uba74, \uac01 age\uc5d0 \ub300\ud558\uc5ec p-value\uac00 \uc720\uc758\ud558\uac8c \ub098\uc624\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc73c\uba70, age\uc758 \uc99d\uac10\uc5d0 \ub530\ub77c Y\uac12 bone height \ub610\ud55c \uc99d\uac00\ub7c9\uc774 \uc720\uc758\ubbf8\ud558\uac8c \uc99d\uac00\ud558\ub294 \ucd94\uc138 \ub610\ud55c \ud655\uc778 \ud560 \uc218 \uc788\ub2e4.","title":"2-4."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#2-5","text":"\uc704\uc758 \ub124\uac00\uc9c0 \ubc29\ubc95\uc5d0\uc11c \ucd94\uc815\ud55c \ubaa8\uc218\uc758 \uac1c\uc218\ub294 \uccab\uc9f8\ub85c CRBD\ubc29\ubc95\uc758 \uacbd\uc6b0\ub294 4\uac1c\uac00 \ub41c\ub2e4. Boy\ub97c block\uc73c\ub85c \uc124\uc815\ud558\uace0, age \uc9d1\ub2e8\uac04\uc758 \ud3c9\uade0\ucc28\uc774\ub97c \ube44\uad50\ud558\uace0\uc790 \ud558\ub294 \uac83\uc73c\ub85c, \uac01 age group\ubcc4 \ud3c9\uade0\uc744 \ucd94\uc815\ud558\uc5ec \uac80\uc815\uc744 \uc2dc\ud589\ud558\uae30\uc5d0 \ucd94\uc815\ub41c \ubaa8\uc218\uc758 \uac1c\uc218\ub294 4\uac1c\uc774\ub2e4. contrast\ubc29\ubc95\uc5d0\uc11c \ucd94\uc815\ud55c \ubaa8\uc218\uc758 \uac1c\uc218 \ub610\ud55c 4\uac1c\uc77c \uac83\uc774\ub2e4. 4\uac1c\uc758 \uc9d1\ub2e8 \uac04\uc758 \ud3c9\uade0\ucc28\uc774\ub97c \ube44\uad50\ud558\ub294 \uac83\uc740 \uc704\uc640 \uac19\ub418, \ub300\ube44\ub97c \uc774\uc6a9\ud558\uc5ec \ub2e4\uc911\ube44\uad50\ub97c \uc2dc\ud589\ud55c \ubc29\ubc95\uc774\uae30 \ub54c\ubb38\uc774\ub2e4. GEE-exchangeable\uc744 \uc0ac\uc6a9\ud558\uc600\uc744 \ub54c \ucd94\uc815\ud558\ub294 \ubaa8\uc218\uc758 \uac1c\uc218\ub294 5\uac1c\uc774\ub2e4. age group\uac04\uc758 covariance matrix\ub97c \ucd94\uc815\ud558\uc5ec \uadf8\ub8f9\uac04\uc758 \ucc28\uc774\ub97c \ud655\uc778\ud558\ub294 \uacfc\uc815\uc5d0\uc11c age\uc758 \uacf5\ubd84\uc0b0\ud589\ub82c\uc740 \\(4\\times4\\) \ud589\ub82c\uc774\uba70, exchangeable\ub85c \ubd84\uc0b0\uc744 \uc81c\uc678\ud55c \uacf5\ubd84\uc0b0\uc6d0\uc18c\ub97c \uac19\uc740 \uac12\uc73c\ub85c \ucd94\uc815\ud558\uc600\uae30 \ub54c\ubb38\uc5d0, \ucd94\uc815\uc5d0 \uc0ac\uc6a9\ub41c \ubaa8\uc218\uc758 \uac1c\uc218\ub294 5\uac1c\ub2e4. Mixed model\uc5d0\uc11c \ucd94\uc815\ub41c \ubaa8\uc218\uc758 \uac1c\uc218\ub294 2\uac1c\uc774\ub2e4. \uc784\uc758\ud6a8\uacfc\ub97c \uacc1\ub4e4\uc778 \ud63c\ud569\ubaa8\ud615\uc5d0\uc11c \uc8fc\ub41c \uad00\uc2ec\uc0ac\ub294 \uc784\uc758\ub85c \uad00\uce21\uce58\ub4e4\uc744 \ucd94\ucd9c\ud558\uc5ec \uadf8\ub4e4\uc758 \ubd84\uc0b0\uc5d0 \ub300\ud574\uc11c \ud655\uc778\ud558\ub294 \uac83\uc774\ub2e4. \ubb38\uc81c\uc5d0\uc11c \uc808\ud3b8\uacfc \uae30\uc6b8\uae30 \ubaa8\ub450\uc5d0\uac8c \uc784\uc758\ud6a8\uacfc\ub97c \ucd94\uac00\ud558\uc5ec \ubaa8\ub378\uc2dd\uc744 \ucd94\uc815\ud558\uc600\uae30\uc5d0, \uc774 \ub450 \ubaa8\uc218\uc5d0 \ub300\ud55c \ucd94\uc815\uac12\uc740 2\uac1c\uc774\ub2e4. \uc704\uc758 4\uac00\uc9c0 \ubc29\ubc95\uc758 \uac00\uc7a5 \ud070 \ucc28\uc774\ub85c\ub294 \uc8fc\ub41c \uad00\uc2ec\uc0ac\uac00 \ubb34\uc5c7\uc774\ub0d0\ub85c \uc811\uadfc\ud560 \uc218 \uc788\ub2e4. CRBD\uc640 contrast \ubc29\ubc95\uc740 \uc9d1\ub2e8 \uac04\uc758 \ud3c9\uade0\ucc28\uc774\uac00 \uc788\ub294\uc9c0\uc758 \uc5ec\ubd80\ub97c \ud655\uc778\ud558\ub294 \uac83\uc73c\ub85c, age\uac00 \uc2dc\uac04\uc5d0 \uc9c0\ub0a8\uc5d0 \ub530\ub77c \uc5b4\ub5a4 \uacbd\ud5a5\uc131\uc744 \ub744\ub294\uc9c0\ub294 \ud655\uc778\ud560 \uc218 \uc5c6\uace0 \uc624\ub85c\uc9c0 \uadf8 \uac12\uc758 \ucc28\uc774\uac00 \uc720\uc758\ubbf8\ud55c\uc9c0\ub9cc\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc GEE\uc640 Mixed model\uc740 \uc2dc\uac04\uc5d0 \ub530\ub978 covariance\ub97c \uace0\ub824\ud558\uae30 \ub54c\ubb38\uc5d0 \ucc28\uc774 \ubfd0\uc544\ub2c8\ub77c, age\uc5d0 \ub530\ub978 Y\uac12\uc758 \uc99d\uac10 \ub4f1\uc758 pattern\uc744 \uccb4\ud06c\ud560 \uc218 \uc788\ub2e4.","title":"2-5."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#3","text":"","title":"3."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#3-1","text":"#gee no <- c(283,30,140,21,274,26,134,18,266,32,134,14) yes <- c(17,20,12,14,24,26,14,21,28,24,22,17) previous <- c(0,1,0,1,0,1,0,1,0,1,0,1) s <- c(0,0,1,1,0,0,1,1,0,0,1,1) t <- c(10,10,10,10,9,9,9,9,8,8,8,8) data <- cbind(no,yes,previous,s,t) data <- as.data.frame(data) library(gee) gee <- gee(cbind(yes,no)~previous+s+t,id=1:nrow(data),family=binomial,corstr='exchangeable',data=data) ## Beginning Cgee S-function, @(#) geeformula.q 4.13 98/01/27 ## running glm to get initial regression estimate ## (Intercept) previous s t ## -0.2925632 2.2110745 0.2959619 -0.2428077 summary(gee) ## ## GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA ## gee S-function, version 4.13 modified 98/01/27 (1998) ## ## Model: ## Link: Logit ## Variance to Mean Relation: Binomial ## Correlation Structure: Exchangeable ## ## Call: ## gee(formula = cbind(yes, no) ~ previous + s + t, id = 1:nrow(data), ## data = data, family = binomial, corstr = \"exchangeable\") ## ## Summary of Residuals: ## Min 1Q Median 3Q Max ## 11.91869 15.79896 20.05870 23.61013 27.90335 ## ## ## Coefficients: ## Estimate Naive S.E. Naive z Robust S.E. Robust z ## (Intercept) -0.2925632 0.52898439 -0.5530658 0.40667174 -0.7194086 ## previous 2.2110745 0.09891071 22.3542483 0.09359790 23.6231200 ## s 0.2959619 0.09775188 3.0276852 0.07310746 4.0483131 ## t -0.2428077 0.05918439 -4.1025632 0.04489908 -5.4078543 ## ## Estimated Scale Parameter: 0.3909426 ## Number of Iterations: 1 ## ## Working Correlation ## [,1] ## [1,] 1 data\ub97c \ub9cc\ub4e4\uace0 GEE method\ub97c \uc0ac\uc6a9\ud55c \uacb0\uacfc\ub294 \uc704\uc640 \uac19\ub2e4. \ubd84\uc11d\ud55c\ub2e4\uba74, \uc2dc\uac04\uc774 \uc9c0\ub0a8\uc5d0 \ub530\ub77c illness\uc5d0 \uac78\ub9b4 \ud655\ub960\uc740 \ub0ae\uc544\uc9d0\uc744 \ud655\uc778\ud560 \uc218 \uc788\uace0, \uc0b0\ubaa8\uc758 \ud761\uc5f0\ub825\uc740 illness\uc5d0 positive\ud55c \uc601\ud5a5\uc744, \uadf8\ub9ac\uace0 \ubc18\ubcf5\uce21\uc815\uc5d0 \uc788\uc5b4\uc11c \uc0ac\uc804\ubcd1\ub825\uc774 \uc774\ud6c4\uc758 age\uc758 illness\ud655\ub960\uc744 \uc99d\uac00\uc2dc\ud0a4\ub294 \uac83\uc73c\ub85c \ud655\uc778\ub41c\ub2e4.","title":"3-1."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#3-2","text":"#regressive logistic model fit1 <- glm(cbind(yes,no)~previous+s+t,family=binomial, data=data) summary(fit1) ## ## Call: ## glm(formula = cbind(yes, no) ~ previous + s + t, family = binomial, ## data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.98143 -0.30047 -0.09456 0.36684 0.96039 ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) -0.29256 0.84603 -0.346 0.7295 ## previous 2.21107 0.15819 13.977 <2e-16 *** ## s 0.29596 0.15634 1.893 0.0583 . ## t -0.24281 0.09466 -2.565 0.0103 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 207.2212 on 11 degrees of freedom ## Residual deviance: 3.1186 on 8 degrees of freedom ## AIC: 64.392 ## ## Number of Fisher Scoring iterations: 3 regressive logistic model\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5bb\uc740 \uacb0\uacfc\ub294 \uc704\uc640 \uac19\ub2e4. \ubaa8\ub378\uc5d0 \ub300\ud574 \ucd94\uc815\ud55c \uacc4\uc218\uac00 \uc704\uc758 gee method\uc640 \uac19\uac8c \ub098\uc624\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc73c\uba70, \ud574\uc11d\uc740 \uc704\uc758 \ubc29\ubc95\uacfc \ub3d9\uc77c\ud558\uac8c \ubc1b\uc544\ub4e4\uc77c \uc218 \uc788\ub2e4. \ucd94\uac00\uc801\uc73c\ub85c \ud655\uc778\ud560 \uc218 \uc788\ub294 \uac83\uc740, p-value\uac00 \ud1b5\uacc4\uc801\uc73c\ub85c \uc720\uc758\ud558\uac8c \ub098\uc624\ub294 \uac83\uc73c\ub85c \ubcf4\uc544 \uac01 \ubcc0\uc218\ub4e4\uc774 illness\uc5d0 \ub300\ud574 \uc720\uc758\ubbf8\ud558\uac8c \uc791\uc6a9\ud558\uace0 \uc788\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4.","title":"3-2."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#3-3","text":"","title":"3-3."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#4","text":"","title":"4."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#4-1-draw-the-histogram","text":"geyser <- c(80,71,57,80,75,77,60,86,77,56,81,50,89,54,90,73,60,83,65,82,84, 54,85,58,79,57,88,68,76,78,74,85,75,65,76,58,91,50,87,48,93,54, 86,53,78,52,83,60,87,49,80,60,92,43,89,60,84,69,74,71,108,50,77, 57,80,61,82,48,81,73,62,79,54,80,73,81,62,81,71,79,81,74,59,81, 66,87,53,80,50,87,51,82,58,81,49,92,50,88,62,93,56,89,51,79,58, 82,52,88,52,78,69,75,77,53,80,55,87,53,85,61,93,54,76,80,81,59, 86,78,71,77,76,94,75,50,83,82,72,77,75,65,79,72,78,77,79,75,78, 64,80,49,88,54,86,51,96,50,80,78,81,72,75,78,87,69,55,83,49,82, 57,84,57,84,73,78,57,79,57,90,62,87,78,52,98,48,78,79,65,84,50, 83,60,80,50,88,50,84,74,76,65,89,49,88,51,78,85,65,75,77,69,92, 68,87,61,81,55,93,53,84,70,73,93,50,87,77,74,72,82,74,80,49,91, 53,86,49,79,89,87,76,59,80,89,45,93,72,71,54,79,74,65,78,57,87, 72,84,47,84,57,87,68,86,75,73,53,82,93,77,54,96,48,89,63,84,76, 62,83,50,85,78,78,81,78,76,74,81,66,84,48,93,47,87,51,78,54,87, 52,85,58,88,79) hist(geyser,main='Histogram of Old faithful geyser Eruption', xlab='# of Eruption',prob=T,breaks=30)","title":"4-1. Draw the histogram"},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#4-2","text":"hist(geyser,main='Histogram of Old faithful geyser Eruption', xlab='# of Eruption',prob=T,col='grey',breaks=30) lines(density(geyser)) lines(density(geyser,bw=1.5,kernel=\"epanechnikov\"),col='red') legend('topright',legend='non-parametric',col='red',lty=1:2,cex=0.8)","title":"4-2."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#4-3-assume-a-normal-distribution-and-esimate-the-mean-and-standarad-deviation","text":"Fitdistr\ud568\uc218\ub97c \ud1b5\ud574 \ub178\ub9d0\ubd84\ud3ec\uc5d0 \ud53c\ud305\ud558\uc5ec \ubaa8\uc218\uac00 \uc5b4\ub5bb\uac8c \ub098\uc624\ub294\uc9c0 \ud655\uc778\ud558\uace0 \uc774\ub97c \uadf8\ub824\ubcf4\uc558\ub2e4. hist(geyser,main='Histogram of Old faithful geyser Eruption', xlab='# of Eruption',prob=T,col='grey',breaks=30) curve(dnorm(x, mean=mean(geyser), sd=sd(geyser)), col=\"blue\", lwd=2, add=T, yaxt=\"n\") legend('topright',legend='Normal Dist',col='blue',lty=1:2,cex=0.8)","title":"4-3. Assume a normal distribution, and esimate the mean and standarad deviation."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#4-4","text":"EM-Algorithm\uc744 \uc2dc\ud589\ud558\uae30 \uc55e\uc11c, \uc774\ub294 geyser \ub370\uc774\ud130\uac00 \ub450\uac1c\uc758 \ubd84\ud3ec\uac00 \uc11e\uc5ec\uc788\ub294 \uac83\uc73c\ub85c \uac00\uc815\ud558\uace0 \ubb38\uc81c\ub97c \uc2dc\uc791\ud574\uc57c\ud558\ubbc0\ub85c, rough\ud55c \ubc29\ubc95\uc774\uc9c0\ub9cc, \ub370\uc774\ud130\ub97c \uc808\ubc18\uc73c\ub85c \ub098\ub208 \ud6c4 (299\uac1c\ub97c 149\uac1c\uc640 150\uac1c) \uc774\ub97c sorting\ud558\uc5ec \uac01\uac01\uc5d0 fitdistr\ub97c \uc0ac\uc6a9\ud558\uc5ec \ubaa8\uc218\ub97c \ucd94\uc815\ud55c \ud6c4, \uc774\ub97c \ucd08\uae30\uac12\uc73c\ub85c \uc0ac\uc6a9\ud558\uc5ec EM-algorithm\uc744 \uc9c4\ud589\ud574\ubcf4\uae30\ub85c \ud558\uc600\ub2e4. library(MASS) ## ## Attaching package: 'MASS' ## The following object is masked _by_ '.GlobalEnv': ## ## geyser x <- sort(geyser) a <-x[1:149] b <- x[150:299] fitdistr(a,'normal') ## mean sd ## 60.7986577 9.5713357 ## ( 0.7841144) ( 0.5544526) fitdistr(b,'normal') ## mean sd ## 83.7600000 5.4548816 ## ( 0.4453892) ( 0.3149377) set.seed(2020311194) x1 <- rnorm(149,60,9) x2 <- rnorm(150,83,5) p <- 0.5 x1_mean <- 60 x1_sd <- 8 x2_mean <- 83 x2_sd <-6 x <- c(x1,x2) for (i in 1:length(geyser)){ den_x <- p*dnorm(x, x1_mean, x1_sd) + (1-p)*dnorm(x, x2_mean, x2_sd) p1 <- p*dnorm(x, x1_mean, x1_sd)/den_x p2 <- 1 - p1 pi <- sum(p1)/length(geyser) x1_mean <- sum(p1*x)/sum(p1) x2_mean <- sum(p2*x)/sum(p2) newx1_sd <- sum(p1 *(x-x1_mean)^2)/ sum(p1) x1_sd<- sqrt(newx1_sd) newx2_sd <- sum(p2 *(x-x2_mean)^2)/ sum(p2) x2_sd<- sqrt(newx2_sd) } x<-seq(min(x),max(x),len=299) density_em2 <- p*dnorm(x,x1_mean,x1_sd) + (1-p)*dnorm(x,x2_mean,x2_sd) hist(geyser,main='Histogram & EM-Algorithm using two dist', xlab='# of Eruption',prob=T,col='grey',breaks=30) lines(x,density_em2,col='deeppink') legend('topright',legend='EM-Algorithm',col='deeppink',lty=1:2,cex=0.8) pi ; x1_mean; x1_sd; x2_mean ; x2_sd ## [1] 0.520283 ## [1] 61.23908 ## [1] 9.351038 ## [1] 83.3176 ## [1] 4.99325 \ucd94\uc815\ud55c 5\uac1c\uc758 \ubaa8\uc218 \\(\\pi_1,\\ \\mu_1,\\ \\sigma_1,\\ \\mu_2,\\ \\sigma_2\\) \ub294 \uac01\uac01 0.52, 61.24, 9.35, 83.3, 4.99\uac00 \ub098\uc628\ub2e4. ####4-5. \uc774\ubc88\uc5d0\ub294 3\uac00\uc9c0\uc758 \ub178\ub9d0\ubd84\ud3ec\ub97c \uac00\uc815\ud558\uace0 \ubb38\uc81c\ub97c \uc811\uadfc\ud574\uc57c\ud558\ubbc0\ub85c, \uc704\uc758 \ubb38\uc81c\uc640 \ub611\uac19\uc740 \ubc29\uc2dd\uc5d0\uc11c \uac1c\uc218\ub9cc \ub298\ub824\uc8fc\ub294 \ud615\uc2dd\uc73c\ub85c, 3\uac1c\ub85c \uad6c\uac04\uc744 \ub098\ub204\uc5b4\uc11c \ub178\ub9d0\ubd84\ud3ec\uc5d0 fitting \uc2dc\ud0a4\uace0 \ubb38\uc81c\ub97c \uc9c4\ud589\ud558\uc600\ub2e4. x <- sort(geyser) a <-x[1:99] b <- x[100:199] c <- x[200:299] fitdistr(a,'normal') ## mean sd ## 54.8383838 5.3251966 ## ( 0.5352024) ( 0.3784452) fitdistr(b,'normal') ## mean sd ## 75.4700000 3.5339921 ## ( 0.3533992) ( 0.2498910) fitdistr(c,'normal') ## mean sd ## 86.4700000 4.6871207 ## ( 0.4687121) ( 0.3314295) set.seed(2020311194) x1 <- rnorm(99,55,5) x2 <- rnorm(100,75,3.5) x3 <- rnorm(100,86,4.7) pi1 <- 0.3 pi2 <- 0.4 x1_mean <- 55 x1_sd <- 6 x2_mean <- 75 x2_sd <-4 x3_mean <- 86 x3_sd <- 5 x <- c(x1,x2,x3) for (i in 1:length(geyser)){ den_x <- pi1*dnorm(x, x1_mean, x1_sd) + pi2*dnorm(x, x2_mean, x2_sd)+ (1-pi1-pi2)*dnorm(x, x3_mean, x3_sd) p1 <- pi1*dnorm(x, x1_mean, x1_sd)/den_x p2 <- pi2*dnorm(x, x2_mean, x2_sd)/den_x p3 <- 1-p1-p2 new_p1 <- sum(p1)/length(geyser) new_p2 <- sum(p2)/length(geyser) x1_mean <- sum(p1*x)/sum(p1) x2_mean <- sum(p2*x)/sum(p2) x3_mean <- sum(p3*x)/sum(p3) newx1_sd <- sum(p1 *(x-x1_mean)^2)/ sum(p1) x1_sd<- sqrt(newx1_sd) newx2_sd <- sum(p2 *(x-x2_mean)^2)/ sum(p2) x2_sd<- sqrt(newx2_sd) newx3_sd <- sum(p3 *(x-x3_mean)^2)/ sum(p3) x3_sd<- sqrt(newx3_sd) } x<-seq(min(x),max(x),len=299) density_em3 <- pi1*dnorm(x,x1_mean,x1_sd) + pi2*dnorm(x,x2_mean,x2_sd)+ (1-pi1-pi2)*dnorm(x,x3_mean,x3_sd) hist(geyser,main='Histogram & EM-Algorithm using three dist', xlab='# of Eruption',prob=T,col='grey',breaks=30) lines(x,density_em3,col='orange') legend('topright',legend='EM-Algorithm2',col='orange',lty=1:2,cex=0.8) new_p1 ; new_p2 ; x1_mean; x1_sd; x2_mean ; x2_sd ; x3_mean ; x3_sd ## [1] 0.3307159 ## [1] 0.3771631 ## [1] 55.10935 ## [1] 4.865457 ## [1] 75.35787 ## [1] 3.730488 ## [1] 87.44965 ## [1] 3.88705 \ucd94\uc815\ud55c 5\uac1c\uc758 \ubaa8\uc218 \\(\\pi_1,\\ \\pi_2,\\ \\mu_1,\\ \\sigma_1,\\ \\mu_2,\\ \\sigma_2,\\ \\mu_3,\\ \\sigma_3\\) \ub294 \uac01\uac01 0.33, 0.377, 55.1, 4.8, 75.35, 3.7, 87.45, 3.88\ub85c \ub098\uc654\ub2e4. \uc774\ub97c EDA\uc801 \uad00\uc810\uc5d0\uc11c \uc0b4\ud3b4\ubcf4\uba74 \uc704\uc758 \ub450\uac00\uc9c0 \ubd84\ud3ec\ub97c \uc0ac\uc6a9\ud558\uc600\ub358 EM-Algorithm\ubcf4\ub2e4\ub294 \ubaa9\ud45c\ud588\ub358 \ub178\ub9d0\ubd84\ud3ec\uc758 \ubaa8\uc218\uc640 \ub2e4\uc18c \ucc28\uc774\uac00 \uc788\ub294\ub370, \uc774\ub294 \ub450\ubc88\uc9f8 \ubd09\uc6b0\ub9ac\uc758 \uc624\ub978\ucabd \ub05d\uc774 \uc57d\uac04 \uc19f\uc544\ub098\uc788\uc9c0\ub9cc \ub450\uac1c\uc758 \ubd84\ud3ec\ub85c \ubcf4\uae30\uc5d0\ub294 \uc57d\uac04 \ubb34\ub9ac\uac00 \uc788\uae30\ub54c\ubb38\uc73c\ub85c \uc0ac\ub8cc\ub41c\ub2e4.","title":"4-4."},{"location":"07%20Intermediate%20Applied%20Statistics/Final/#4-6-which-one-is-the-best-among-12345","text":"\uc721\uc548\uc73c\ub85c \uc0b4\ud3b4\ubcf8\ub2e4\uba74 \uba3c\uc800 \ub450\uac1c\uc758 \ubd09\uc6b0\ub9ac\ub97c check\ud55c \uac83\ub4e4 \uc911\uc5d0\uc11c best\ub97c \ubf51\uc73c\ub824 \ud55c\ub2e4. \ucd94\uac00\ub85c non-parametric method\ub294 fitting\uc758 \uad00\uc810\uc5d0\uc11c\ub294 \uc5bc\ucd94 \uad1c\ucc2e\uc544 \ubcf4\uc774\uc9c0\ub9cc, smoothness\ub97c \uace0\ub824\ud574\ubcf4\uc558\uc744 \ub54c, \ucd5c\uc801\uc758 \uc120\ud0dd\uc73c\ub85c \ucc44\ud0dd\ud558\uae30\uc5d0\ub294 \uadfc\uac70\uac00 \uc870\uae08 \ubd80\uc871\ud55c \uac83\uc73c\ub85c \ud310\ub2e8\ud574 best select\uc5d0\uc11c \uc81c\uc678\ud558\uae30\ub85c \ud558\uc600\ub2e4. \ub298 \uc774\ub7f0 \ud310\ub2e8\uc5d0\uc11c\ub294 bias-variance\uac04\uc758 trade off\uac00 \ubc1c\uc0dd\ud558\uc9c0\ub9cc, \ub108\ubb34 underfitting\ud558\uc9c0 \uc54a\uc740 \uac83\uc744 \ucc44\ud0dd\ud558\uae30 \uc704\ud574 \uc774\ubd09\ubd84\ud3ec\ub97c detect\ud558\uc9c0 \ubabb\ud55c model\uc740 \ubc84\ub824\ub0b4\uba74\uc11c Bias\ub97c \uc870\uae08 \ub35c\uc5b4\ub0b4\uace0, \ub0a8\uc740 \ubaa8\ub378 \uc911\uc5d0 data\uc758 variance\ub3c4 \uace0\ub824\ud558\uace0\uc790 \ud55c\ub2e4. \uba3c\uc800 \uc0ac\uc6a9\ud588\ub358 \ubaa8\ub4e0 \ubaa8\ub378\ub4e4\uc758 density line\ub4e4\uc744 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc5d0 \ub367\ubd99\ud600 \uadf8\ub9ac\uba74 \uc544\ub798\uc640 \uac19\ub2e4. hist(geyser,main='Histogram of Old faithful geyser Eruption', xlab='# of Eruption',prob=T,breaks=30) lines(density(geyser,bw=1.5,kernel=\"epanechnikov\"),col='red') curve(dnorm(x, mean=mean(geyser), sd=sd(geyser)), col=\"blue\", lwd=2, add=T, yaxt=\"n\") lines(x,density_em2,col='deeppink') lines(x,density_em3,col='orange') legend('topright',legend=c('non-parametric','Normal','EM-ALgorithm','EM-Algorithm2'), col=c('red','blue','deeppink','orange'),lty=1:4,cex=0.8) \ub9c9\uc0c1 \uc801\uc5c8\uc9c0\ub9cc clear criterion\ub97c \ub17c\ud558\ub294 \uac83\uc744 \uc0dd\uac01\ud558\uae30\uac00 \uc5b4\ub824\uc6cc \uc5ec\uae30\uae4c\uc9c0\ub9cc \ubb38\uc81c\ub97c \ud480\uc5c8\uc2b5\ub2c8\ub2e4.. \uac10\uc0ac\ud569\ub2c8\ub2e4","title":"4-6. which one is the best among 1,2,3,4,5 ?"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/","text":"Intermediate Applied Statistics Quiz 1 1. \uc77c\uc815\uae30\uac04 \ud6c4 7\ub9c8\ub9ac \uc911 4\ub9c8\ub9ac\uac00 \uc0ac\ub9dd\ud558\uc600\ub2e4. 1) \ubaa8\ub4e0 \uac1c\uccb4\uc758 \uc0ac\ub9dd\ud560 \ud655\ub960\uc744 p\ub85c \ub3d9\uc77c\ud558\ub2e4\uace0 \ud558\uace0 \uac01 \uac1d\uccb4\uc758 \uc0ac\ub9dd\uc5ec\ubd80\uac00 \ub3c5\ub9bd\uc774\ub77c\ub294 \uac00\uc815 \ud558\uc5d0\uc11c \uc774\ud56d\ubd84\ud3ec\ub97c \uc774\uc6a9\ud558\uc5ec likelihood function\uc744 \uc4f0\uc2dc\uc624. \uac01 \uac1c\uccb4\ub4e4\uc758 \uc0ac\ub9dd\ud655\ub960\uc774 \ub3d9\uc77c\ud558\uace0, \uc11c\ub85c \ub3c5\ub9bd\uc774\ub77c\ub294 \uac00\uc815\uc744 \ud55c\ub2e4\uba74, \uc774\ub294 \uc774\ud56d\ubd84\ud3ec \\(Bin(7,p)\\) \ub85c \ud45c\ud604\ud560 \uc218 \uc788\uc73c\uba70, \uc774\ud56d\ubd84\ud3ec\ub294 \uc11c\ub85c \ub3c5\ub9bd\uc778 \ubca0\ub974\ub204\uc774 \uc2dc\ud589\uc744 n\ubc88 \ubc18\ubcf5\ud55c \ud615\ud0dc\uc774\ubbc0\ub85c, \uac01 \uac1c\uccb4\ub4e4( \\(X_i,\\ i=1,..,7\\) )\uc740 \\(Ber(1,p)\\) \uc758 \ubd84\ud3ec\ub97c \uac16\ub294\ub2e4\uace0 \ubcfc \uc218 \uc788\ub2e4. \uc774\uc5d0 \ub300\ud55c likelihood function\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \\(L(p;x)=\\prod_{i=1}^7f(x_i;p)\\) \\(= p^{x_1}(1-p)^{1-x_1}p^{x_2}(1-p)^{1-x_2}..p^{x_7}(1-p)^{1-x_7}\\) \\(= p^{\\sum_{i=1}^7 {x_i}}(1-p)^{n-\\sum_{i=1}^7{x_i}},\\ n=7\\) \uc5ec\uae30\uc5d0 \ucd94\uac00\ub85c \uba87\ubc88\uc9f8 \uac1c\uccb4\uc5d0\uc11c \uc774\ubca4\ud2b8\uac00 \ubc1c\uc0dd\ud588\ub294\uc9c0\uc5d0 \ub300\ud55c \uc870\ud569\uc758 \uc218\ub85c \\(nCx,\\ \\sum{x_i}=x\\) \uac00 \uc788\uc9c0\ub9cc, likelihood \ud568\uc218\ub85c \uad00\uc2ec\ubaa8\uc218\ub97c \ucd94\uc815\ud558\ub294\ub370\uc5d0 \uc788\uc5b4\uc11c\ub294 \\(nCx\\) \ub294 \ubaa8\uc218 p\uc5d0 \ub300\ud574 \ubbf8\ubd84\uc744 \ud560 \uc2dc \uc0ac\ub77c\uc9c8\ubfd0 \uc544\ub2c8\ub77c, \ud544\uc694\ud55c \uc815\ubcf4\uac00 \uc544\ub2c8\uae30 \ub54c\ubb38\uc5d0 \uc0dd\ub7b5\ud558\uc5ec \ud45c\ud604\ud574\ub3c4 \ubb34\ubc29\ud558\ub2e4. 2) Score function\uc744 \uad6c\ud558\uace0 p\uc5d0 \ub530\ub978 score function\uc744 \uadf8\ub9ac\uc2dc\uc624. Score function\uc740 log-likelihood\ub97c \uad00\uc2ec\ubaa8\uc218\uc5d0 \ub300\ud574\uc11c 1\ucc28\ubbf8\ubd84\uc744 \ud55c \ud568\uc218\uc774\ub2e4. \uc774\ub97c \uacc4\uc0b0\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\(S(p) = \\frac{\\partial }{\\partial p}logL(p)\\) $=\\frac{\\partial }{\\partial p}(\\sum{x_i}logp + (n-\\sum{x_i})log(1-p)) $ \\(= \\frac{\\sum{x_i}}{p}+\\frac{\\sum{x_i}-n}{1-p}\\) \\(=\\frac{\\sum{x_i}-np}{p(1-p)}\\) score function\uc740 likelihood function\uc758 \uae30\uc6b8\uae30\ub97c \ub098\ud0c0\ub0b4\ub294 \ud568\uc218\uc774\ubbc0\ub85c, p\uac00 mle\uc77c\ub54c 0\uc744 \uc9c0\ub098\uac00\ub294 \ubaa8\uc2b5\uc744 \uac16\uac8c\ub41c\ub2e4. \uae30\uc6b8\uae30\ub97c \ub098\ud0c0\ub0b4\ub294 \ud568\uc218\uc774\uae30\uc5d0, score function\uc740 \uc790\uc5f0\uc2a4\ub7fd\uac8c decreasing function\uc758 form\uc744 \uac16\uac8c \ub418\uba70, \uc774\ub97c \uc5bc\ucd94 \uadf8\ub9bc\uc73c\ub85c \ud45c\ud604\ud55c\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc744 \uac83\uc774\ub2e4. 3) likelihood based confidence interval\uc744 \uad6c\ud558\uc2dc\uc624 likelihood \uc5d0 \uae30\ubc18\ud574 \uc2e0\ub8b0\uad6c\uac04\uc744 \uad6c\ud55c\ub2e4\uba74, likelihood ratio\ub97c \uc774\uc6a9\ud558\uc5ec \uc774 ratio\uac00 \ud2b9\uc815 cutoff \uc9c0\uc810\ubcf4\ub2e4 \ud06c\uac8c \ub9cc\ub4dc\ub294 p\uc758 \ubc94\uc704\ub97c \uad6c\ud558\ub294 \uac83\uc774\ub2e4. \uc774 \uc608\uc81c\uc758 likelihood ratio\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(\\frac{p^y(1-p)^{n-y}}{\\hat{p}^y(1-\\hat{p})^{n-y}},\\ \\hat{p}\\ is\\ mle\\ of\\ p,\\ y=\\sum{x_i}\\) \\(=\\frac{p^4(1-p)^3}{(4/7)^4(3/7)^3}\\) mle\ub97c \uad6c\ud574\ubcf4\uba74 \\(\\frac{\\sum{x_i}}{n}\\) \uc774 \ub098\uc624\ubbc0\ub85c, \\(\\frac{4}{7}\\) \uc744 \uc0ac\uc6a9\ud55c \uac83\uc774\ub2e4. \uacc4\uc18d\ud574\uc11c \uc804\uac1c\ud558\uba74, \\(\\frac{p^4(1-p)^3}{(4/7)^4(3/7)^3} > c\\) \uc774\uba70 \uc774\ub294 \\(-2log\\frac{p^4(1-p)^3}{(4/7)^4(3/7)^3} < -2logc =3.84\\) \uc774\uba70 \uc774\ub294 \\(\\{p^4(1-p)^3 > 0.019 \\}\\) \uc640 \uac19\uace0 \uc774\ub97c p\uc5d0 \ub300\ud574\uc11c \ud480\uba74, (0.22,0.87) \uc744 \uc5bb\uc744 \uc218 \uc788\uc73c\uba70, \uc774\uac83\uc774 \uace7 likelihood based\ub85c \uad6c\ud55c 95% \uc2e0\ub8b0\uad6c\uac04\uc774 \ub41c\ub2e4. 4) p=0.3\uc784\uc744 \uac80\uc815\ud558\uace0\uc790\ud55c\ub2e4. likelihood ratio function\uc744 \uadf8\ub9ac\uace0 0.05\uc758 \uc720\uc758\uc218\uc900\uc5d0\uc11c\uc758 cutoff\ub97c \uad6c\ud558\uc5ec \uac80\uc815\ud558\uc2dc\uc624. likelihood ratio\ub294 \uc704\uc5d0\uc11c \uad6c\ud55c \uac83\uc744 \uc774\uc6a9\ud558\uc5ec \ud655\uc778\ud558\uba74 \uc704\uc640 \uac19\uc740 \uadf8\ub9bc\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4. \ud568\uc218\uc758 \ubd84\ubaa8 \ubd80\ubd84\uc758 \\(\\hat{\\theta}\\) \ub294 MLE\ub97c \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc5d0 \\(\\frac{4}{7}\\) \ub97c \ub123\uc5c8\ub2e4. \uadf8\ub798\ud504\ub294 \ucd5c\uace0\ucc28\ud56d\uc774 7\uc778 \uace0\ucc28\ud568\uc218\uc758 \ud615\ud0dc\ub97c \ub744\uace0 \uc788\uc73c\ub098, \ubaa8\uc218 p\uc758 support\uac00 [0,1] \uc774\uae30 \ub54c\ubb38\uc5d0 0\uacfc 1\uc0ac\uc774\uc5d0\uc11c\uc758 \uadf8\ub798\ud504\ub97c \ud655\uc778\ud558\uba74 \ub41c\ub2e4. \uc5ec\uae30\uc11c cutpoint\uc778 y\ucd95\uc774 0.15\uac00 \ub418\ub294 \uc9c0\uc810\uc744 \ud655\uc778\ud558\uba74, p\uac12\uc774 \uc57d 0.22\uc640 0.86\uc77c\ub54c\uac00 \ud568\uc218\uac12\uc774 0.15\uc5d0 \ud574\ub2f9\ud558\ub294 \uc9c0\uc810\uc774 \ub41c\ub2e4. \uace0\ub85c \uac80\uc815\ud558\uace0\uc790 \ud558\ub294 p\uac12\uc774 (0.22,0.86) \uc0ac\uc774\uc5d0 \uc874\uc7ac\ud558\uac8c\ub418\uba74, \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud560 \uc218 \uc5c6\uc744 \uac83\uc774\ub2e4. LRT\uc5d0\uc11c \\(R(p)= \\frac{(0.3)^4(0.7)^3}{(4/7)^4(3/7)^3}\\) \uc774\uba70, \uc720\uc758\uc218\uc900 0.05\uc5d0\uc11c LRT\uc758 cutoff point\ub294 ratio\uc758 \uac12\uc774 0.15\uac00 \ub418\ub294 \uc9c0\uc810\uc774 \ub41c\ub2e4. \uace0\ub85c ratio \uac12\uc774 0.15\ubcf4\ub2e4 \uc791\uc744 \uc2dc, \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud558\uac8c \ub41c\ub2e4. \uc774\ub97c \uacc4\uc0b0\ud574\ubcf4\uba74 0.33\uc774 \ub098\uc624\uac8c \ub418\ubbc0\ub85c, \uc774\uc5d0 \ub530\ub77c p=0.3\uc778 \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud560 \uc218 \uc5c6\ub2e4\ub294 \uacb0\ub860\uc744 \ub0b4\ub9b4 \uc218 \uc788\ub2e4. 2. \ub2e4\uc74c\uc740 \ub3c5\uc57d\uc758 \uc591\uc5d0 \ub530\ub978 \uc77c\uc815\uae30\uac04 \ud6c4\uc758 \uc0ac\ub9dd\uc5ec\ubd80\uc5d0 \ub300\ud55c \uc790\ub8cc\uc774\ub2e4. (\ub3c5\uc591\uc758 \uc591, \uc0ac\ub9dd\uc5ec\ubd80) = (1,Dead), (2,Dead), (1,Alive), (1,Alive), (3,Alive), (3,Dead), (2,Dead). \uac01 \uac1c\uccb4\uc758 \uc0ac\ub9dd\ud560 \ud655\ub960\uc744 \\(p_i\\) \ub77c \ud560 \ub54c likelihood function\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(L(p_1,p_2,..p_7)=p_1p_2(1-p_3)(1-p_4)(1-p_5)p_6p_7\\) 1) Likelihood\uac00 \ucd5c\ub300\uac00 \ub418\ub294 \uac01 p\uac12\uc744 \uad6c\ud558\uc2dc\uc624. \uc5ec\uae30\uc11c p\ub294 \uc0ac\ub9dd\ud560 \ud655\ub960\uc744 \ub098\ud0c0\ub0b4\ub294 \uac83\uc774\ubbc0\ub85c, \ub2e8\uc21c\ud558\uac8c \uc811\uadfc\ud558\uba74 \uc774 \ud480\uc774\ub294 \uc27d\uac8c \uc811\uadfc\uc774 \uac00\ub2a5\ud558\ub2e4. \uc8fd\uc740 \uc0ac\ub78c\uc758 p\uac12\uc5d0 1\uc744 \uc8fc\uace0, \uc0b4\uc544\ub0a8\uc740 \uc0ac\ub78c\uc758 p\uac12\uc5d0 0\uc744 \uc8fc\uac8c\ub41c\ub2e4\uba74, Likelihood\ub294 1\ub85c \ucd5c\ub300\uac00 \ub418\uac8c \ub41c\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 likelihood\ub97c \ucd5c\ub300\ub85c \ud558\ub294 p\uac12\ub4e4\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(p_1=1,\\ p_2=1,\\ p_3=0,\\ p_4=0,\\ p_5=0,\\ p_6=1,\\ p_7=1\\) 2) \\(p_i=\\alpha\\) \uc77c\ub54c likelihood\uac00 \ucd5c\ub300\uac00 \ub418\ub294 \\alpha\ub97c \uad6c\ud558\uace0 \uc774\ub7ec\ud55c \ubaa8\ud615\uc774 \uc801\ud569\ud55c\uc9c0\ub97c \ub17c\ud558\uc2dc\uc624. \ubaa8\ub4e0 p\uac12\uc774 \uac19\ub2e4\uba74 likelihood function\uc740 \ub2e4\uc74c\uacfc \uac19\uc774 \ud45c\ud604\uc774 \uac00\ub2a5\ud558\ub2e4. \\(L(\\alpha)=\\alpha^4(1-\\alpha)^3\\) likelihood\ub97c \ucd5c\ub300\ub85c \ud558\uae30\uc704\ud574 MLE\ub97c \uad6c\ud574\ubcf4\uba74, \\(\\frac{\\partial}{\\partial \\alpha}logL(\\alpha)= \\frac{4}{\\alpha}+ \\frac{-3}{1-\\alpha}\\) \ucc98\ub7fc \ud45c\ud604\uac00\ub2a5\ud558\uace0, \uc774 \uc2dd\uc774 0\uc73c\ub85c \ub418\uac8c\ud558\ub294 \\(\\alpha\\) \ub97c \ucc3e\uc73c\uba74 \ub41c\ub2e4. \uc774\ub97c \uacc4\uc0b0\ud558\uba74 \\(\\alpha = \\frac{4}{7}\\) \ub97c \uc5bb\uac8c \ub41c\ub2e4. \ud558\uc9c0\ub9cc \uc774 \ubaa8\ud615\uc774 \uc801\ud569\ud55c\uc9c0\ub97c \ub17c\ud55c\ub2e4\uba74, '\uc801\ud569\ud558\uc9c0 \uc54a\ub2e4' \ub77c\uace0 \ub9d0\ud574\uc57c \ud560 \uac83\uc774\ub2e4. \ubaa8\ub4e0 \uac1c\uccb4\ub4e4\uc774 \uc0ac\ub9dd\ud560 \ud655\ub960\uc774 \uac19\uace0 \uc0ac\ub9dd\ud655\ub960 \uc790\uccb4 \uc678\uc5d0 \ub2e4\ub978 \uc694\uc778\uc774 \ud3ec\ud568\ub418\uc5b4\uc788\uc9c0\uc54a\ub2e4\uba74, \uc62c\ubc14\ub978 \ubaa8\ud615\uc801\ud569\uc774\ub77c\uace0 \ub9d0\ud560 \uc218 \uc788\uaca0\uc9c0\ub9cc, \uc704\uc758 \uc0d8\ud50c\uc5d0\ub294 \ud22c\uc5ec\ud55c \ub3c5\uc57d\uc758 \uc591\uc774\ub77c\ub294 \uc815\ubcf4\uac00 \ud558\ub098 \ub354 \ucd94\uac00\ub418\uc5b4\uc788\ub2e4. \uc774\uc640 \uac19\uc740 \ubaa8\ud615\uc801\ud569\uc744 \ud558\uac8c\ub41c\ub2e4\uba74, \ub3c5\uc57d\uc758 \ud22c\uc5ec\uc815\ub3c4\uc640\ub294 \uc0c1\uad00\uc5c6\uc774 likehood\ub97c \uacc4\uc0b0\ud558\uac8c \ub418\ub294 \uac83\uc774\ubbc0\ub85c, \ubaa8\ub4e0 \uc694\uc778\uc744 \uace0\ub824\ud558\uc9c0 \uc54a\uc558\uae30 \ub54c\ubb38\uc5d0 \uc801\uc808\ud55c \ubaa8\ud615\uc774\ub77c\uace0 \ub9d0\ud560 \uc218 \uc5c6\uc744 \uac83\uc774\ub2e4. 3) \\(p_i=\\alpha+\\beta{x_i}\\) \uc77c \ub54c likelihood\ub97c \uc4f0\uace0 \ucd5c\ub300\uac00 \ub418\ub294 \\(\\alpha,\\ \\beta\\) \ub97c \uc5b4\ub5bb\uac8c \uad6c\ud560 \uc218 \uc788\uaca0\ub294\uac00? \uba3c\uc800 likelihood\uc2dd\uc744 \uc804\uac1c\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\((\\alpha+\\beta{x_1})(\\alpha+\\beta{x_5})(\\alpha+\\beta{x_6})(\\alpha+\\beta{x_7})(1-\\alpha-\\beta{x_3})(1-\\alpha-\\beta{x_4})(1-\\alpha-\\beta{x_5})\\) \uc774\ub97c \ucd5c\ub300\uac00 \ub418\uac8c \ud558\ub824\uba74, \uae30\uc874 likelihood\ub97c \ucd5c\ub300\ud654 \ud558\ub294 \uac83\ucc98\ub7fc \uc704 \uc2dd\uc5d0 log\ub97c \ucde8\ud574 \ubbf8\ubd84\ud55c \ub4a4 \uadf8 \uc2dd\uc744 0\uc774 \ub418\uac8c\ud558\ub294 \uac01\uac01\uc758 \\(\\alpha\\) \uc640 \\(\\beta\\) \ub97c \ucc3e\uc73c\uba74 \ub420 \uac83\uc774\ub2e4. \\(p_i\\) \uac12\uc774 \ubaa8\ub450 \uac01\uac01 \uad00\ub828\uc5c6\ub294 \ub2e4\ub978 \uac12\ub4e4\uc774\ub77c\uba74 \uae30\uc874\uc811\uadfc\ubc29\uc2dd\ucc98\ub7fc \ub2e4\uac00\uac08 \ub54c\ub294 \uad00\uc2ec\ubaa8\uc218\uac00 7\uac1c\uac00 \ub418\uaca0\uc9c0\ub9cc, \uc774 \uc608\uc81c\uc5d0\uc11c\ub294 \uad00\uc2ec \ubaa8\uc218\uac00 2\uac1c\ub85c \uc881\ud600\uc9c4\ub2e4. \\(p_i\\) \uc758 \uac12\uc774 \\(x_i\\) \ub4e4\uc758 \uc120\ud615\uacb0\ud569\uc2dd\uc73c\ub85c \ub418\uc5b4 \uc788\ub294 \ud615\ud0dc\uc774\uae30 \ub54c\ubb38\uc5d0, \uc6b0\ub9ac\uac00 \\(x_i\\) \ub85c \uc774\ub8e8\uc5b4\uc9c4 \uc774 \uacb0\ud569\uc2dd\uc5d0\uc11c \uae30\uc6b8\uae30\uc640 \uc808\ud3b8\uc744 \ucd94\uc815\ud560 \uc218 \uc788\ub2e4\uba74, \uc774 \uac83\uc774 \\(p_i\\) \ub4e4\uc5d0 \ub300\ud55c \ucd94\uc815\uc73c\ub85c \uc774\uc5b4\uc9c0\ub294 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc likelihood\uc2dd\uc744 \uc804\uac1c\ud560\ub54c \ucd5c\uace0\ucc28\ud56d\uc774 \ub9e4\uc6b0 \ub192\uc544\uc9c0\uba70 \uacc4\uc0b0\ud558\uae30\uc5d0 \uc544\uc8fc \ubcf5\uc7a1\ud574\uc9c0\ub294 \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud55c\ub2e4. \uac1c\uc778\uc801\uc778 \uacac\ud574\ub85c\ub294 '\uae30\uc874\uc5d0 \uac01\uae30 \ub2e4\ub978 7\uac1c\uc758 \ubaa8\uc218\ub97c \ucd94\uc815\ud558\ub294 \uac83\uacfc \ub2e4\ub974\uac8c, 3\ubc88\uacfc \uac19\uc740 \uc608\uc81c\ub294 \uc624\ud788\ub824 \uc0d8\ud50c\uacfc \uc228\uaca8\uc9c4 \ub450\uac00\uc9c0 \ubaa8\uc218\uc758 \uacb0\ud569\uc2dd\uc73c\ub85c \uae30\uc874 \ubaa8\uc218\ub97c \ucd94\uc815\ud558\uc5ec \ucd94\uc815\ud558\ub294 \ubaa8\uc218\uc758 \uac1c\uc218\ub97c \uc904\uc5ec \uc880 \ub354 \uc27d\uac8c \ubb38\uc81c\ub97c \ud574\uacb0\ud560 \uc218 \uc788\ub294 \uc810\ub3c4 \uc874\uc7ac\ub294 \ud558\uaca0\uc9c0\ub9cc, \uac01 \ud558\ub098\uc758 \uac1c\uccb4\uc5d0 \ud574\ub2f9\ud558\ub294 \ud558\ub098\uc758 \ubaa8\uc218\ub97c \ub354 \uc798\uac8c\ucabc\uac1c \ub450\uac00\uc9c0 \ubaa8\uc218\uc758 \uc120\ud615\uacb0\ud569\uc73c\ub85c \ud45c\ud604\ud558\uc600\uae30\uc5d0 \uc124\ub839 \uadf8 \ub450\uac00\uc9c0 \ubaa8\uc218\ub97c \uc798 \ucd94\uc815\ud558\uc5ec \uad6c\ud588\ub2e4\uace0 \ud558\ub354\ub77c\ub3c4, \ud574\uc11d\uacfc \uc124\uba85\ub825\uc5d0 \uc788\uc5b4\uc11c \ubcf5\uc7a1\ud568\uc744 \ub354 \uc99d\uac00\uc2dc\ud0a4\uc9c0\uc54a\uc744\uae4c' \ub77c\ub294 \uc0dd\uac01\uc744 \ud558\uc600\ub2e4. ( \uc624\ub85c\uc9c0 \uc800\uc758 \uac1c\uc778\uc801\uc778 \uc0c1\uc0c1\ubfd0\uc774\uc5b4\uc11c, \uc774\uc5d0 \ub300\ud574 \uad50\uc218\ub2d8\uaed8\uc11c \uc124\uba85\ud574\uc8fc\uc2dc\uba74 \ub9e4\uc6b0 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4. ) 4) \uae30\uc874\uc758 \uc811\uadfc\uc740 \uac1c\uccb4\ub97c \uae30\uc900\uc73c\ub85c grouping\uc744 \ud55c \uac83\uc774\ub77c\uba74, 4\ubc88 \uc608\uc81c\ub294 \ub3c5\uc591\uc758 \ud22c\uc5ec\uc591\uc5d0 \ub530\ub77c \uc0c8\ub86d\uac8c grouping\uc744 \ud55c \uc608\uc81c\uc774\ub2e4. 1\ubc88\uc758 case\ub294 \uc624\ub85c\uc9c0 \uac1c\uccb4 \uae30\uc900\uc758 \uc0ac\ub9dd\ud655\ub960\uc744 \ub17c\ud558\uc5ec \ubaa8\uc218\uc758 \uac1c\uc218\uac00 7\uac1c\uc600\uace0, \ub3c5\uc57d\uc758 \uc591\uc744 \uace0\ub824\ud558\uc9c0 \uc54a\uc558\ub2e4. \ud558\uc9c0\ub9cc 4\ubc88\uc758 case\ub294 \ub3c5\uc57d\ud22c\uc5ec\uc5d0 \ub530\ub77c \uc0ac\ub9dd\ud655\ub960\uc744 \uc815\ud558\uc600\uae30 \ub54c\ubb38\uc5d0, \ud22c\uc57d\uc758 class \uac2f\uc218\uc640 \uac19\uac8c \ucd94\uc815\ud558\ub294 \ubaa8\uc218\uac00 3\uac1c\ub85c \uc904\uc5b4\ub4e4\uac8c \ub418\uc5c8\ub2e4. \uc774\ub294 \uc624\ub85c\uc9c0 \uc0ac\ub9dd\uc5ec\ubd80\ub9cc \uace0\ub824\ud558\uc9c0 \uc54a\uace0, \uc57d\ubb3c \ud22c\uc5ec\uae4c\uc9c0 \uace0\ub824\ud558\uc5ec likelihood\ub97c \ucd94\uc815\ud558\ub294 \uac83\uc774\uae30 \ub54c\ubb38\uc5d0 \uc870\uae08 \ub354 \ud569\ub9ac\uc801\uc778 \ucd94\uc815\uc774\ub77c\uace0 \ud560 \uc218 \uc788\uc744 \uac83\uc774\ub2e4. \ub610\ud55c \ucd94\uc815\ud558\ub294 \ubaa8\uc218\uc758 \uac1c\uc218 \ub610\ud55c \uc904\uc5b4\ub4e4\uc5c8\uae30 \ub54c\ubb38\uc5d0, \uacc4\uc0b0\uc5d0\uc11c\uc758 \uc6a9\uc774\ud568 \ub610\ud55c \uc5bb\uc744 \uc218 \uc788\uc744 \uac83\uc774\ub2e4.","title":"R Notebook"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/#intermediate-applied-statistics-quiz-1","text":"","title":"Intermediate Applied Statistics Quiz 1"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/#1-7-4","text":"","title":"1. \uc77c\uc815\uae30\uac04 \ud6c4 7\ub9c8\ub9ac \uc911 4\ub9c8\ub9ac\uac00 \uc0ac\ub9dd\ud558\uc600\ub2e4."},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/#1-p-likelihood-function","text":"\uac01 \uac1c\uccb4\ub4e4\uc758 \uc0ac\ub9dd\ud655\ub960\uc774 \ub3d9\uc77c\ud558\uace0, \uc11c\ub85c \ub3c5\ub9bd\uc774\ub77c\ub294 \uac00\uc815\uc744 \ud55c\ub2e4\uba74, \uc774\ub294 \uc774\ud56d\ubd84\ud3ec \\(Bin(7,p)\\) \ub85c \ud45c\ud604\ud560 \uc218 \uc788\uc73c\uba70, \uc774\ud56d\ubd84\ud3ec\ub294 \uc11c\ub85c \ub3c5\ub9bd\uc778 \ubca0\ub974\ub204\uc774 \uc2dc\ud589\uc744 n\ubc88 \ubc18\ubcf5\ud55c \ud615\ud0dc\uc774\ubbc0\ub85c, \uac01 \uac1c\uccb4\ub4e4( \\(X_i,\\ i=1,..,7\\) )\uc740 \\(Ber(1,p)\\) \uc758 \ubd84\ud3ec\ub97c \uac16\ub294\ub2e4\uace0 \ubcfc \uc218 \uc788\ub2e4. \uc774\uc5d0 \ub300\ud55c likelihood function\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \\(L(p;x)=\\prod_{i=1}^7f(x_i;p)\\) \\(= p^{x_1}(1-p)^{1-x_1}p^{x_2}(1-p)^{1-x_2}..p^{x_7}(1-p)^{1-x_7}\\) \\(= p^{\\sum_{i=1}^7 {x_i}}(1-p)^{n-\\sum_{i=1}^7{x_i}},\\ n=7\\) \uc5ec\uae30\uc5d0 \ucd94\uac00\ub85c \uba87\ubc88\uc9f8 \uac1c\uccb4\uc5d0\uc11c \uc774\ubca4\ud2b8\uac00 \ubc1c\uc0dd\ud588\ub294\uc9c0\uc5d0 \ub300\ud55c \uc870\ud569\uc758 \uc218\ub85c \\(nCx,\\ \\sum{x_i}=x\\) \uac00 \uc788\uc9c0\ub9cc, likelihood \ud568\uc218\ub85c \uad00\uc2ec\ubaa8\uc218\ub97c \ucd94\uc815\ud558\ub294\ub370\uc5d0 \uc788\uc5b4\uc11c\ub294 \\(nCx\\) \ub294 \ubaa8\uc218 p\uc5d0 \ub300\ud574 \ubbf8\ubd84\uc744 \ud560 \uc2dc \uc0ac\ub77c\uc9c8\ubfd0 \uc544\ub2c8\ub77c, \ud544\uc694\ud55c \uc815\ubcf4\uac00 \uc544\ub2c8\uae30 \ub54c\ubb38\uc5d0 \uc0dd\ub7b5\ud558\uc5ec \ud45c\ud604\ud574\ub3c4 \ubb34\ubc29\ud558\ub2e4.","title":"1) \ubaa8\ub4e0 \uac1c\uccb4\uc758 \uc0ac\ub9dd\ud560 \ud655\ub960\uc744 p\ub85c \ub3d9\uc77c\ud558\ub2e4\uace0 \ud558\uace0 \uac01 \uac1d\uccb4\uc758 \uc0ac\ub9dd\uc5ec\ubd80\uac00 \ub3c5\ub9bd\uc774\ub77c\ub294 \uac00\uc815 \ud558\uc5d0\uc11c \uc774\ud56d\ubd84\ud3ec\ub97c \uc774\uc6a9\ud558\uc5ec likelihood function\uc744 \uc4f0\uc2dc\uc624."},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/#2-score-function-p-score-function","text":"Score function\uc740 log-likelihood\ub97c \uad00\uc2ec\ubaa8\uc218\uc5d0 \ub300\ud574\uc11c 1\ucc28\ubbf8\ubd84\uc744 \ud55c \ud568\uc218\uc774\ub2e4. \uc774\ub97c \uacc4\uc0b0\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\(S(p) = \\frac{\\partial }{\\partial p}logL(p)\\) $=\\frac{\\partial }{\\partial p}(\\sum{x_i}logp + (n-\\sum{x_i})log(1-p)) $ \\(= \\frac{\\sum{x_i}}{p}+\\frac{\\sum{x_i}-n}{1-p}\\) \\(=\\frac{\\sum{x_i}-np}{p(1-p)}\\) score function\uc740 likelihood function\uc758 \uae30\uc6b8\uae30\ub97c \ub098\ud0c0\ub0b4\ub294 \ud568\uc218\uc774\ubbc0\ub85c, p\uac00 mle\uc77c\ub54c 0\uc744 \uc9c0\ub098\uac00\ub294 \ubaa8\uc2b5\uc744 \uac16\uac8c\ub41c\ub2e4. \uae30\uc6b8\uae30\ub97c \ub098\ud0c0\ub0b4\ub294 \ud568\uc218\uc774\uae30\uc5d0, score function\uc740 \uc790\uc5f0\uc2a4\ub7fd\uac8c decreasing function\uc758 form\uc744 \uac16\uac8c \ub418\uba70, \uc774\ub97c \uc5bc\ucd94 \uadf8\ub9bc\uc73c\ub85c \ud45c\ud604\ud55c\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc744 \uac83\uc774\ub2e4.","title":"2) Score function\uc744 \uad6c\ud558\uace0 p\uc5d0 \ub530\ub978 score function\uc744 \uadf8\ub9ac\uc2dc\uc624."},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/#3-likelihood-based-confidence-interval","text":"likelihood \uc5d0 \uae30\ubc18\ud574 \uc2e0\ub8b0\uad6c\uac04\uc744 \uad6c\ud55c\ub2e4\uba74, likelihood ratio\ub97c \uc774\uc6a9\ud558\uc5ec \uc774 ratio\uac00 \ud2b9\uc815 cutoff \uc9c0\uc810\ubcf4\ub2e4 \ud06c\uac8c \ub9cc\ub4dc\ub294 p\uc758 \ubc94\uc704\ub97c \uad6c\ud558\ub294 \uac83\uc774\ub2e4. \uc774 \uc608\uc81c\uc758 likelihood ratio\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(\\frac{p^y(1-p)^{n-y}}{\\hat{p}^y(1-\\hat{p})^{n-y}},\\ \\hat{p}\\ is\\ mle\\ of\\ p,\\ y=\\sum{x_i}\\) \\(=\\frac{p^4(1-p)^3}{(4/7)^4(3/7)^3}\\) mle\ub97c \uad6c\ud574\ubcf4\uba74 \\(\\frac{\\sum{x_i}}{n}\\) \uc774 \ub098\uc624\ubbc0\ub85c, \\(\\frac{4}{7}\\) \uc744 \uc0ac\uc6a9\ud55c \uac83\uc774\ub2e4. \uacc4\uc18d\ud574\uc11c \uc804\uac1c\ud558\uba74, \\(\\frac{p^4(1-p)^3}{(4/7)^4(3/7)^3} > c\\) \uc774\uba70 \uc774\ub294 \\(-2log\\frac{p^4(1-p)^3}{(4/7)^4(3/7)^3} < -2logc =3.84\\) \uc774\uba70 \uc774\ub294 \\(\\{p^4(1-p)^3 > 0.019 \\}\\) \uc640 \uac19\uace0 \uc774\ub97c p\uc5d0 \ub300\ud574\uc11c \ud480\uba74, (0.22,0.87) \uc744 \uc5bb\uc744 \uc218 \uc788\uc73c\uba70, \uc774\uac83\uc774 \uace7 likelihood based\ub85c \uad6c\ud55c 95% \uc2e0\ub8b0\uad6c\uac04\uc774 \ub41c\ub2e4.","title":"3) likelihood based confidence interval\uc744 \uad6c\ud558\uc2dc\uc624"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/#4-p03-likelihood-ratio-function-005-cutoff","text":"likelihood ratio\ub294 \uc704\uc5d0\uc11c \uad6c\ud55c \uac83\uc744 \uc774\uc6a9\ud558\uc5ec \ud655\uc778\ud558\uba74 \uc704\uc640 \uac19\uc740 \uadf8\ub9bc\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4. \ud568\uc218\uc758 \ubd84\ubaa8 \ubd80\ubd84\uc758 \\(\\hat{\\theta}\\) \ub294 MLE\ub97c \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc5d0 \\(\\frac{4}{7}\\) \ub97c \ub123\uc5c8\ub2e4. \uadf8\ub798\ud504\ub294 \ucd5c\uace0\ucc28\ud56d\uc774 7\uc778 \uace0\ucc28\ud568\uc218\uc758 \ud615\ud0dc\ub97c \ub744\uace0 \uc788\uc73c\ub098, \ubaa8\uc218 p\uc758 support\uac00 [0,1] \uc774\uae30 \ub54c\ubb38\uc5d0 0\uacfc 1\uc0ac\uc774\uc5d0\uc11c\uc758 \uadf8\ub798\ud504\ub97c \ud655\uc778\ud558\uba74 \ub41c\ub2e4. \uc5ec\uae30\uc11c cutpoint\uc778 y\ucd95\uc774 0.15\uac00 \ub418\ub294 \uc9c0\uc810\uc744 \ud655\uc778\ud558\uba74, p\uac12\uc774 \uc57d 0.22\uc640 0.86\uc77c\ub54c\uac00 \ud568\uc218\uac12\uc774 0.15\uc5d0 \ud574\ub2f9\ud558\ub294 \uc9c0\uc810\uc774 \ub41c\ub2e4. \uace0\ub85c \uac80\uc815\ud558\uace0\uc790 \ud558\ub294 p\uac12\uc774 (0.22,0.86) \uc0ac\uc774\uc5d0 \uc874\uc7ac\ud558\uac8c\ub418\uba74, \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud560 \uc218 \uc5c6\uc744 \uac83\uc774\ub2e4. LRT\uc5d0\uc11c \\(R(p)= \\frac{(0.3)^4(0.7)^3}{(4/7)^4(3/7)^3}\\) \uc774\uba70, \uc720\uc758\uc218\uc900 0.05\uc5d0\uc11c LRT\uc758 cutoff point\ub294 ratio\uc758 \uac12\uc774 0.15\uac00 \ub418\ub294 \uc9c0\uc810\uc774 \ub41c\ub2e4. \uace0\ub85c ratio \uac12\uc774 0.15\ubcf4\ub2e4 \uc791\uc744 \uc2dc, \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud558\uac8c \ub41c\ub2e4. \uc774\ub97c \uacc4\uc0b0\ud574\ubcf4\uba74 0.33\uc774 \ub098\uc624\uac8c \ub418\ubbc0\ub85c, \uc774\uc5d0 \ub530\ub77c p=0.3\uc778 \uadc0\ubb34\uac00\uc124\uc744 \uae30\uac01\ud560 \uc218 \uc5c6\ub2e4\ub294 \uacb0\ub860\uc744 \ub0b4\ub9b4 \uc218 \uc788\ub2e4.","title":"4) p=0.3\uc784\uc744 \uac80\uc815\ud558\uace0\uc790\ud55c\ub2e4. likelihood ratio function\uc744 \uadf8\ub9ac\uace0 0.05\uc758 \uc720\uc758\uc218\uc900\uc5d0\uc11c\uc758 cutoff\ub97c \uad6c\ud558\uc5ec \uac80\uc815\ud558\uc2dc\uc624."},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/#2","text":"","title":"2. \ub2e4\uc74c\uc740 \ub3c5\uc57d\uc758 \uc591\uc5d0 \ub530\ub978 \uc77c\uc815\uae30\uac04 \ud6c4\uc758 \uc0ac\ub9dd\uc5ec\ubd80\uc5d0 \ub300\ud55c \uc790\ub8cc\uc774\ub2e4."},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/#1dead-2dead-1alive-1alive-3alive-3dead-2dead","text":"","title":"(\ub3c5\uc591\uc758 \uc591, \uc0ac\ub9dd\uc5ec\ubd80) = (1,Dead), (2,Dead), (1,Alive), (1,Alive), (3,Alive), (3,Dead), (2,Dead)."},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/#p_i-likelihood-function","text":"","title":"\uac01 \uac1c\uccb4\uc758 \uc0ac\ub9dd\ud560 \ud655\ub960\uc744 \\(p_i\\)\ub77c \ud560 \ub54c likelihood function\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4."},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/#lp_1p_2p_7p_1p_21-p_31-p_41-p_5p_6p_7","text":"","title":"\\(L(p_1,p_2,..p_7)=p_1p_2(1-p_3)(1-p_4)(1-p_5)p_6p_7\\)"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/#1-likelihood-p","text":"\uc5ec\uae30\uc11c p\ub294 \uc0ac\ub9dd\ud560 \ud655\ub960\uc744 \ub098\ud0c0\ub0b4\ub294 \uac83\uc774\ubbc0\ub85c, \ub2e8\uc21c\ud558\uac8c \uc811\uadfc\ud558\uba74 \uc774 \ud480\uc774\ub294 \uc27d\uac8c \uc811\uadfc\uc774 \uac00\ub2a5\ud558\ub2e4. \uc8fd\uc740 \uc0ac\ub78c\uc758 p\uac12\uc5d0 1\uc744 \uc8fc\uace0, \uc0b4\uc544\ub0a8\uc740 \uc0ac\ub78c\uc758 p\uac12\uc5d0 0\uc744 \uc8fc\uac8c\ub41c\ub2e4\uba74, Likelihood\ub294 1\ub85c \ucd5c\ub300\uac00 \ub418\uac8c \ub41c\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 likelihood\ub97c \ucd5c\ub300\ub85c \ud558\ub294 p\uac12\ub4e4\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(p_1=1,\\ p_2=1,\\ p_3=0,\\ p_4=0,\\ p_5=0,\\ p_6=1,\\ p_7=1\\)","title":"1) Likelihood\uac00 \ucd5c\ub300\uac00 \ub418\ub294 \uac01 p\uac12\uc744 \uad6c\ud558\uc2dc\uc624."},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/#2-p_ialpha-likelihood-alpha","text":"\ubaa8\ub4e0 p\uac12\uc774 \uac19\ub2e4\uba74 likelihood function\uc740 \ub2e4\uc74c\uacfc \uac19\uc774 \ud45c\ud604\uc774 \uac00\ub2a5\ud558\ub2e4. \\(L(\\alpha)=\\alpha^4(1-\\alpha)^3\\) likelihood\ub97c \ucd5c\ub300\ub85c \ud558\uae30\uc704\ud574 MLE\ub97c \uad6c\ud574\ubcf4\uba74, \\(\\frac{\\partial}{\\partial \\alpha}logL(\\alpha)= \\frac{4}{\\alpha}+ \\frac{-3}{1-\\alpha}\\) \ucc98\ub7fc \ud45c\ud604\uac00\ub2a5\ud558\uace0, \uc774 \uc2dd\uc774 0\uc73c\ub85c \ub418\uac8c\ud558\ub294 \\(\\alpha\\) \ub97c \ucc3e\uc73c\uba74 \ub41c\ub2e4. \uc774\ub97c \uacc4\uc0b0\ud558\uba74 \\(\\alpha = \\frac{4}{7}\\) \ub97c \uc5bb\uac8c \ub41c\ub2e4. \ud558\uc9c0\ub9cc \uc774 \ubaa8\ud615\uc774 \uc801\ud569\ud55c\uc9c0\ub97c \ub17c\ud55c\ub2e4\uba74, '\uc801\ud569\ud558\uc9c0 \uc54a\ub2e4' \ub77c\uace0 \ub9d0\ud574\uc57c \ud560 \uac83\uc774\ub2e4. \ubaa8\ub4e0 \uac1c\uccb4\ub4e4\uc774 \uc0ac\ub9dd\ud560 \ud655\ub960\uc774 \uac19\uace0 \uc0ac\ub9dd\ud655\ub960 \uc790\uccb4 \uc678\uc5d0 \ub2e4\ub978 \uc694\uc778\uc774 \ud3ec\ud568\ub418\uc5b4\uc788\uc9c0\uc54a\ub2e4\uba74, \uc62c\ubc14\ub978 \ubaa8\ud615\uc801\ud569\uc774\ub77c\uace0 \ub9d0\ud560 \uc218 \uc788\uaca0\uc9c0\ub9cc, \uc704\uc758 \uc0d8\ud50c\uc5d0\ub294 \ud22c\uc5ec\ud55c \ub3c5\uc57d\uc758 \uc591\uc774\ub77c\ub294 \uc815\ubcf4\uac00 \ud558\ub098 \ub354 \ucd94\uac00\ub418\uc5b4\uc788\ub2e4. \uc774\uc640 \uac19\uc740 \ubaa8\ud615\uc801\ud569\uc744 \ud558\uac8c\ub41c\ub2e4\uba74, \ub3c5\uc57d\uc758 \ud22c\uc5ec\uc815\ub3c4\uc640\ub294 \uc0c1\uad00\uc5c6\uc774 likehood\ub97c \uacc4\uc0b0\ud558\uac8c \ub418\ub294 \uac83\uc774\ubbc0\ub85c, \ubaa8\ub4e0 \uc694\uc778\uc744 \uace0\ub824\ud558\uc9c0 \uc54a\uc558\uae30 \ub54c\ubb38\uc5d0 \uc801\uc808\ud55c \ubaa8\ud615\uc774\ub77c\uace0 \ub9d0\ud560 \uc218 \uc5c6\uc744 \uac83\uc774\ub2e4.","title":"2) \\(p_i=\\alpha\\) \uc77c\ub54c likelihood\uac00 \ucd5c\ub300\uac00 \ub418\ub294 \\alpha\ub97c \uad6c\ud558\uace0 \uc774\ub7ec\ud55c \ubaa8\ud615\uc774 \uc801\ud569\ud55c\uc9c0\ub97c \ub17c\ud558\uc2dc\uc624."},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/#3-p_ialphabetax_i-likelihood-alpha-beta","text":"\uba3c\uc800 likelihood\uc2dd\uc744 \uc804\uac1c\ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\((\\alpha+\\beta{x_1})(\\alpha+\\beta{x_5})(\\alpha+\\beta{x_6})(\\alpha+\\beta{x_7})(1-\\alpha-\\beta{x_3})(1-\\alpha-\\beta{x_4})(1-\\alpha-\\beta{x_5})\\) \uc774\ub97c \ucd5c\ub300\uac00 \ub418\uac8c \ud558\ub824\uba74, \uae30\uc874 likelihood\ub97c \ucd5c\ub300\ud654 \ud558\ub294 \uac83\ucc98\ub7fc \uc704 \uc2dd\uc5d0 log\ub97c \ucde8\ud574 \ubbf8\ubd84\ud55c \ub4a4 \uadf8 \uc2dd\uc744 0\uc774 \ub418\uac8c\ud558\ub294 \uac01\uac01\uc758 \\(\\alpha\\) \uc640 \\(\\beta\\) \ub97c \ucc3e\uc73c\uba74 \ub420 \uac83\uc774\ub2e4. \\(p_i\\) \uac12\uc774 \ubaa8\ub450 \uac01\uac01 \uad00\ub828\uc5c6\ub294 \ub2e4\ub978 \uac12\ub4e4\uc774\ub77c\uba74 \uae30\uc874\uc811\uadfc\ubc29\uc2dd\ucc98\ub7fc \ub2e4\uac00\uac08 \ub54c\ub294 \uad00\uc2ec\ubaa8\uc218\uac00 7\uac1c\uac00 \ub418\uaca0\uc9c0\ub9cc, \uc774 \uc608\uc81c\uc5d0\uc11c\ub294 \uad00\uc2ec \ubaa8\uc218\uac00 2\uac1c\ub85c \uc881\ud600\uc9c4\ub2e4. \\(p_i\\) \uc758 \uac12\uc774 \\(x_i\\) \ub4e4\uc758 \uc120\ud615\uacb0\ud569\uc2dd\uc73c\ub85c \ub418\uc5b4 \uc788\ub294 \ud615\ud0dc\uc774\uae30 \ub54c\ubb38\uc5d0, \uc6b0\ub9ac\uac00 \\(x_i\\) \ub85c \uc774\ub8e8\uc5b4\uc9c4 \uc774 \uacb0\ud569\uc2dd\uc5d0\uc11c \uae30\uc6b8\uae30\uc640 \uc808\ud3b8\uc744 \ucd94\uc815\ud560 \uc218 \uc788\ub2e4\uba74, \uc774 \uac83\uc774 \\(p_i\\) \ub4e4\uc5d0 \ub300\ud55c \ucd94\uc815\uc73c\ub85c \uc774\uc5b4\uc9c0\ub294 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc likelihood\uc2dd\uc744 \uc804\uac1c\ud560\ub54c \ucd5c\uace0\ucc28\ud56d\uc774 \ub9e4\uc6b0 \ub192\uc544\uc9c0\uba70 \uacc4\uc0b0\ud558\uae30\uc5d0 \uc544\uc8fc \ubcf5\uc7a1\ud574\uc9c0\ub294 \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud55c\ub2e4. \uac1c\uc778\uc801\uc778 \uacac\ud574\ub85c\ub294 '\uae30\uc874\uc5d0 \uac01\uae30 \ub2e4\ub978 7\uac1c\uc758 \ubaa8\uc218\ub97c \ucd94\uc815\ud558\ub294 \uac83\uacfc \ub2e4\ub974\uac8c, 3\ubc88\uacfc \uac19\uc740 \uc608\uc81c\ub294 \uc624\ud788\ub824 \uc0d8\ud50c\uacfc \uc228\uaca8\uc9c4 \ub450\uac00\uc9c0 \ubaa8\uc218\uc758 \uacb0\ud569\uc2dd\uc73c\ub85c \uae30\uc874 \ubaa8\uc218\ub97c \ucd94\uc815\ud558\uc5ec \ucd94\uc815\ud558\ub294 \ubaa8\uc218\uc758 \uac1c\uc218\ub97c \uc904\uc5ec \uc880 \ub354 \uc27d\uac8c \ubb38\uc81c\ub97c \ud574\uacb0\ud560 \uc218 \uc788\ub294 \uc810\ub3c4 \uc874\uc7ac\ub294 \ud558\uaca0\uc9c0\ub9cc, \uac01 \ud558\ub098\uc758 \uac1c\uccb4\uc5d0 \ud574\ub2f9\ud558\ub294 \ud558\ub098\uc758 \ubaa8\uc218\ub97c \ub354 \uc798\uac8c\ucabc\uac1c \ub450\uac00\uc9c0 \ubaa8\uc218\uc758 \uc120\ud615\uacb0\ud569\uc73c\ub85c \ud45c\ud604\ud558\uc600\uae30\uc5d0 \uc124\ub839 \uadf8 \ub450\uac00\uc9c0 \ubaa8\uc218\ub97c \uc798 \ucd94\uc815\ud558\uc5ec \uad6c\ud588\ub2e4\uace0 \ud558\ub354\ub77c\ub3c4, \ud574\uc11d\uacfc \uc124\uba85\ub825\uc5d0 \uc788\uc5b4\uc11c \ubcf5\uc7a1\ud568\uc744 \ub354 \uc99d\uac00\uc2dc\ud0a4\uc9c0\uc54a\uc744\uae4c' \ub77c\ub294 \uc0dd\uac01\uc744 \ud558\uc600\ub2e4. ( \uc624\ub85c\uc9c0 \uc800\uc758 \uac1c\uc778\uc801\uc778 \uc0c1\uc0c1\ubfd0\uc774\uc5b4\uc11c, \uc774\uc5d0 \ub300\ud574 \uad50\uc218\ub2d8\uaed8\uc11c \uc124\uba85\ud574\uc8fc\uc2dc\uba74 \ub9e4\uc6b0 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4. )","title":"3) \\(p_i=\\alpha+\\beta{x_i}\\) \uc77c \ub54c likelihood\ub97c \uc4f0\uace0 \ucd5c\ub300\uac00 \ub418\ub294 \\(\\alpha,\\ \\beta\\)\ub97c \uc5b4\ub5bb\uac8c \uad6c\ud560 \uc218 \uc788\uaca0\ub294\uac00?"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz1/#4","text":"\uae30\uc874\uc758 \uc811\uadfc\uc740 \uac1c\uccb4\ub97c \uae30\uc900\uc73c\ub85c grouping\uc744 \ud55c \uac83\uc774\ub77c\uba74, 4\ubc88 \uc608\uc81c\ub294 \ub3c5\uc591\uc758 \ud22c\uc5ec\uc591\uc5d0 \ub530\ub77c \uc0c8\ub86d\uac8c grouping\uc744 \ud55c \uc608\uc81c\uc774\ub2e4. 1\ubc88\uc758 case\ub294 \uc624\ub85c\uc9c0 \uac1c\uccb4 \uae30\uc900\uc758 \uc0ac\ub9dd\ud655\ub960\uc744 \ub17c\ud558\uc5ec \ubaa8\uc218\uc758 \uac1c\uc218\uac00 7\uac1c\uc600\uace0, \ub3c5\uc57d\uc758 \uc591\uc744 \uace0\ub824\ud558\uc9c0 \uc54a\uc558\ub2e4. \ud558\uc9c0\ub9cc 4\ubc88\uc758 case\ub294 \ub3c5\uc57d\ud22c\uc5ec\uc5d0 \ub530\ub77c \uc0ac\ub9dd\ud655\ub960\uc744 \uc815\ud558\uc600\uae30 \ub54c\ubb38\uc5d0, \ud22c\uc57d\uc758 class \uac2f\uc218\uc640 \uac19\uac8c \ucd94\uc815\ud558\ub294 \ubaa8\uc218\uac00 3\uac1c\ub85c \uc904\uc5b4\ub4e4\uac8c \ub418\uc5c8\ub2e4. \uc774\ub294 \uc624\ub85c\uc9c0 \uc0ac\ub9dd\uc5ec\ubd80\ub9cc \uace0\ub824\ud558\uc9c0 \uc54a\uace0, \uc57d\ubb3c \ud22c\uc5ec\uae4c\uc9c0 \uace0\ub824\ud558\uc5ec likelihood\ub97c \ucd94\uc815\ud558\ub294 \uac83\uc774\uae30 \ub54c\ubb38\uc5d0 \uc870\uae08 \ub354 \ud569\ub9ac\uc801\uc778 \ucd94\uc815\uc774\ub77c\uace0 \ud560 \uc218 \uc788\uc744 \uac83\uc774\ub2e4. \ub610\ud55c \ucd94\uc815\ud558\ub294 \ubaa8\uc218\uc758 \uac1c\uc218 \ub610\ud55c \uc904\uc5b4\ub4e4\uc5c8\uae30 \ub54c\ubb38\uc5d0, \uacc4\uc0b0\uc5d0\uc11c\uc758 \uc6a9\uc774\ud568 \ub610\ud55c \uc5bb\uc744 \uc218 \uc788\uc744 \uac83\uc774\ub2e4.","title":"4)"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz2/","text":"Intermediate Applied Statistics Quiz II 1. \ucca8\ubd80\ub41c \ub17c\ubb381\uc740 \ud608\uc561\ud615\uacfc COVID-19\uc758 \uac10\uc5fc\uc5d0 \ub300\ud55c \ubd84\uc11d\uc774\ub2e4. Table 1, S1\uc5d0\ub294 Data\uc640 \uc774\ub97c \uc694\uc57d\ud55c Pearson \uce74\uc774\uc81c\uacf1 \uac80\uc815\ud1b5\uacc4\ub7c9, p-value, OR, 95% CI\ub4f1\uc774 \ub098\uc640 \uc788\ub2e4. \uc774\ub4e4\uc774 \uc5b4\ub5bb\uac8c \uad6c\ud574\uc9c4 \uac83\uc778\uc9c0 \ud558\ub098\uc758 \uc608 \ub97c \ub4e4\uc5b4 \ubcf4\uc774\uc2dc\uc624. \uc704\uc758 \ud14c\uc774\ube14\uc5d0\uc11c \ud558\ub098\uc758 \uadf8\ub8f9\uc744 \ud0dd\ud558\uc5ec \uac19\uc740 \ud1b5\uacc4\ub7c9\uc774 \ub098\uc624\ub294\uc9c0\ub97c \ud655\uc778\ud574\ubcf4\uc790. \uc774\ub97c \ud655\uc778\ud574\ubcf4\uba74 \uc6b0\ud55c\uc9c0\uc5ed\uc758 Normal control \uc9d1\ub2e8\uc740 \ucd1d 3694\uba85\uacfc, Patients \uc9d1\ub2e8\uc740 1775\uba85\uc73c\ub85c \uc774\ub8e8\uc5b4\uc838\uc788\ub2e4. \uac01 \ud608\uc561\ud615 \uadf8\ub8f9\ub0b4\uc5d0\uc11c control\uacfc treatment\ub97c \ube44\uad50\ud558\uace0\uc2f6\ub2e4\uba74, \uadf8\ub54c\uc758 \ud589\ub82c\uc740 \\(2 \\times 2\\) \ud615\ud0dc\uc5d0 \uc5f4\uc740 \ud2b9\uc815 \ud608\uc561\ud615\uc9d1\ub2e8\uacfc \uadf8\uc5d0 \ub300\ud55c \uc5ec\uc9d1\ud569\uc9d1\ub2e8\uc73c\ub85c \uc774\ub8e8\uc5b4\uc838\uc57c\ud55c\ub2e4. library(epitools) a <- matrix(c(469,1775-469,920,3694-920),2,2) chisq.test(a) ## ## Pearson's Chi-squared test with Yates' continuity correction ## ## data: a ## X-squared = 1.3777, df = 1, p-value = 0.2405 odds.ratio <- (469/1306)/(920/2774) oddsratio.wald(a) ## $data ## Outcome ## Predictor Disease1 Disease2 Total ## Exposed1 469 920 1389 ## Exposed2 1306 2774 4080 ## Total 1775 3694 5469 ## ## $measure ## odds ratio with 95% C.I. ## Predictor estimate lower upper ## Exposed1 1.0000 NA NA ## Exposed2 1.0828 0.9515635 1.232136 ## ## $p.value ## two-sided ## Predictor midp.exact fisher.exact chi.square ## Exposed1 NA NA NA ## Exposed2 0.2280304 0.2324257 0.2274537 ## ## $correction ## [1] FALSE ## ## attr(,\"method\") ## [1] \"Unconditional MLE & normal approximation (Wald) CI\" \uc704\uc758 \ucf54\ub4dc\ub294 B\ud615 \ud608\uc561\ud615\uc744 \uae30\uc900\uc73c\ub85c Control\uacfc Treatment \uc9d1\ub2e8\uc744 \ub098\ub204\uc5b4 \ud1b5\uacc4\ub7c9\uac12\uc744 \uad6c\ud574\ubcf8 \uacb0\uacfc\uc774\ub2e4. \ud655\uc778\ud574\ubcf4\uba74, Oddsratio\uc640 Pearson chi-square Statistics, oddsraio\uc758 CI \ub4f1\uc774 \uac19\uac8c \ub098\uc634\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. 2. \ucca8\ubd80\ub41c \ub17c\ubb382\ub294 \uc628\ub3c4 \ubc0f \uc2b5\ub3c4\uc758 COVID-19\uc758 \uc804\ud30c\ub825\uacfc\uc758 \uad00\uacc4\uc5d0 \ub300\ud55c \ubd84\uc11d\uc774\ub2e4. \ubd84\uc11d\ubc29\ubc95 \uc804\ubc18\uc5d0 \uac78\uccd0 comment\ud558\uc2dc\uc624. \uc704\uc758 \ub17c\ubb38\uc744 \uc0b4\ud3b4\ubcf4\uba74, \ucf54\ub85c\ub098\ubc14\uc774\ub7ec\uc2a4\uac00 \uace0\uc628 \ud639\uc740 \ub2e4\uc2b5\ud55c \ub0a0\uc528\uc5d0\uc11c transmission\ud558\ub294 \uc804\ud30c\ub825\uc774 \ub0ae\uc544\uc9c4\ub2e4\ub294 \uc8fc\uc7a5\uc744 \ud558\uace0 \uc788\ub2e4. \uc5f0\uad6c\uc9c4\ub4e4\uc740 \uc5ed\ud559\uc870\uc0ac\ub370\uc774\ud130\ub97c \uc5bb\uace0 \ub098\uc11c \ud2b9\uc815\uae30\uac04(3\uc77c) \ub3d9\uc548 \uc57d 100\uc5ec \uacf3\uc758 \ub3c4\uc2dc\ub4e4\uc758 \uc2b5\ub3c4\uc640 \uc628\ub3c4 \ub370\uc774\ud130\ub97c \uc5bb\uc5b4 \uc7ac\uc0dd\uc0b0\uc9c0\uc218\uc640 \ub3c5\ub9bd\ubcc0\uc218\ub4e4 \uac04\uc758 \uad00\uacc4\ub97c \ud30c\uc545\ud558\uace0\uc790 \uba87\uac00\uc9c0 \ud68c\uadc0\ubd84\uc11d\uc744 \uc0ac\uc6a9\ud558\uc600\ub2e4. COVID-19\uac00 \uc9c0\uae08 \ud070 \uc601\ud5a5\uc744 \uc804 \uc138\uacc4\uc5d0 \uc8fc\uace0 \uc788\uc9c0\ub9cc, \uc774 \ubc14\uc774\ub7ec\uc2a4\uac00 \ubc1c\ud604\ud558\uac8c \ub41c \uc2dc\uae30\ub294 \uadf8\ub2e5 \uae34 \uc2dc\uae30\uac00 \uc544\ub2cc\ub370, \uc5f0\uad6c\uc9c4\ub4e4\uc740 \ubd84\uc11d\uc744 \ud560 \ub584, 2020\ub144 1\uc6d4 \uc911\uc21c\uc5d0\uc11c 3\uc77c\ub3d9\uc548, 21\uc77c \ubd80\ud130 23\uc77c\uae4c\uc9c0\uc758 \uc628\ub3c4\uc640 \uc2b5\ub3c4\ub97c \uc5bb\uc5b4 \uc774\ub4e4\uc758 \ud3c9\uade0\uce58\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc7ac\uc0dd\uc0b0\uc9c0\uc218\uc640\uc758 \uad00\uacc4\ub97c \ubd84\uc11d\ud558\uc600\ub2e4. \ubb3c\ub860 \ucf54\ub85c\ub098\uc0ac\ud0dc \uc774\ud6c4 \ub3d9\uc548 \uc624\ub79c\uae30\uac04\uc774 \uc9c0\ub0ac\ub358 \uac83 \uc740 \uc544\ub2c8\uc9c0\ub9cc, panel regression\uc5d0 \uc801\uc6a9\ud560 \ub54c \uc870\uae08 \ub354 \uc624\ub79c\uae30\uac04\ub3d9\uc548\uc758 \uc2b5\ub3c4\uc640 \uc628\ub3c4\ub4e4\uc758 \ud3c9\uade0\uce58\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc9c0 \uc54a\uc558\ub098\uc5d0 \ub300\ud55c \uc0dd\uac01\uc774 \ub4e4\uc5c8\ub2e4. \ub17c\ubb38\uc758 \uc55e \ubd80\ubd84\uc5d0\ub294 \uc778\ud50c\ub8e8\uc5d4\uc790\uc758 \uc804\ud30c\ub825\uc744 \uc124\uba85\ud558\uba74\uc11c, \ucf54\ub85c\ub098\uc640 \ud0c0 \ubc14\uc774\ub7ec\uc2a4(\uc0ac\uc2a4, \uc778\ud50c\ub8e8\uc5d4\uc790)\uac00 \uc628\ub3c4\uc640 \uc2b5\ub3c4\ub85c \uc778\ud574 \ubcc0\ud558\ub294 \uc601\ud5a5\ub825\uc774 \ube44\uc2b7\ud558\ub2e4\uace0 \uc5b8\uae09\uc744 \ud55c \ubd80\ubd84\uc774 \uc788\uc5c8\ub294\ub370, \uc5f0\uad6c\ubd84\uc11d\uc744 \uc704\ud574 \ub370\uc774\ud130\ub97c \uc870\uae08 \ub354 \uc624\ub79c\uae30\uac04 \ub3d9\uc548 \uce21\uc815\ud558\uc5ec \ubd84\uc11d\uc744 \ud558\uac70\ub098, \uc774 \uc804 \ubc14\uc774\ub7ec\uc2a4\uc758 \uc804\ud30c\ub825\uc5d0 \ub300\ud55c \uc5f0\uad6c\uc5d0\uc11c \uce21\uc815\ub2e8\uc704\ub97c \uc815\ud558\ub294 \ubd80\ubd84\uc5d0\uc11c \uc870\uae08 \ub354 \ucc38\uace0\ub97c \ud558\uac70\ub098 \uc774\uc5d0 \ub300\ud55c \uc124\uba85\uc774 \ucd94\uac00\ub85c \ub418\uc5b4\uc788\uc5c8\ub2e4\uba74 \ub354 \uc88b\uc558\uc744 \uac83\uc774\ub77c \uc0dd\uac01\ud55c\ub2e4.","title":"Intermediate Applied Statistics Quiz II"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz2/#intermediate-applied-statistics-quiz-ii","text":"","title":"Intermediate Applied Statistics Quiz II"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz2/#1-1-covid-19-table-1-s1-data-pearson-p-value-or-95-ci","text":"\uc704\uc758 \ud14c\uc774\ube14\uc5d0\uc11c \ud558\ub098\uc758 \uadf8\ub8f9\uc744 \ud0dd\ud558\uc5ec \uac19\uc740 \ud1b5\uacc4\ub7c9\uc774 \ub098\uc624\ub294\uc9c0\ub97c \ud655\uc778\ud574\ubcf4\uc790. \uc774\ub97c \ud655\uc778\ud574\ubcf4\uba74 \uc6b0\ud55c\uc9c0\uc5ed\uc758 Normal control \uc9d1\ub2e8\uc740 \ucd1d 3694\uba85\uacfc, Patients \uc9d1\ub2e8\uc740 1775\uba85\uc73c\ub85c \uc774\ub8e8\uc5b4\uc838\uc788\ub2e4. \uac01 \ud608\uc561\ud615 \uadf8\ub8f9\ub0b4\uc5d0\uc11c control\uacfc treatment\ub97c \ube44\uad50\ud558\uace0\uc2f6\ub2e4\uba74, \uadf8\ub54c\uc758 \ud589\ub82c\uc740 \\(2 \\times 2\\) \ud615\ud0dc\uc5d0 \uc5f4\uc740 \ud2b9\uc815 \ud608\uc561\ud615\uc9d1\ub2e8\uacfc \uadf8\uc5d0 \ub300\ud55c \uc5ec\uc9d1\ud569\uc9d1\ub2e8\uc73c\ub85c \uc774\ub8e8\uc5b4\uc838\uc57c\ud55c\ub2e4. library(epitools) a <- matrix(c(469,1775-469,920,3694-920),2,2) chisq.test(a) ## ## Pearson's Chi-squared test with Yates' continuity correction ## ## data: a ## X-squared = 1.3777, df = 1, p-value = 0.2405 odds.ratio <- (469/1306)/(920/2774) oddsratio.wald(a) ## $data ## Outcome ## Predictor Disease1 Disease2 Total ## Exposed1 469 920 1389 ## Exposed2 1306 2774 4080 ## Total 1775 3694 5469 ## ## $measure ## odds ratio with 95% C.I. ## Predictor estimate lower upper ## Exposed1 1.0000 NA NA ## Exposed2 1.0828 0.9515635 1.232136 ## ## $p.value ## two-sided ## Predictor midp.exact fisher.exact chi.square ## Exposed1 NA NA NA ## Exposed2 0.2280304 0.2324257 0.2274537 ## ## $correction ## [1] FALSE ## ## attr(,\"method\") ## [1] \"Unconditional MLE & normal approximation (Wald) CI\" \uc704\uc758 \ucf54\ub4dc\ub294 B\ud615 \ud608\uc561\ud615\uc744 \uae30\uc900\uc73c\ub85c Control\uacfc Treatment \uc9d1\ub2e8\uc744 \ub098\ub204\uc5b4 \ud1b5\uacc4\ub7c9\uac12\uc744 \uad6c\ud574\ubcf8 \uacb0\uacfc\uc774\ub2e4. \ud655\uc778\ud574\ubcf4\uba74, Oddsratio\uc640 Pearson chi-square Statistics, oddsraio\uc758 CI \ub4f1\uc774 \uac19\uac8c \ub098\uc634\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.","title":"1. \ucca8\ubd80\ub41c \ub17c\ubb381\uc740 \ud608\uc561\ud615\uacfc COVID-19\uc758 \uac10\uc5fc\uc5d0 \ub300\ud55c \ubd84\uc11d\uc774\ub2e4. Table 1, S1\uc5d0\ub294 Data\uc640 \uc774\ub97c \uc694\uc57d\ud55c Pearson \uce74\uc774\uc81c\uacf1 \uac80\uc815\ud1b5\uacc4\ub7c9, p-value, OR, 95% CI\ub4f1\uc774 \ub098\uc640 \uc788\ub2e4. \uc774\ub4e4\uc774 \uc5b4\ub5bb\uac8c \uad6c\ud574\uc9c4 \uac83\uc778\uc9c0 \ud558\ub098\uc758 \uc608 \ub97c \ub4e4\uc5b4 \ubcf4\uc774\uc2dc\uc624."},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz2/#2-2-covid-19-comment","text":"\uc704\uc758 \ub17c\ubb38\uc744 \uc0b4\ud3b4\ubcf4\uba74, \ucf54\ub85c\ub098\ubc14\uc774\ub7ec\uc2a4\uac00 \uace0\uc628 \ud639\uc740 \ub2e4\uc2b5\ud55c \ub0a0\uc528\uc5d0\uc11c transmission\ud558\ub294 \uc804\ud30c\ub825\uc774 \ub0ae\uc544\uc9c4\ub2e4\ub294 \uc8fc\uc7a5\uc744 \ud558\uace0 \uc788\ub2e4. \uc5f0\uad6c\uc9c4\ub4e4\uc740 \uc5ed\ud559\uc870\uc0ac\ub370\uc774\ud130\ub97c \uc5bb\uace0 \ub098\uc11c \ud2b9\uc815\uae30\uac04(3\uc77c) \ub3d9\uc548 \uc57d 100\uc5ec \uacf3\uc758 \ub3c4\uc2dc\ub4e4\uc758 \uc2b5\ub3c4\uc640 \uc628\ub3c4 \ub370\uc774\ud130\ub97c \uc5bb\uc5b4 \uc7ac\uc0dd\uc0b0\uc9c0\uc218\uc640 \ub3c5\ub9bd\ubcc0\uc218\ub4e4 \uac04\uc758 \uad00\uacc4\ub97c \ud30c\uc545\ud558\uace0\uc790 \uba87\uac00\uc9c0 \ud68c\uadc0\ubd84\uc11d\uc744 \uc0ac\uc6a9\ud558\uc600\ub2e4. COVID-19\uac00 \uc9c0\uae08 \ud070 \uc601\ud5a5\uc744 \uc804 \uc138\uacc4\uc5d0 \uc8fc\uace0 \uc788\uc9c0\ub9cc, \uc774 \ubc14\uc774\ub7ec\uc2a4\uac00 \ubc1c\ud604\ud558\uac8c \ub41c \uc2dc\uae30\ub294 \uadf8\ub2e5 \uae34 \uc2dc\uae30\uac00 \uc544\ub2cc\ub370, \uc5f0\uad6c\uc9c4\ub4e4\uc740 \ubd84\uc11d\uc744 \ud560 \ub584, 2020\ub144 1\uc6d4 \uc911\uc21c\uc5d0\uc11c 3\uc77c\ub3d9\uc548, 21\uc77c \ubd80\ud130 23\uc77c\uae4c\uc9c0\uc758 \uc628\ub3c4\uc640 \uc2b5\ub3c4\ub97c \uc5bb\uc5b4 \uc774\ub4e4\uc758 \ud3c9\uade0\uce58\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc7ac\uc0dd\uc0b0\uc9c0\uc218\uc640\uc758 \uad00\uacc4\ub97c \ubd84\uc11d\ud558\uc600\ub2e4. \ubb3c\ub860 \ucf54\ub85c\ub098\uc0ac\ud0dc \uc774\ud6c4 \ub3d9\uc548 \uc624\ub79c\uae30\uac04\uc774 \uc9c0\ub0ac\ub358 \uac83 \uc740 \uc544\ub2c8\uc9c0\ub9cc, panel regression\uc5d0 \uc801\uc6a9\ud560 \ub54c \uc870\uae08 \ub354 \uc624\ub79c\uae30\uac04\ub3d9\uc548\uc758 \uc2b5\ub3c4\uc640 \uc628\ub3c4\ub4e4\uc758 \ud3c9\uade0\uce58\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc9c0 \uc54a\uc558\ub098\uc5d0 \ub300\ud55c \uc0dd\uac01\uc774 \ub4e4\uc5c8\ub2e4. \ub17c\ubb38\uc758 \uc55e \ubd80\ubd84\uc5d0\ub294 \uc778\ud50c\ub8e8\uc5d4\uc790\uc758 \uc804\ud30c\ub825\uc744 \uc124\uba85\ud558\uba74\uc11c, \ucf54\ub85c\ub098\uc640 \ud0c0 \ubc14\uc774\ub7ec\uc2a4(\uc0ac\uc2a4, \uc778\ud50c\ub8e8\uc5d4\uc790)\uac00 \uc628\ub3c4\uc640 \uc2b5\ub3c4\ub85c \uc778\ud574 \ubcc0\ud558\ub294 \uc601\ud5a5\ub825\uc774 \ube44\uc2b7\ud558\ub2e4\uace0 \uc5b8\uae09\uc744 \ud55c \ubd80\ubd84\uc774 \uc788\uc5c8\ub294\ub370, \uc5f0\uad6c\ubd84\uc11d\uc744 \uc704\ud574 \ub370\uc774\ud130\ub97c \uc870\uae08 \ub354 \uc624\ub79c\uae30\uac04 \ub3d9\uc548 \uce21\uc815\ud558\uc5ec \ubd84\uc11d\uc744 \ud558\uac70\ub098, \uc774 \uc804 \ubc14\uc774\ub7ec\uc2a4\uc758 \uc804\ud30c\ub825\uc5d0 \ub300\ud55c \uc5f0\uad6c\uc5d0\uc11c \uce21\uc815\ub2e8\uc704\ub97c \uc815\ud558\ub294 \ubd80\ubd84\uc5d0\uc11c \uc870\uae08 \ub354 \ucc38\uace0\ub97c \ud558\uac70\ub098 \uc774\uc5d0 \ub300\ud55c \uc124\uba85\uc774 \ucd94\uac00\ub85c \ub418\uc5b4\uc788\uc5c8\ub2e4\uba74 \ub354 \uc88b\uc558\uc744 \uac83\uc774\ub77c \uc0dd\uac01\ud55c\ub2e4.","title":"2. \ucca8\ubd80\ub41c \ub17c\ubb382\ub294 \uc628\ub3c4 \ubc0f \uc2b5\ub3c4\uc758 COVID-19\uc758 \uc804\ud30c\ub825\uacfc\uc758 \uad00\uacc4\uc5d0 \ub300\ud55c \ubd84\uc11d\uc774\ub2e4. \ubd84\uc11d\ubc29\ubc95 \uc804\ubc18\uc5d0 \uac78\uccd0 comment\ud558\uc2dc\uc624."},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz3/","text":"Intermediate Applied Statistics Quiz III 1. 2.(a) Write the likelihood functions X\uc640 Y\uac00 \ub3c5\ub9bd\uc774\ubbc0\ub85c \ub458\uc758 joint likelihood\ub294 \uc544\ub798\uc640 \uac19\ub2e4. \\(X \\sim B(m,p_1),\\ Y \\sim B(n,p_2)\\) \\(L(p_1,p_2)=\\ p_1^x(1-p_1)^{(m-x)}p_2^y(1-p_2)^{(n-y)}\\) 2.(b) Rewrite the likelihood function in terms of \\(\\theta = log\\frac{p_1(1-p_1)}{p_2(1-p_2)}\\ and\\ \\eta = log\\frac{p_2}{1-p_2}\\) \\(p_1 = \\frac{e^{\\eta}}{1+e^{\\eta}},\\ p_2 = \\frac{e^{\\theta + \\eta}}{1+ e^{\\theta+\\eta}}.\\) \\(L(\\theta, \\eta) = (\\frac{p_1}{1-p_1})^x(1-p_1)^m(\\frac{p_2}{1-p_2})^y(1-p_2)^n\\) \\(= (\\frac{p_1/(1-p_1)}{p_2/(1-p_2)})^x(\\frac{p_2}{1-p_2})^{x+y}(1-p_1)^m(1-p_2)^n\\) \\(= e^{{\\theta}x}e^{\\eta(x+y)}(1+e^{\\theta+\\eta})^{-m}(1-p_2)^n\\) 2.(c) Use the profile likelihood to get the mle of \\(\\theta\\) \uc5ec\uae30\uc11c target parameter\ub294 \\(\\theta\\) \uc77c \uac83\uc774\uace0, \\(\\eta\\) \uac00 nuisance parameter\uc774\uae30 \ub54c\ubb38\uc5d0 at each fixed value \\(\\theta\\) \uc5d0\uc11c \\(\\eta\\) \uc758 mle\ub97c \uad6c\ud558\ub294 \uac83\uc774 profile likelihood\ub97c \ucc3e\ub294 \uac83\uc774\ub2e4\u001c \uc774\ub97c \uacc4\uc0b0\ud574\ubcf4\uba74 \\(Let\\ \\frac{\\partial{L(\\theta,\\eta)}}{\\partial\\eta} = {\\theta}x + \\eta(x+y) -mlog(1+e^{\\theta+\\eta}) - nlog(1+e^{\\eta}) = 0\\) \ud5c8\ub098 \uc774 \uc2dd\uc740 MLE\uc5d0 \ub300\ud55c closed form\uc744 \uac16\uc9c0 \uc54a\uc73c\ubbc0\ub85c numerical \ud55c \ubc29\ubc95\uc73c\ub85c profile likelihood\ub97c \uad6c\ud560 \uc218 \uc788\ub2e4. \\(L(\\theta)= max_\\eta{L(\\theta,\\eta)}\\) . \\(\\theta\\) \uc758 MLE\ub294 invariance property\uc5d0 \uc758\ud574 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(\\hat{\\theta} = log\\frac{x/(m-x)}{y/(n-y)}\\) . 3.(a)-(e) set.seed(2020311194) obs <- c(2.08, 2.6, 2.67, 2.7, 2.94, 3.08, 3.71, 4.66, 4.71, 5.2) exp(mean(obs)) ## [1] 31.03141 iter <- 5000 esti <- rep(0,iter) ###Assume parent Normal### for ( i in 1:iter) esti[i] <- exp(mean(rnorm(10,mean(obs),sd(obs)))) print(sd(esti)) ## [1] 11.49627 ###Assume Exponential### for ( i in 1:iter) esti[i] <- exp(mean(rexp(10,1/mean(obs)))) print(sd(esti)) ## [1] 253.4486 ###Approximation method### ((exp(mean(obs)))^2)*(var(obs)/length(obs)) ## [1] 110.7054 ###Bootstrap### for ( i in 1:iter) esti[i] = exp(mean(sample(obs,10,replace=T))) print(sd(esti)) ## [1] 11.3162 ###Jackknife### n = length(obs) esti <- rep(0,n) lxbar <- exp(mean(obs)) for ( i in 1:n) esti[i] <- exp(mean(obs[-i])) sqrt((n-1)*mean((esti-lxbar)^2)) ## [1] 10.28417 3-(f) Bootstrap \ubc29\ubc95\uc774 \ubd84\uc11d\uc790\uc5d0 \ub530\ub77c \ub2e4\ub978 \uac12\ub4e4\uc744 \uac16\uae34 \ud558\uc9c0\ub9cc, sample \uad00\uce21\uc5d0 \uc5b4\ub824\uc6c0\uc744 \ub290\ub07c\ub294 \ub4f1\uc758 \uc81c\uc57d\uc774 \uc788\uc744 \ub54c \uc27d\uac8c \uc751\uc6a9\ud560 \uc218 \uc788\ub294 \ubc29\ubc95\uc774\ubbc0\ub85c, ootstrap\uc758 \ubc29\ubc95\uc744 \ucc44\ud0dd\ud558\ub294 \uac83\uc744 \uc120\ud638\ud55c\ub2e4.","title":"R Notebook"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz3/#intermediate-applied-statistics-quiz-iii","text":"","title":"Intermediate Applied Statistics Quiz III"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz3/#1","text":"","title":"1."},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz3/#2a-write-the-likelihood-functions","text":"X\uc640 Y\uac00 \ub3c5\ub9bd\uc774\ubbc0\ub85c \ub458\uc758 joint likelihood\ub294 \uc544\ub798\uc640 \uac19\ub2e4. \\(X \\sim B(m,p_1),\\ Y \\sim B(n,p_2)\\) \\(L(p_1,p_2)=\\ p_1^x(1-p_1)^{(m-x)}p_2^y(1-p_2)^{(n-y)}\\)","title":"2.(a) Write the likelihood functions"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz3/#2b-rewrite-the-likelihood-function-in-terms-of-theta-logfracp_11-p_1p_21-p_2-and-eta-logfracp_21-p_2","text":"\\(p_1 = \\frac{e^{\\eta}}{1+e^{\\eta}},\\ p_2 = \\frac{e^{\\theta + \\eta}}{1+ e^{\\theta+\\eta}}.\\) \\(L(\\theta, \\eta) = (\\frac{p_1}{1-p_1})^x(1-p_1)^m(\\frac{p_2}{1-p_2})^y(1-p_2)^n\\) \\(= (\\frac{p_1/(1-p_1)}{p_2/(1-p_2)})^x(\\frac{p_2}{1-p_2})^{x+y}(1-p_1)^m(1-p_2)^n\\) \\(= e^{{\\theta}x}e^{\\eta(x+y)}(1+e^{\\theta+\\eta})^{-m}(1-p_2)^n\\)","title":"2.(b) Rewrite the likelihood function in terms of \\(\\theta = log\\frac{p_1(1-p_1)}{p_2(1-p_2)}\\ and\\ \\eta = log\\frac{p_2}{1-p_2}\\)"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz3/#2c-use-the-profile-likelihood-to-get-the-mle-of-theta","text":"\uc5ec\uae30\uc11c target parameter\ub294 \\(\\theta\\) \uc77c \uac83\uc774\uace0, \\(\\eta\\) \uac00 nuisance parameter\uc774\uae30 \ub54c\ubb38\uc5d0 at each fixed value \\(\\theta\\) \uc5d0\uc11c \\(\\eta\\) \uc758 mle\ub97c \uad6c\ud558\ub294 \uac83\uc774 profile likelihood\ub97c \ucc3e\ub294 \uac83\uc774\ub2e4\u001c \uc774\ub97c \uacc4\uc0b0\ud574\ubcf4\uba74 \\(Let\\ \\frac{\\partial{L(\\theta,\\eta)}}{\\partial\\eta} = {\\theta}x + \\eta(x+y) -mlog(1+e^{\\theta+\\eta}) - nlog(1+e^{\\eta}) = 0\\) \ud5c8\ub098 \uc774 \uc2dd\uc740 MLE\uc5d0 \ub300\ud55c closed form\uc744 \uac16\uc9c0 \uc54a\uc73c\ubbc0\ub85c numerical \ud55c \ubc29\ubc95\uc73c\ub85c profile likelihood\ub97c \uad6c\ud560 \uc218 \uc788\ub2e4. \\(L(\\theta)= max_\\eta{L(\\theta,\\eta)}\\) . \\(\\theta\\) \uc758 MLE\ub294 invariance property\uc5d0 \uc758\ud574 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(\\hat{\\theta} = log\\frac{x/(m-x)}{y/(n-y)}\\) .","title":"2.(c) Use the profile likelihood to get the mle of \\(\\theta\\)"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz3/#3a-e","text":"set.seed(2020311194) obs <- c(2.08, 2.6, 2.67, 2.7, 2.94, 3.08, 3.71, 4.66, 4.71, 5.2) exp(mean(obs)) ## [1] 31.03141 iter <- 5000 esti <- rep(0,iter) ###Assume parent Normal### for ( i in 1:iter) esti[i] <- exp(mean(rnorm(10,mean(obs),sd(obs)))) print(sd(esti)) ## [1] 11.49627 ###Assume Exponential### for ( i in 1:iter) esti[i] <- exp(mean(rexp(10,1/mean(obs)))) print(sd(esti)) ## [1] 253.4486 ###Approximation method### ((exp(mean(obs)))^2)*(var(obs)/length(obs)) ## [1] 110.7054 ###Bootstrap### for ( i in 1:iter) esti[i] = exp(mean(sample(obs,10,replace=T))) print(sd(esti)) ## [1] 11.3162 ###Jackknife### n = length(obs) esti <- rep(0,n) lxbar <- exp(mean(obs)) for ( i in 1:n) esti[i] <- exp(mean(obs[-i])) sqrt((n-1)*mean((esti-lxbar)^2)) ## [1] 10.28417","title":"3.(a)-(e)"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz3/#3-f","text":"Bootstrap \ubc29\ubc95\uc774 \ubd84\uc11d\uc790\uc5d0 \ub530\ub77c \ub2e4\ub978 \uac12\ub4e4\uc744 \uac16\uae34 \ud558\uc9c0\ub9cc, sample \uad00\uce21\uc5d0 \uc5b4\ub824\uc6c0\uc744 \ub290\ub07c\ub294 \ub4f1\uc758 \uc81c\uc57d\uc774 \uc788\uc744 \ub54c \uc27d\uac8c \uc751\uc6a9\ud560 \uc218 \uc788\ub294 \ubc29\ubc95\uc774\ubbc0\ub85c, ootstrap\uc758 \ubc29\ubc95\uc744 \ucc44\ud0dd\ud558\ub294 \uac83\uc744 \uc120\ud638\ud55c\ub2e4.","title":"3-(f)"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz4/","text":"Intermediate Applied Statistics Quiz4 1.(a) Traffic Control Measure \uc124\uce58 \uc804 \ud6c4\uc758 \uad00\uce21\ud55c Years\uac00 \ub2e4\ub974\uae30 \ub584\ubb38\uc5d0 \uc774 \uae30\uac04\uc744 \uace0\ub824\ud560 \uc218 \uc5c6\uc73c\ubbc0\ub85c, \\(\\bar{m_{i1}},\\ \\bar{m_{i2}}\\) \uac04\uc758 \ube44\uad50\ub294 \uc801\uc808\ud558\uc9c0 \uc54a\ub2e4. 1.(b) two-way ANOVA \uc5ed\uc2dc Trafiic Control Measure \uc124\uce58 \uc804 \ud6c4\uc758 Years\uac04\uc758 \ube44\uad50\ub97c \uace0\ub824\ud560 \uc218 \uc5c6\uc73c\ubbc0\ub85c, \uc801\uc808\ud558\uc9c0 \uc54a\uc740 \uc811\uadfc \ubc29\ubc95\uc774\ub2e4. 1.(c) Poisson Regression model\uc744 \uc801\uc6a9\ud55c\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc740 \uc2dd\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4. \\(log{\\lambda_{ij}}=\\lambda_0+l_i+\\tau_j\\) \\(L(\\lambda_0,l,\\tau;y_{ij})= \\prod\\frac{e^{-\\lambda_{ij}(\\lambda_{ij})^{y_{ij}}}}{y_{ij}!}\\) \\(logL= -\\sum{e^{(\\lambda_0+l_i+\\tau_j)}}+\\sum{y_{ij}(\\lambda_0+l_i+\\tau_j)}-\\sum log(y_{ij}!)\\) data <- matrix(0,16,3) data[,1] <- c(0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1) data[,2] <- c(1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8) data[,3] <- c(13,6,30,30,10,15,7,13,0,2,4,0,0,6,1,2) data <- data.frame(data) names(data) <- c(\"Traffic_Measure\",\"Locations\",\"Accidents\") data ## Traffic_Measure Locations Accidents ## 1 0 1 13 ## 2 0 2 6 ## 3 0 3 30 ## 4 0 4 30 ## 5 0 5 10 ## 6 0 6 15 ## 7 0 7 7 ## 8 0 8 13 ## 9 1 1 0 ## 10 1 2 2 ## 11 1 3 4 ## 12 1 4 0 ## 13 1 5 0 ## 14 1 6 6 ## 15 1 7 1 ## 16 1 8 2 fit <- glm(Accidents~Traffic_Measure+Locations,family=poisson(link='log'),data=data) summary(fit) ## ## Call: ## glm(formula = Accidents ~ Traffic_Measure + Locations, family = poisson(link = \"log\"), ## data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.0196 -1.9267 -0.4212 0.4995 3.2014 ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.87723 0.18421 15.619 < 2e-16 *** ## Traffic_Measure -2.11223 0.27336 -7.727 1.1e-14 *** ## Locations -0.03086 0.03708 -0.832 0.405 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 154.691 on 15 degrees of freedom ## Residual deviance: 56.414 on 13 degrees of freedom ## AIC: 112.06 ## ## Number of Fisher Scoring iterations: 5 exp(-2.112) ## [1] 0.1209957 parameter\ub97c \ucd94\uc815\ud558\uba74 \uc704\uc640 \uac19\uc740 \uc2dd\uc744 \uc5bb\uc744 \uc218 \uc788\uc73c\uba70, \uacc4\uc218\uc758 \ud574\uc11d\uc740 Traffic control measure \uc124\uce58 \ud6c4\uc758 accident rate\uac00 0.12 \uac10\uc18c\ud568\uc744 \uc758\ubbf8\ud55c\ub2e4.","title":"R Notebook"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz4/#intermediate-applied-statistics-quiz4","text":"","title":"Intermediate Applied Statistics Quiz4"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz4/#1a","text":"Traffic Control Measure \uc124\uce58 \uc804 \ud6c4\uc758 \uad00\uce21\ud55c Years\uac00 \ub2e4\ub974\uae30 \ub584\ubb38\uc5d0 \uc774 \uae30\uac04\uc744 \uace0\ub824\ud560 \uc218 \uc5c6\uc73c\ubbc0\ub85c, \\(\\bar{m_{i1}},\\ \\bar{m_{i2}}\\) \uac04\uc758 \ube44\uad50\ub294 \uc801\uc808\ud558\uc9c0 \uc54a\ub2e4.","title":"1.(a)"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz4/#1b","text":"two-way ANOVA \uc5ed\uc2dc Trafiic Control Measure \uc124\uce58 \uc804 \ud6c4\uc758 Years\uac04\uc758 \ube44\uad50\ub97c \uace0\ub824\ud560 \uc218 \uc5c6\uc73c\ubbc0\ub85c, \uc801\uc808\ud558\uc9c0 \uc54a\uc740 \uc811\uadfc \ubc29\ubc95\uc774\ub2e4.","title":"1.(b)"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz4/#1c","text":"Poisson Regression model\uc744 \uc801\uc6a9\ud55c\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc740 \uc2dd\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4. \\(log{\\lambda_{ij}}=\\lambda_0+l_i+\\tau_j\\) \\(L(\\lambda_0,l,\\tau;y_{ij})= \\prod\\frac{e^{-\\lambda_{ij}(\\lambda_{ij})^{y_{ij}}}}{y_{ij}!}\\) \\(logL= -\\sum{e^{(\\lambda_0+l_i+\\tau_j)}}+\\sum{y_{ij}(\\lambda_0+l_i+\\tau_j)}-\\sum log(y_{ij}!)\\) data <- matrix(0,16,3) data[,1] <- c(0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1) data[,2] <- c(1,2,3,4,5,6,7,8,1,2,3,4,5,6,7,8) data[,3] <- c(13,6,30,30,10,15,7,13,0,2,4,0,0,6,1,2) data <- data.frame(data) names(data) <- c(\"Traffic_Measure\",\"Locations\",\"Accidents\") data ## Traffic_Measure Locations Accidents ## 1 0 1 13 ## 2 0 2 6 ## 3 0 3 30 ## 4 0 4 30 ## 5 0 5 10 ## 6 0 6 15 ## 7 0 7 7 ## 8 0 8 13 ## 9 1 1 0 ## 10 1 2 2 ## 11 1 3 4 ## 12 1 4 0 ## 13 1 5 0 ## 14 1 6 6 ## 15 1 7 1 ## 16 1 8 2 fit <- glm(Accidents~Traffic_Measure+Locations,family=poisson(link='log'),data=data) summary(fit) ## ## Call: ## glm(formula = Accidents ~ Traffic_Measure + Locations, family = poisson(link = \"log\"), ## data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.0196 -1.9267 -0.4212 0.4995 3.2014 ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 2.87723 0.18421 15.619 < 2e-16 *** ## Traffic_Measure -2.11223 0.27336 -7.727 1.1e-14 *** ## Locations -0.03086 0.03708 -0.832 0.405 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 154.691 on 15 degrees of freedom ## Residual deviance: 56.414 on 13 degrees of freedom ## AIC: 112.06 ## ## Number of Fisher Scoring iterations: 5 exp(-2.112) ## [1] 0.1209957 parameter\ub97c \ucd94\uc815\ud558\uba74 \uc704\uc640 \uac19\uc740 \uc2dd\uc744 \uc5bb\uc744 \uc218 \uc788\uc73c\uba70, \uacc4\uc218\uc758 \ud574\uc11d\uc740 Traffic control measure \uc124\uce58 \ud6c4\uc758 accident rate\uac00 0.12 \uac10\uc18c\ud568\uc744 \uc758\ubbf8\ud55c\ub2e4.","title":"1.(c)"},{"location":"07%20Intermediate%20Applied%20Statistics/Quiz5/","text":"1. \\(p_0=0.5,\\ \\mu_1 = 10,\\ \\sigma_1=4,\\ \\mu_2=50,\\ \\sigma_2=10\\) \\(p=\\pi_1=0.333,\\ \\mu_1=9.965,\\ \\sigma_1=2.826,\\ \\mu_2 = 50.994\\, \\sigma_2=6.612\\) \\(\\sum p_i =1,\\ \\sum x_ip_i = \\theta\\)","title":"R Notebook"},{"location":"10%20Bayesian%20Statistics/HW1/","text":"Bayesian Statistics HW1 1.1 (a) For \\(\\sigma=2\\) , write the formula for the marginal probability density for \\(Y\\) and sketch it. \\[f(y)= \\sum_\\theta{f(y,\\theta)}=\\sum_\\theta{P(\\theta)f(y|\\theta)}\\] \\[=P(\\theta=1)f(y|\\theta=1)+P(\\theta=2)f(y|\\theta=2)\\] \\[0.5N(Y|1,2^2)+0.5N(Y|2,2^2)\\] y <- seq(-20,20,0.01) py <- 0.5*dnorm(y,1,2)+0.5*dnorm(y,2,2) plot(y,py,type='l',main='p(y)',xlab='y',ylab='p(y)') (b) What is \\(P(\\theta=1|y=1)\\) again supposing \\(\\sigma=2\\) ? By bayes theorem, $ \\(P(\\theta=1|y=1)=\\frac{P(\\theta=1,y=1)}{f(y=1)}\\) $ $ \\(=\\frac{P(\\theta=1)f(y=1|\\theta=1)}{\\sum_\\theta{P(\\theta)f(y=1|\\theta)}}\\) $ $ \\(=\\frac{0.5N(1|1,2^2)}{0.5N(1|1,2^2)+0.5N(1|2,2^2)}\\) $ $ \\(=\\frac{1}{1+exp(-1/8)}\\) $ (c) Describe how the posterior density of \\(\\theta\\) changes in shape as \\(\\sigma\\) as increased and as it is decreased. If as \\(\\sigma \\rightarrow \\infty\\) , \\[P(\\theta=1|y) = \\frac{P(\\theta=1)f(y|1,\\sigma^2)}{P(\\theta=1)f(y|1,\\sigma^2)+P(\\theta=2)f(y|2,\\sigma^2)}$$ $$=\\frac{P(\\theta=1)exp(-\\frac{(y-1)^2}{2\\sigma^2})}{P(\\theta=1)exp(-\\frac{(y-1)^2}{2\\sigma^2})+P(\\theta=2)exp(-\\frac{(y-2)^2}{2\\sigma^2})}\\] \\[\\rightarrow P(\\theta=1)\\] As \\(\\sigma \\rightarrow \\infty\\) , \\[P(\\theta=1|y)=\\frac{exp(-\\frac{(y-1)^2}{2\\sigma^2})}{exp(-\\frac{(y-1)^2}{2\\sigma^2})+exp(-\\frac{(y-2)^2}{2\\sigma^2})}$$ $$= \\frac{1}{1+exp(-\\frac{(2y-3)}{2\\sigma^2})}\\] \\[ 0, \\ y>3/2\\] $ \\(1,\\ y< 3/2\\) $ posterior density \\(P(\\theta|y)\\) goes to prior \\(P(\\theta)\\) if \\(\\sigma\\) goes to infinity, and if \\(\\sigma\\) goes to zero, posterior density \\(P(\\theta|y)\\) will have high variability which depends on data. 1.2 Show that (1.8) and (1.9) hold if u is a vector Suppose \\(\\mathbf{U} = (U_1,..,U_n)^T \\in \\mathbf{R^n}\\) is n-dimensional random vector. Then \\(E(\\mathbf{U})= (E(U_1),..,E(U_n))^T=(E(E(U_1|V)),..,E(E(U_n|V)))^T\\) . Then (1.8) still holds for vector \\(\\mathbf{U}\\) 's components. (1.9) also holds for diagonal elements \\(V(U_i)=E(V(U_i|V))+V(E(U_i|V))\\) and also for off diagonal elements. \\[E(Cov(U_i,U_j|V))+Cov(E(U_i|V),E(U_j|V))\\] $ \\(=E(E(U_i,U_j|V)-E(U_i|V)(E(U_j|V)) + E(E(U_i|V)E(U_j|V))-E(E(U_i|V)E(U_j|V))\\) $ $ \\(=E(U_i,U_j)-E(E(U_i|V)E(U_j|V))+E(E(U_i|V)E(U_j|V))-E(E(U_i|V)E(U_j|V))\\) $ $ \\(=E(U_i,U_j)-E(U_i)E(U_j)=Cov(U_i,U_j)\\) $. 1.3 \\[P(child\\ is\\ Xx| child\\ has\\ brown\\ eyes\\ \\&\\ parents\\ have\\ brown\\ eyes )\\] \\[=\\frac{0\\cdot(1-p)^4+\\frac{1}{2}\\cdot4p(1-p)^3+\\frac{1}{2}\\cdot4p^2(1-p)^2} {1\\cdot(1-p)^4+1\\cdot4p(1-p)^3+\\frac{3}{4}4p^2(1-p)^2}\\] \\[=\\frac{2p(1-p)+2p^2}{(1-p)^2+4p(1-p)+3p^2} = \\frac{2p}{1+2p}\\] \\[P(Judy\\ is\\ Xx|n\\ children\\ all\\ have\\ brown\\ eyes\\ \\&\\ all\\ previous\\ information)$$ $$= \\frac{\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n}{\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n}+\\frac{1}{1+2p}\\cdot1\\] \\[P(Judy's\\ child\\ is\\ Xx| all\\ the\\ given\\ information)$$ $$=\\frac{\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n}{\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n+\\frac{1}{1+2p}}(\\frac{2}{3})+\\frac{\\frac{1}{1+2p}}{\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n+\\frac{1}{1+2p}}(\\frac{1}{2})\\] \\[P(Grandchild\\ is\\ xx| all\\ the\\ given\\ information)\\] \\[=\\frac{\\frac{2}{3}\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n+\\frac{1}{2}\\frac{1}{1+2p}} {\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n+\\frac{1}{1+2p}}(\\frac{1}{4}2p(1-p)+\\frac{1}{2}p^2)\\] \\[=\\frac{\\frac{2}{3}\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n+\\frac{1}{2}\\frac{1}{1+2p}} {\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n+\\frac{1}{1+2p}}(\\frac{1}{2}p)\\] 1.4 (a). \\[P(favorite\\ wins|point\\ spreads=8)=\\frac{8}{12}\\] \\[P(favorite\\ wins\\ by\\ at\\ least\\ 8| point\\ spread=8)=\\frac{5}{12}\\] $ \\(P(favorite\\ wins\\ by\\ at\\ least\\ 8|point\\ spread=8\\ and\\ favorite\\ wins)=\\frac{5}{8}\\) $ (b). Let d= (outcome-point spread). Then \\(d \\approx N(\\bar{d}=-1.25,s^2=(10.1^2))\\) \\[P(favorite\\ wins|point\\ spreads=8)=P(d\\geq0)=0.45\\] \\[P(favorite\\ wins\\ by\\ at\\ least\\ 8| point\\ spread=8)=P(d\\geq8)=0.18\\] \\[P(favorite\\ wins\\ by\\ at\\ least\\ 8|point\\ spread=8\\ and\\ favorite\\ wins)=\\frac{0.18}{0.45}=0.4\\] 1.5 (a). Our goal is guessing probability \\(P(election\\ is\\ tied|given\\ information)\\) . Assume that in two party, each candidate receives between 30% - 70% of the vote that follows Uniform distribution ( \\(\\frac{y}{n} \\sim Unif(0.3,0.7)\\) ). Then we can express $ \\(P(election\\ is\\ tied|n)= P(\\frac{y}{n}=0.5) = 1/0.4n,\\ n\\ is\\ even,\\ o.w\\ zero\\) $ n is the total number of votes and y be the number received by candidate. We can write \\(P(election\\ is\\ tied,n=even)= P(election\\ is\\ tied|n=even)P(n=even)= \\frac{1}{0.8n}\\) National election has 435 individual districts, then the probability of at least one of them being tied is, \\[P(at\\ least\\ one\\ election\\ is\\ tied)=1-(1-\\frac{1}{0.8n})^{435} \\approx \\frac{435}{0.8n}\\] as n goes to infinity. (b). Given information implies \\(\\hat{P}(election\\ decided\\ within\\ 100\\ votes)=\\frac{49}{20597}\\) and vote gap written as \\(|2y-n| \\leq 100\\) . Let \\(d=\\ 2y-n,\\) \\[P(election\\ is\\ tied\\ within\\ 100\\ votes)=P(d=0|d \\leq 100)P(d \\leq 100)\\] $ \\(=\\frac{1}{100-(-100)+1}\\frac{49}{20597} = \\frac{1}{201}\\frac{49}{20597}\\) $. By using (a), \\[P(at\\ least\\ one\\ election\\ is\\ tied)$$ $$= 1- (1-P(election\\ is\\ tied\\ within\\ 100\\ votes))^{435} \\approx 435 \\times\\frac{49}{201\\times20597}= 0.00514\\] 1.6 \\[P(identical\\ twins| twin\\ brother)$$ $$= \\frac{P(identical\\ twins)P(twin\\ brother|identical\\ twins)}{\\sum_{twins}P(twins)P(twin\\ brother|twins)} = \\frac{\\frac{1}{300}\\frac{1}{2}}{\\frac{1}{300}\\frac{1}{2}+\\frac{1}{125}\\frac{1}{4}}=\\frac{5}{11}\\] 1.8 (a). Person A already saw rolled die, then his belief about probability about issue is updated and will be biased. This situation seens to have subjectivity, but i think, if two people A and B's are both rational persons, A & B build same probablity about 6 appears as 1/6. (b). A and B have different belief about soccer. B's knowledge about soccer affect hisb belief about probability, then B allocate probablity about Brazil's win more higher than A, who will allocate probability about all countries equally. 1.9 (a). simulation = function(seed=NULL){ if(!is.null(seed)){ set.seed(seed) } time <- cumsum(rexp(50,1/10)) time <- time[time <= 420] patients <- length(time) wait_pat <- 0 wait_time <- 0 doc <- c(0,0,0) for (i in (1:length(time))){ wait <- max(min(doc)-time[i],0) wait_time <- wait_time + wait wait_pat <- wait_pat + (wait > 0) doc_start <- max(c(min(doc),time[i])) doc_end <- doc_start + runif(1,5,20) doc[which.min(doc)] = doc_end } result <- c('patients'=patients,'wait_pat'=wait_pat, 'avg_wait_time'=avg_wait_time <- ifelse(wait_pat==0,0,wait_time/wait_pat) ,'hospital_closed'=hospital_closed <- max(max(doc,420)) ) return(result) } simulation(2020311194) ## patients wait_pat avg_wait_time hospital_closed ## 47.000000 4.000000 3.958816 428.354702 (b). iteration <- replicate(100,simulation()) q25 <- apply(iteration,1,quantile,0.25) q75 <- apply(iteration,1,quantile,0.75) q25; q75 ## patients wait_pat avg_wait_time hospital_closed ## 39.000000 3.000000 2.885335 420.000000 ## patients wait_pat avg_wait_time hospital_closed ## 46.000000 8.000000 5.363889 430.994808","title":"R Notebook"},{"location":"10%20Bayesian%20Statistics/HW1/#bayesian-statistics-hw1","text":"","title":"Bayesian Statistics HW1"},{"location":"10%20Bayesian%20Statistics/HW1/#11","text":"(a) For \\(\\sigma=2\\) , write the formula for the marginal probability density for \\(Y\\) and sketch it. \\[f(y)= \\sum_\\theta{f(y,\\theta)}=\\sum_\\theta{P(\\theta)f(y|\\theta)}\\] \\[=P(\\theta=1)f(y|\\theta=1)+P(\\theta=2)f(y|\\theta=2)\\] \\[0.5N(Y|1,2^2)+0.5N(Y|2,2^2)\\] y <- seq(-20,20,0.01) py <- 0.5*dnorm(y,1,2)+0.5*dnorm(y,2,2) plot(y,py,type='l',main='p(y)',xlab='y',ylab='p(y)') (b) What is \\(P(\\theta=1|y=1)\\) again supposing \\(\\sigma=2\\) ? By bayes theorem, $ \\(P(\\theta=1|y=1)=\\frac{P(\\theta=1,y=1)}{f(y=1)}\\) $ $ \\(=\\frac{P(\\theta=1)f(y=1|\\theta=1)}{\\sum_\\theta{P(\\theta)f(y=1|\\theta)}}\\) $ $ \\(=\\frac{0.5N(1|1,2^2)}{0.5N(1|1,2^2)+0.5N(1|2,2^2)}\\) $ $ \\(=\\frac{1}{1+exp(-1/8)}\\) $ (c) Describe how the posterior density of \\(\\theta\\) changes in shape as \\(\\sigma\\) as increased and as it is decreased. If as \\(\\sigma \\rightarrow \\infty\\) , \\[P(\\theta=1|y) = \\frac{P(\\theta=1)f(y|1,\\sigma^2)}{P(\\theta=1)f(y|1,\\sigma^2)+P(\\theta=2)f(y|2,\\sigma^2)}$$ $$=\\frac{P(\\theta=1)exp(-\\frac{(y-1)^2}{2\\sigma^2})}{P(\\theta=1)exp(-\\frac{(y-1)^2}{2\\sigma^2})+P(\\theta=2)exp(-\\frac{(y-2)^2}{2\\sigma^2})}\\] \\[\\rightarrow P(\\theta=1)\\] As \\(\\sigma \\rightarrow \\infty\\) , \\[P(\\theta=1|y)=\\frac{exp(-\\frac{(y-1)^2}{2\\sigma^2})}{exp(-\\frac{(y-1)^2}{2\\sigma^2})+exp(-\\frac{(y-2)^2}{2\\sigma^2})}$$ $$= \\frac{1}{1+exp(-\\frac{(2y-3)}{2\\sigma^2})}\\] \\[ 0, \\ y>3/2\\] $ \\(1,\\ y< 3/2\\) $ posterior density \\(P(\\theta|y)\\) goes to prior \\(P(\\theta)\\) if \\(\\sigma\\) goes to infinity, and if \\(\\sigma\\) goes to zero, posterior density \\(P(\\theta|y)\\) will have high variability which depends on data.","title":"1.1"},{"location":"10%20Bayesian%20Statistics/HW1/#12-show-that-18-and-19-hold-if-u-is-a-vector","text":"Suppose \\(\\mathbf{U} = (U_1,..,U_n)^T \\in \\mathbf{R^n}\\) is n-dimensional random vector. Then \\(E(\\mathbf{U})= (E(U_1),..,E(U_n))^T=(E(E(U_1|V)),..,E(E(U_n|V)))^T\\) . Then (1.8) still holds for vector \\(\\mathbf{U}\\) 's components. (1.9) also holds for diagonal elements \\(V(U_i)=E(V(U_i|V))+V(E(U_i|V))\\) and also for off diagonal elements. \\[E(Cov(U_i,U_j|V))+Cov(E(U_i|V),E(U_j|V))\\] $ \\(=E(E(U_i,U_j|V)-E(U_i|V)(E(U_j|V)) + E(E(U_i|V)E(U_j|V))-E(E(U_i|V)E(U_j|V))\\) $ $ \\(=E(U_i,U_j)-E(E(U_i|V)E(U_j|V))+E(E(U_i|V)E(U_j|V))-E(E(U_i|V)E(U_j|V))\\) $ $ \\(=E(U_i,U_j)-E(U_i)E(U_j)=Cov(U_i,U_j)\\) $.","title":"1.2 Show that (1.8) and (1.9) hold if u is a vector"},{"location":"10%20Bayesian%20Statistics/HW1/#13","text":"\\[P(child\\ is\\ Xx| child\\ has\\ brown\\ eyes\\ \\&\\ parents\\ have\\ brown\\ eyes )\\] \\[=\\frac{0\\cdot(1-p)^4+\\frac{1}{2}\\cdot4p(1-p)^3+\\frac{1}{2}\\cdot4p^2(1-p)^2} {1\\cdot(1-p)^4+1\\cdot4p(1-p)^3+\\frac{3}{4}4p^2(1-p)^2}\\] \\[=\\frac{2p(1-p)+2p^2}{(1-p)^2+4p(1-p)+3p^2} = \\frac{2p}{1+2p}\\] \\[P(Judy\\ is\\ Xx|n\\ children\\ all\\ have\\ brown\\ eyes\\ \\&\\ all\\ previous\\ information)$$ $$= \\frac{\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n}{\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n}+\\frac{1}{1+2p}\\cdot1\\] \\[P(Judy's\\ child\\ is\\ Xx| all\\ the\\ given\\ information)$$ $$=\\frac{\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n}{\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n+\\frac{1}{1+2p}}(\\frac{2}{3})+\\frac{\\frac{1}{1+2p}}{\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n+\\frac{1}{1+2p}}(\\frac{1}{2})\\] \\[P(Grandchild\\ is\\ xx| all\\ the\\ given\\ information)\\] \\[=\\frac{\\frac{2}{3}\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n+\\frac{1}{2}\\frac{1}{1+2p}} {\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n+\\frac{1}{1+2p}}(\\frac{1}{4}2p(1-p)+\\frac{1}{2}p^2)\\] \\[=\\frac{\\frac{2}{3}\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n+\\frac{1}{2}\\frac{1}{1+2p}} {\\frac{2p}{1+2p}\\cdot(\\frac{3}{4})^n+\\frac{1}{1+2p}}(\\frac{1}{2}p)\\]","title":"1.3"},{"location":"10%20Bayesian%20Statistics/HW1/#14","text":"(a). \\[P(favorite\\ wins|point\\ spreads=8)=\\frac{8}{12}\\] \\[P(favorite\\ wins\\ by\\ at\\ least\\ 8| point\\ spread=8)=\\frac{5}{12}\\] $ \\(P(favorite\\ wins\\ by\\ at\\ least\\ 8|point\\ spread=8\\ and\\ favorite\\ wins)=\\frac{5}{8}\\) $ (b). Let d= (outcome-point spread). Then \\(d \\approx N(\\bar{d}=-1.25,s^2=(10.1^2))\\) \\[P(favorite\\ wins|point\\ spreads=8)=P(d\\geq0)=0.45\\] \\[P(favorite\\ wins\\ by\\ at\\ least\\ 8| point\\ spread=8)=P(d\\geq8)=0.18\\] \\[P(favorite\\ wins\\ by\\ at\\ least\\ 8|point\\ spread=8\\ and\\ favorite\\ wins)=\\frac{0.18}{0.45}=0.4\\]","title":"1.4"},{"location":"10%20Bayesian%20Statistics/HW1/#15","text":"(a). Our goal is guessing probability \\(P(election\\ is\\ tied|given\\ information)\\) . Assume that in two party, each candidate receives between 30% - 70% of the vote that follows Uniform distribution ( \\(\\frac{y}{n} \\sim Unif(0.3,0.7)\\) ). Then we can express $ \\(P(election\\ is\\ tied|n)= P(\\frac{y}{n}=0.5) = 1/0.4n,\\ n\\ is\\ even,\\ o.w\\ zero\\) $ n is the total number of votes and y be the number received by candidate. We can write \\(P(election\\ is\\ tied,n=even)= P(election\\ is\\ tied|n=even)P(n=even)= \\frac{1}{0.8n}\\) National election has 435 individual districts, then the probability of at least one of them being tied is, \\[P(at\\ least\\ one\\ election\\ is\\ tied)=1-(1-\\frac{1}{0.8n})^{435} \\approx \\frac{435}{0.8n}\\] as n goes to infinity. (b). Given information implies \\(\\hat{P}(election\\ decided\\ within\\ 100\\ votes)=\\frac{49}{20597}\\) and vote gap written as \\(|2y-n| \\leq 100\\) . Let \\(d=\\ 2y-n,\\) \\[P(election\\ is\\ tied\\ within\\ 100\\ votes)=P(d=0|d \\leq 100)P(d \\leq 100)\\] $ \\(=\\frac{1}{100-(-100)+1}\\frac{49}{20597} = \\frac{1}{201}\\frac{49}{20597}\\) $. By using (a), \\[P(at\\ least\\ one\\ election\\ is\\ tied)$$ $$= 1- (1-P(election\\ is\\ tied\\ within\\ 100\\ votes))^{435} \\approx 435 \\times\\frac{49}{201\\times20597}= 0.00514\\]","title":"1.5"},{"location":"10%20Bayesian%20Statistics/HW1/#16","text":"\\[P(identical\\ twins| twin\\ brother)$$ $$= \\frac{P(identical\\ twins)P(twin\\ brother|identical\\ twins)}{\\sum_{twins}P(twins)P(twin\\ brother|twins)} = \\frac{\\frac{1}{300}\\frac{1}{2}}{\\frac{1}{300}\\frac{1}{2}+\\frac{1}{125}\\frac{1}{4}}=\\frac{5}{11}\\]","title":"1.6"},{"location":"10%20Bayesian%20Statistics/HW1/#18","text":"(a). Person A already saw rolled die, then his belief about probability about issue is updated and will be biased. This situation seens to have subjectivity, but i think, if two people A and B's are both rational persons, A & B build same probablity about 6 appears as 1/6. (b). A and B have different belief about soccer. B's knowledge about soccer affect hisb belief about probability, then B allocate probablity about Brazil's win more higher than A, who will allocate probability about all countries equally.","title":"1.8"},{"location":"10%20Bayesian%20Statistics/HW1/#19","text":"(a). simulation = function(seed=NULL){ if(!is.null(seed)){ set.seed(seed) } time <- cumsum(rexp(50,1/10)) time <- time[time <= 420] patients <- length(time) wait_pat <- 0 wait_time <- 0 doc <- c(0,0,0) for (i in (1:length(time))){ wait <- max(min(doc)-time[i],0) wait_time <- wait_time + wait wait_pat <- wait_pat + (wait > 0) doc_start <- max(c(min(doc),time[i])) doc_end <- doc_start + runif(1,5,20) doc[which.min(doc)] = doc_end } result <- c('patients'=patients,'wait_pat'=wait_pat, 'avg_wait_time'=avg_wait_time <- ifelse(wait_pat==0,0,wait_time/wait_pat) ,'hospital_closed'=hospital_closed <- max(max(doc,420)) ) return(result) } simulation(2020311194) ## patients wait_pat avg_wait_time hospital_closed ## 47.000000 4.000000 3.958816 428.354702 (b). iteration <- replicate(100,simulation()) q25 <- apply(iteration,1,quantile,0.25) q75 <- apply(iteration,1,quantile,0.75) q25; q75 ## patients wait_pat avg_wait_time hospital_closed ## 39.000000 3.000000 2.885335 420.000000 ## patients wait_pat avg_wait_time hospital_closed ## 46.000000 8.000000 5.363889 430.994808","title":"1.9"},{"location":"10%20Bayesian%20Statistics/HW2/","text":"Bayesian Statistics HW2 2-1. \ub3d9\uc804\uc758 \uc55e\uba74\uc774 \ub098\uc62c \ud655\ub960 \\(\\theta\\) \uc5d0 \ub300\ud55c Prior distribution\uc774 \\(Beta(4,4)\\) \ub97c \ub530\ub974\uba70, \ub3d9\uc804\uc744 10\ubc88 \ub358\uc84c\uc744 \ub54c \ub3d9\uc804\uc758 \uc55e\uba74\uc774 3\ubc88 \ubbf8\ub9cc\uc73c\ub85c \ub098\uc624\ub294 \uc0ac\uac74\uc5d0 \ub300\ud558\uc5ec prior\uc640 likelihood\ub97c \uc815\ub9ac\ud558\uba74 \ub2e4\uc74c\uacfc \uac19\ub2e4. $p(\\theta) \\sim \\theta^3(1-\\theta)^3,\\ Beta(\\alpha, \\beta) \\propto \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} $ \\(f(y|\\theta) = \\dbinom{10}{0}(1-\\theta)^{10}+\\dbinom{10}{1}\\theta(1-\\theta)^{9}+\\dbinom{10}{2}\\theta^2(1-\\theta)^8 = (1-\\theta)^{10}+10\\theta(1-\\theta)^{9}+45\\theta^2(1-\\theta)^{8}\\) . Posterior Density\ub294 prior\uc640 likelihood\uc758 \uacf1\uc5d0 \ube44\ub840\ud558\ubbc0\ub85c, \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\(p(\\theta|y) \\propto \\theta^3(1-\\theta)^{13}+10\\theta^4(1-\\theta)^{12}+45\\theta^5(1-\\theta)^{11}\\) Posterior density\uc758 sketch\ub294 \uc544\ub798\uc640 \uac19\ub2e4. theta <- seq(0,1,.01) dens <- theta^3*(1-theta)^13 + 10*theta^4*(1-theta)^12 + 45*theta^5*(1-theta)^11 plot (dens~theta, main='Posterior', type=\"l\", xlab=\"theta\", yaxt=\"n\", cex=2) 2-2. \ub3d9\uc804\uc758 \uc55e\uba74\uc774 \ub098\uc62c \ud655\ub960\uc744 \\(\\pi\\) \ub77c \ud558\uace0, \ub3d9\uc804\uc774 \uc55e\uba74\uc774 \ub098\uc62c \ub54c \uae4c\uc9c0 \ucd94\uac00\ub85c \ub358\uc9c4 \ud69f\uc218\ub97c N\uc774\ub77c\uace0 \ud560 \uc2dc , \uc774\uc5d0 \ub300\ud55c \uae30\ub313\uac12\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(E[N|\\pi] = 1\\cdot\\pi+2\\cdot(1-\\pi)\\pi+3\\cdot(1-\\pi)^2\\pi+ \\cdots = 1/\\pi\\) \uccab \ub450\ubc88 \uc9f8 toss\uac00 tail\uc774 \ub098\uc628 \ud6c4 \ub3d9\uc804 \\(C_1\\) \uc640 \\(C_2\\) \uc911 \ub79c\ub364\ud558\uac8c \ub3d9\uc804\uc744 \uc120\ud0dd\ud558\ub294 \ud655\ub960\uc740 \ubca0\uc774\uc988\uc815\ub9ac\uc5d0 \ub530\ub77c \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604 \ud560 \uc218 \uc788\ub2e4. \\(P(C=C_1|TT) = \\frac{P(C=C_1)P(TT|C=C_1)}{P(C=C_1)P(TT|C=C_1)+P(C=C_2)P(TT|C=C_2)}= \\frac{0.5(0.4^2)}{0.5(0.4^2)+0.5(0.6^2)}=\\frac{16}{52}\\) . \uc774\ud6c4 \ub3d9\uc804\uc774 \uc55e\uba74\uc774 \ub098\uc62c \ub54c \uae4c\uc9c0\uc758 \uc2dc\ud589\ud69f\uc218 N\uc5d0 \ub300\ud55c posterior expectation\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(E(N|TT) = E[E(N|TT,C)|TT] = P(C=C_1|TT)E(N|C=C_1,TT)+P(C=C_2|TT)E(N|C=C_2,TT)\\) \\(=\\frac{16}{52}\\frac{1}{0.6}+\\frac{36}{52}\\frac{1}{0.4}=2.24\\) . 2-3. \\(E[y] = 1000\\cdot\\frac{1}{6} = 166.7,\\ sd[y] = \\sqrt{1000\\cdot\\frac{1}{6}\\frac{5}{6}}=11.8.\\) y <- seq(100,240,1) dens <- dnorm(y,1000*(1/6),sqrt(1000*(1/6)*(5/6))) plot(dens~y, main=\"Approx Dist\", type='l',xlab='y',yaxt='n',cex=2) 5%,25%,50%,75%,95% points\ub294 \ucc28\ub840\ub300\ub85c \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(5\\%\\ point = 166.7 - 1.65(11.8) = 147.2\\) \\(25\\%\\ point = 166.7 - 0.67(11.8) = 158.8\\) \\(50\\%\\ point = 166.7\\) \\(75\\%\\ point = 166.7 + 0.67(11.8) = 174.6\\) \\(95\\%\\ point = 166.7 + 1.65(11.8) = 186.1\\) rounding\ud558\uba74 \ucc28\ub840\ub300\ub85c \uac01 point\ub4e4\uc740 147, 150, 167, 175, 186\uc774\ub2e4. 2-4. 2-4a. \\(E(y|\\theta=\\frac{1}{12}) = 83.3, sd(y|\\theta=\\frac{1}{12}) = 8.7\\) , \\(E(y|\\theta=\\frac{1}{6}) = 166.7, sd(y|\\theta=\\frac{1}{6}) = 11.8\\) , \\(E(y|\\theta=\\frac{1}{4}) = 250, sd(y|\\theta=\\frac{1}{4}) = 13.7\\) . y <- seq(50,300,1) dens <- function(x,theta){ dnorm(x,1000*theta, sqrt(1000*theta*(1-theta))) } mixture_dens <- 0.25*dens(y,1/12)+0.5*dens(y,1/6)+0.25*dens(y,1/4) plot(mixture_dens~y,main=\"prior predictive aprrox dist\", yaxt='n',type='l',cex=2) 2-4b. \uc704\uc5d0\uc11c \uadf8\ub824\uc9c4 data y\uc758 \ubd84\ud3ec\ub294 \uc815\uaddc\ubd84\ud3ec\ub97c \ub530\ub974\uc9c0 \uc54a\uc9c0\ub9cc, \uadfc\uc0ac\uc801\uc73c\ub85c \uc815\uaddc\ubd84\ud3ec 3\uac1c\uac00 \uc11c\ub85c overlap \ub418\uc5b4\uc788\ub294 \ubaa8\uc2b5\uc73c\ub85c \ubcfc \uc218 \uc788\ub2e4. \uace0\ub85c \uc804\uccb4 \ub370\uc774\ud130\uc758 5%\uc5d0 \ud574\ub2f9\ud558\ub294 point\ub294 y\uc804\uccb4 \ub370\uc774\ud130\uc758 25%\ub97c \ucc28\uc9c0\ud558\uace0 \uc788\ub294 \uccab\ubc88\uc9f8 \uc815\uaddc\ubd84\ud3ec\uc5d0\uc11c 20%\uc5d0 \ud574\ub2f9\ud558\ub294 \ubd80\ubd84\uc73c\ub85c \uc0dd\uac01\ud560 \uc218 \uc788\ub2e4. \uc774\ub7ec\ud55c \ubc29\ubc95\uc73c\ub85c \ub370\uc774\ud130\uc758 25%\uc5d0 \ud574\ub2f9\ud558\ub294 \ud3ec\uc778\ud2b8\ub294 \uccab\ubc88 \uc9f8 \ubd09\uc6b0\ub9ac\uc640 \ub450\ubc88\uc9f8 \ubd09\uc6b0\ub9ac\uc758 \uc0ac\uc774\ub85c \ubcfc \uc218 \uc788\uc73c\uba70, 50% \ud3ec\uc778\ud2b8\ub294 \ub370\uc774\ud130\uc758 \uc815\uc911\uc559, 75% \ud3ec\uc778\ud2b8\ub294 \ub450\ubc88 \uc9f8 \ubd09\uc6b0\ub9ac\uc640 \uc138\ubc88 \uc9f8 \ubd09\uc6b0\ub9ac\uc758 \uc0ac\uc774, 95% \ud3ec\uc778\ud2b8\ub294 \uc138\ubc88\uc9f8 \ubd09\uc6b0\ub9ac\uc758 80%\uc5d0 \ud574\ub2f9\ud558\ub294 \ud3ec\uc778\ud2b8\uc640 \ub300\uc751\ub41c\ub2e4. \uace0\ub85c \uac01 point\ub4e4\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \\(5\\%\\ point = 83.3-(0.84)8.7 = 75.9\\) \\(25\\%\\ point \\sim 120\\ (from\\ the\\ graph)\\) \\(50\\%\\ point = 166.7\\) \\(75\\%\\ point = 205\\sim210\\) \\(95\\%\\ point = 250+(0.84)13.7 = 262\\) . 2-5. 2-5a. \\(P(y=k) =\\int_0^1P(y=k|\\theta)d\\theta\\) \\(=\\int_0^1\\dbinom{n}{k}\\theta^k(1-\\theta)^{n-k}d\\theta\\) \\(=\\dbinom{n}{k}\\frac{\\Gamma(k+1)\\Gamma(n-k+1)}{\\Gamma(n+2)} = \\frac{1}{n+1}\\) . 2-5b. \\(\\theta\\) \uc758 posterior mean \\(\\frac{\\alpha+y}{\\alpha+\\beta+n}\\) \uc774 \\(\\frac{\\alpha}{\\alpha+\\beta}\\) \uc640 \\(\\frac{y}{n}\\) \uc0ac\uc774\uc5d0 \uc788\ub2e4\ub294 \uac83\uc744 \ubcf4\uc774\uae30 \uc704\ud574\uc11c\ub294, \\(\\frac{\\alpha+y}{\\alpha+\\beta+n} = \\lambda\\frac{\\alpha}{\\alpha+\\beta}+(1-\\lambda)\\frac{y}{n},\\ \\lambda \\in (0,1)\\) \uc784\uc744 \ubcf4\uc774\uba74 \ub41c\ub2e4. \uc774\ub97c \\(\\lambda\\) \uc5d0 \uad00\ud55c \uc2dd\uc73c\ub85c \ud480\uba74, \\(\\frac{\\alpha+y}{\\alpha+\\beta+n} = \\frac{y}{n}+\\lambda(\\frac{\\alpha}{\\alpha+\\beta}-\\frac{y}{n})\\) , \\(\\frac{n\\alpha-\\alpha{y}-\\beta{y}}{(\\alpha+\\beta+n)n}=\\lambda(\\frac{n\\alpha-\\alpha{y}-\\beta{y}}{(\\alpha+\\beta)n})\\) . \\(\\lambda= \\frac{\\alpha+\\beta}{\\alpha+\\beta+n}\\) \uac00 \ub418\uba70, \uc774\ub294 \ub298 0\uacfc 1 \uc0ac\uc774\uc5d0 \uc874\uc7ac\ud55c\ub2e4. \uace0\ub85c posterior mean\uc740 prior mean\uacfc data\uc758 \uac00\uc911\ud3c9\uade0\uc774 \ub41c\ub2e4. 2-5c. Uniform prior distribution\uc740 \\(Beta(1,1)\\) \uc774\uba70, \uace0\ub85c prior variance\ub294 \\(\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} = \\frac{1}{12}\\) \uc774\ub2e4. Posterior variance\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(\\frac{(1+y)(1+n-y)}{(2+n)^2(3+n)}=(\\frac{1+y}{2+n})(\\frac{1+n-y}{2+n})(\\frac{1}{3+n})\\) . \uc55e\uc758 \ub450 factor\ub4e4\uc740 \ub450\uac1c\uc758 \ud569\uc774 1\uc774 \ub418\ub294 \uac12\ub4e4\uc774\uba70, \uc774\ub4e4\uc758 \uacf1\uc740 \\(\\frac{1}{4}\\) \uadfc\ucc98\uc5d0\uc11c \uba38\ubb34\ub97c \uac83\uc774\ub2e4. \uc138\ubc88\uc9f8 factor\ub294 n\uc774 1\ubcf4\ub2e4 \ud06c\uac70\ub098 \uac19\ub2e4\uba74, \\(\\frac{1}{3}\\) \ubcf4\ub2e4 \uc791\ub2e4. \uace0\ub85c Posterior variance\ub294 prior variance\ubcf4\ub2e4 \uc791\uc740 \uac12\uc744 \uac16\ub294\ub2e4. 2-5d. \uc0ac\uc804\ubd84\ud3ec\uac00 \ubca0\ud0c0\ubd84\ud3ec\ub97c \ub530\ub97c \ub54c, \uc0ac\ud6c4\ubd84\ud3ec\uc758 \ubd84\uc0b0\uc774 \uc0ac\uc804\ubd84\ud3ec\uc758 \ubd84\uc0b0\ubcf4\ub2e4 \uc791\uc740 \uac83\uc744 \ubcf4\uc774\ub294 \uacbd\uc6b0\uc758 \uc218\ub294 \ubb34\uc218\ud788 \ub9ce\ub2e4. \uc77c\ub840\ub85c n\uc774 \ub9e4\uc6b0 \ucee4\uc9c4\ub2e4\uba74, \uc0ac\ud6c4\ubd84\ud3ec\ub294 \uba85\ubc31\ud788 \uc0ac\uc804\ubd84\ud3ec\uc758 \ubd84\uc0b0\ubcf4\ub2e4 \uc791\uc740 \uac12\uc744 \uac00\uc9c0\uac8c \ub41c\ub2e4. \ub9cc\uc57d n\uacfc y\uac00 \ubaa8\ub450 1\uc778 \uacbd\uc6b0\ub97c \uc0dd\uac01\ud574\ubcf4\uc790. \uc774 \ub54c \uc0ac\uc804\ubd84\ud3ec\uac00 \\(Beta(1,4)\\) \ub97c \ub530\ub978\ub2e4\uba74, \uc0ac\ud6c4\ubd84\ud3ec\uc758 \ubd84\uc0b0\uc740 0.055\uc774\uba70, \uc0ac\uc804\ubd84\ud3ec\uc758 \ubd84\uc0b0\uc740 0.026\uc744 \uac16\ub294\ub2e4. 2-7 2-7a. \uc774\ud56d\ubd84\ud3ec\ub294 exponential family\uc5d0 \uc18d\ud558\uba70, natural parameter\uc5d0 \ub300\ud55c \ubd80\ubd84\uc740 \ub2e4\uc74c\uacfc \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\(\\phi(\\theta) = log(\\frac{\\theta}{1-\\theta})\\) . Uniform prior density\uc778 $\\phi(\\theta), p(\\theta) \\propto 1 $\ub294 \\(\\theta = e^{\\phi}/(1+e^{\\phi})\\) \ub85c \ud45c\ud604\uc774 \uac00\ub2a5\ud558\ub2e4. \uace0\ub85c, $ \\(q(\\theta) = p(\\frac{e^{\\phi}}{1+e^{\\phi}})|\\frac{d}{d\\theta}log(\\frac{\\theta}{1-\\theta})| \\propto \\theta^{-1}(1-\\theta)^{-1}\\) $. 2-7b. \ub9cc\uc57d y\uac00 0 \uc774\ub77c\uba74, \\(p(\\theta|y) \\propto \\theta^{-1}(1-\\theta)^{n-1}\\) \ub294 \\(\\theta=0\\) \uc8fc\ubcc0\uc5d0\uc11c \ubb34\ud55c\ud788 \uc801\ubd84\ub418\uc5b4 \uacc4\uc0b0\uc774 \ubd88\uac00\ub2a5\ud558\uba70, y\uac00 n\uc778 \uacbd\uc6b0\uc5d0\ub294 \\(\\theta=1\\) \uc8fc\ubcc0\uc5d0\uc11c \uc704\uc640 \uac19\ub2e4. 2-8 2-8a \\[\\theta|y \\sim N(\\frac{\\frac{1}{40^2}180+\\frac{n}{20^2}150}{\\frac{1}{40^2}+\\frac{n}{20^2}},\\frac{1}{\\frac{1}{40^2}+\\frac{n}{20^2}})\\] 2-8b \\[\\tilde{y} |y \\sim N(\\frac{\\frac{1}{40^2}180+\\frac{n}{20^2}150}{\\frac{1}{40^2}+\\frac{n}{20^2}},\\frac{1}{\\frac{1}{40^2}+\\frac{n}{20^2}}+20^2)\\] 2-8c 95% posterior interval for \\(\\theta|y=150,\\ n=10:\\ 150.7 \\pm 1.96(6.25) = [138,163]\\) 95% posterior interval for \\(\\tilde{y}|y=150,\\ n=10:\\ 150.7 \\pm 1.96(20.95) = [110,192]\\) 2-8d 95% posterior interval for \\(\\theta|y=150,\\ n=100:[146,154]\\) 95% posterior interval for \\(\\tilde{y}|y=150,\\ n=100:[111,189]\\) 2-9 2-9a. \\(\\alpha + \\beta = \\frac{E(\\theta)(1-E(\\theta))}{var(\\theta)}-1 = 1.67\\) \\(\\alpha = (\\alpha+\\beta)E(\\theta) = 1\\) \\(\\beta= (\\alpha+\\beta)(1-E(\\theta))=0.67\\) theta <- seq(0,1,.001) dens <- dbeta(theta,1,.67) plot (theta, dens, type=\"l\", xlab=\"theta\", ylab=\"\", yaxt=\"n\", cex=2) lines (c(1,1),c(0,3),col=0) lines (c(1,1),c(0,3),lty=3) 2-9b \\(n=1000,\\ y=650\\) \uc774\uba74, \\(p(\\theta|y)=Beta(\\alpha+650,\\beta+350)=Beta(651,350,67)\\) \uc774\ub2e4. theta <- seq(0,1,.001) dens <- dbeta(theta,651,350.67) cond <- dens/max(dens) > 0.001 plot (theta[cond], dens[cond], type=\"l\", xlab=\"theta\", ylab=\"\", yaxt=\"n\", cex=2) 2-10 2-10a \\[p(data|N) = \\begin{cases} \\frac{1}{N}\\ if\\ N \\geq 203\\\\ 0\\ ohterwise \\end{cases} \\ \\] \\[p(N|data) \\propto p(n)p(data|N)\\] \\[=\\frac{1}{N}(0.01)(0.99)^{N-1}\\ for\\ N \\geq 203\\] $ \\(\\frac{1}{N}(0.99)^N\\ for\\ N \\geq 203\\) $. 2-10b $ \\(p(N|data) = c\\frac{1}{N}(0.99)^N\\) $ \\(\\sum_Np(N|data)=1\\) \uc774\ubbc0\ub85c, c\ub294 \ub2e4\uc74c\uc744 \ud1b5\ud574 \uacc4\uc0b0\ud560 \uc218 \uc788\ub2e4. $ \\(\\frac{1}{c} = \\sum_{N=203}^\\infty\\frac{1}{N}(0.99)^N\\) $ \uc774\ub294 analytically \ud558\uac8c \uacc4\uc0b0\ud560 \uc218 \uc788\uc9c0\ub9cc, \ucef4\ud4e8\ud130\uc758 numerical computation\uc73c\ub85c \ub3c4\ucd9c\ud558\ub294 \uac83\uc774 \ud6e8\uc52c \uc27d\ub2e4. \\[Approximation\\ on\\ the\\ computer:\\ \\sum_{N=203}^{1000}\\frac{1}{N}(0.99)^N = 0.04658\\] \\[Error\\ in\\ the\\ approximation:\\ \\sum_{N=1001}^\\infty\\frac{1}{N}(0.99)^N < \\frac{1}{1001}\\sum_{N=1001}^\\infty(0.99)^N\\] $ \\(=4.3 \\times 10^{-6}\\) $ \uace0\ub85c \\(\\frac{1}{c} = 0.04658,\\ c=21.47\\) \uc774\ub2e4. \\[E(N|data) = \\sum_{N=203}^\\infty Np(N|data)\\] \\[c\\sum_{N=203}^\\infty(0.99)^N = 21.47\\cdot\\frac{(0.99)^{203}}{1-0.99} = 279.1\\] \\[sd(N|data) = \\sqrt{\\sum_{N=203}^\\infty (N-279.1)^2c\\frac{1}{N}(0.99)^N}\\] $ \\(\\approx \\sqrt{\\sum_{N=203}^{1000}(N-279.1)^221.47\\frac{1}{N}(0.99)^N} = 79.6\\) $. 2-10c improper discrete uniform prior density on \\(N:p(N) \\propto 1\\) \uc774 \ud55c\uac00\uc9c0 \ubc29\ubc95\uc774 \ub420 \uc218 \uc788\ub2e4. \uc774 density\ub294 improper posterior density($p(N) \\propto \\frac{1}{N},\\ for\\ N \\geq203 $)\ub97c \uac16\ub294\ub2e4. prior( \\(p(N) \\propto 1/N\\) ) \ub294 improper\ud558\uc9c0\ub9cc, proper prior density\ub97c \uac16\ub294\ub370, \\(\\sum_N1/N^2\\) \uac00 \uc218\ub834\ud558\uae30 \ub54c\ubb38\uc774\ub2e4. \ub610\ud55c data point\uac00 \ud55c \uac1c \uc774\uc0c1\uc774\ub77c\uba74, \uc0ac\ud6c4\ubd84\ud3ec\ub294 \uc704\uc758 \uc0ac\uc804\ubd84\ud3ec\ub97c \uac16\ub294 \ud558\uc5d0\uc11c proper\ud558\uba70 data point\uac00 \ub2e8 \ud558\ub098\ubfd0\uc774\ub77c\uba74 noninformative prior\ub97c \uc4f0\ub294 \uac83\ubcf4\ub2e4 \uc2e4\uc6a9\uc801\uc774\uc9c0 \ubabb\ud55c \uacb0\uacfc\ub97c \uac16\ub294\ub2e4. 2-11 2-11a dens <- function (y, th){ dens0 <- NULL for (i in 1:length(th)) dens0 <- c(dens0, prod (dcauchy (y, th[i], 1))) dens0} y <- c(-2, -1, 0, 1.5, 2.5) step <- .01 theta <- seq(step/2, 1-step/2, step) dens.unnorm <- dens(y,theta) dens.norm <- dens.unnorm/(step*sum(dens.unnorm)) plot (theta, dens.norm, ylim=c(0,1.1*max(dens.norm)), type=\"l\", xlab=\"theta\", ylab=\"normalized density\", xaxs=\"i\", yaxs=\"i\", cex=2) 2-11b thetas <- sample (theta, 1000, step*dens.norm, replace=TRUE) hist (thetas, xlab=\"theta\", yaxt=\"n\", breaks=seq(0,1,.05), cex=2) 2-11c y6 <- rcauchy (length(thetas), thetas, 1) hist (y6, xlab=\"new observation\", yaxt=\"n\", nclass=100, cex=2) 2-12 \ud3ec\uc544\uc1a1 \ubd84\ud3ec\uc758 pdf\ub294 \ub2e4\uc74c\uacfc \uac19\uc73c\uba70 \\(p(y|\\theta) = \\theta^ye^{-\\theta}/y!\\) , \\(J(\\theta)=E(d^2logp(y|\\theta)/d\\theta^2|\\theta)=E(y/\\theta^2)=1/\\theta\\) \uc774\ub2e4. \uc774\ub294 improper gamma density \\(\\Gamma(1/2,0)\\) \uc640 \uc0c1\uc751\ud55c\ub2e4. 2-13 2-13a \\(y_i\\) \ub97c fatal accidents in year i\uc758 \ube48\ub3c4\ub77c \ud558\uace0 \\(\\theta\\) \ub97c \ud55c \ud574\uc5d0 \uc77c\uc5b4\ub0a0 \uc0ac\uace0\ube48\ub3c4\uc758 \uae30\ub313\uac12\uc774\ub77c \ud558\uc790. \uadf8\ub807\ub2e4\uba74 \\(y_i|\\theta \\sim Pois(\\theta)\\) \ub97c \uac16\ub294\ub2e4. \ud3b8\uc758\ub97c \uc704\ud574 conjugate family\ub97c \uc774\uc6a9\ud558\uc790. \ub9cc\uc57d \\(\\theta\\) \uc758 \uc0ac\uc804\ubd84\ud3ec\uac00 \\(\\Gamma(\\alpha,\\beta)\\) \ub77c\uba74, \uc0ac\ud6c4\ubd84\ud3ec\ub294 \\(\\Gamma(\\alpha+10\\bar{y},\\beta+10)\\) \uc744 \uac16\ub294\ub2e4. \\(\\alpha,\\beta\\) \uac00 \ubaa8\ub450 0\uc778 noninformative prior distribution\uc744 \uac00\uc815\ud558\uc790.(n=10) \uc0ac\ud6c4\ubd84\ud3ec\ub294 \\(\\theta|y \\sim \\Gamma(238,10)\\) \uc744 \uac16\ub294\ub2e4. \\(\\tilde{y}\\) \ub97c 1986\ub144\uc758 fatal accidents\uc758 \ud69f\uc218\ub77c \ud558\uc790. \\(\\theta\\) \uac00 \uc8fc\uc5b4\uc84c\uc744 \ub54c predictive distribution for \\(\\tilde{y}\\) \ub294 \\(Pois(\\theta)\\) \ub97c \uac16\ub294\ub2e4. \uc544\ub798\ub294 \\(\\tilde{y}\\) \uc758 95% posterior interval\uc744 \uad6c\ud558\ub294 \ub450 \uac00\uc9c0 \ubc29\ubc95\uc774\ub2e4. Simulation theta <- rgamma(1000,238)/10 y1986 <- rpois(1000,theta) print (sort(y1986)[c(25,976)]) ## [1] 15 34 Normal approximation \uac10\ub9c8\ubd84\ud3ec\ub85c\ubd80\ud130 \\(E(\\theta|y) = 238/10=23.8,\\ sd(\\theta|y)=\\sqrt{238/10} =1.54\\) \ub97c \uc5bb\uc744 \uc218 \uc788\uc73c\uba70 \ud3ec\uc544\uc1a1 \ubd84\ud3ec\ub85c\ubd80\ud130 \\(E(\\tilde{y}|\\theta)=\\theta,\\ sd(\\tilde{y}|\\theta)=\\sqrt{\\theta}\\) \ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4. Formula (1.6)\uacfc (1.7)\ub85c \ubd80\ud130 \ub2e4\uc74c\uacfc \uac19\uc740 \uc2dd\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4. \\[E(\\tilde{y}|y)=E(E(\\tilde{y}|\\theta,y|y)=E(\\theta|y)=23.8\\] \\[var(\\tilde{y}|y) = E(var(\\tilde{y}|\\theta,y)|y)+var(E(\\tilde{y}|\\theta,y)|y)\\] $ \\(=E(\\theta|y)+var(\\theta|y) = 26.2=5.12^2\\) $. \\(p(\\tilde{y}|y)\\) \uc758 \uc815\uaddc\uadfc\uc0ac\ub97c \ud1b5\ud574 \\(\\tilde{y}\\) \uc758 95% interval\uc740 \\([23.8 \\pm 1.96(5.12)] = [13.8,33.8]\\) \ub97c \uac16\ub294\ub2e4. \\(\\tilde{y}\\) \ub294 \ubc18\ub4dc\uc2dc \uc815\uc218\uc5ec\uc57c\ud558\uba70, 95% \uc2e0\uc6a9\uad6c\uac04\uc740 \ucd5c\uc18c\ud55c [13,34]\ub97c \ud3ec\ud568\ud55c\ub2e4. 2-13b 1976\ub144\uacfc 1977\ub144\uc758 \ucd94\uc815\ub41c numbers of passenger miles\ub294 \uac01\uac01 \\(3.863 \\times 10^{11},\\ 4.3 \\times 10^{11}\\) \uc774\ub2e4. \\(x_i\\) =number of passenger miles flown in year i, \\(\\theta\\) =expected accident rate per passenger mile\uc774\ub77c\uace0 \ud558\uc790. \\(y_i|x_i,\\theta \\sim Pois(x_i\\theta)\\) . \\(\\theta\\) \uc5d0 \ub300\ud55c prior\ub85c \\(\\Gamma(0,0)\\) \uc744 \uc0ac\uc6a9\ud558\uba74 \uc774\uc5d0 \ub300\ud55c \uc0ac\ud6c4\ubd84\ud3ec\ub294 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(y|\\theta \\sim \\Gamma(10\\bar{y},10\\bar{x})=\\Gamma(238,5.716 \\times 10^{12})\\) $. Given \\(\\theta\\) \uc5d0\uc11c predictive distribution for \\(\\tilde{y}\\) \ub294 \\(Pois(\\tilde{x}\\theta)=Pois(8\\times10^{11}\\theta)\\) \uc774\ub2e4. \\(tilde{y}\\) \uc758 95% posterior interval\uc744 \uad6c\ud558\ub294 \ub450 \uac00\uc9c0 \ubc29\ubc95\uc740 \uc544\ub798\uc640 \uac19\ub2e4. Simulation theta <- rgamma(1000,238)/5.716e12 y1986 <- rpois(1000,theta*8e11) print (sort(y1986)[c(25,976)]) ## [1] 22 47 Normal Approximation \\(E(\\theta|y) = 238/(5.716 \\times 10^{12}) = 4.164 \\times 10^{-11},\\ sd(\\theta|y) = \\sqrt{238}/(5.716 \\times 10^{12}) = 0.270 \\times 10^{-11}\\) , \\(E(\\tilde{y}|\\theta)=(8\\times 10^{11})\\theta,\\ sd(\\tilde{y}|\\tilde{x},\\theta)=\\sqrt{(8 \\times 10^{11})\\theta)}\\) . \\[E(\\tilde{y}|y) = E(E(\\tilde{y}|\\theta,y)|y) = 33.3\\] \\[var(\\tilde{y}|y) = E(var(\\tilde{y}|\\theta,y)|y) +var(E(\\tilde{y}|\\theta,y)|y)\\] $ \\((8 \\times 10^{11})(4.164 \\times 10^{-11})+(8 \\times 10^{11})(0.270 \\times 10^{-11})^2 = 38=6.2^2\\) $. \\(\\tilde{y}\\) \ub294 \uc815\uc218\uac12\uc744 \uac00\uc838\uc57c \ud558\ubbc0\ub85c, interval\uc740 \uc801\uc5b4\ub3c4 [21,46]\uc744 \ud3ec\ud568\ud574\uc57c\ud55c\ub2e4. 2-13c 238\uc744 6919(total number of deaths in the data)\ub85c \ub300\uccb4\ud558\uc5ec (a)\ub97c \ubc18\ubcf5\ud558\uba74, 1000\ubc88\uc758 simulation\uc758 \uacb0\uacfc\ub294 95% posterior interval [638,750]\uc744 \uac16\ub294\ub2e4. 2-13d \uc704\uc758 c\uc640 \uac19\uc774 \uac12\uc744 \ub300\uccb4\ud558\uc5ec (b)\ub97c \ubc18\ubcf5\ud558\uba74, 95% posterior interval\uc740 [900,1035]\uc744 \uac16\ub294\ub2e4. 2-13e \ud55c \ud574\uc5d0 \ub354 \ub9ce\uc740 mile\uc744 \ube44\ud589\ud560\uc218\ub85d \ub354 \ub9ce\uc740 \uc0ac\uace0\uac00 \uae30\ub300\ub418\uae30\uc5d0, \uc77c\ubc18\uc801\uc73c\ub85c model (b)\uc640 (d)\uc5d0\uc11c\ub294 Poisson model\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ubcf4\ub2e4 \ud569\ub9ac\uc801\uc774\ub2e4. \ubc18\uba74 \ub9cc\uc57d \ud56d\uacf5\uc0ac\uc758 \uc548\uc804\uc774 \uac1c\uc120\ub41c\ub2e4\uba74, \uc0ac\uace0\uc728\uc774 \uc2dc\uac04\uc5d0 \ub530\ub77c \uac10\uc18c\ud560 \uac83\uc73c\ub85c \uae30\ub300\ub418\uba70, \uc0ac\uace0\ube48\ub3c4\uc758 \uae30\ub313\uac12\uc740 roughly\ud558\uac8c constant\uac12\uc744 \uac00\uc9c8 \uac83\uc774\ub2e4. \uc774\uc5d0\ub294 time trend\ub97c \ubaa8\ub378\uc5d0 \ub123\ub294 \uac83\uc774 \ud569\ub9ac\uc801\uc73c\ub85c \uc0ac\ub8cc\ub41c\ub2e4. \ub610\ud55c \uc0ac\uace0\ub294 \ub3c5\ub9bd\uc801\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c0\ub294 \uac83\uc73c\ub85c \uae30\ub300\ub41c\ub2e4. \ud5c8\ub098 \uc2b9\uac1d\ub4e4\uc758 \uc0ac\ub9dd\uc740 \ub3c5\ub9bd\uc774 \uc544\ub2c8\ub2e4.(\ube44\ud589\uae30\uc0ac\uace0\uac00 \ub098\uba74 \ud0d1\uc2b9\uac1d\ub4e4\uc740 \ud55c \ubb36\uc74c\uc5d0 \uc5ee\uc5ec\uc788\uc73c\ubbc0\ub85c) \uace0\ub85c \ud3ec\uc544\uc1a1 \ubaa8\ub378\uc740 \ucd1d\uc0ac\ub9dd\ube48\ub3c4\ubcf4\ub2e4\ub294 \uc0ac\uace0\ube48\ub3c4\uc5d0 \uc801\uc6a9\ud558\ub294 \uac83\uc774 \ud569\ub9ac\uc801\uc774\ub2e4. \ub9cc\uc57d \uad00\uc2ec\ubd84\uc57c\uac00 \ucd1d\uc0ac\ub9dd\uc790\ub77c\uba74 \uc5b4\ub5bb\uac8c \ub420 \uac83\uc778\uac00? \uc774\ub54c\ub294 compound model\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ud569\ub9ac\uc801\uc778\ub370, \uc608\ub97c \ub4e4\uc5b4 accidents\uc5d0 \ub300\ud574\uc11c\ub294 \ud3ec\uc544\uc1a1\ubd84\ud3ec\ub97c \uc801\uc6a9\ud558\uace0, \uc0ac\uace0\uac00 \uc77c\uc5b4\ub0ac\uc744 \ub54c \uc0ac\ub9dd\uc5d0 \ub300\ud558\uc5ec\ub294 \ub2e4\ub978 \ubd84\ud3ec\ub97c \uc801\uc6a9\ud558\ub294 \uac83\uc774\ub2e4. 2-16 2-16a \\[p(y) \\int p(y|\\theta)p(\\theta)d\\theta\\] \\[\\int_0^1\\binom{n}{y}\\theta^y(1-\\theta)^{n-y}\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\int_0^1\\theta^{\\alpha+1}(1-\\theta)^{\\beta-1}d\\theta\\] \\[\\frac{\\Gamma(n+1)}{\\Gamma(y+1)\\Gamma(n-y+1)}\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\int_0^1\\theta^{y+\\alpha-1}(1-\\theta)^{n-y+\\beta-1}d\\theta\\] \\[\\frac{\\Gamma(n+1)}{\\Gamma(y+1)\\Gamma(n-y+1)}\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\frac{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)}{\\Gamma(n+\\alpha+\\beta)}\\] 2-16b \uc6b0\ub9ac\ub294 y\uc5d0 \ub530\ub77c \\(p(y)\\) \uac00 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0\ub97c \uc54c\uace0 \uc2f6\uae30 \ub54c\ubb38\uc5d0, \\(p(y)\\) factor\uc5d0 \uad00\uc2ec\uc744 \uac16\ub294\ub2e4. \uace0\ub85c \\(\\Gamma(a+y)\\Gamma(b+n-y)/\\Gamma(y+1)\\Gamma(n-y+1)\\) \uac00 y\uc5d0 \ub530\ub77c \ubcc0\ud558\ub294 \uac83\uc744 \ud655\uc778\ud558\uba74 \ub41c\ub2e4. \uc704\uc758 \uc2dd\uc740 \\(a=b=1\\) \uc77c \ub54c 1\uc774\uba70, \\(p(y)\\) \ub294 \uc774 \ub54c constant\uc774\ub2e4. \\(p(y)\\) \uac00 in y\uc5d0\uc11c constant\ub77c \uac00\uc815\ud574\ubcf4\uc790. (\ud2b9\ud788 \\(p(0)=p(n)\\) \uacfc \\(p(0)=p(1)\\) \uc5d0\uc11c) \uccab\ubc88\uc9f8 \uc2dd\uc744 \ud1b5\ud574 \ub2e4\uc74c\uacfc \uac19\uc740 \uc2dd \ub3c4\ucd9c\uc774 \uac00\ub2a5\ud558\ub2e4. $ \\(\\Gamma(a)\\Gamma(b+n)=\\Gamma(a+n)\\Gamma(b)\\) $. \\(\\Gamma(t)=(t-1)\\Gamma(t-1)\\) \uc744 \ub5a0\uc62c\ub824\ubcf8\ub2e4\uba74, \uc6b0\ub9ac\ub294 \\(\\Gamma(a)\\Gamma(b)(b+n-1)\\cdots(b+1)b=\\Gamma(a)\\Gamma(b)(a+n-1)\\cdots(a+1)a\\) \ub97c \uc131\ub9bd\uc2dc\ucf1c\uc57c\ud55c\ub2e4. \uc5ec\uae30\uc11c \ubaa8\ub4e0 product\uc758 \ubaa8\ub4e0 \ud56d\ub4e4\uc774 positive\uc774\uae30\ub54c\ubb38\uc5d0, \\(a=b\\) \uc774\ub2e4. \\(p(0)=p(1)\\) \uc740 \\(\\Gamma(a)\\Gamma(b+n)/\\Gamma(1)\\Gamma(n+1)=\\Gamma(a+1)\\Gamma(b+n-1)/\\Gamma(2)\\Gamma(n)\\) \uc744 \ub0b4\ud3ec\ud558\uba70, \uc704\uc640 \uac19\uc740 \ubc29\uc2dd\uc73c\ub85c \\(b+n-1=na\\) \ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4. \\(a=b\\) \uc774\uae30 \ub54c\ubb38\uc5d0, \uc774\ub294 \\(a+n-1=na\\) \ub85c \uc815\ub9ac\ub418\uba70, \uc774 \uc2dd\uc758 \uc720\uc77c\ud55c \ud574\ub294 \\(a=1\\) \uc774\ub2e4. \uadf8\ub7ec\ubbc0\ub85c \\(a=b=1\\) \uc740 \\(p(y)\\) \uac00 constant in y\uac00 \ub418\uae30 \uc704\ud55c \ud544\uc694\ucda9\ubd84\uc870\uac74\uc774\ub2e4. 2-17 2-17a Let \\(\\mu = \\sigma^2\\) , so that \\(\\sigma = \\sqrt{\\mu}\\) . Then \\(p(\\sigma^2)=p(\\mu)=p(\\sqrt{\\mu})d\\sqrt{\\mu}/d\\mu=p(\\sigma)(1/2)\\mu^{-1/2}=(1/2)p(\\sigma)/\\sigma\\) , which is proportional to \\(1/\\sigma^2\\) if \\(p(\\sigma) \\propto \\sigma^{-1}\\) . 2-17b \\(p(\\sigma/data) \\propto \\sigma^{-1-n}exp(-c/\\sigma^2)\\) , which may be written as \\((\\sigma^2)^{-1/2-n/2}exp(-c/\\sigma^2)\\) . \\(p(\\sigma^2|data) \\propto (\\sigma^2)^{-1-n/2}exp(-c/\\sigma^2)\\) . We have defined \\(c=nv/2\\) , and assume this quantity to be positive. Let \\((\\sqrt{a},\\sqrt{b})\\) be the 95% interval of highest density for \\(p(\\sigma|data)\\) . Then $ \\(a^{-1/2-n/2}exp(-c/a)=b^{-1/2-n/2}exp(-c/b)\\) $. Equivalently, \\((-1/2-n/2)loga-c/a=(-1/2-n/2)logb-c/b\\) . if (a,b) were the 95% interval of highest density for \\(p(\\sigma^2|data)\\) , then \\[a^{-1-n/2}exp(-c/a)=b^{-1-n/2}exp(-c/b)\\] That is, \\((-1-n/2)loga-c/a=(-1-n/2)logb-c/b\\) . Combining the two equations, \\(1/2loga=1/2logb\\) , so that \\(a=b\\) , in which case [a,b] cannot be a 95% interval, and we have a contradiction. 2-20 2-20a \\[p(\\theta|y \\geq 100) \\propto p(y \\geq 100|\\theta)p(\\theta)\\] \\[\\propto exp(-100\\theta)\\theta^{\\alpha-1}exp(-\\beta\\theta)\\] \\[p(\\theta|y \\geq 100) = \\Gamma(\\theta|\\alpha,\\beta+100)\\] 2-20b \\[p(\\theta|y=100) \\propto p(y=100|\\theta)p(\\theta)\\] \\[\\propto \\theta exp(-100\\theta)\\theta^{\\alpha-1}exp(-\\beta\\theta)\\] \\[p(\\theta|y=100) = \\Gamma(\\theta|\\alpha+1,\\beta+100)\\] 2-20c The variance of \\(\\theta\\) decreases given more information : in this case, \\[E(var(\\theta|y)|y \\geq 100) \\leq var(\\theta|y \\geq 100)\\] Plugging in y=100 to get \\(var(\\theta|y=100)\\) is not the sa,e as averaging over the distribution of \\(y|y \\geq 100\\) on the left side of above inequality. Probability of a girl birth given placenta previa library(ggplot2) theme_set(theme_minimal()) df1 <- data.frame(theta = seq(0.375, 0.525, 0.001)) a <- 438 b <- 544 # dbeta computes the posterior density df1$p <- dbeta(df1$theta, a, b) df2 <- data.frame(theta = seq(qbeta(0.025, a, b), qbeta(0.975, a, b), length.out = 100)) # compute the posterior density df2$p <- dbeta(df2$theta, a, b) ggplot(mapping = aes(theta, p)) + geom_line(data = df1) + # Add a layer of colorized 95% posterior interval geom_area(data = df2, aes(fill='1')) + # Add the proportion of girl babies in general population geom_vline(xintercept = 0.488, linetype='dotted') + # Decorate the plot a little labs(title='Uniform prior -> Posterior is Beta(438,544)', y = '') + scale_y_continuous(expand = c(0, 0.1), breaks = NULL) + scale_fill_manual(values = 'lightblue', labels = '95% posterior interval') + theme(legend.position = 'bottom', legend.title = element_blank()) Illustrate the effect of prior in binomial model library(ggplot2) theme_set(theme_minimal()) library(tidyr) a <- 437 b <- 543 df1 <- data.frame(theta = seq(0.375, 0.525, 0.001)) df1$pu <- dbeta(df1$theta, a+1, b+1) n <- c(2, 20, 200) # prior counts apr <- 0.488 # prior ratio of success helperf <- function(n, apr, a, b, df) cbind(df, pr = dbeta(df$theta, n*apr, n*(1-apr)), po = dbeta(df$theta, n*apr + a, n*(1-apr) + b), n = n) df2 <- lapply(n, helperf, apr, a, b, df1) %>% do.call(rbind, args = .) %>% gather(grp, p, -c(theta, n), factor_key = T) df2$title <- factor(paste0('alpha/(alpha+beta)=0.488, alpha+beta=',df2$n)) levels(df2$grp) <- c('Posterior with unif prior', 'Prior', 'Posterior') ggplot(data = df2) + geom_line(aes(theta, p, color = grp)) + geom_vline(xintercept = 0.488, linetype = 'dotted') + facet_wrap(~title, ncol = 1) + labs(x = '', y = '') + scale_y_continuous(breaks = NULL) + theme(legend.position = 'bottom', legend.title = element_blank()) Derive Jefferey's prior for Binomial Distribution. \\[p(y|\\theta = \\binom{n}{y}\\theta^y(1-\\theta)^{n-y}\\] Reparameterize the model with \\(\\phi=g(\\theta)\\) to get $ \\(p(y|\\phi) = \\binom{n}{y}h(\\phi)^y(1-h(\\phi))^{n-y}\\) $. Denote \\(p_J(\\phi)\\) Jeffrey's prior, $ \\(p_J(\\phi) = p_J(h(\\phi))|\\frac{dh}{d\\phi}|\\) $. \\[l: log(p(y|\\theta)) \\propto ylog(\\theta) + (n-y)log(1-\\theta)\\] \\[\\frac{\\partial l}{\\partial \\theta}=\\frac{y}{\\theta}-\\frac{n-y}{1-\\theta}\\] \\[\\frac{\\partial^2 l}{\\partial \\theta^2}=-\\frac{y}{\\theta^2}-\\frac{n-y}{(1-\\theta)^2}\\] \\[I(\\theta) = -E(\\frac{\\partial^2 l}{\\partial \\theta^2}|\\theta)\\] \\[\\frac{n\\theta}{\\theta^2}+\\frac{n-n\\theta}{(1-\\theta)^2}\\] \\[\\frac{n}{\\theta(1-\\theta)}\\] \\[\\theta^{-1}(1-\\theta)^{-1}\\] \\[p_J(\\theta) = \\sqrt{I(\\theta)}\\] \\[\\propto \\theta^{-1/2}(1-\\theta)^{-1/2}\\] which is \\(Beta(1/2,1/2)\\)","title":"R Notebook"},{"location":"10%20Bayesian%20Statistics/HW2/#bayesian-statistics-hw2","text":"","title":"Bayesian Statistics HW2"},{"location":"10%20Bayesian%20Statistics/HW2/#2-1","text":"\ub3d9\uc804\uc758 \uc55e\uba74\uc774 \ub098\uc62c \ud655\ub960 \\(\\theta\\) \uc5d0 \ub300\ud55c Prior distribution\uc774 \\(Beta(4,4)\\) \ub97c \ub530\ub974\uba70, \ub3d9\uc804\uc744 10\ubc88 \ub358\uc84c\uc744 \ub54c \ub3d9\uc804\uc758 \uc55e\uba74\uc774 3\ubc88 \ubbf8\ub9cc\uc73c\ub85c \ub098\uc624\ub294 \uc0ac\uac74\uc5d0 \ub300\ud558\uc5ec prior\uc640 likelihood\ub97c \uc815\ub9ac\ud558\uba74 \ub2e4\uc74c\uacfc \uac19\ub2e4. $p(\\theta) \\sim \\theta^3(1-\\theta)^3,\\ Beta(\\alpha, \\beta) \\propto \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} $ \\(f(y|\\theta) = \\dbinom{10}{0}(1-\\theta)^{10}+\\dbinom{10}{1}\\theta(1-\\theta)^{9}+\\dbinom{10}{2}\\theta^2(1-\\theta)^8 = (1-\\theta)^{10}+10\\theta(1-\\theta)^{9}+45\\theta^2(1-\\theta)^{8}\\) . Posterior Density\ub294 prior\uc640 likelihood\uc758 \uacf1\uc5d0 \ube44\ub840\ud558\ubbc0\ub85c, \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\(p(\\theta|y) \\propto \\theta^3(1-\\theta)^{13}+10\\theta^4(1-\\theta)^{12}+45\\theta^5(1-\\theta)^{11}\\) Posterior density\uc758 sketch\ub294 \uc544\ub798\uc640 \uac19\ub2e4. theta <- seq(0,1,.01) dens <- theta^3*(1-theta)^13 + 10*theta^4*(1-theta)^12 + 45*theta^5*(1-theta)^11 plot (dens~theta, main='Posterior', type=\"l\", xlab=\"theta\", yaxt=\"n\", cex=2)","title":"2-1."},{"location":"10%20Bayesian%20Statistics/HW2/#2-2","text":"\ub3d9\uc804\uc758 \uc55e\uba74\uc774 \ub098\uc62c \ud655\ub960\uc744 \\(\\pi\\) \ub77c \ud558\uace0, \ub3d9\uc804\uc774 \uc55e\uba74\uc774 \ub098\uc62c \ub54c \uae4c\uc9c0 \ucd94\uac00\ub85c \ub358\uc9c4 \ud69f\uc218\ub97c N\uc774\ub77c\uace0 \ud560 \uc2dc , \uc774\uc5d0 \ub300\ud55c \uae30\ub313\uac12\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(E[N|\\pi] = 1\\cdot\\pi+2\\cdot(1-\\pi)\\pi+3\\cdot(1-\\pi)^2\\pi+ \\cdots = 1/\\pi\\) \uccab \ub450\ubc88 \uc9f8 toss\uac00 tail\uc774 \ub098\uc628 \ud6c4 \ub3d9\uc804 \\(C_1\\) \uc640 \\(C_2\\) \uc911 \ub79c\ub364\ud558\uac8c \ub3d9\uc804\uc744 \uc120\ud0dd\ud558\ub294 \ud655\ub960\uc740 \ubca0\uc774\uc988\uc815\ub9ac\uc5d0 \ub530\ub77c \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604 \ud560 \uc218 \uc788\ub2e4. \\(P(C=C_1|TT) = \\frac{P(C=C_1)P(TT|C=C_1)}{P(C=C_1)P(TT|C=C_1)+P(C=C_2)P(TT|C=C_2)}= \\frac{0.5(0.4^2)}{0.5(0.4^2)+0.5(0.6^2)}=\\frac{16}{52}\\) . \uc774\ud6c4 \ub3d9\uc804\uc774 \uc55e\uba74\uc774 \ub098\uc62c \ub54c \uae4c\uc9c0\uc758 \uc2dc\ud589\ud69f\uc218 N\uc5d0 \ub300\ud55c posterior expectation\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(E(N|TT) = E[E(N|TT,C)|TT] = P(C=C_1|TT)E(N|C=C_1,TT)+P(C=C_2|TT)E(N|C=C_2,TT)\\) \\(=\\frac{16}{52}\\frac{1}{0.6}+\\frac{36}{52}\\frac{1}{0.4}=2.24\\) .","title":"2-2."},{"location":"10%20Bayesian%20Statistics/HW2/#2-3","text":"\\(E[y] = 1000\\cdot\\frac{1}{6} = 166.7,\\ sd[y] = \\sqrt{1000\\cdot\\frac{1}{6}\\frac{5}{6}}=11.8.\\) y <- seq(100,240,1) dens <- dnorm(y,1000*(1/6),sqrt(1000*(1/6)*(5/6))) plot(dens~y, main=\"Approx Dist\", type='l',xlab='y',yaxt='n',cex=2) 5%,25%,50%,75%,95% points\ub294 \ucc28\ub840\ub300\ub85c \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(5\\%\\ point = 166.7 - 1.65(11.8) = 147.2\\) \\(25\\%\\ point = 166.7 - 0.67(11.8) = 158.8\\) \\(50\\%\\ point = 166.7\\) \\(75\\%\\ point = 166.7 + 0.67(11.8) = 174.6\\) \\(95\\%\\ point = 166.7 + 1.65(11.8) = 186.1\\) rounding\ud558\uba74 \ucc28\ub840\ub300\ub85c \uac01 point\ub4e4\uc740 147, 150, 167, 175, 186\uc774\ub2e4.","title":"2-3."},{"location":"10%20Bayesian%20Statistics/HW2/#2-4","text":"","title":"2-4."},{"location":"10%20Bayesian%20Statistics/HW2/#2-4a","text":"\\(E(y|\\theta=\\frac{1}{12}) = 83.3, sd(y|\\theta=\\frac{1}{12}) = 8.7\\) , \\(E(y|\\theta=\\frac{1}{6}) = 166.7, sd(y|\\theta=\\frac{1}{6}) = 11.8\\) , \\(E(y|\\theta=\\frac{1}{4}) = 250, sd(y|\\theta=\\frac{1}{4}) = 13.7\\) . y <- seq(50,300,1) dens <- function(x,theta){ dnorm(x,1000*theta, sqrt(1000*theta*(1-theta))) } mixture_dens <- 0.25*dens(y,1/12)+0.5*dens(y,1/6)+0.25*dens(y,1/4) plot(mixture_dens~y,main=\"prior predictive aprrox dist\", yaxt='n',type='l',cex=2)","title":"2-4a."},{"location":"10%20Bayesian%20Statistics/HW2/#2-4b","text":"\uc704\uc5d0\uc11c \uadf8\ub824\uc9c4 data y\uc758 \ubd84\ud3ec\ub294 \uc815\uaddc\ubd84\ud3ec\ub97c \ub530\ub974\uc9c0 \uc54a\uc9c0\ub9cc, \uadfc\uc0ac\uc801\uc73c\ub85c \uc815\uaddc\ubd84\ud3ec 3\uac1c\uac00 \uc11c\ub85c overlap \ub418\uc5b4\uc788\ub294 \ubaa8\uc2b5\uc73c\ub85c \ubcfc \uc218 \uc788\ub2e4. \uace0\ub85c \uc804\uccb4 \ub370\uc774\ud130\uc758 5%\uc5d0 \ud574\ub2f9\ud558\ub294 point\ub294 y\uc804\uccb4 \ub370\uc774\ud130\uc758 25%\ub97c \ucc28\uc9c0\ud558\uace0 \uc788\ub294 \uccab\ubc88\uc9f8 \uc815\uaddc\ubd84\ud3ec\uc5d0\uc11c 20%\uc5d0 \ud574\ub2f9\ud558\ub294 \ubd80\ubd84\uc73c\ub85c \uc0dd\uac01\ud560 \uc218 \uc788\ub2e4. \uc774\ub7ec\ud55c \ubc29\ubc95\uc73c\ub85c \ub370\uc774\ud130\uc758 25%\uc5d0 \ud574\ub2f9\ud558\ub294 \ud3ec\uc778\ud2b8\ub294 \uccab\ubc88 \uc9f8 \ubd09\uc6b0\ub9ac\uc640 \ub450\ubc88\uc9f8 \ubd09\uc6b0\ub9ac\uc758 \uc0ac\uc774\ub85c \ubcfc \uc218 \uc788\uc73c\uba70, 50% \ud3ec\uc778\ud2b8\ub294 \ub370\uc774\ud130\uc758 \uc815\uc911\uc559, 75% \ud3ec\uc778\ud2b8\ub294 \ub450\ubc88 \uc9f8 \ubd09\uc6b0\ub9ac\uc640 \uc138\ubc88 \uc9f8 \ubd09\uc6b0\ub9ac\uc758 \uc0ac\uc774, 95% \ud3ec\uc778\ud2b8\ub294 \uc138\ubc88\uc9f8 \ubd09\uc6b0\ub9ac\uc758 80%\uc5d0 \ud574\ub2f9\ud558\ub294 \ud3ec\uc778\ud2b8\uc640 \ub300\uc751\ub41c\ub2e4. \uace0\ub85c \uac01 point\ub4e4\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \\(5\\%\\ point = 83.3-(0.84)8.7 = 75.9\\) \\(25\\%\\ point \\sim 120\\ (from\\ the\\ graph)\\) \\(50\\%\\ point = 166.7\\) \\(75\\%\\ point = 205\\sim210\\) \\(95\\%\\ point = 250+(0.84)13.7 = 262\\) .","title":"2-4b."},{"location":"10%20Bayesian%20Statistics/HW2/#2-5","text":"","title":"2-5."},{"location":"10%20Bayesian%20Statistics/HW2/#2-5a","text":"\\(P(y=k) =\\int_0^1P(y=k|\\theta)d\\theta\\) \\(=\\int_0^1\\dbinom{n}{k}\\theta^k(1-\\theta)^{n-k}d\\theta\\) \\(=\\dbinom{n}{k}\\frac{\\Gamma(k+1)\\Gamma(n-k+1)}{\\Gamma(n+2)} = \\frac{1}{n+1}\\) .","title":"2-5a."},{"location":"10%20Bayesian%20Statistics/HW2/#2-5b","text":"\\(\\theta\\) \uc758 posterior mean \\(\\frac{\\alpha+y}{\\alpha+\\beta+n}\\) \uc774 \\(\\frac{\\alpha}{\\alpha+\\beta}\\) \uc640 \\(\\frac{y}{n}\\) \uc0ac\uc774\uc5d0 \uc788\ub2e4\ub294 \uac83\uc744 \ubcf4\uc774\uae30 \uc704\ud574\uc11c\ub294, \\(\\frac{\\alpha+y}{\\alpha+\\beta+n} = \\lambda\\frac{\\alpha}{\\alpha+\\beta}+(1-\\lambda)\\frac{y}{n},\\ \\lambda \\in (0,1)\\) \uc784\uc744 \ubcf4\uc774\uba74 \ub41c\ub2e4. \uc774\ub97c \\(\\lambda\\) \uc5d0 \uad00\ud55c \uc2dd\uc73c\ub85c \ud480\uba74, \\(\\frac{\\alpha+y}{\\alpha+\\beta+n} = \\frac{y}{n}+\\lambda(\\frac{\\alpha}{\\alpha+\\beta}-\\frac{y}{n})\\) , \\(\\frac{n\\alpha-\\alpha{y}-\\beta{y}}{(\\alpha+\\beta+n)n}=\\lambda(\\frac{n\\alpha-\\alpha{y}-\\beta{y}}{(\\alpha+\\beta)n})\\) . \\(\\lambda= \\frac{\\alpha+\\beta}{\\alpha+\\beta+n}\\) \uac00 \ub418\uba70, \uc774\ub294 \ub298 0\uacfc 1 \uc0ac\uc774\uc5d0 \uc874\uc7ac\ud55c\ub2e4. \uace0\ub85c posterior mean\uc740 prior mean\uacfc data\uc758 \uac00\uc911\ud3c9\uade0\uc774 \ub41c\ub2e4.","title":"2-5b."},{"location":"10%20Bayesian%20Statistics/HW2/#2-5c","text":"Uniform prior distribution\uc740 \\(Beta(1,1)\\) \uc774\uba70, \uace0\ub85c prior variance\ub294 \\(\\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)} = \\frac{1}{12}\\) \uc774\ub2e4. Posterior variance\ub294 \ub2e4\uc74c\uacfc \uac19\ub2e4. \\(\\frac{(1+y)(1+n-y)}{(2+n)^2(3+n)}=(\\frac{1+y}{2+n})(\\frac{1+n-y}{2+n})(\\frac{1}{3+n})\\) . \uc55e\uc758 \ub450 factor\ub4e4\uc740 \ub450\uac1c\uc758 \ud569\uc774 1\uc774 \ub418\ub294 \uac12\ub4e4\uc774\uba70, \uc774\ub4e4\uc758 \uacf1\uc740 \\(\\frac{1}{4}\\) \uadfc\ucc98\uc5d0\uc11c \uba38\ubb34\ub97c \uac83\uc774\ub2e4. \uc138\ubc88\uc9f8 factor\ub294 n\uc774 1\ubcf4\ub2e4 \ud06c\uac70\ub098 \uac19\ub2e4\uba74, \\(\\frac{1}{3}\\) \ubcf4\ub2e4 \uc791\ub2e4. \uace0\ub85c Posterior variance\ub294 prior variance\ubcf4\ub2e4 \uc791\uc740 \uac12\uc744 \uac16\ub294\ub2e4.","title":"2-5c."},{"location":"10%20Bayesian%20Statistics/HW2/#2-5d","text":"\uc0ac\uc804\ubd84\ud3ec\uac00 \ubca0\ud0c0\ubd84\ud3ec\ub97c \ub530\ub97c \ub54c, \uc0ac\ud6c4\ubd84\ud3ec\uc758 \ubd84\uc0b0\uc774 \uc0ac\uc804\ubd84\ud3ec\uc758 \ubd84\uc0b0\ubcf4\ub2e4 \uc791\uc740 \uac83\uc744 \ubcf4\uc774\ub294 \uacbd\uc6b0\uc758 \uc218\ub294 \ubb34\uc218\ud788 \ub9ce\ub2e4. \uc77c\ub840\ub85c n\uc774 \ub9e4\uc6b0 \ucee4\uc9c4\ub2e4\uba74, \uc0ac\ud6c4\ubd84\ud3ec\ub294 \uba85\ubc31\ud788 \uc0ac\uc804\ubd84\ud3ec\uc758 \ubd84\uc0b0\ubcf4\ub2e4 \uc791\uc740 \uac12\uc744 \uac00\uc9c0\uac8c \ub41c\ub2e4. \ub9cc\uc57d n\uacfc y\uac00 \ubaa8\ub450 1\uc778 \uacbd\uc6b0\ub97c \uc0dd\uac01\ud574\ubcf4\uc790. \uc774 \ub54c \uc0ac\uc804\ubd84\ud3ec\uac00 \\(Beta(1,4)\\) \ub97c \ub530\ub978\ub2e4\uba74, \uc0ac\ud6c4\ubd84\ud3ec\uc758 \ubd84\uc0b0\uc740 0.055\uc774\uba70, \uc0ac\uc804\ubd84\ud3ec\uc758 \ubd84\uc0b0\uc740 0.026\uc744 \uac16\ub294\ub2e4.","title":"2-5d."},{"location":"10%20Bayesian%20Statistics/HW2/#2-7","text":"","title":"2-7"},{"location":"10%20Bayesian%20Statistics/HW2/#2-7a","text":"\uc774\ud56d\ubd84\ud3ec\ub294 exponential family\uc5d0 \uc18d\ud558\uba70, natural parameter\uc5d0 \ub300\ud55c \ubd80\ubd84\uc740 \ub2e4\uc74c\uacfc \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\(\\phi(\\theta) = log(\\frac{\\theta}{1-\\theta})\\) . Uniform prior density\uc778 $\\phi(\\theta), p(\\theta) \\propto 1 $\ub294 \\(\\theta = e^{\\phi}/(1+e^{\\phi})\\) \ub85c \ud45c\ud604\uc774 \uac00\ub2a5\ud558\ub2e4. \uace0\ub85c, $ \\(q(\\theta) = p(\\frac{e^{\\phi}}{1+e^{\\phi}})|\\frac{d}{d\\theta}log(\\frac{\\theta}{1-\\theta})| \\propto \\theta^{-1}(1-\\theta)^{-1}\\) $.","title":"2-7a."},{"location":"10%20Bayesian%20Statistics/HW2/#2-7b","text":"\ub9cc\uc57d y\uac00 0 \uc774\ub77c\uba74, \\(p(\\theta|y) \\propto \\theta^{-1}(1-\\theta)^{n-1}\\) \ub294 \\(\\theta=0\\) \uc8fc\ubcc0\uc5d0\uc11c \ubb34\ud55c\ud788 \uc801\ubd84\ub418\uc5b4 \uacc4\uc0b0\uc774 \ubd88\uac00\ub2a5\ud558\uba70, y\uac00 n\uc778 \uacbd\uc6b0\uc5d0\ub294 \\(\\theta=1\\) \uc8fc\ubcc0\uc5d0\uc11c \uc704\uc640 \uac19\ub2e4.","title":"2-7b."},{"location":"10%20Bayesian%20Statistics/HW2/#2-8","text":"","title":"2-8"},{"location":"10%20Bayesian%20Statistics/HW2/#2-8a","text":"\\[\\theta|y \\sim N(\\frac{\\frac{1}{40^2}180+\\frac{n}{20^2}150}{\\frac{1}{40^2}+\\frac{n}{20^2}},\\frac{1}{\\frac{1}{40^2}+\\frac{n}{20^2}})\\]","title":"2-8a"},{"location":"10%20Bayesian%20Statistics/HW2/#2-8b","text":"\\[\\tilde{y} |y \\sim N(\\frac{\\frac{1}{40^2}180+\\frac{n}{20^2}150}{\\frac{1}{40^2}+\\frac{n}{20^2}},\\frac{1}{\\frac{1}{40^2}+\\frac{n}{20^2}}+20^2)\\]","title":"2-8b"},{"location":"10%20Bayesian%20Statistics/HW2/#2-8c","text":"95% posterior interval for \\(\\theta|y=150,\\ n=10:\\ 150.7 \\pm 1.96(6.25) = [138,163]\\) 95% posterior interval for \\(\\tilde{y}|y=150,\\ n=10:\\ 150.7 \\pm 1.96(20.95) = [110,192]\\)","title":"2-8c"},{"location":"10%20Bayesian%20Statistics/HW2/#2-8d","text":"95% posterior interval for \\(\\theta|y=150,\\ n=100:[146,154]\\) 95% posterior interval for \\(\\tilde{y}|y=150,\\ n=100:[111,189]\\)","title":"2-8d"},{"location":"10%20Bayesian%20Statistics/HW2/#2-9","text":"","title":"2-9"},{"location":"10%20Bayesian%20Statistics/HW2/#2-9a","text":"\\(\\alpha + \\beta = \\frac{E(\\theta)(1-E(\\theta))}{var(\\theta)}-1 = 1.67\\) \\(\\alpha = (\\alpha+\\beta)E(\\theta) = 1\\) \\(\\beta= (\\alpha+\\beta)(1-E(\\theta))=0.67\\) theta <- seq(0,1,.001) dens <- dbeta(theta,1,.67) plot (theta, dens, type=\"l\", xlab=\"theta\", ylab=\"\", yaxt=\"n\", cex=2) lines (c(1,1),c(0,3),col=0) lines (c(1,1),c(0,3),lty=3)","title":"2-9a."},{"location":"10%20Bayesian%20Statistics/HW2/#2-9b","text":"\\(n=1000,\\ y=650\\) \uc774\uba74, \\(p(\\theta|y)=Beta(\\alpha+650,\\beta+350)=Beta(651,350,67)\\) \uc774\ub2e4. theta <- seq(0,1,.001) dens <- dbeta(theta,651,350.67) cond <- dens/max(dens) > 0.001 plot (theta[cond], dens[cond], type=\"l\", xlab=\"theta\", ylab=\"\", yaxt=\"n\", cex=2)","title":"2-9b"},{"location":"10%20Bayesian%20Statistics/HW2/#2-10","text":"","title":"2-10"},{"location":"10%20Bayesian%20Statistics/HW2/#2-10a","text":"\\[p(data|N) = \\begin{cases} \\frac{1}{N}\\ if\\ N \\geq 203\\\\ 0\\ ohterwise \\end{cases} \\ \\] \\[p(N|data) \\propto p(n)p(data|N)\\] \\[=\\frac{1}{N}(0.01)(0.99)^{N-1}\\ for\\ N \\geq 203\\] $ \\(\\frac{1}{N}(0.99)^N\\ for\\ N \\geq 203\\) $.","title":"2-10a"},{"location":"10%20Bayesian%20Statistics/HW2/#2-10b","text":"$ \\(p(N|data) = c\\frac{1}{N}(0.99)^N\\) $ \\(\\sum_Np(N|data)=1\\) \uc774\ubbc0\ub85c, c\ub294 \ub2e4\uc74c\uc744 \ud1b5\ud574 \uacc4\uc0b0\ud560 \uc218 \uc788\ub2e4. $ \\(\\frac{1}{c} = \\sum_{N=203}^\\infty\\frac{1}{N}(0.99)^N\\) $ \uc774\ub294 analytically \ud558\uac8c \uacc4\uc0b0\ud560 \uc218 \uc788\uc9c0\ub9cc, \ucef4\ud4e8\ud130\uc758 numerical computation\uc73c\ub85c \ub3c4\ucd9c\ud558\ub294 \uac83\uc774 \ud6e8\uc52c \uc27d\ub2e4. \\[Approximation\\ on\\ the\\ computer:\\ \\sum_{N=203}^{1000}\\frac{1}{N}(0.99)^N = 0.04658\\] \\[Error\\ in\\ the\\ approximation:\\ \\sum_{N=1001}^\\infty\\frac{1}{N}(0.99)^N < \\frac{1}{1001}\\sum_{N=1001}^\\infty(0.99)^N\\] $ \\(=4.3 \\times 10^{-6}\\) $ \uace0\ub85c \\(\\frac{1}{c} = 0.04658,\\ c=21.47\\) \uc774\ub2e4. \\[E(N|data) = \\sum_{N=203}^\\infty Np(N|data)\\] \\[c\\sum_{N=203}^\\infty(0.99)^N = 21.47\\cdot\\frac{(0.99)^{203}}{1-0.99} = 279.1\\] \\[sd(N|data) = \\sqrt{\\sum_{N=203}^\\infty (N-279.1)^2c\\frac{1}{N}(0.99)^N}\\] $ \\(\\approx \\sqrt{\\sum_{N=203}^{1000}(N-279.1)^221.47\\frac{1}{N}(0.99)^N} = 79.6\\) $.","title":"2-10b"},{"location":"10%20Bayesian%20Statistics/HW2/#2-10c","text":"improper discrete uniform prior density on \\(N:p(N) \\propto 1\\) \uc774 \ud55c\uac00\uc9c0 \ubc29\ubc95\uc774 \ub420 \uc218 \uc788\ub2e4. \uc774 density\ub294 improper posterior density($p(N) \\propto \\frac{1}{N},\\ for\\ N \\geq203 $)\ub97c \uac16\ub294\ub2e4. prior( \\(p(N) \\propto 1/N\\) ) \ub294 improper\ud558\uc9c0\ub9cc, proper prior density\ub97c \uac16\ub294\ub370, \\(\\sum_N1/N^2\\) \uac00 \uc218\ub834\ud558\uae30 \ub54c\ubb38\uc774\ub2e4. \ub610\ud55c data point\uac00 \ud55c \uac1c \uc774\uc0c1\uc774\ub77c\uba74, \uc0ac\ud6c4\ubd84\ud3ec\ub294 \uc704\uc758 \uc0ac\uc804\ubd84\ud3ec\ub97c \uac16\ub294 \ud558\uc5d0\uc11c proper\ud558\uba70 data point\uac00 \ub2e8 \ud558\ub098\ubfd0\uc774\ub77c\uba74 noninformative prior\ub97c \uc4f0\ub294 \uac83\ubcf4\ub2e4 \uc2e4\uc6a9\uc801\uc774\uc9c0 \ubabb\ud55c \uacb0\uacfc\ub97c \uac16\ub294\ub2e4.","title":"2-10c"},{"location":"10%20Bayesian%20Statistics/HW2/#2-11","text":"","title":"2-11"},{"location":"10%20Bayesian%20Statistics/HW2/#2-11a","text":"dens <- function (y, th){ dens0 <- NULL for (i in 1:length(th)) dens0 <- c(dens0, prod (dcauchy (y, th[i], 1))) dens0} y <- c(-2, -1, 0, 1.5, 2.5) step <- .01 theta <- seq(step/2, 1-step/2, step) dens.unnorm <- dens(y,theta) dens.norm <- dens.unnorm/(step*sum(dens.unnorm)) plot (theta, dens.norm, ylim=c(0,1.1*max(dens.norm)), type=\"l\", xlab=\"theta\", ylab=\"normalized density\", xaxs=\"i\", yaxs=\"i\", cex=2)","title":"2-11a"},{"location":"10%20Bayesian%20Statistics/HW2/#2-11b","text":"thetas <- sample (theta, 1000, step*dens.norm, replace=TRUE) hist (thetas, xlab=\"theta\", yaxt=\"n\", breaks=seq(0,1,.05), cex=2)","title":"2-11b"},{"location":"10%20Bayesian%20Statistics/HW2/#2-11c","text":"y6 <- rcauchy (length(thetas), thetas, 1) hist (y6, xlab=\"new observation\", yaxt=\"n\", nclass=100, cex=2)","title":"2-11c"},{"location":"10%20Bayesian%20Statistics/HW2/#2-12","text":"\ud3ec\uc544\uc1a1 \ubd84\ud3ec\uc758 pdf\ub294 \ub2e4\uc74c\uacfc \uac19\uc73c\uba70 \\(p(y|\\theta) = \\theta^ye^{-\\theta}/y!\\) , \\(J(\\theta)=E(d^2logp(y|\\theta)/d\\theta^2|\\theta)=E(y/\\theta^2)=1/\\theta\\) \uc774\ub2e4. \uc774\ub294 improper gamma density \\(\\Gamma(1/2,0)\\) \uc640 \uc0c1\uc751\ud55c\ub2e4.","title":"2-12"},{"location":"10%20Bayesian%20Statistics/HW2/#2-13","text":"","title":"2-13"},{"location":"10%20Bayesian%20Statistics/HW2/#2-13a","text":"\\(y_i\\) \ub97c fatal accidents in year i\uc758 \ube48\ub3c4\ub77c \ud558\uace0 \\(\\theta\\) \ub97c \ud55c \ud574\uc5d0 \uc77c\uc5b4\ub0a0 \uc0ac\uace0\ube48\ub3c4\uc758 \uae30\ub313\uac12\uc774\ub77c \ud558\uc790. \uadf8\ub807\ub2e4\uba74 \\(y_i|\\theta \\sim Pois(\\theta)\\) \ub97c \uac16\ub294\ub2e4. \ud3b8\uc758\ub97c \uc704\ud574 conjugate family\ub97c \uc774\uc6a9\ud558\uc790. \ub9cc\uc57d \\(\\theta\\) \uc758 \uc0ac\uc804\ubd84\ud3ec\uac00 \\(\\Gamma(\\alpha,\\beta)\\) \ub77c\uba74, \uc0ac\ud6c4\ubd84\ud3ec\ub294 \\(\\Gamma(\\alpha+10\\bar{y},\\beta+10)\\) \uc744 \uac16\ub294\ub2e4. \\(\\alpha,\\beta\\) \uac00 \ubaa8\ub450 0\uc778 noninformative prior distribution\uc744 \uac00\uc815\ud558\uc790.(n=10) \uc0ac\ud6c4\ubd84\ud3ec\ub294 \\(\\theta|y \\sim \\Gamma(238,10)\\) \uc744 \uac16\ub294\ub2e4. \\(\\tilde{y}\\) \ub97c 1986\ub144\uc758 fatal accidents\uc758 \ud69f\uc218\ub77c \ud558\uc790. \\(\\theta\\) \uac00 \uc8fc\uc5b4\uc84c\uc744 \ub54c predictive distribution for \\(\\tilde{y}\\) \ub294 \\(Pois(\\theta)\\) \ub97c \uac16\ub294\ub2e4. \uc544\ub798\ub294 \\(\\tilde{y}\\) \uc758 95% posterior interval\uc744 \uad6c\ud558\ub294 \ub450 \uac00\uc9c0 \ubc29\ubc95\uc774\ub2e4. Simulation theta <- rgamma(1000,238)/10 y1986 <- rpois(1000,theta) print (sort(y1986)[c(25,976)]) ## [1] 15 34 Normal approximation \uac10\ub9c8\ubd84\ud3ec\ub85c\ubd80\ud130 \\(E(\\theta|y) = 238/10=23.8,\\ sd(\\theta|y)=\\sqrt{238/10} =1.54\\) \ub97c \uc5bb\uc744 \uc218 \uc788\uc73c\uba70 \ud3ec\uc544\uc1a1 \ubd84\ud3ec\ub85c\ubd80\ud130 \\(E(\\tilde{y}|\\theta)=\\theta,\\ sd(\\tilde{y}|\\theta)=\\sqrt{\\theta}\\) \ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4. Formula (1.6)\uacfc (1.7)\ub85c \ubd80\ud130 \ub2e4\uc74c\uacfc \uac19\uc740 \uc2dd\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4. \\[E(\\tilde{y}|y)=E(E(\\tilde{y}|\\theta,y|y)=E(\\theta|y)=23.8\\] \\[var(\\tilde{y}|y) = E(var(\\tilde{y}|\\theta,y)|y)+var(E(\\tilde{y}|\\theta,y)|y)\\] $ \\(=E(\\theta|y)+var(\\theta|y) = 26.2=5.12^2\\) $. \\(p(\\tilde{y}|y)\\) \uc758 \uc815\uaddc\uadfc\uc0ac\ub97c \ud1b5\ud574 \\(\\tilde{y}\\) \uc758 95% interval\uc740 \\([23.8 \\pm 1.96(5.12)] = [13.8,33.8]\\) \ub97c \uac16\ub294\ub2e4. \\(\\tilde{y}\\) \ub294 \ubc18\ub4dc\uc2dc \uc815\uc218\uc5ec\uc57c\ud558\uba70, 95% \uc2e0\uc6a9\uad6c\uac04\uc740 \ucd5c\uc18c\ud55c [13,34]\ub97c \ud3ec\ud568\ud55c\ub2e4.","title":"2-13a"},{"location":"10%20Bayesian%20Statistics/HW2/#2-13b","text":"1976\ub144\uacfc 1977\ub144\uc758 \ucd94\uc815\ub41c numbers of passenger miles\ub294 \uac01\uac01 \\(3.863 \\times 10^{11},\\ 4.3 \\times 10^{11}\\) \uc774\ub2e4. \\(x_i\\) =number of passenger miles flown in year i, \\(\\theta\\) =expected accident rate per passenger mile\uc774\ub77c\uace0 \ud558\uc790. \\(y_i|x_i,\\theta \\sim Pois(x_i\\theta)\\) . \\(\\theta\\) \uc5d0 \ub300\ud55c prior\ub85c \\(\\Gamma(0,0)\\) \uc744 \uc0ac\uc6a9\ud558\uba74 \uc774\uc5d0 \ub300\ud55c \uc0ac\ud6c4\ubd84\ud3ec\ub294 \uc544\ub798\uc640 \uac19\ub2e4. $ \\(y|\\theta \\sim \\Gamma(10\\bar{y},10\\bar{x})=\\Gamma(238,5.716 \\times 10^{12})\\) $. Given \\(\\theta\\) \uc5d0\uc11c predictive distribution for \\(\\tilde{y}\\) \ub294 \\(Pois(\\tilde{x}\\theta)=Pois(8\\times10^{11}\\theta)\\) \uc774\ub2e4. \\(tilde{y}\\) \uc758 95% posterior interval\uc744 \uad6c\ud558\ub294 \ub450 \uac00\uc9c0 \ubc29\ubc95\uc740 \uc544\ub798\uc640 \uac19\ub2e4. Simulation theta <- rgamma(1000,238)/5.716e12 y1986 <- rpois(1000,theta*8e11) print (sort(y1986)[c(25,976)]) ## [1] 22 47 Normal Approximation \\(E(\\theta|y) = 238/(5.716 \\times 10^{12}) = 4.164 \\times 10^{-11},\\ sd(\\theta|y) = \\sqrt{238}/(5.716 \\times 10^{12}) = 0.270 \\times 10^{-11}\\) , \\(E(\\tilde{y}|\\theta)=(8\\times 10^{11})\\theta,\\ sd(\\tilde{y}|\\tilde{x},\\theta)=\\sqrt{(8 \\times 10^{11})\\theta)}\\) . \\[E(\\tilde{y}|y) = E(E(\\tilde{y}|\\theta,y)|y) = 33.3\\] \\[var(\\tilde{y}|y) = E(var(\\tilde{y}|\\theta,y)|y) +var(E(\\tilde{y}|\\theta,y)|y)\\] $ \\((8 \\times 10^{11})(4.164 \\times 10^{-11})+(8 \\times 10^{11})(0.270 \\times 10^{-11})^2 = 38=6.2^2\\) $. \\(\\tilde{y}\\) \ub294 \uc815\uc218\uac12\uc744 \uac00\uc838\uc57c \ud558\ubbc0\ub85c, interval\uc740 \uc801\uc5b4\ub3c4 [21,46]\uc744 \ud3ec\ud568\ud574\uc57c\ud55c\ub2e4.","title":"2-13b"},{"location":"10%20Bayesian%20Statistics/HW2/#2-13c","text":"238\uc744 6919(total number of deaths in the data)\ub85c \ub300\uccb4\ud558\uc5ec (a)\ub97c \ubc18\ubcf5\ud558\uba74, 1000\ubc88\uc758 simulation\uc758 \uacb0\uacfc\ub294 95% posterior interval [638,750]\uc744 \uac16\ub294\ub2e4.","title":"2-13c"},{"location":"10%20Bayesian%20Statistics/HW2/#2-13d","text":"\uc704\uc758 c\uc640 \uac19\uc774 \uac12\uc744 \ub300\uccb4\ud558\uc5ec (b)\ub97c \ubc18\ubcf5\ud558\uba74, 95% posterior interval\uc740 [900,1035]\uc744 \uac16\ub294\ub2e4.","title":"2-13d"},{"location":"10%20Bayesian%20Statistics/HW2/#2-13e","text":"\ud55c \ud574\uc5d0 \ub354 \ub9ce\uc740 mile\uc744 \ube44\ud589\ud560\uc218\ub85d \ub354 \ub9ce\uc740 \uc0ac\uace0\uac00 \uae30\ub300\ub418\uae30\uc5d0, \uc77c\ubc18\uc801\uc73c\ub85c model (b)\uc640 (d)\uc5d0\uc11c\ub294 Poisson model\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ubcf4\ub2e4 \ud569\ub9ac\uc801\uc774\ub2e4. \ubc18\uba74 \ub9cc\uc57d \ud56d\uacf5\uc0ac\uc758 \uc548\uc804\uc774 \uac1c\uc120\ub41c\ub2e4\uba74, \uc0ac\uace0\uc728\uc774 \uc2dc\uac04\uc5d0 \ub530\ub77c \uac10\uc18c\ud560 \uac83\uc73c\ub85c \uae30\ub300\ub418\uba70, \uc0ac\uace0\ube48\ub3c4\uc758 \uae30\ub313\uac12\uc740 roughly\ud558\uac8c constant\uac12\uc744 \uac00\uc9c8 \uac83\uc774\ub2e4. \uc774\uc5d0\ub294 time trend\ub97c \ubaa8\ub378\uc5d0 \ub123\ub294 \uac83\uc774 \ud569\ub9ac\uc801\uc73c\ub85c \uc0ac\ub8cc\ub41c\ub2e4. \ub610\ud55c \uc0ac\uace0\ub294 \ub3c5\ub9bd\uc801\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c0\ub294 \uac83\uc73c\ub85c \uae30\ub300\ub41c\ub2e4. \ud5c8\ub098 \uc2b9\uac1d\ub4e4\uc758 \uc0ac\ub9dd\uc740 \ub3c5\ub9bd\uc774 \uc544\ub2c8\ub2e4.(\ube44\ud589\uae30\uc0ac\uace0\uac00 \ub098\uba74 \ud0d1\uc2b9\uac1d\ub4e4\uc740 \ud55c \ubb36\uc74c\uc5d0 \uc5ee\uc5ec\uc788\uc73c\ubbc0\ub85c) \uace0\ub85c \ud3ec\uc544\uc1a1 \ubaa8\ub378\uc740 \ucd1d\uc0ac\ub9dd\ube48\ub3c4\ubcf4\ub2e4\ub294 \uc0ac\uace0\ube48\ub3c4\uc5d0 \uc801\uc6a9\ud558\ub294 \uac83\uc774 \ud569\ub9ac\uc801\uc774\ub2e4. \ub9cc\uc57d \uad00\uc2ec\ubd84\uc57c\uac00 \ucd1d\uc0ac\ub9dd\uc790\ub77c\uba74 \uc5b4\ub5bb\uac8c \ub420 \uac83\uc778\uac00? \uc774\ub54c\ub294 compound model\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ud569\ub9ac\uc801\uc778\ub370, \uc608\ub97c \ub4e4\uc5b4 accidents\uc5d0 \ub300\ud574\uc11c\ub294 \ud3ec\uc544\uc1a1\ubd84\ud3ec\ub97c \uc801\uc6a9\ud558\uace0, \uc0ac\uace0\uac00 \uc77c\uc5b4\ub0ac\uc744 \ub54c \uc0ac\ub9dd\uc5d0 \ub300\ud558\uc5ec\ub294 \ub2e4\ub978 \ubd84\ud3ec\ub97c \uc801\uc6a9\ud558\ub294 \uac83\uc774\ub2e4.","title":"2-13e"},{"location":"10%20Bayesian%20Statistics/HW2/#2-16","text":"","title":"2-16"},{"location":"10%20Bayesian%20Statistics/HW2/#2-16a","text":"\\[p(y) \\int p(y|\\theta)p(\\theta)d\\theta\\] \\[\\int_0^1\\binom{n}{y}\\theta^y(1-\\theta)^{n-y}\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\int_0^1\\theta^{\\alpha+1}(1-\\theta)^{\\beta-1}d\\theta\\] \\[\\frac{\\Gamma(n+1)}{\\Gamma(y+1)\\Gamma(n-y+1)}\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\int_0^1\\theta^{y+\\alpha-1}(1-\\theta)^{n-y+\\beta-1}d\\theta\\] \\[\\frac{\\Gamma(n+1)}{\\Gamma(y+1)\\Gamma(n-y+1)}\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\frac{\\Gamma(y+\\alpha)\\Gamma(n-y+\\beta)}{\\Gamma(n+\\alpha+\\beta)}\\]","title":"2-16a"},{"location":"10%20Bayesian%20Statistics/HW2/#2-16b","text":"\uc6b0\ub9ac\ub294 y\uc5d0 \ub530\ub77c \\(p(y)\\) \uac00 \uc5b4\ub5bb\uac8c \ubcc0\ud558\ub294\uc9c0\ub97c \uc54c\uace0 \uc2f6\uae30 \ub54c\ubb38\uc5d0, \\(p(y)\\) factor\uc5d0 \uad00\uc2ec\uc744 \uac16\ub294\ub2e4. \uace0\ub85c \\(\\Gamma(a+y)\\Gamma(b+n-y)/\\Gamma(y+1)\\Gamma(n-y+1)\\) \uac00 y\uc5d0 \ub530\ub77c \ubcc0\ud558\ub294 \uac83\uc744 \ud655\uc778\ud558\uba74 \ub41c\ub2e4. \uc704\uc758 \uc2dd\uc740 \\(a=b=1\\) \uc77c \ub54c 1\uc774\uba70, \\(p(y)\\) \ub294 \uc774 \ub54c constant\uc774\ub2e4. \\(p(y)\\) \uac00 in y\uc5d0\uc11c constant\ub77c \uac00\uc815\ud574\ubcf4\uc790. (\ud2b9\ud788 \\(p(0)=p(n)\\) \uacfc \\(p(0)=p(1)\\) \uc5d0\uc11c) \uccab\ubc88\uc9f8 \uc2dd\uc744 \ud1b5\ud574 \ub2e4\uc74c\uacfc \uac19\uc740 \uc2dd \ub3c4\ucd9c\uc774 \uac00\ub2a5\ud558\ub2e4. $ \\(\\Gamma(a)\\Gamma(b+n)=\\Gamma(a+n)\\Gamma(b)\\) $. \\(\\Gamma(t)=(t-1)\\Gamma(t-1)\\) \uc744 \ub5a0\uc62c\ub824\ubcf8\ub2e4\uba74, \uc6b0\ub9ac\ub294 \\(\\Gamma(a)\\Gamma(b)(b+n-1)\\cdots(b+1)b=\\Gamma(a)\\Gamma(b)(a+n-1)\\cdots(a+1)a\\) \ub97c \uc131\ub9bd\uc2dc\ucf1c\uc57c\ud55c\ub2e4. \uc5ec\uae30\uc11c \ubaa8\ub4e0 product\uc758 \ubaa8\ub4e0 \ud56d\ub4e4\uc774 positive\uc774\uae30\ub54c\ubb38\uc5d0, \\(a=b\\) \uc774\ub2e4. \\(p(0)=p(1)\\) \uc740 \\(\\Gamma(a)\\Gamma(b+n)/\\Gamma(1)\\Gamma(n+1)=\\Gamma(a+1)\\Gamma(b+n-1)/\\Gamma(2)\\Gamma(n)\\) \uc744 \ub0b4\ud3ec\ud558\uba70, \uc704\uc640 \uac19\uc740 \ubc29\uc2dd\uc73c\ub85c \\(b+n-1=na\\) \ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4. \\(a=b\\) \uc774\uae30 \ub54c\ubb38\uc5d0, \uc774\ub294 \\(a+n-1=na\\) \ub85c \uc815\ub9ac\ub418\uba70, \uc774 \uc2dd\uc758 \uc720\uc77c\ud55c \ud574\ub294 \\(a=1\\) \uc774\ub2e4. \uadf8\ub7ec\ubbc0\ub85c \\(a=b=1\\) \uc740 \\(p(y)\\) \uac00 constant in y\uac00 \ub418\uae30 \uc704\ud55c \ud544\uc694\ucda9\ubd84\uc870\uac74\uc774\ub2e4.","title":"2-16b"},{"location":"10%20Bayesian%20Statistics/HW2/#2-17","text":"","title":"2-17"},{"location":"10%20Bayesian%20Statistics/HW2/#2-17a","text":"Let \\(\\mu = \\sigma^2\\) , so that \\(\\sigma = \\sqrt{\\mu}\\) . Then \\(p(\\sigma^2)=p(\\mu)=p(\\sqrt{\\mu})d\\sqrt{\\mu}/d\\mu=p(\\sigma)(1/2)\\mu^{-1/2}=(1/2)p(\\sigma)/\\sigma\\) , which is proportional to \\(1/\\sigma^2\\) if \\(p(\\sigma) \\propto \\sigma^{-1}\\) .","title":"2-17a"},{"location":"10%20Bayesian%20Statistics/HW2/#2-17b","text":"\\(p(\\sigma/data) \\propto \\sigma^{-1-n}exp(-c/\\sigma^2)\\) , which may be written as \\((\\sigma^2)^{-1/2-n/2}exp(-c/\\sigma^2)\\) . \\(p(\\sigma^2|data) \\propto (\\sigma^2)^{-1-n/2}exp(-c/\\sigma^2)\\) . We have defined \\(c=nv/2\\) , and assume this quantity to be positive. Let \\((\\sqrt{a},\\sqrt{b})\\) be the 95% interval of highest density for \\(p(\\sigma|data)\\) . Then $ \\(a^{-1/2-n/2}exp(-c/a)=b^{-1/2-n/2}exp(-c/b)\\) $. Equivalently, \\((-1/2-n/2)loga-c/a=(-1/2-n/2)logb-c/b\\) . if (a,b) were the 95% interval of highest density for \\(p(\\sigma^2|data)\\) , then \\[a^{-1-n/2}exp(-c/a)=b^{-1-n/2}exp(-c/b)\\] That is, \\((-1-n/2)loga-c/a=(-1-n/2)logb-c/b\\) . Combining the two equations, \\(1/2loga=1/2logb\\) , so that \\(a=b\\) , in which case [a,b] cannot be a 95% interval, and we have a contradiction.","title":"2-17b"},{"location":"10%20Bayesian%20Statistics/HW2/#2-20","text":"","title":"2-20"},{"location":"10%20Bayesian%20Statistics/HW2/#2-20a","text":"\\[p(\\theta|y \\geq 100) \\propto p(y \\geq 100|\\theta)p(\\theta)\\] \\[\\propto exp(-100\\theta)\\theta^{\\alpha-1}exp(-\\beta\\theta)\\] \\[p(\\theta|y \\geq 100) = \\Gamma(\\theta|\\alpha,\\beta+100)\\]","title":"2-20a"},{"location":"10%20Bayesian%20Statistics/HW2/#2-20b","text":"\\[p(\\theta|y=100) \\propto p(y=100|\\theta)p(\\theta)\\] \\[\\propto \\theta exp(-100\\theta)\\theta^{\\alpha-1}exp(-\\beta\\theta)\\] \\[p(\\theta|y=100) = \\Gamma(\\theta|\\alpha+1,\\beta+100)\\]","title":"2-20b"},{"location":"10%20Bayesian%20Statistics/HW2/#2-20c","text":"The variance of \\(\\theta\\) decreases given more information : in this case, \\[E(var(\\theta|y)|y \\geq 100) \\leq var(\\theta|y \\geq 100)\\] Plugging in y=100 to get \\(var(\\theta|y=100)\\) is not the sa,e as averaging over the distribution of \\(y|y \\geq 100\\) on the left side of above inequality.","title":"2-20c"},{"location":"10%20Bayesian%20Statistics/HW2/#probability-of-a-girl-birth-given-placenta-previa","text":"library(ggplot2) theme_set(theme_minimal()) df1 <- data.frame(theta = seq(0.375, 0.525, 0.001)) a <- 438 b <- 544 # dbeta computes the posterior density df1$p <- dbeta(df1$theta, a, b) df2 <- data.frame(theta = seq(qbeta(0.025, a, b), qbeta(0.975, a, b), length.out = 100)) # compute the posterior density df2$p <- dbeta(df2$theta, a, b) ggplot(mapping = aes(theta, p)) + geom_line(data = df1) + # Add a layer of colorized 95% posterior interval geom_area(data = df2, aes(fill='1')) + # Add the proportion of girl babies in general population geom_vline(xintercept = 0.488, linetype='dotted') + # Decorate the plot a little labs(title='Uniform prior -> Posterior is Beta(438,544)', y = '') + scale_y_continuous(expand = c(0, 0.1), breaks = NULL) + scale_fill_manual(values = 'lightblue', labels = '95% posterior interval') + theme(legend.position = 'bottom', legend.title = element_blank())","title":"Probability of a girl birth given placenta previa"},{"location":"10%20Bayesian%20Statistics/HW2/#illustrate-the-effect-of-prior-in-binomial-model","text":"library(ggplot2) theme_set(theme_minimal()) library(tidyr) a <- 437 b <- 543 df1 <- data.frame(theta = seq(0.375, 0.525, 0.001)) df1$pu <- dbeta(df1$theta, a+1, b+1) n <- c(2, 20, 200) # prior counts apr <- 0.488 # prior ratio of success helperf <- function(n, apr, a, b, df) cbind(df, pr = dbeta(df$theta, n*apr, n*(1-apr)), po = dbeta(df$theta, n*apr + a, n*(1-apr) + b), n = n) df2 <- lapply(n, helperf, apr, a, b, df1) %>% do.call(rbind, args = .) %>% gather(grp, p, -c(theta, n), factor_key = T) df2$title <- factor(paste0('alpha/(alpha+beta)=0.488, alpha+beta=',df2$n)) levels(df2$grp) <- c('Posterior with unif prior', 'Prior', 'Posterior') ggplot(data = df2) + geom_line(aes(theta, p, color = grp)) + geom_vline(xintercept = 0.488, linetype = 'dotted') + facet_wrap(~title, ncol = 1) + labs(x = '', y = '') + scale_y_continuous(breaks = NULL) + theme(legend.position = 'bottom', legend.title = element_blank())","title":"Illustrate the effect of prior in binomial model"},{"location":"10%20Bayesian%20Statistics/HW2/#derive-jeffereys-prior-for-binomial-distribution","text":"\\[p(y|\\theta = \\binom{n}{y}\\theta^y(1-\\theta)^{n-y}\\] Reparameterize the model with \\(\\phi=g(\\theta)\\) to get $ \\(p(y|\\phi) = \\binom{n}{y}h(\\phi)^y(1-h(\\phi))^{n-y}\\) $. Denote \\(p_J(\\phi)\\) Jeffrey's prior, $ \\(p_J(\\phi) = p_J(h(\\phi))|\\frac{dh}{d\\phi}|\\) $. \\[l: log(p(y|\\theta)) \\propto ylog(\\theta) + (n-y)log(1-\\theta)\\] \\[\\frac{\\partial l}{\\partial \\theta}=\\frac{y}{\\theta}-\\frac{n-y}{1-\\theta}\\] \\[\\frac{\\partial^2 l}{\\partial \\theta^2}=-\\frac{y}{\\theta^2}-\\frac{n-y}{(1-\\theta)^2}\\] \\[I(\\theta) = -E(\\frac{\\partial^2 l}{\\partial \\theta^2}|\\theta)\\] \\[\\frac{n\\theta}{\\theta^2}+\\frac{n-n\\theta}{(1-\\theta)^2}\\] \\[\\frac{n}{\\theta(1-\\theta)}\\] \\[\\theta^{-1}(1-\\theta)^{-1}\\] \\[p_J(\\theta) = \\sqrt{I(\\theta)}\\] \\[\\propto \\theta^{-1/2}(1-\\theta)^{-1/2}\\] which is \\(Beta(1/2,1/2)\\)","title":"Derive Jefferey's prior for Binomial Distribution."},{"location":"10%20Bayesian%20Statistics/HW3/","text":"Bayes Statistics HW3 1 1-(a) Write the marginal posterior distribution for \\(\\alpha\\) Lets denote \\(p(\\theta)\\) as \\(Dirichlet(a_1,..,a_n)\\) . Then posterior distribution is \\(p(\\theta|y) = Dirichlet(y_1+a_1,..,y_n+a_n\\) . By the properties of Dirichlet, marginal posterior dist of ( \\(\\theta_1,\\theta_2,1-\\theta_1-\\theta_2\\) ) is also Dirichlet : \\(p(\\theta_1,\\theta_2|y) \\propto \\theta_1^{y_1+a_1-1}\\theta_2^{y_2+a_2-1}(1-\\theta_1-\\theta_2)^{y_{rest}+a_{rest}-1}\\) , where \\(y_{rest}=y_3+..y_J\\) , \\(a_{rest}=a_3+..a_J\\) . do a variable transformation to \\((\\alpha,\\beta)=(\\frac{\\theta_1}{\\theta_1+\\theta_2},\\theta_1+\\theta_2)\\) . Jacobian is \\(|1/\\beta|\\) , \\[p(\\alpha,\\beta|y) \\propto \\beta(\\alpha\\beta)^{y_1+a_1-1}((1-\\alpha)\\beta)^{y_2+a_2-1}(1-\\beta)^{y_{rest}+a_{rest}-1}\\] \\[= \\alpha^{y_1+a_1-1}(1-\\alpha)^{y_2+a_2-1}\\beta_{y_1+y_2+a_1+a_2-1}(1-\\beta)^{y_{rest}+a_{rest}-1}\\] $ \\(\\propto Beta(\\alpha|y_1+a_1,y_2+a_2)Beta(\\beta|y_1+y_2+a_1+a_2,y+{rest}+a_{rest})\\) $ Posterior density is separated by two factors for \\(\\alpha\\) and \\(\\beta\\) , they are independent, therefore \\(\\alpha|y \\sim Beta(y_1+a_2,y_2+a_2)\\) . (1-b) show that this distribution is identical to the posterior distribution for \\(\\alpha\\) obtained by treating \\(y_1\\) as an observation from the binomial distribution with probability \\(\\alpha\\) and sample size \\(y_1+y_2\\) , ignoring the data \\(y_3,...,j_J.\\) The \\(Beta(y_1+a_1,y_2+a_2)\\) posterior distribution also be derived from a \\(Beta(a_1,a_2)\\) prior dist and a binomial observation \\(y_1\\) with sample size \\(y_1+y_2\\) . 2 Assume independent uniform prior distributions on the multinomial parameters. Then the posteriors are independent multinomial : $ \\((\\pi_1,\\pi_2,\\pi_3)|y \\sim Dirichlet(295,308,39)\\) $ $ \\((\\pi_1^*,\\pi_2^*,\\pi_3^*)|y \\sim Dirichlet(289,333,20)\\) $, and \\(\\alpha_1=\\frac{\\pi_1}{\\pi_1+\\pi_2},\\alpha_2=\\frac{\\pi_1^*}{\\pi_1^*+\\pi_2^*}\\) . From the properties of Dirichlet distribution, \\(\\alpha_1|y \\sim Beta(295,308)\\) \\(\\alpha_2|y \\sim Beta(289,333)\\) . set.seed(2020311194) alpha.1 <- rbeta(2000,295,308) alpha.2 <- rbeta(2000,289,333) diff <- alpha.2- alpha.1 hist(diff,xlab=\"alpha2-alpha1\",yaxt=\"n\", breaks = seq(-.15,.09,.01),cex=2) print (mean(diff>0)) ## [1] 0.191 This is histogram of 2000 draws from posterior. Based on histogram, posterior probability that there was a shift toward Bush is 19.4%. If we use normal approximations for the dist of \\(\\alpha_1\\) and \\(\\alpha_2\\) with means and st.d computed from relevant beta dist, can get the same answer. 3 3-(a) Data dist is \\(p(y|\\mu_c,\\mu_t,\\sigma_c,\\sigma_t)= \\prod_{i=1}^32N(y_{ci}|\\mu_c,\\sigma_c^2)\\prod_{i=1}^{36}N(y_{ti}|\\mu_t,\\sigma_t^2)\\) . \\((\\mu_c,\\sigma_c)\\) are independent of \\((\\mu_t,\\sigma_t)\\) in the posterior. So in this model, we can analyze two separately. The marginal posterior for \\(\\mu_c\\) and \\(\\mu_t\\) are : \\[\\mu_c|y \\sim t_{31}(1.013,0.24^2/32)\\] $ \\(\\mu_t|y \\sim t_{35}(1.173,0.20^2/36)\\) $. 3-(b) mu.c <- 1.013 + (0.24/sqrt(32))*rt(1000,31) mu.t <- 1.173 + (0.20/sqrt(36))*rt(1000,35) dif <- mu.t - mu.c hist (dif, xlab=\"mu_t - mu_c\", yaxt=\"n\", breaks=seq(-.1,.4,.02), cex=2) print (sort(dif)[c(25,976)]) ## [1] 0.05868759 0.26396430 Based on histogram, 95% posterior interval for average treatment effect is [0.05,0.27] 4 4-(a),(b) Set Uniform prior for our model's prior. In binomial model, Uniform can be noninformative prior. Let Uniform prior( \\(Beta(1/2,1/2)\\) ) as our prior. Then \\(p(p_0) \\propto 1,\\ p(p_1) \\propto 1\\) , \\(p(p_0,p_1|n_i,y_i) \\propto p(p_0,p_1)\\prod_{i=1}^2f(y_i|p_0,p_1)\\) \\(\\propto p_0^{39}(1-p_0)^{674}p_1^{22}(1-p_1)^{680}\\) . Two experiments are independent, they can dealt seperately. \\(p(p_0) \\sim Beta(40,675)\\) \\(p(p_1) \\sim Beta(23,681)\\) set.seed(2020311194) p0 <- rbeta(1000,40,675) p1 <- rbeta(1000,22,680) hist(p0, breaks=20, col='grey', yaxt='n',main=\"p0\") hist(p1, breaks=20, col='grey', yaxt='n',main=\"p1\") odds_r <- (p1/(1-p1))/(p0/(1-p0)) hist(odds_r,breaks=30,col='grey',yaxt='n',main='Odds Ratio') 4-(c) #Compare Jeffrey and Uniform prior set.seed(2020311194) p0 <- rbeta(1000,39.5,674.5) p1 <- rbeta(1000,21.5,679.5) odds_r1 <- (p1/(1-p1))/(p0/(1-p0)) mean(odds_r) ; mean(odds_r1) ## [1] 0.564724 ## [1] 0.559396 var(odds_r) ; var(odds_r1) ## [1] 0.02306329 ## [1] 0.0235932 quantile(odds_r,c(0.25,0.5,0.75,0.99)) ; quantile(odds_r1,c(0.25,0.5,0.75,0.99)) ## 25% 50% 75% 99% ## 0.4557233 0.5458081 0.6511201 1.0087703 ## 25% 50% 75% 99% ## 0.4500328 0.5366413 0.6471813 1.0028043 Comparing with Jeffrey's prior and my choice, difference is small. 5 5-(a) \\(p(\\sigma^2|y) \\sim Inv-\\chi^2(n-1,s^2),\\ p(\\mu|\\sigma^2,y) \\sim N(\\bar{y},\\sigma^2/n)\\) with n=5, \\(\\bar{y}\\) =10.4, \\(s^2\\) = 1.3. 5-(b) \\[p(\\mu,\\sigma^2|y) \\propto \\frac{1}{\\sigma^2}\\prod_{i=1}^n(\\Phi(\\frac{y_i+0.5-\\mu}{\\sigma})-\\Phi(\\frac{y_i-0.5-\\mu}{\\sigma}))\\] 5-(c) post.a <- function(mu,sd,y){ ldens <- 0 for (i in 1:length(y)) ldens <- ldens + log(dnorm(y[i],mu,sd)) ldens} post.b <- function(mu,sd,y){ ldens <- 0 for (i in 1:length(y)) ldens <- ldens + log(pnorm(y[i]+0.5,mu,sd) - pnorm(y[i]-0.5,mu,sd)) ldens} summ <- function(x){c(mean(x),sqrt(var(x)), quantile(x, c(.025,.25,.5,.75,.975)))} nsim <- 2000 y <- c(10,10,12,11,9) n <- length(y) ybar <- mean(y) s2 <- sum((y-mean(y))^2)/(n-1) mugrid <- seq(3,18,length=200) logsdgrid <- seq(-2,4,length=200) contours <- c(.0001,.001,.01,seq(.05,.95,.05)) logdens <- outer (mugrid, exp(logsdgrid), post.a, y) dens <- exp(logdens - max(logdens)) contour (mugrid, logsdgrid, dens, levels=contours, xlab=\"mu\", ylab=\"log sigma\", label=0, cex=2) mtext (\"Posterior density, ignoring rounding\", 3) sd <- sqrt((n-1)*s2/rchisq(nsim,4)) mu <- rnorm(nsim,ybar,sd/sqrt(n)) print (rbind (summ(mu),summ(sd))) ## 2.5% 25% 50% 75% 97.5% ## [1,] 10.384610 0.8058220 9.0456124 10.0022985 10.384453 10.770302 11.779308 ## [2,] 1.439714 0.8063883 0.6947574 0.9888535 1.239987 1.640318 3.426055 logdens <- outer (mugrid, exp(logsdgrid), post.b, y) dens <- exp(logdens - max(logdens)) contour (mugrid, logsdgrid, dens, levels=contours, xlab=\"mu\", ylab=\"log sigma\", labex=0, cex=2) ## Warning in plot.window(xlim, ylim, ...): \"labex\" is not a graphical parameter ## Warning in title(...): \"labex\" is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): \"labex\" is not a ## graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): \"labex\" is not a ## graphical parameter ## Warning in box(...): \"labex\" is not a graphical parameter mtext (\"Posterior density, accounting for rounding\", cex=2, 3) dens.mu <- apply(dens,1,sum) muindex <- sample (1:length(mugrid), nsim, replace=T, prob=dens.mu) mu <- mugrid[muindex] sd <- rep (NA,nsim) for (i in (1:nsim)) sd[i] <- exp (sample (logsdgrid, 1, prob=dens[muindex[i],])) print (rbind (summ(mu),summ(sd))) ## 2.5% 25% 50% 75% 97.5% ## [1,] 10.3702 0.6956568 8.8793970 10.0100503 10.386935 10.763819 11.668342 ## [2,] 1.3473 0.7143081 0.5929691 0.9043828 1.186318 1.556144 3.208582 5-(d) z <- matrix (NA, nsim, length(y)) for (i in 1:length(y)){ lower <- pnorm (y[i]-.5, mu, sd) upper <- pnorm (y[i]+.5, mu, sd) z[,i] <- qnorm (lower + runif(nsim)*(upper-lower), mu, sd)} mean ((z[,1]-z[,2])^2) ## [1] 0.1582345 7 Poisson's parameter is relative with frequency of some event. We are given that total number of bicycles and others is \\(b+v\\) . The likelihood we are trying to find is \\(p(b|b+v)\\) . Using Bayes' Rule, we can derive likelihood. \\[p(b|b+v) = \\frac{p(b+v|b)p(b)}{p(b+v)}$$ $$=\\frac{p(v)p(b)}{p(b+v)}\\] \\[=\\frac{Pois(\\theta_v)Pois(\\theta_v)}{Pois(\\theta_b+\\theta_v)}\\] Sum of Pois dist is also Pois dist, then we can get that denominator. \\[p(b|b+v) = \\frac{\\frac{e^{-\\theta_v}\\theta_v^v}{v!}\\frac{e^{-\\theta_b}\\theta_b^b}{b!}}{\\frac{e^{-(\\theta_b+\\theta_v)(\\theta_b+\\theta_v)^{b+v}}}{(b+v)!}}$$ $$=\\frac{(b+v)!}{b!v!}(\\frac{\\theta_b}{\\theta_b+\\theta_v})^b(\\frac{\\theta_v}{\\theta_b+\\theta_v})^v$$ $$=\\frac{(b+v)!}{b!v!}(\\frac{\\theta_b}{\\theta_b+\\theta_v})^b(1-\\frac{\\theta_b}{\\theta_b+\\theta_v})^v\\] This is a binomial distribution with \\(b+v\\) trials and prop \\(\\frac{\\theta_b}{\\theta_b+\\theta_v}\\) . 9 \\[p(\\mu,\\sigma^2|y) \\propto p(y|\\mu,\\sigma^2)p(\\mu,\\sigma^2)\\] $$ \\propto \\sigma^{-1}(\\sigma^2)^{-((\\nu_0+n)/2+1)}exp(-\\frac{\\nu_0\\sigma^2_0+(n-1)s^2+\\frac{n\\kappa_0(\\bar{y}-\\mu_0)^2}{n_\\kappa{_0}}+(n+\\kappa_0)(\\mu-\\frac{\\mu_0\\kappa_0+n\\bar{y}}{n+\\kappa_0})^2}{2\\sigma^2}) $$. So, \\(\\mu,\\sigma^2|y \\sim N-Inv-\\chi^2(\\frac{\\mu_0\\kappa_0+n\\bar{y}}{n+\\kappa_0},\\frac{\\sigma^2_n}{n+\\kappa_0};n+\\nu_0,\\sigma^2_n),\\ where\\ \\sigma^2_n =\\frac{\\nu_0\\sigma^2_0+(n-1)s^2+\\frac{n\\kappa_0(\\bar{y}-\\mu_0)^2}{n+\\kappa_0}}{n+\\nu_0}\\) . 10 \\(p(\\sigma^2_j|y) \\propto (\\sigma^2_j)^{-n/2-1/2}exp(-(n-1)s^2/2\\sigma^2_j)\\) for each j. Thus \\(p(1/\\sigma^2_j|y) \\propto (1/\\sigma^2_j)^{n/2-3/2}exp(-(n-1)s^2/2\\sigma^2_j)\\) , which implies that \\((n-1)s^2/\\sigma^2_j\\) has a \\(\\chi^2_{n-1}\\) distribution. Two independent \\(\\chi^2\\) divided by each degree of freedom, it is F distribution. By this fact, \\(\\frac{s_1^2/\\sigma^2_1}{s_2^2/\\sigma^2_2}\\) has the \\(F_{n_1-1,n_2-1}\\) distribution. Normal model with unknown mean and variance library(ggplot2) theme_set(theme_minimal()) library(grid) library(gridExtra) library(tidyr) y <- c(93, 112, 122, 135, 122, 150, 118, 90, 124, 114) n <- length(y) s2 <- var(y) my <- mean(y) rsinvchisq <- function(n, nu, s2, ...) nu*s2 / rchisq(n , nu, ...) dsinvchisq <- function(x, nu, s2){ exp(log(nu/2)*nu/2 - lgamma(nu/2) + log(s2)/2*nu - log(x)*(nu/2+1) - (nu*s2/2)/x) } #' Sample 1000 random numbers from p(sigma2|y) ns <- 1000 sigma2 <- rsinvchisq(ns, n-1, s2) #' Sample from p(mu|sigma2,y) mu <- my + sqrt(sigma2/n)*rnorm(length(sigma2)) #' Create a variable sigma and #' sample from predictive distribution p(ynew|y) for each draw of (mu, sigma) sigma <- sqrt(sigma2) ynew <- rnorm(ns, mu, sigma) #' For mu, sigma and ynew compute the density in a grid #' ranges for the grids t1l <- c(90, 150) t2l <- c(10, 60) nl <- c(50, 185) t1 <- seq(t1l[1], t1l[2], length.out = ns) t2 <- seq(t2l[1], t2l[2], length.out = ns) xynew <- seq(nl[1], nl[2], length.out = ns) #' Compute the exact marginal density of mu # multiplication by 1./sqrt(s2/n) is due to the transformation of # variable z=(x-mean(y))/sqrt(s2/n), see BDA3 p. 21 pm <- dt((t1-my) / sqrt(s2/n), n-1) / sqrt(s2/n) #' Estimate the marginal density using samples #' and ad hoc Gaussian kernel approximation pmk <- density(mu, adjust = 2, n = ns, from = t1l[1], to = t1l[2])$y #' Compute the exact marginal density of sigma # the multiplication by 2*t2 is due to the transformation of # variable z=t2^2, see BDA3 p. 21 ps <- dsinvchisq(t2^2, n-1, s2) * 2*t2 #' Estimate the marginal density using samples #' and ad hoc Gaussian kernel approximation psk <- density(sigma, n = ns, from = t2l[1], to = t2l[2])$y #' Compute the exact predictive density # multiplication by 1./sqrt(s2/n) is due to the transformation of variable # see BDA3 p. 21 p_new <- dt((xynew-my) / sqrt(s2*(1+1/n)), n-1) / sqrt(s2*(1+1/n)) #' Evaluate the joint density in a grid. #' Note that the following is not normalized, but for plotting #' contours it does not matter. # Combine grid points into another data frame # with all pairwise combinations dfj <- data.frame(t1 = rep(t1, each = length(t2)), t2 = rep(t2, length(t1))) dfj$z <- dsinvchisq(dfj$t2^2, n-1, s2) * 2*dfj$t2 * dnorm(dfj$t1, my, dfj$t2/sqrt(n)) # breaks for plotting the contours cl <- seq(1e-5, max(dfj$z), length.out = 6) #' ### Demo 3.1 Visualise the joint and marginal densities #' Visualise the joint density and marginal densities of the posterior #' of normal distribution with unknown mean and variance. #' #' Create a plot of the marginal density of mu dfm <- data.frame(t1, Exact = pm, Empirical = pmk) %>% gather(grp, p, -t1) margmu <- ggplot(dfm) + geom_line(aes(t1, p, color = grp)) + coord_cartesian(xlim = t1l) + labs(title = 'Marginal of mu', x = '', y = '') + scale_y_continuous(breaks = NULL) + theme(legend.background = element_blank(), legend.position = c(0.75, 0.8), legend.title = element_blank()) #' Create a plot of the marginal density of sigma dfs <- data.frame(t2, Exact = ps, Empirical = psk) %>% gather(grp, p, -t2) margsig <- ggplot(dfs) + geom_line(aes(t2, p, color = grp)) + coord_cartesian(xlim = t2l) + coord_flip() + labs(title = 'Marginal of sigma', x = '', y = '') + scale_y_continuous(breaks = NULL) + theme(legend.background = element_blank(), legend.position = c(0.75, 0.8), legend.title = element_blank()) ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. #' Create a plot of the joint density joint1labs <- c('Samples','Exact contour') joint1 <- ggplot() + geom_point(data = data.frame(mu,sigma), aes(mu, sigma, col = '1'), size = 0.1) + geom_contour(data = dfj, aes(t1, t2, z = z, col = '2'), breaks = cl) + coord_cartesian(xlim = t1l,ylim = t2l) + labs(title = 'Joint posterior', x = '', y = '') + scale_y_continuous(labels = NULL) + scale_x_continuous(labels = NULL) + scale_color_manual(values=c('blue', 'black'), labels = joint1labs) + guides(color = guide_legend(nrow = 1, override.aes = list( shape = c(16, NA), linetype = c(0, 1), size = c(2, 1)))) + theme(legend.background = element_blank(), legend.position = c(0.5, 0.9), legend.title = element_blank()) #' Combine the plots #+ blank, fig.show='hide' # blank plot for combining the plots bp <- grid.rect(gp = gpar(col = 'white')) #+ combined grid.arrange(joint1, margsig, margmu, bp, nrow = 2) #' ### Demo 3.2 Visualise factored distribution #' Visualise factored sampling and the corresponding #' marginal and conditional densities. #' #' Create another plot of the joint posterior # data frame for the conditional of mu and marginal of sigma dfc <- data.frame(mu = t1, marg = rep(sigma[1], length(t1)), cond = sigma[1] + dnorm(t1 ,my, sqrt(sigma2[1]/n)) * 100) %>% gather(grp, p, marg, cond) # legend labels for the following plot joint2labs <- c('Exact contour plot', 'Sample from joint post.', 'Cond. distribution of mu', 'Sample from the marg. of sigma') joint2 <- ggplot() + geom_contour(data = dfj, aes(t1, t2, z = z, col = '1'), breaks = cl) + geom_point(data = data.frame(m = mu[1], s = sigma[1]), aes(m , s, color = '2')) + geom_line(data = dfc, aes(mu, p, color = grp), linetype = 'dashed') + coord_cartesian(xlim = t1l,ylim = t2l) + labs(title = 'Joint posterior', x = '', y = '') + scale_x_continuous(labels = NULL) + scale_color_manual(values=c('blue', 'darkgreen','darkgreen','black'), labels = joint2labs) + guides(color = guide_legend(nrow = 2, override.aes = list( shape = c(NA, 16, NA, NA), linetype = c(1, 0, 1, 1)))) + theme(legend.background = element_blank(), legend.position = c(0.5, 0.85), legend.title = element_blank()) #' Create another plot of the marginal density of sigma margsig2 <- ggplot(data = data.frame(t2, ps)) + geom_line(aes(t2, ps), color = 'blue') + coord_cartesian(xlim = t2l) + coord_flip() + labs(title = 'Marginal of sigma', x = '', y = '') + scale_y_continuous(labels = NULL) ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. #' Combine the plots grid.arrange(joint2, margsig2, ncol = 2) #' ### Demo 3.3 Visualise the marginal distribution of mu #' Visualise the marginal distribution of mu as a mixture of normals. #' #' Calculate conditional pdfs for each sample condpdfs <- sapply(t1, function(x) dnorm(x, my, sqrt(sigma2/n))) #' Create a plot of some of them # data frame of 25 first samples dfm25 <- data.frame(t1, t(condpdfs[1:25,])) %>% gather(grp, p, -t1) condmu <- ggplot(data = dfm25) + geom_line(aes(t1, p, group = grp), linetype = 'dashed') + labs(title = 'Conditional distribution of mu for first 25 samples', y = '', x = '') + scale_y_continuous(breaks = NULL) #' create a plot of their mean dfsam <- data.frame(t1, colMeans(condpdfs), pm) %>% gather(grp,p,-t1) # labels mulabs <- c('avg of sampled conds', 'exact marginal of mu') meanmu <- ggplot(data = dfsam) + geom_line(aes(t1, p, size = grp, color = grp)) + labs(y = '', x = 'mu') + scale_y_continuous(breaks = NULL) + scale_size_manual(values = c(2, 0.8), labels = mulabs) + scale_color_manual(values = c('orange', 'black'), labels = mulabs) + theme(legend.position = c(0.8, 0.8), legend.background = element_blank(), legend.title = element_blank()) #' Combine the plots grid.arrange(condmu, meanmu, ncol = 1) #' ### Demo 3.4 Visualise posterior predictive distribution. #' Visualise sampling from the posterior predictive distribution. #' Calculate each predictive pdf with given mu and sigma ynewdists <- sapply(xynew, function(x) dnorm(x, mu, sigma)) #' Create plot of the joint posterior with a draw #' from the posterior predictive distribution, highlight the first sample #' create a plot of the joint density # data frame of dirst draw from sample the predictive along with the exact value for plotting dfp <- data.frame(xynew, draw = ynewdists[1,], exact = p_new) # data frame for plotting the samples dfy <- data.frame(ynew, p = 0.02*max(ynewdists)) # legend labels pred1labs <- c('Sample from the predictive distribution', 'Predictive distribution given the posterior sample') pred2labs <- c('Samples from the predictive distribution', 'Exact predictive distribution') joint3labs <- c('Samples', 'Exact contour') joint3 <- ggplot() + geom_point(data = data.frame(mu, sigma), aes(mu, sigma, col = '1'), size = 0.1) + geom_contour(data = dfj, aes(t1, t2, z = z, col = '2'), breaks = cl) + geom_point(data = data.frame(x = mu[1], y = sigma[1]), aes(x, y), color = 'red') + coord_cartesian(xlim = t1l,ylim = t2l) + labs(title = 'Joint posterior', x = 'mu', y = 'sigma') + scale_color_manual(values=c('blue', 'black'), labels = joint3labs) + guides(color = guide_legend(nrow = 1, override.aes = list( shape = c(16, NA), linetype = c(0, 1), size = c(2, 1)))) + theme(legend.background = element_blank(), legend.position=c(0.5 ,0.9), legend.title = element_blank()) #' Create a plot of the predicitive distribution and the respective sample pred1 <- ggplot() + geom_line(data = dfp, aes(xynew, draw, color = '2')) + geom_point(data = dfy, aes(ynew[1], p, color = '1')) + coord_cartesian(xlim = nl, ylim = c(0,0.04)) + labs(title = 'Posterior predictive distribution', x = 'ytilde', y = '') + scale_y_continuous(breaks = NULL) + scale_color_manual(values = c('red', 'blue'), labels = pred1labs) + guides(color = guide_legend(nrow = 2, override.aes = list( linetype = c(0, 1), shape=c(16, NA), labels = pred1labs))) + theme(legend.background = element_blank(), legend.position = c(0.5 ,0.9), legend.title = element_blank()) #' Create a plot for all ynews pred2 <- ggplot() + geom_line(data = dfp, aes(xynew, draw, color = '2')) + geom_point(data = dfy, aes(ynew, p, color = '1'), alpha = 0.05) + coord_cartesian(xlim = nl, ylim = c(0,0.04)) + labs(x = 'ytilde', y = '') + scale_y_continuous(breaks=NULL) + scale_color_manual(values=c('darkgreen','blue'),labels=pred2labs) + guides(color = guide_legend(nrow = 2, override.aes=list( linetype = c(0, 1), shape = c(16, NA), alpha = c(1, 1) ,labels = pred2labs))) + theme(legend.background = element_blank(), legend.position = c(0.5 ,0.9), legend.title = element_blank()) #' Combine the plots grid.arrange(joint3, pred1, bp, pred2, nrow = 2) Estimating the speed of light using normal model Binomial regression and grid sampling with bioassay data","title":"R Notebook"},{"location":"10%20Bayesian%20Statistics/HW3/#bayes-statistics-hw3","text":"","title":"Bayes Statistics HW3"},{"location":"10%20Bayesian%20Statistics/HW3/#1","text":"","title":"1"},{"location":"10%20Bayesian%20Statistics/HW3/#1-a-write-the-marginal-posterior-distribution-for-alpha","text":"Lets denote \\(p(\\theta)\\) as \\(Dirichlet(a_1,..,a_n)\\) . Then posterior distribution is \\(p(\\theta|y) = Dirichlet(y_1+a_1,..,y_n+a_n\\) . By the properties of Dirichlet, marginal posterior dist of ( \\(\\theta_1,\\theta_2,1-\\theta_1-\\theta_2\\) ) is also Dirichlet : \\(p(\\theta_1,\\theta_2|y) \\propto \\theta_1^{y_1+a_1-1}\\theta_2^{y_2+a_2-1}(1-\\theta_1-\\theta_2)^{y_{rest}+a_{rest}-1}\\) , where \\(y_{rest}=y_3+..y_J\\) , \\(a_{rest}=a_3+..a_J\\) . do a variable transformation to \\((\\alpha,\\beta)=(\\frac{\\theta_1}{\\theta_1+\\theta_2},\\theta_1+\\theta_2)\\) . Jacobian is \\(|1/\\beta|\\) , \\[p(\\alpha,\\beta|y) \\propto \\beta(\\alpha\\beta)^{y_1+a_1-1}((1-\\alpha)\\beta)^{y_2+a_2-1}(1-\\beta)^{y_{rest}+a_{rest}-1}\\] \\[= \\alpha^{y_1+a_1-1}(1-\\alpha)^{y_2+a_2-1}\\beta_{y_1+y_2+a_1+a_2-1}(1-\\beta)^{y_{rest}+a_{rest}-1}\\] $ \\(\\propto Beta(\\alpha|y_1+a_1,y_2+a_2)Beta(\\beta|y_1+y_2+a_1+a_2,y+{rest}+a_{rest})\\) $ Posterior density is separated by two factors for \\(\\alpha\\) and \\(\\beta\\) , they are independent, therefore \\(\\alpha|y \\sim Beta(y_1+a_2,y_2+a_2)\\) .","title":"1-(a) Write the marginal posterior distribution for \\(\\alpha\\)"},{"location":"10%20Bayesian%20Statistics/HW3/#1-b-show-that-this-distribution-is-identical-to-the-posterior-distribution-for-alpha-obtained-by-treating-y_1-as-an-observation-from-the-binomial-distribution-with-probability-alpha-and-sample-size-y_1y_2-ignoring-the-data-y_3j_j","text":"The \\(Beta(y_1+a_1,y_2+a_2)\\) posterior distribution also be derived from a \\(Beta(a_1,a_2)\\) prior dist and a binomial observation \\(y_1\\) with sample size \\(y_1+y_2\\) .","title":"(1-b) show that this distribution is identical to the posterior distribution for \\(\\alpha\\) obtained by treating \\(y_1\\) as an observation from the binomial distribution with probability \\(\\alpha\\) and sample size \\(y_1+y_2\\), ignoring the data \\(y_3,...,j_J.\\)"},{"location":"10%20Bayesian%20Statistics/HW3/#2","text":"Assume independent uniform prior distributions on the multinomial parameters. Then the posteriors are independent multinomial : $ \\((\\pi_1,\\pi_2,\\pi_3)|y \\sim Dirichlet(295,308,39)\\) $ $ \\((\\pi_1^*,\\pi_2^*,\\pi_3^*)|y \\sim Dirichlet(289,333,20)\\) $, and \\(\\alpha_1=\\frac{\\pi_1}{\\pi_1+\\pi_2},\\alpha_2=\\frac{\\pi_1^*}{\\pi_1^*+\\pi_2^*}\\) . From the properties of Dirichlet distribution, \\(\\alpha_1|y \\sim Beta(295,308)\\) \\(\\alpha_2|y \\sim Beta(289,333)\\) . set.seed(2020311194) alpha.1 <- rbeta(2000,295,308) alpha.2 <- rbeta(2000,289,333) diff <- alpha.2- alpha.1 hist(diff,xlab=\"alpha2-alpha1\",yaxt=\"n\", breaks = seq(-.15,.09,.01),cex=2) print (mean(diff>0)) ## [1] 0.191 This is histogram of 2000 draws from posterior. Based on histogram, posterior probability that there was a shift toward Bush is 19.4%. If we use normal approximations for the dist of \\(\\alpha_1\\) and \\(\\alpha_2\\) with means and st.d computed from relevant beta dist, can get the same answer.","title":"2"},{"location":"10%20Bayesian%20Statistics/HW3/#3","text":"","title":"3"},{"location":"10%20Bayesian%20Statistics/HW3/#3-a","text":"Data dist is \\(p(y|\\mu_c,\\mu_t,\\sigma_c,\\sigma_t)= \\prod_{i=1}^32N(y_{ci}|\\mu_c,\\sigma_c^2)\\prod_{i=1}^{36}N(y_{ti}|\\mu_t,\\sigma_t^2)\\) . \\((\\mu_c,\\sigma_c)\\) are independent of \\((\\mu_t,\\sigma_t)\\) in the posterior. So in this model, we can analyze two separately. The marginal posterior for \\(\\mu_c\\) and \\(\\mu_t\\) are : \\[\\mu_c|y \\sim t_{31}(1.013,0.24^2/32)\\] $ \\(\\mu_t|y \\sim t_{35}(1.173,0.20^2/36)\\) $.","title":"3-(a)"},{"location":"10%20Bayesian%20Statistics/HW3/#3-b","text":"mu.c <- 1.013 + (0.24/sqrt(32))*rt(1000,31) mu.t <- 1.173 + (0.20/sqrt(36))*rt(1000,35) dif <- mu.t - mu.c hist (dif, xlab=\"mu_t - mu_c\", yaxt=\"n\", breaks=seq(-.1,.4,.02), cex=2) print (sort(dif)[c(25,976)]) ## [1] 0.05868759 0.26396430 Based on histogram, 95% posterior interval for average treatment effect is [0.05,0.27]","title":"3-(b)"},{"location":"10%20Bayesian%20Statistics/HW3/#4","text":"","title":"4"},{"location":"10%20Bayesian%20Statistics/HW3/#4-ab","text":"Set Uniform prior for our model's prior. In binomial model, Uniform can be noninformative prior. Let Uniform prior( \\(Beta(1/2,1/2)\\) ) as our prior. Then \\(p(p_0) \\propto 1,\\ p(p_1) \\propto 1\\) , \\(p(p_0,p_1|n_i,y_i) \\propto p(p_0,p_1)\\prod_{i=1}^2f(y_i|p_0,p_1)\\) \\(\\propto p_0^{39}(1-p_0)^{674}p_1^{22}(1-p_1)^{680}\\) . Two experiments are independent, they can dealt seperately. \\(p(p_0) \\sim Beta(40,675)\\) \\(p(p_1) \\sim Beta(23,681)\\) set.seed(2020311194) p0 <- rbeta(1000,40,675) p1 <- rbeta(1000,22,680) hist(p0, breaks=20, col='grey', yaxt='n',main=\"p0\") hist(p1, breaks=20, col='grey', yaxt='n',main=\"p1\") odds_r <- (p1/(1-p1))/(p0/(1-p0)) hist(odds_r,breaks=30,col='grey',yaxt='n',main='Odds Ratio')","title":"4-(a),(b)"},{"location":"10%20Bayesian%20Statistics/HW3/#4-c","text":"#Compare Jeffrey and Uniform prior set.seed(2020311194) p0 <- rbeta(1000,39.5,674.5) p1 <- rbeta(1000,21.5,679.5) odds_r1 <- (p1/(1-p1))/(p0/(1-p0)) mean(odds_r) ; mean(odds_r1) ## [1] 0.564724 ## [1] 0.559396 var(odds_r) ; var(odds_r1) ## [1] 0.02306329 ## [1] 0.0235932 quantile(odds_r,c(0.25,0.5,0.75,0.99)) ; quantile(odds_r1,c(0.25,0.5,0.75,0.99)) ## 25% 50% 75% 99% ## 0.4557233 0.5458081 0.6511201 1.0087703 ## 25% 50% 75% 99% ## 0.4500328 0.5366413 0.6471813 1.0028043 Comparing with Jeffrey's prior and my choice, difference is small.","title":"4-(c)"},{"location":"10%20Bayesian%20Statistics/HW3/#5","text":"","title":"5"},{"location":"10%20Bayesian%20Statistics/HW3/#5-a","text":"\\(p(\\sigma^2|y) \\sim Inv-\\chi^2(n-1,s^2),\\ p(\\mu|\\sigma^2,y) \\sim N(\\bar{y},\\sigma^2/n)\\) with n=5, \\(\\bar{y}\\) =10.4, \\(s^2\\) = 1.3.","title":"5-(a)"},{"location":"10%20Bayesian%20Statistics/HW3/#5-b","text":"\\[p(\\mu,\\sigma^2|y) \\propto \\frac{1}{\\sigma^2}\\prod_{i=1}^n(\\Phi(\\frac{y_i+0.5-\\mu}{\\sigma})-\\Phi(\\frac{y_i-0.5-\\mu}{\\sigma}))\\]","title":"5-(b)"},{"location":"10%20Bayesian%20Statistics/HW3/#5-c","text":"post.a <- function(mu,sd,y){ ldens <- 0 for (i in 1:length(y)) ldens <- ldens + log(dnorm(y[i],mu,sd)) ldens} post.b <- function(mu,sd,y){ ldens <- 0 for (i in 1:length(y)) ldens <- ldens + log(pnorm(y[i]+0.5,mu,sd) - pnorm(y[i]-0.5,mu,sd)) ldens} summ <- function(x){c(mean(x),sqrt(var(x)), quantile(x, c(.025,.25,.5,.75,.975)))} nsim <- 2000 y <- c(10,10,12,11,9) n <- length(y) ybar <- mean(y) s2 <- sum((y-mean(y))^2)/(n-1) mugrid <- seq(3,18,length=200) logsdgrid <- seq(-2,4,length=200) contours <- c(.0001,.001,.01,seq(.05,.95,.05)) logdens <- outer (mugrid, exp(logsdgrid), post.a, y) dens <- exp(logdens - max(logdens)) contour (mugrid, logsdgrid, dens, levels=contours, xlab=\"mu\", ylab=\"log sigma\", label=0, cex=2) mtext (\"Posterior density, ignoring rounding\", 3) sd <- sqrt((n-1)*s2/rchisq(nsim,4)) mu <- rnorm(nsim,ybar,sd/sqrt(n)) print (rbind (summ(mu),summ(sd))) ## 2.5% 25% 50% 75% 97.5% ## [1,] 10.384610 0.8058220 9.0456124 10.0022985 10.384453 10.770302 11.779308 ## [2,] 1.439714 0.8063883 0.6947574 0.9888535 1.239987 1.640318 3.426055 logdens <- outer (mugrid, exp(logsdgrid), post.b, y) dens <- exp(logdens - max(logdens)) contour (mugrid, logsdgrid, dens, levels=contours, xlab=\"mu\", ylab=\"log sigma\", labex=0, cex=2) ## Warning in plot.window(xlim, ylim, ...): \"labex\" is not a graphical parameter ## Warning in title(...): \"labex\" is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): \"labex\" is not a ## graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): \"labex\" is not a ## graphical parameter ## Warning in box(...): \"labex\" is not a graphical parameter mtext (\"Posterior density, accounting for rounding\", cex=2, 3) dens.mu <- apply(dens,1,sum) muindex <- sample (1:length(mugrid), nsim, replace=T, prob=dens.mu) mu <- mugrid[muindex] sd <- rep (NA,nsim) for (i in (1:nsim)) sd[i] <- exp (sample (logsdgrid, 1, prob=dens[muindex[i],])) print (rbind (summ(mu),summ(sd))) ## 2.5% 25% 50% 75% 97.5% ## [1,] 10.3702 0.6956568 8.8793970 10.0100503 10.386935 10.763819 11.668342 ## [2,] 1.3473 0.7143081 0.5929691 0.9043828 1.186318 1.556144 3.208582","title":"5-(c)"},{"location":"10%20Bayesian%20Statistics/HW3/#5-d","text":"z <- matrix (NA, nsim, length(y)) for (i in 1:length(y)){ lower <- pnorm (y[i]-.5, mu, sd) upper <- pnorm (y[i]+.5, mu, sd) z[,i] <- qnorm (lower + runif(nsim)*(upper-lower), mu, sd)} mean ((z[,1]-z[,2])^2) ## [1] 0.1582345","title":"5-(d)"},{"location":"10%20Bayesian%20Statistics/HW3/#7","text":"Poisson's parameter is relative with frequency of some event. We are given that total number of bicycles and others is \\(b+v\\) . The likelihood we are trying to find is \\(p(b|b+v)\\) . Using Bayes' Rule, we can derive likelihood. \\[p(b|b+v) = \\frac{p(b+v|b)p(b)}{p(b+v)}$$ $$=\\frac{p(v)p(b)}{p(b+v)}\\] \\[=\\frac{Pois(\\theta_v)Pois(\\theta_v)}{Pois(\\theta_b+\\theta_v)}\\] Sum of Pois dist is also Pois dist, then we can get that denominator. \\[p(b|b+v) = \\frac{\\frac{e^{-\\theta_v}\\theta_v^v}{v!}\\frac{e^{-\\theta_b}\\theta_b^b}{b!}}{\\frac{e^{-(\\theta_b+\\theta_v)(\\theta_b+\\theta_v)^{b+v}}}{(b+v)!}}$$ $$=\\frac{(b+v)!}{b!v!}(\\frac{\\theta_b}{\\theta_b+\\theta_v})^b(\\frac{\\theta_v}{\\theta_b+\\theta_v})^v$$ $$=\\frac{(b+v)!}{b!v!}(\\frac{\\theta_b}{\\theta_b+\\theta_v})^b(1-\\frac{\\theta_b}{\\theta_b+\\theta_v})^v\\] This is a binomial distribution with \\(b+v\\) trials and prop \\(\\frac{\\theta_b}{\\theta_b+\\theta_v}\\) .","title":"7"},{"location":"10%20Bayesian%20Statistics/HW3/#9","text":"\\[p(\\mu,\\sigma^2|y) \\propto p(y|\\mu,\\sigma^2)p(\\mu,\\sigma^2)\\] $$ \\propto \\sigma^{-1}(\\sigma^2)^{-((\\nu_0+n)/2+1)}exp(-\\frac{\\nu_0\\sigma^2_0+(n-1)s^2+\\frac{n\\kappa_0(\\bar{y}-\\mu_0)^2}{n_\\kappa{_0}}+(n+\\kappa_0)(\\mu-\\frac{\\mu_0\\kappa_0+n\\bar{y}}{n+\\kappa_0})^2}{2\\sigma^2}) $$. So, \\(\\mu,\\sigma^2|y \\sim N-Inv-\\chi^2(\\frac{\\mu_0\\kappa_0+n\\bar{y}}{n+\\kappa_0},\\frac{\\sigma^2_n}{n+\\kappa_0};n+\\nu_0,\\sigma^2_n),\\ where\\ \\sigma^2_n =\\frac{\\nu_0\\sigma^2_0+(n-1)s^2+\\frac{n\\kappa_0(\\bar{y}-\\mu_0)^2}{n+\\kappa_0}}{n+\\nu_0}\\) .","title":"9"},{"location":"10%20Bayesian%20Statistics/HW3/#10","text":"\\(p(\\sigma^2_j|y) \\propto (\\sigma^2_j)^{-n/2-1/2}exp(-(n-1)s^2/2\\sigma^2_j)\\) for each j. Thus \\(p(1/\\sigma^2_j|y) \\propto (1/\\sigma^2_j)^{n/2-3/2}exp(-(n-1)s^2/2\\sigma^2_j)\\) , which implies that \\((n-1)s^2/\\sigma^2_j\\) has a \\(\\chi^2_{n-1}\\) distribution. Two independent \\(\\chi^2\\) divided by each degree of freedom, it is F distribution. By this fact, \\(\\frac{s_1^2/\\sigma^2_1}{s_2^2/\\sigma^2_2}\\) has the \\(F_{n_1-1,n_2-1}\\) distribution.","title":"10"},{"location":"10%20Bayesian%20Statistics/HW3/#normal-model-with-unknown-mean-and-variance","text":"library(ggplot2) theme_set(theme_minimal()) library(grid) library(gridExtra) library(tidyr) y <- c(93, 112, 122, 135, 122, 150, 118, 90, 124, 114) n <- length(y) s2 <- var(y) my <- mean(y) rsinvchisq <- function(n, nu, s2, ...) nu*s2 / rchisq(n , nu, ...) dsinvchisq <- function(x, nu, s2){ exp(log(nu/2)*nu/2 - lgamma(nu/2) + log(s2)/2*nu - log(x)*(nu/2+1) - (nu*s2/2)/x) } #' Sample 1000 random numbers from p(sigma2|y) ns <- 1000 sigma2 <- rsinvchisq(ns, n-1, s2) #' Sample from p(mu|sigma2,y) mu <- my + sqrt(sigma2/n)*rnorm(length(sigma2)) #' Create a variable sigma and #' sample from predictive distribution p(ynew|y) for each draw of (mu, sigma) sigma <- sqrt(sigma2) ynew <- rnorm(ns, mu, sigma) #' For mu, sigma and ynew compute the density in a grid #' ranges for the grids t1l <- c(90, 150) t2l <- c(10, 60) nl <- c(50, 185) t1 <- seq(t1l[1], t1l[2], length.out = ns) t2 <- seq(t2l[1], t2l[2], length.out = ns) xynew <- seq(nl[1], nl[2], length.out = ns) #' Compute the exact marginal density of mu # multiplication by 1./sqrt(s2/n) is due to the transformation of # variable z=(x-mean(y))/sqrt(s2/n), see BDA3 p. 21 pm <- dt((t1-my) / sqrt(s2/n), n-1) / sqrt(s2/n) #' Estimate the marginal density using samples #' and ad hoc Gaussian kernel approximation pmk <- density(mu, adjust = 2, n = ns, from = t1l[1], to = t1l[2])$y #' Compute the exact marginal density of sigma # the multiplication by 2*t2 is due to the transformation of # variable z=t2^2, see BDA3 p. 21 ps <- dsinvchisq(t2^2, n-1, s2) * 2*t2 #' Estimate the marginal density using samples #' and ad hoc Gaussian kernel approximation psk <- density(sigma, n = ns, from = t2l[1], to = t2l[2])$y #' Compute the exact predictive density # multiplication by 1./sqrt(s2/n) is due to the transformation of variable # see BDA3 p. 21 p_new <- dt((xynew-my) / sqrt(s2*(1+1/n)), n-1) / sqrt(s2*(1+1/n)) #' Evaluate the joint density in a grid. #' Note that the following is not normalized, but for plotting #' contours it does not matter. # Combine grid points into another data frame # with all pairwise combinations dfj <- data.frame(t1 = rep(t1, each = length(t2)), t2 = rep(t2, length(t1))) dfj$z <- dsinvchisq(dfj$t2^2, n-1, s2) * 2*dfj$t2 * dnorm(dfj$t1, my, dfj$t2/sqrt(n)) # breaks for plotting the contours cl <- seq(1e-5, max(dfj$z), length.out = 6) #' ### Demo 3.1 Visualise the joint and marginal densities #' Visualise the joint density and marginal densities of the posterior #' of normal distribution with unknown mean and variance. #' #' Create a plot of the marginal density of mu dfm <- data.frame(t1, Exact = pm, Empirical = pmk) %>% gather(grp, p, -t1) margmu <- ggplot(dfm) + geom_line(aes(t1, p, color = grp)) + coord_cartesian(xlim = t1l) + labs(title = 'Marginal of mu', x = '', y = '') + scale_y_continuous(breaks = NULL) + theme(legend.background = element_blank(), legend.position = c(0.75, 0.8), legend.title = element_blank()) #' Create a plot of the marginal density of sigma dfs <- data.frame(t2, Exact = ps, Empirical = psk) %>% gather(grp, p, -t2) margsig <- ggplot(dfs) + geom_line(aes(t2, p, color = grp)) + coord_cartesian(xlim = t2l) + coord_flip() + labs(title = 'Marginal of sigma', x = '', y = '') + scale_y_continuous(breaks = NULL) + theme(legend.background = element_blank(), legend.position = c(0.75, 0.8), legend.title = element_blank()) ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. #' Create a plot of the joint density joint1labs <- c('Samples','Exact contour') joint1 <- ggplot() + geom_point(data = data.frame(mu,sigma), aes(mu, sigma, col = '1'), size = 0.1) + geom_contour(data = dfj, aes(t1, t2, z = z, col = '2'), breaks = cl) + coord_cartesian(xlim = t1l,ylim = t2l) + labs(title = 'Joint posterior', x = '', y = '') + scale_y_continuous(labels = NULL) + scale_x_continuous(labels = NULL) + scale_color_manual(values=c('blue', 'black'), labels = joint1labs) + guides(color = guide_legend(nrow = 1, override.aes = list( shape = c(16, NA), linetype = c(0, 1), size = c(2, 1)))) + theme(legend.background = element_blank(), legend.position = c(0.5, 0.9), legend.title = element_blank()) #' Combine the plots #+ blank, fig.show='hide' # blank plot for combining the plots bp <- grid.rect(gp = gpar(col = 'white')) #+ combined grid.arrange(joint1, margsig, margmu, bp, nrow = 2) #' ### Demo 3.2 Visualise factored distribution #' Visualise factored sampling and the corresponding #' marginal and conditional densities. #' #' Create another plot of the joint posterior # data frame for the conditional of mu and marginal of sigma dfc <- data.frame(mu = t1, marg = rep(sigma[1], length(t1)), cond = sigma[1] + dnorm(t1 ,my, sqrt(sigma2[1]/n)) * 100) %>% gather(grp, p, marg, cond) # legend labels for the following plot joint2labs <- c('Exact contour plot', 'Sample from joint post.', 'Cond. distribution of mu', 'Sample from the marg. of sigma') joint2 <- ggplot() + geom_contour(data = dfj, aes(t1, t2, z = z, col = '1'), breaks = cl) + geom_point(data = data.frame(m = mu[1], s = sigma[1]), aes(m , s, color = '2')) + geom_line(data = dfc, aes(mu, p, color = grp), linetype = 'dashed') + coord_cartesian(xlim = t1l,ylim = t2l) + labs(title = 'Joint posterior', x = '', y = '') + scale_x_continuous(labels = NULL) + scale_color_manual(values=c('blue', 'darkgreen','darkgreen','black'), labels = joint2labs) + guides(color = guide_legend(nrow = 2, override.aes = list( shape = c(NA, 16, NA, NA), linetype = c(1, 0, 1, 1)))) + theme(legend.background = element_blank(), legend.position = c(0.5, 0.85), legend.title = element_blank()) #' Create another plot of the marginal density of sigma margsig2 <- ggplot(data = data.frame(t2, ps)) + geom_line(aes(t2, ps), color = 'blue') + coord_cartesian(xlim = t2l) + coord_flip() + labs(title = 'Marginal of sigma', x = '', y = '') + scale_y_continuous(labels = NULL) ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. #' Combine the plots grid.arrange(joint2, margsig2, ncol = 2) #' ### Demo 3.3 Visualise the marginal distribution of mu #' Visualise the marginal distribution of mu as a mixture of normals. #' #' Calculate conditional pdfs for each sample condpdfs <- sapply(t1, function(x) dnorm(x, my, sqrt(sigma2/n))) #' Create a plot of some of them # data frame of 25 first samples dfm25 <- data.frame(t1, t(condpdfs[1:25,])) %>% gather(grp, p, -t1) condmu <- ggplot(data = dfm25) + geom_line(aes(t1, p, group = grp), linetype = 'dashed') + labs(title = 'Conditional distribution of mu for first 25 samples', y = '', x = '') + scale_y_continuous(breaks = NULL) #' create a plot of their mean dfsam <- data.frame(t1, colMeans(condpdfs), pm) %>% gather(grp,p,-t1) # labels mulabs <- c('avg of sampled conds', 'exact marginal of mu') meanmu <- ggplot(data = dfsam) + geom_line(aes(t1, p, size = grp, color = grp)) + labs(y = '', x = 'mu') + scale_y_continuous(breaks = NULL) + scale_size_manual(values = c(2, 0.8), labels = mulabs) + scale_color_manual(values = c('orange', 'black'), labels = mulabs) + theme(legend.position = c(0.8, 0.8), legend.background = element_blank(), legend.title = element_blank()) #' Combine the plots grid.arrange(condmu, meanmu, ncol = 1) #' ### Demo 3.4 Visualise posterior predictive distribution. #' Visualise sampling from the posterior predictive distribution. #' Calculate each predictive pdf with given mu and sigma ynewdists <- sapply(xynew, function(x) dnorm(x, mu, sigma)) #' Create plot of the joint posterior with a draw #' from the posterior predictive distribution, highlight the first sample #' create a plot of the joint density # data frame of dirst draw from sample the predictive along with the exact value for plotting dfp <- data.frame(xynew, draw = ynewdists[1,], exact = p_new) # data frame for plotting the samples dfy <- data.frame(ynew, p = 0.02*max(ynewdists)) # legend labels pred1labs <- c('Sample from the predictive distribution', 'Predictive distribution given the posterior sample') pred2labs <- c('Samples from the predictive distribution', 'Exact predictive distribution') joint3labs <- c('Samples', 'Exact contour') joint3 <- ggplot() + geom_point(data = data.frame(mu, sigma), aes(mu, sigma, col = '1'), size = 0.1) + geom_contour(data = dfj, aes(t1, t2, z = z, col = '2'), breaks = cl) + geom_point(data = data.frame(x = mu[1], y = sigma[1]), aes(x, y), color = 'red') + coord_cartesian(xlim = t1l,ylim = t2l) + labs(title = 'Joint posterior', x = 'mu', y = 'sigma') + scale_color_manual(values=c('blue', 'black'), labels = joint3labs) + guides(color = guide_legend(nrow = 1, override.aes = list( shape = c(16, NA), linetype = c(0, 1), size = c(2, 1)))) + theme(legend.background = element_blank(), legend.position=c(0.5 ,0.9), legend.title = element_blank()) #' Create a plot of the predicitive distribution and the respective sample pred1 <- ggplot() + geom_line(data = dfp, aes(xynew, draw, color = '2')) + geom_point(data = dfy, aes(ynew[1], p, color = '1')) + coord_cartesian(xlim = nl, ylim = c(0,0.04)) + labs(title = 'Posterior predictive distribution', x = 'ytilde', y = '') + scale_y_continuous(breaks = NULL) + scale_color_manual(values = c('red', 'blue'), labels = pred1labs) + guides(color = guide_legend(nrow = 2, override.aes = list( linetype = c(0, 1), shape=c(16, NA), labels = pred1labs))) + theme(legend.background = element_blank(), legend.position = c(0.5 ,0.9), legend.title = element_blank()) #' Create a plot for all ynews pred2 <- ggplot() + geom_line(data = dfp, aes(xynew, draw, color = '2')) + geom_point(data = dfy, aes(ynew, p, color = '1'), alpha = 0.05) + coord_cartesian(xlim = nl, ylim = c(0,0.04)) + labs(x = 'ytilde', y = '') + scale_y_continuous(breaks=NULL) + scale_color_manual(values=c('darkgreen','blue'),labels=pred2labs) + guides(color = guide_legend(nrow = 2, override.aes=list( linetype = c(0, 1), shape = c(16, NA), alpha = c(1, 1) ,labels = pred2labs))) + theme(legend.background = element_blank(), legend.position = c(0.5 ,0.9), legend.title = element_blank()) #' Combine the plots grid.arrange(joint3, pred1, bp, pred2, nrow = 2)","title":"Normal model with unknown mean and variance"},{"location":"10%20Bayesian%20Statistics/HW3/#estimating-the-speed-of-light-using-normal-model","text":"","title":"Estimating the speed of light using normal model"},{"location":"10%20Bayesian%20Statistics/HW3/#binomial-regression-and-grid-sampling-with-bioassay-data","text":"","title":"Binomial regression and grid sampling with bioassay data"},{"location":"10%20Bayesian%20Statistics/HW4/","text":"Bayes Statistics HW4 4-2 \\(p(y_i|\\theta) \\propto (logit^{-1}(\\alpha+\\beta x_i))^{y_i}(1-logit^{-1}(\\alpha+\\beta x_i))^{n_i-y_i}\\) \\(l_i = logp(y_i|\\theta) = constant+y_ilog(logit^{-1}(\\alpha+\\beta x_i))+(n_i-y_i)log(1-logit^{-1}(\\alpha+\\beta x_i))\\) \\(\\frac{d^2l_i}{d\\alpha^2} = -\\frac{n_iexp(\\alpha+\\beta x_i)}{(1+exp(\\alpha+\\beta x_i))^2}\\) \\(\\frac{d^2l_i}{d\\alpha d\\beta} = -\\frac{n_i x_iexp(\\alpha+\\beta x_i)}{(1+exp(\\alpha+\\beta x_i))^2}\\) \\(\\frac{d^2l_i}{d\\beta^2} = -\\frac{n_i x_i^2exp(\\alpha+\\beta x_i)}{(1+exp(\\alpha+\\beta x_i))^2}\\) The prior density on ( \\(\\alpha\\) , \\(\\beta\\) ) is uniform, so \\(logp(\\theta|y) = constant+\\sum_{i=1}^4l_i\\) , and \\[\\begin{equation*} I(\\hat\\theta) = \\begin{pmatrix} \\sum_{i=1}^4\\frac{n_iexp(\\alpha+\\beta x_i)}{(1+exp(\\alpha+\\beta x_i))^2} & \\sum_{i=1}^4\\frac{n_i x_iexp(\\alpha+\\beta x_i)}{(1+exp(\\alpha+\\beta x_i))^2} \\\\ \\sum_{i=1}^4\\frac{n_i x_iexp(\\alpha+\\beta x_i)}{(1+exp(\\alpha+\\beta x_i))^2} & \\sum_{i=1}^4\\frac{n_i x_i^2exp(\\alpha+\\beta x_i)}{(1+exp(\\alpha+\\beta x_i))^2} \\\\ \\end{pmatrix} |(\\alpha,\\beta)=(\\hat\\alpha,\\hat\\beta) \\end{equation*}\\] where \\(\\hat\\theta = (\\hat\\alpha,\\hat\\beta)\\) is the posterior mode. Denoteing I as \\begin{equation } \\begin{pmatrix} a & c \\ c & b \\ \\end{pmatrix} \\end{equation }, the normal approximation variances of \\(\\alpha\\) and \\(\\beta\\) are the diagonal elements of \\(I^{-1}:\\frac{b}{ab-c^2}\\) , and \\(\\frac{a}{ab-c^2}\\) , respectively. 4-3. Let \\(\\theta = LD50\\) ; also introduce \\(\\nu=\\beta\\) . Formula (4.1) suggests that the (asymptotic) posterior median and mode should coincide, and the (asymptotic) posterior standard variance should be the inverse of observed information, evaluated at the posterior mode. With some effort , it is possible to obtain decent numerical estimates for the posterior mode and standard deviation associated with this data set; we will take these values as proxies for their asymptotic analogues. One way to proceed is : Observe that \\(p(\\theta|y) = \\int p(\\theta,\\nu|y)d\\nu\\) , where \\(\\alpha = -\\theta\\nu\\) and \\(\\beta = \\nu\\) , and \\(|\\nu|\\) is the Jacobian related with the change in coordinates. Fixing \\(\\theta\\) equal to a value around \u22120.12, which appears to be near the posterior mode, we may compute \\(\\int p(\\alpha,\\beta|y)|\\nu|d\\nu\\) numericaally. The region of integration is infinite, but the integrand decays rapidly after \\(\\nu\\) passes 50, so that it suffices to estimate the integral assuming that \\(\\nu\\) runs from 0 to, say, 70. This procedure can be repeated fgor several values of \\(\\theta\\) near -0.12. The values may be compared directly to find the posterior mode for \\(\\theta\\) . To three decimal places, we obtain -0.114. One can fix a small value of \\(h\\) , such as \\(h=0.002\\) , and compute \\(d^2/d\\theta^2logp(\\theta|y)\\) , evaluated at \\(\\theta\\) equal to the posterior mode, bt the expression [ \\(logp(-0.114+h|y)-2logp(-0.114|y)+logp(-0.114-h|y)\\) ]/ \\(h^2\\) . 4-4. As \\(n \\rightarrow \\infty\\) , the posterior variance approaches zero - that is, the posterior distribution becomes thick near a single point. Any one-to-one continuous transformation on the real numbers is locally linear in the neighborhood of that point. 4-6. 4-6a $ \\(\\frac{d}{da}E(L(a|y))=\\frac{d}{da}\\int(\\theta-a)^2p(\\theta|y)d\\theta = -\\int(\\theta-a)p(\\theta|y)d\\theta = -2(E(\\theta|y)-a)=0\\ if\\ a=E(\\theta|y)\\) $. This calculus shows that \\(L(a|y)\\) has zero derivative at \\(a=E(\\theta|y)\\) . Also the second derivative is positive so this is minimum value. 4-6b Can apply the argument from 4-6c with \\(k_o=k_1=1\\) . 4-6c \\[\\frac{d}{da}E(L(a|y))=\\frac{d}{da}(\\int_{-\\infty}^a k_1(a-\\theta)p(\\theta|y)d\\theta+\\int_{a}^{\\infty}k_0(\\theta-a)p(\\theta|y)d\\theta)\\] \\[=k_1\\int_{-\\infty}^ap(\\theta|y)d\\theta-k_0\\int_{a}^{\\infty}p(\\theta|y)d\\theta\\] $ \\(=(k_1+k_0)\\int_{-\\infty}^ap(\\theta|y)d\\theta-k_0\\) $. This calculus shows that \\(L(a|y)\\) has zero derivative at any a for which \\(\\int_{-\\infty}^ap(\\theta|y)d\\theta=k_0/(k_0+k_1)\\) . Also second derivative is positive, that is minimizing value. 4-7. Denote posterior mean \\(m(y)=E(\\theta|y)\\) and consider \\(m(y)\\) as an estimatior of \\(\\theta\\) . Unbiasedness means that \\(E(m(y)|\\theta)=\\theta\\) . The marginal Expectation of \\(\\theta m(y)\\) is \\(E(\\theta m(y))=E[E(\\theta m(y)|\\theta)]=E[\\theta^2]\\) . Also we can write \\(E(\\theta m(y))=E[E(\\theta m(y)|y)]=E[m(y)^2]\\) . It follows that \\(E[(m(y))-\\theta)^2]=0\\) . This can only hold in degenerate problems for which \\(m(y)=\\theta\\) with probability 1. 4-9. Our goal is to show that the Bayes estimate has lower MSE than the maximum likelihood estimate for any value of \\(\\theta \\in [0,1]\\) when sigma is sufficiently large. The maximum likelihood estimate which restricted to the interval [0,1], takes value 0 with probability \\(\\Phi(-c/\\sigma)\\) and takes value 1 with probability \\(1-\\Phi((1-c)/\\sigma)\\) ; these are just the probabilities that y is less than 0 or greater than 1. For very large \\(\\sigma\\) , these probabilities both approach \\(\\Phi(0) =1/2\\) . Thus, the MSE of maximum likelihood estimate is approximately \\(1/2[(1-\\theta)^2+\\theta^2]=1/2-\\theta+\\theta^2\\) . On the other hand, \\(p(\\theta|y) \\propto \\ N(\\theta|y,\\sigma^2)\\) for \\(\\theta \\in [0,1]\\) . \\(E(\\theta|y)\\ is \\ \\int_0^1N(\\theta|y,\\sigma^2)d\\theta/\\int_0^1N(\\theta|y,\\sigma)d\\theta\\) . For very large \\(\\sigma\\) , \\(N(\\theta|y,\\sigma^2)\\) is approximately constant over small ranges of \\(\\theta\\) . So the Bayes estimate is close to \\(\\int_0^1\\theta d\\theta = 1/2\\) . Hence, for large \\(\\sigma\\) , the MSE of the \\(E(\\theta|y)\\) is about \\((\\theta-1/2)^2=1/4-\\theta+\\theta^2\\) . The difference in MSE is independent of the truv value of \\(\\theta\\) . Also, for large \\(\\sigma\\) the maximum likelihood estimate generally chooses 0 or 1, each with probability almost 1/2, whereas the Bayes estimate chooses 1/2 with probability almost 1. 4-11. For this problems, the prior should be a mixture of a spike at \\(\\theta=0\\) and a flat prior for \\(\\theta \\neq 0\\) . Let's write prior as $ \\(p(\\theta)=\\lambda N(\\theta|0,\\tau_1^2)+(1-\\lambda)N(\\theta|0,\\tau_2^2)\\) $, work through the algebra, and then take the limit \\(\\tau_1 \\rightarrow 0\\) and \\(\\tau_2 \\rightarrow \\infty\\) . \\(p(\\theta|y) \\propto p(\\theta)N(\\bar{y}|\\theta,\\sigma^2/n)\\) \\(\\propto \\lambda N(\\theta|0,\\tau_1^2)N(\\bar{y}|\\theta,\\sigma^2/n)+(1-\\lambda)N(\\theta|0,\\tau_2^2)N(\\bar{y}|\\theta,\\sigma^2/n)\\) \\(\\propto \\lambda N(\\bar{y}|0,\\tau_1^2+\\sigma^2/n)N(\\theta|\\frac{\\frac{n}{\\sigma^2}\\bar{y}^2}{\\frac{1}{\\tau_1^2}+\\frac{n}{\\sigma^2}},\\frac{1}{\\frac{1}{\\tau_1^2}+\\frac{n}{\\sigma^2}})\\) + \\((1-\\lambda)N(\\bar{y}|0,\\tau_2^2+\\sigma^2/n)N(\\theta|\\frac{\\frac{n}{\\sigma^2}\\bar{y}^2}{\\frac{1}{\\tau_2^2}+\\frac{n}{\\sigma^2}},\\frac{1}{\\frac{1}{\\tau_2^2}+\\frac{n}{\\sigma^2}})\\) In last step, we have replaced the factorization \\(p(\\theta)p(y|\\theta)\\) by the factorization \\(p(y)p(\\theta|y)\\) . The result is a mixture of two mormal densities in \\(\\theta\\) /. In the limit \\(\\tau_1 \\rightarrow 0\\) and \\(\\tau_2 \\rightarrow \\infty\\) , this is $ \\(p(\\theta|y) = \\lambda N(\\bar{y}|0,\\sigma^2/n)N(\\theta|0,-\\tau_1^2)_(1-\\lambda) N(\\theta|\\bar{y},\\sigma^2/n)\\) $. The estimate \\(\\hat\\theta\\) cannot be the posterior mean. Since the two normal densities have much different variances, it would also not make sense to use a posterior mode estimate. The mode of the hump of the posterior which has greater mass is more reasonable estimate. That is, \\[Set\\ \\hat\\theta=0\\ if: \\lambda N(\\bar{y}|0,\\sigma^2/n) > (1-\\lambda)N(\\bar{y}|0,\\tau_2^2)\\] $ \\(\\lambda\\frac{1}{\\sqrt{2\\pi/n\\sigma}}exp(-\\frac{1}{2}\\frac{n}{\\sigma^2}y^2) > (1-\\lambda)\\frac{1}{\\sqrt{2\\pi\\tau_2}}\\) $, and set \\(\\hat\\theta=\\bar{y}\\) o.w. For condition on above to be equivalent to \"if \\(\\bar{y} < 1.96\\sigma/\\sqrt{n}\\) \", as specified, we must have \\[0.146\\lambda\\frac{1}{\\sqrt{2\\pi/n\\sigma}}=(1-\\lambda)\\frac{1}{\\sqrt{2\\pi\\tau_2}}\\] $ \\(\\frac{\\lambda}{1-\\lambda} = \\frac{\\sigma}{0.146\\sqrt{n/\\tau_2}}\\) $. Since we consider the limit \\(\\tau_2 \\rightarrow \\infty\\) , this means that \\(\\lambda\\) goes to 0. That would be acceptable, nbut the more serious problem here is that the limiting value for \\(\\lambda \\tau_2\\) depends on n, and thus the prior for \\(\\theta\\) depends on n. Prior cannot depend on the data, so there is no prior for which the given estimate $\\hat\\theta $ is a reasonable posterior summary. Ex. library(ggplot2) library(gridExtra) library(tidyr) library(MASS) df1 <- data.frame( x = c(-0.86, -0.30, -0.05, 0.73), n = c(5, 5, 5, 5), y = c(0, 1, 3, 5) ) A = seq(-1.5, 7, length.out = 100) B = seq(-5, 35, length.out = 100) # make vectors that contain all pairwise combinations of A and B cA <- rep(A, each = length(B)) cB <- rep(B, length(A)) # a helper function to calculate the log likelihood logl <- function(df, a, b) df['y']*(a + b*df['x']) - df['n']*log1p(exp(a + b*df['x'])) # calculate likelihoods: apply logl function for each observation # ie. each row of data frame of x, n and y p <- apply(df1, 1, logl, cA, cB) %>% rowSums() %>% exp() nsamp <- 1000 samp_indices <- sample(length(p), size = nsamp, replace = T, prob = p/sum(p)) samp_A <- cA[samp_indices[1:nsamp]] samp_B <- cB[samp_indices[1:nsamp]] # add random jitter, see BDA3 p. 76 samp_A <- samp_A + runif(nsamp, A[1] - A[2], A[2] - A[1]) samp_B <- samp_B + runif(nsamp, B[1] - B[2], B[2] - B[1]) #Compute LD50 conditional beta >0 bpi <- samp_B > 0 samp_ld50 <- -samp_A[bpi]/samp_B[bpi] xl <- c(-1.5, 7) yl <- c(-5, 35) pos <- ggplot(data = data.frame(cA ,cB, p), aes(x = cA, y = cB)) + geom_raster(aes(fill = p, alpha = p), interpolate = T) + geom_contour(aes(z = p), colour = 'black', size = 0.2) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') + scale_fill_gradient(low = 'yellow', high = 'red', guide = F) + scale_alpha(range = c(0, 1), guide = F) sam <- ggplot(data = data.frame(samp_A, samp_B)) + geom_point(aes(samp_A, samp_B), color = 'blue', size = 0.3) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') his <- ggplot() + geom_histogram(aes(samp_ld50), binwidth = 0.04, fill = 'steelblue', color = 'black') + coord_cartesian(xlim = c(-0.8, 0.8)) + labs(x = 'LD50 = -alpha/beta') bioassayfun <- function(w, df) { z <- w[1] + w[2]*df$x -sum(df$y*(z) - df$n*log1p(exp(z))) } w0 <- c(0,0) optim_res <- optim(w0, bioassayfun, gr = NULL, df1, hessian = T) w <- optim_res$par S <- solve(optim_res$hessian) dmvnorm <- function(x, mu, sig) exp(-0.5*(length(x)*log(2*pi) + log(det(sig)) + (x-mu)%*%solve(sig, x-mu))) p <- apply(cbind(cA, cB), 1, dmvnorm, w, S) # sample from the multivariate normal normsamp <- mvrnorm(nsamp, w, S) bpi <- normsamp[,2] > 0 normsamp_ld50 <- -normsamp[bpi,1]/normsamp[bpi,2] pos_norm <- ggplot(data = data.frame(cA ,cB, p), aes(x = cA, y = cB)) + geom_raster(aes(fill = p, alpha = p), interpolate = T) + geom_contour(aes(z = p), colour = 'black', size = 0.2) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') + scale_fill_gradient(low = 'yellow', high = 'red', guide = F) + scale_alpha(range = c(0, 1), guide = F) sam_norm <- ggplot(data = data.frame(samp_A=normsamp[,1], samp_B=normsamp[,2])) + geom_point(aes(samp_A, samp_B), color = 'blue', size = 0.3) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') his_norm <- ggplot() + geom_histogram(aes(normsamp_ld50), binwidth = 0.04, fill = 'steelblue', color = 'black') + coord_cartesian(xlim = c(-0.8, 0.8)) + labs(x = 'LD50 = -alpha/beta, beta > 0') grid.arrange(pos, sam, his, pos_norm, sam_norm, his_norm, ncol = 3)","title":"R Notebook"},{"location":"10%20Bayesian%20Statistics/HW4/#bayes-statistics-hw4","text":"","title":"Bayes Statistics HW4"},{"location":"10%20Bayesian%20Statistics/HW4/#4-2","text":"\\(p(y_i|\\theta) \\propto (logit^{-1}(\\alpha+\\beta x_i))^{y_i}(1-logit^{-1}(\\alpha+\\beta x_i))^{n_i-y_i}\\) \\(l_i = logp(y_i|\\theta) = constant+y_ilog(logit^{-1}(\\alpha+\\beta x_i))+(n_i-y_i)log(1-logit^{-1}(\\alpha+\\beta x_i))\\) \\(\\frac{d^2l_i}{d\\alpha^2} = -\\frac{n_iexp(\\alpha+\\beta x_i)}{(1+exp(\\alpha+\\beta x_i))^2}\\) \\(\\frac{d^2l_i}{d\\alpha d\\beta} = -\\frac{n_i x_iexp(\\alpha+\\beta x_i)}{(1+exp(\\alpha+\\beta x_i))^2}\\) \\(\\frac{d^2l_i}{d\\beta^2} = -\\frac{n_i x_i^2exp(\\alpha+\\beta x_i)}{(1+exp(\\alpha+\\beta x_i))^2}\\) The prior density on ( \\(\\alpha\\) , \\(\\beta\\) ) is uniform, so \\(logp(\\theta|y) = constant+\\sum_{i=1}^4l_i\\) , and \\[\\begin{equation*} I(\\hat\\theta) = \\begin{pmatrix} \\sum_{i=1}^4\\frac{n_iexp(\\alpha+\\beta x_i)}{(1+exp(\\alpha+\\beta x_i))^2} & \\sum_{i=1}^4\\frac{n_i x_iexp(\\alpha+\\beta x_i)}{(1+exp(\\alpha+\\beta x_i))^2} \\\\ \\sum_{i=1}^4\\frac{n_i x_iexp(\\alpha+\\beta x_i)}{(1+exp(\\alpha+\\beta x_i))^2} & \\sum_{i=1}^4\\frac{n_i x_i^2exp(\\alpha+\\beta x_i)}{(1+exp(\\alpha+\\beta x_i))^2} \\\\ \\end{pmatrix} |(\\alpha,\\beta)=(\\hat\\alpha,\\hat\\beta) \\end{equation*}\\] where \\(\\hat\\theta = (\\hat\\alpha,\\hat\\beta)\\) is the posterior mode. Denoteing I as \\begin{equation } \\begin{pmatrix} a & c \\ c & b \\ \\end{pmatrix} \\end{equation }, the normal approximation variances of \\(\\alpha\\) and \\(\\beta\\) are the diagonal elements of \\(I^{-1}:\\frac{b}{ab-c^2}\\) , and \\(\\frac{a}{ab-c^2}\\) , respectively.","title":"4-2"},{"location":"10%20Bayesian%20Statistics/HW4/#4-3","text":"Let \\(\\theta = LD50\\) ; also introduce \\(\\nu=\\beta\\) . Formula (4.1) suggests that the (asymptotic) posterior median and mode should coincide, and the (asymptotic) posterior standard variance should be the inverse of observed information, evaluated at the posterior mode. With some effort , it is possible to obtain decent numerical estimates for the posterior mode and standard deviation associated with this data set; we will take these values as proxies for their asymptotic analogues. One way to proceed is : Observe that \\(p(\\theta|y) = \\int p(\\theta,\\nu|y)d\\nu\\) , where \\(\\alpha = -\\theta\\nu\\) and \\(\\beta = \\nu\\) , and \\(|\\nu|\\) is the Jacobian related with the change in coordinates. Fixing \\(\\theta\\) equal to a value around \u22120.12, which appears to be near the posterior mode, we may compute \\(\\int p(\\alpha,\\beta|y)|\\nu|d\\nu\\) numericaally. The region of integration is infinite, but the integrand decays rapidly after \\(\\nu\\) passes 50, so that it suffices to estimate the integral assuming that \\(\\nu\\) runs from 0 to, say, 70. This procedure can be repeated fgor several values of \\(\\theta\\) near -0.12. The values may be compared directly to find the posterior mode for \\(\\theta\\) . To three decimal places, we obtain -0.114. One can fix a small value of \\(h\\) , such as \\(h=0.002\\) , and compute \\(d^2/d\\theta^2logp(\\theta|y)\\) , evaluated at \\(\\theta\\) equal to the posterior mode, bt the expression [ \\(logp(-0.114+h|y)-2logp(-0.114|y)+logp(-0.114-h|y)\\) ]/ \\(h^2\\) .","title":"4-3."},{"location":"10%20Bayesian%20Statistics/HW4/#4-4","text":"As \\(n \\rightarrow \\infty\\) , the posterior variance approaches zero - that is, the posterior distribution becomes thick near a single point. Any one-to-one continuous transformation on the real numbers is locally linear in the neighborhood of that point.","title":"4-4."},{"location":"10%20Bayesian%20Statistics/HW4/#4-6","text":"","title":"4-6."},{"location":"10%20Bayesian%20Statistics/HW4/#4-6a","text":"$ \\(\\frac{d}{da}E(L(a|y))=\\frac{d}{da}\\int(\\theta-a)^2p(\\theta|y)d\\theta = -\\int(\\theta-a)p(\\theta|y)d\\theta = -2(E(\\theta|y)-a)=0\\ if\\ a=E(\\theta|y)\\) $. This calculus shows that \\(L(a|y)\\) has zero derivative at \\(a=E(\\theta|y)\\) . Also the second derivative is positive so this is minimum value.","title":"4-6a"},{"location":"10%20Bayesian%20Statistics/HW4/#4-6b","text":"Can apply the argument from 4-6c with \\(k_o=k_1=1\\) .","title":"4-6b"},{"location":"10%20Bayesian%20Statistics/HW4/#4-6c","text":"\\[\\frac{d}{da}E(L(a|y))=\\frac{d}{da}(\\int_{-\\infty}^a k_1(a-\\theta)p(\\theta|y)d\\theta+\\int_{a}^{\\infty}k_0(\\theta-a)p(\\theta|y)d\\theta)\\] \\[=k_1\\int_{-\\infty}^ap(\\theta|y)d\\theta-k_0\\int_{a}^{\\infty}p(\\theta|y)d\\theta\\] $ \\(=(k_1+k_0)\\int_{-\\infty}^ap(\\theta|y)d\\theta-k_0\\) $. This calculus shows that \\(L(a|y)\\) has zero derivative at any a for which \\(\\int_{-\\infty}^ap(\\theta|y)d\\theta=k_0/(k_0+k_1)\\) . Also second derivative is positive, that is minimizing value.","title":"4-6c"},{"location":"10%20Bayesian%20Statistics/HW4/#4-7","text":"Denote posterior mean \\(m(y)=E(\\theta|y)\\) and consider \\(m(y)\\) as an estimatior of \\(\\theta\\) . Unbiasedness means that \\(E(m(y)|\\theta)=\\theta\\) . The marginal Expectation of \\(\\theta m(y)\\) is \\(E(\\theta m(y))=E[E(\\theta m(y)|\\theta)]=E[\\theta^2]\\) . Also we can write \\(E(\\theta m(y))=E[E(\\theta m(y)|y)]=E[m(y)^2]\\) . It follows that \\(E[(m(y))-\\theta)^2]=0\\) . This can only hold in degenerate problems for which \\(m(y)=\\theta\\) with probability 1.","title":"4-7."},{"location":"10%20Bayesian%20Statistics/HW4/#4-9","text":"Our goal is to show that the Bayes estimate has lower MSE than the maximum likelihood estimate for any value of \\(\\theta \\in [0,1]\\) when sigma is sufficiently large. The maximum likelihood estimate which restricted to the interval [0,1], takes value 0 with probability \\(\\Phi(-c/\\sigma)\\) and takes value 1 with probability \\(1-\\Phi((1-c)/\\sigma)\\) ; these are just the probabilities that y is less than 0 or greater than 1. For very large \\(\\sigma\\) , these probabilities both approach \\(\\Phi(0) =1/2\\) . Thus, the MSE of maximum likelihood estimate is approximately \\(1/2[(1-\\theta)^2+\\theta^2]=1/2-\\theta+\\theta^2\\) . On the other hand, \\(p(\\theta|y) \\propto \\ N(\\theta|y,\\sigma^2)\\) for \\(\\theta \\in [0,1]\\) . \\(E(\\theta|y)\\ is \\ \\int_0^1N(\\theta|y,\\sigma^2)d\\theta/\\int_0^1N(\\theta|y,\\sigma)d\\theta\\) . For very large \\(\\sigma\\) , \\(N(\\theta|y,\\sigma^2)\\) is approximately constant over small ranges of \\(\\theta\\) . So the Bayes estimate is close to \\(\\int_0^1\\theta d\\theta = 1/2\\) . Hence, for large \\(\\sigma\\) , the MSE of the \\(E(\\theta|y)\\) is about \\((\\theta-1/2)^2=1/4-\\theta+\\theta^2\\) . The difference in MSE is independent of the truv value of \\(\\theta\\) . Also, for large \\(\\sigma\\) the maximum likelihood estimate generally chooses 0 or 1, each with probability almost 1/2, whereas the Bayes estimate chooses 1/2 with probability almost 1.","title":"4-9."},{"location":"10%20Bayesian%20Statistics/HW4/#4-11","text":"For this problems, the prior should be a mixture of a spike at \\(\\theta=0\\) and a flat prior for \\(\\theta \\neq 0\\) . Let's write prior as $ \\(p(\\theta)=\\lambda N(\\theta|0,\\tau_1^2)+(1-\\lambda)N(\\theta|0,\\tau_2^2)\\) $, work through the algebra, and then take the limit \\(\\tau_1 \\rightarrow 0\\) and \\(\\tau_2 \\rightarrow \\infty\\) . \\(p(\\theta|y) \\propto p(\\theta)N(\\bar{y}|\\theta,\\sigma^2/n)\\) \\(\\propto \\lambda N(\\theta|0,\\tau_1^2)N(\\bar{y}|\\theta,\\sigma^2/n)+(1-\\lambda)N(\\theta|0,\\tau_2^2)N(\\bar{y}|\\theta,\\sigma^2/n)\\) \\(\\propto \\lambda N(\\bar{y}|0,\\tau_1^2+\\sigma^2/n)N(\\theta|\\frac{\\frac{n}{\\sigma^2}\\bar{y}^2}{\\frac{1}{\\tau_1^2}+\\frac{n}{\\sigma^2}},\\frac{1}{\\frac{1}{\\tau_1^2}+\\frac{n}{\\sigma^2}})\\) + \\((1-\\lambda)N(\\bar{y}|0,\\tau_2^2+\\sigma^2/n)N(\\theta|\\frac{\\frac{n}{\\sigma^2}\\bar{y}^2}{\\frac{1}{\\tau_2^2}+\\frac{n}{\\sigma^2}},\\frac{1}{\\frac{1}{\\tau_2^2}+\\frac{n}{\\sigma^2}})\\) In last step, we have replaced the factorization \\(p(\\theta)p(y|\\theta)\\) by the factorization \\(p(y)p(\\theta|y)\\) . The result is a mixture of two mormal densities in \\(\\theta\\) /. In the limit \\(\\tau_1 \\rightarrow 0\\) and \\(\\tau_2 \\rightarrow \\infty\\) , this is $ \\(p(\\theta|y) = \\lambda N(\\bar{y}|0,\\sigma^2/n)N(\\theta|0,-\\tau_1^2)_(1-\\lambda) N(\\theta|\\bar{y},\\sigma^2/n)\\) $. The estimate \\(\\hat\\theta\\) cannot be the posterior mean. Since the two normal densities have much different variances, it would also not make sense to use a posterior mode estimate. The mode of the hump of the posterior which has greater mass is more reasonable estimate. That is, \\[Set\\ \\hat\\theta=0\\ if: \\lambda N(\\bar{y}|0,\\sigma^2/n) > (1-\\lambda)N(\\bar{y}|0,\\tau_2^2)\\] $ \\(\\lambda\\frac{1}{\\sqrt{2\\pi/n\\sigma}}exp(-\\frac{1}{2}\\frac{n}{\\sigma^2}y^2) > (1-\\lambda)\\frac{1}{\\sqrt{2\\pi\\tau_2}}\\) $, and set \\(\\hat\\theta=\\bar{y}\\) o.w. For condition on above to be equivalent to \"if \\(\\bar{y} < 1.96\\sigma/\\sqrt{n}\\) \", as specified, we must have \\[0.146\\lambda\\frac{1}{\\sqrt{2\\pi/n\\sigma}}=(1-\\lambda)\\frac{1}{\\sqrt{2\\pi\\tau_2}}\\] $ \\(\\frac{\\lambda}{1-\\lambda} = \\frac{\\sigma}{0.146\\sqrt{n/\\tau_2}}\\) $. Since we consider the limit \\(\\tau_2 \\rightarrow \\infty\\) , this means that \\(\\lambda\\) goes to 0. That would be acceptable, nbut the more serious problem here is that the limiting value for \\(\\lambda \\tau_2\\) depends on n, and thus the prior for \\(\\theta\\) depends on n. Prior cannot depend on the data, so there is no prior for which the given estimate $\\hat\\theta $ is a reasonable posterior summary.","title":"4-11."},{"location":"10%20Bayesian%20Statistics/HW4/#ex","text":"library(ggplot2) library(gridExtra) library(tidyr) library(MASS) df1 <- data.frame( x = c(-0.86, -0.30, -0.05, 0.73), n = c(5, 5, 5, 5), y = c(0, 1, 3, 5) ) A = seq(-1.5, 7, length.out = 100) B = seq(-5, 35, length.out = 100) # make vectors that contain all pairwise combinations of A and B cA <- rep(A, each = length(B)) cB <- rep(B, length(A)) # a helper function to calculate the log likelihood logl <- function(df, a, b) df['y']*(a + b*df['x']) - df['n']*log1p(exp(a + b*df['x'])) # calculate likelihoods: apply logl function for each observation # ie. each row of data frame of x, n and y p <- apply(df1, 1, logl, cA, cB) %>% rowSums() %>% exp() nsamp <- 1000 samp_indices <- sample(length(p), size = nsamp, replace = T, prob = p/sum(p)) samp_A <- cA[samp_indices[1:nsamp]] samp_B <- cB[samp_indices[1:nsamp]] # add random jitter, see BDA3 p. 76 samp_A <- samp_A + runif(nsamp, A[1] - A[2], A[2] - A[1]) samp_B <- samp_B + runif(nsamp, B[1] - B[2], B[2] - B[1]) #Compute LD50 conditional beta >0 bpi <- samp_B > 0 samp_ld50 <- -samp_A[bpi]/samp_B[bpi] xl <- c(-1.5, 7) yl <- c(-5, 35) pos <- ggplot(data = data.frame(cA ,cB, p), aes(x = cA, y = cB)) + geom_raster(aes(fill = p, alpha = p), interpolate = T) + geom_contour(aes(z = p), colour = 'black', size = 0.2) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') + scale_fill_gradient(low = 'yellow', high = 'red', guide = F) + scale_alpha(range = c(0, 1), guide = F) sam <- ggplot(data = data.frame(samp_A, samp_B)) + geom_point(aes(samp_A, samp_B), color = 'blue', size = 0.3) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') his <- ggplot() + geom_histogram(aes(samp_ld50), binwidth = 0.04, fill = 'steelblue', color = 'black') + coord_cartesian(xlim = c(-0.8, 0.8)) + labs(x = 'LD50 = -alpha/beta') bioassayfun <- function(w, df) { z <- w[1] + w[2]*df$x -sum(df$y*(z) - df$n*log1p(exp(z))) } w0 <- c(0,0) optim_res <- optim(w0, bioassayfun, gr = NULL, df1, hessian = T) w <- optim_res$par S <- solve(optim_res$hessian) dmvnorm <- function(x, mu, sig) exp(-0.5*(length(x)*log(2*pi) + log(det(sig)) + (x-mu)%*%solve(sig, x-mu))) p <- apply(cbind(cA, cB), 1, dmvnorm, w, S) # sample from the multivariate normal normsamp <- mvrnorm(nsamp, w, S) bpi <- normsamp[,2] > 0 normsamp_ld50 <- -normsamp[bpi,1]/normsamp[bpi,2] pos_norm <- ggplot(data = data.frame(cA ,cB, p), aes(x = cA, y = cB)) + geom_raster(aes(fill = p, alpha = p), interpolate = T) + geom_contour(aes(z = p), colour = 'black', size = 0.2) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') + scale_fill_gradient(low = 'yellow', high = 'red', guide = F) + scale_alpha(range = c(0, 1), guide = F) sam_norm <- ggplot(data = data.frame(samp_A=normsamp[,1], samp_B=normsamp[,2])) + geom_point(aes(samp_A, samp_B), color = 'blue', size = 0.3) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') his_norm <- ggplot() + geom_histogram(aes(normsamp_ld50), binwidth = 0.04, fill = 'steelblue', color = 'black') + coord_cartesian(xlim = c(-0.8, 0.8)) + labs(x = 'LD50 = -alpha/beta, beta > 0') grid.arrange(pos, sam, his, pos_norm, sam_norm, his_norm, ncol = 3)","title":"Ex."},{"location":"10%20Bayesian%20Statistics/HW5/","text":"Bayes Statistics HW5 5-4. 5-4a Yes. The joint distribution is $ \\(p(\\theta_1,..,\\theta_{2J}) = {2J \\choose J}(\\prod_{j=1}^JN(\\theta_{p(j)}|1,1)N(\\theta_{p(j)}|-1,1))\\) $, where the sum is over all permutations p of (1,...,2J). The density above is obviously invariant permutations of the indexex (1,...,2J). 5-4b The covariance of \\(\\theta_i, \\theta_j\\) is negative. If \\(\\theta_i\\) is large, then it probably comes from the N(1,1) distribution, which means that it is more likely than not that \\(\\theta_j\\) from the N(-1,1) distribution, which means that \\(\\theta_j\\) will probably be negative. Conversely, if \\(\\theta_i\\) is negative, \\(\\theta_j\\) is most likely positive. Then, \\((\\theta_1,..,\\theta_{2J})\\) can't be written as a mixture form. 5-4c As \\(J \\rightarrow \\infty\\) , the negative \\(Cor(\\theta_i,\\theta_j)\\) approaches zero, and the joint distribution approaches iid. To put it another way, the distinction disappears between independently assigning each \\(\\theta_j\\) to one of two groups, and picking exactly half of the \\(\\theta_j\\) 's each group. 5-5 Let \\(\\mu(\\phi) =E(\\theta_j|\\phi)\\) . \\[cov(\\theta_i,\\theta_j) = E(cov(\\theta_i,\\theta_j|\\phi))+cov(E(\\theta_i|\\phi),E(\\theta_j|\\phi))\\] \\[0+cov(\\mu(\\phi)),\\mu(\\phi))\\] \\[=var(\\mu(\\phi)) \\geq 0.\\] 5-7 5-7(a) From formula (2.7), \\(E(y) = E(E(y|\\theta)) = \\theta = \\alpha/\\beta\\) . Because \\(y|\\theta \\sim Pois(\\theta), E(y|\\theta) = \\theta\\) . From formula (2.8), $ \\(var(y)=E(var(y|\\theta))+var(E(y|\\theta))=E(\\theta)+var(\\theta)=\\alpha/\\beta+\\alpha/\\beta^2 = \\alpha/\\beta^2(\\beta+1),\\ var(y|\\theta)=\\theta\\) $. 5-7(b) To solve this problems, \\(n,s, \\bar{y}\\) are essentially treated like constants. From (2.7) and (3.3), \\[E(\\sqrt{n}(\\mu-\\bar{y})/s|y)=E(E(\\sqrt{n}(\\mu-\\bar{y})/s|\\sigma,y)|y) =E((\\sqrt{n}/s)E(\\mu-\\bar{y}|\\sigma,y|y)=E((\\sqrt{n}/s\\cdot0|y) =E(0|y)=0.\\] We must have \\(n > 1\\) for \\(s\\) to be defined, but in order for the expectation to exist, we must have \\(n >2\\) . We can compute from (2.8), (3.3), and (3.5) that \\[var(\\sqrt{n}(\\mu-\\bar{y}/s|y)=var(E(\\sqrt{n}(\\mu-\\bar{y}/s|\\sigma,y|y)+E(var(\\sqrt{n}(\\mu-\\bar{y})/s|\\sigma,y)|y)\\] \\[=var(0|y)+E((n/s^2)var(\\mu|\\sigma,y)|y)\\] \\[E((n/s^2)\\sigma^2/n|y)\\] $ \\(E(\\sigma^2|y)/s^2 =\\frac{n-1}{n-3}\\) $. For this to work, we need \\(n > 3\\) . 5-8 Let \\(p_m(\\theta|y)\\) denote the posterior density of \\(\\theta\\) corresponding to the prior density \\(p_m(\\theta)\\) .If \\(p(\\theta) = \\sum_n\\lambda_mp_m(\\theta),\\) the posterior density of \\(\\theta\\) is proportional to \\(\\sum_m\\lambda_mp_m(\\theta)p(y|\\theta)=\\sum_m\\lambda_mp_m(y)p_m(\\theta|y):\\) mixture of the posterior densities \\(p_m(\\theta|y)\\) with weights proportional to \\(\\lambda_mp_m(y)\\) . Since each \\(p_m(\\theta)\\) is conjugate for the model for y given \\(\\theta\\) , the preceding computation demonstrates that the class of finite mixture prior densities is also conjugate. Consider an example : \\(p_1(\\theta) \\sim N(1,0.5^2), p_2(\\theta) \\sim N(-1,0.5^2)\\) , and suppose \\(\\lambda_1=0.9, \\lambda_2=0.1\\) . We know that \\(p_1(\\theta|y) \\sim N(1.5/14,1/14)\\) and \\(p_2(\\theta|y) \\sim N(-6.5/14,1/14)\\) . We also know \\(p(\\theta|y)\\) will be a weighted sum of these conditional posterior densities with weights \\(\\lambda_mp_m(y)/\\sum_k\\lambda_kp_k(y)\\) for \\(m=1,2\\) . \\(p_1(y) = N(-0.25|1,0.5^2+1/10)=0.072\\) , and \\(p_2(y) = N(-0.25|-1,0.5^2+1/10)=0.302\\) . So the weights for \\(p_1(\\theta|y)\\) and \\(p_2(y)\\) are not 0.9 and 0.1 but are, \\(\\frac{0.9\\cdot0.072}{0.9\\cdot0.072+0.1\\cdot0.302}=0.68\\) and \\(\\frac{0.1\\cdot0.302}{0.9\\cdot0.072+0.1\\cdot0.302}=0.32\\) . theta <- seq(-3,3,.01) prior <- c (0.9, 0.1) dens <- prior[1]*dnorm(theta,1,0.5) + prior[2]*dnorm(theta,-1,0.5) plot (theta, dens, ylim=c(0,1.1*max(dens)), type=\"l\", xlab=\"theta\", ylab=\"\", xaxs=\"i\", yaxs=\"i\", yaxt=\"n\", bty=\"n\", cex=2) mtext (\"prior density\", cex=2, 3) marg <- dnorm(-.25,c(1,-1),sqrt(c(0.5,0.5)^2+1/10)) posterior <- prior*marg/sum(prior*marg) dens <- posterior[1]*dnorm(theta,1.5/14,sqrt(1/14)) + posterior[2]*dnorm(theta,-6.5/14,sqrt(1/14)) plot (theta, dens, ylim=c(0,1.1*max(dens)), type=\"l\", xlab=\"theta\", ylab=\"\", xaxs=\"i\", yaxs=\"i\", yaxt=\"n\", bty=\"n\", cex=2) mtext (\"posterior density\", cex=2, 3) 5-9 5-9a Consider the limit \\((\\alpha+\\beta) \\rightarrow \\infty\\) with \\(\\alpha/\\beta\\) fixed at any nonzero value. The likelihood is \\[p(y|\\alpha,\\beta) \\propto \\prod_{j=1}^J\\frac{[\\alpha\\cdots(\\alpha+y_i-1)][\\beta\\cdot n_j-y_j-1)]}{(\\alpha+\\beta)\\cdots(\\alpha+\\beta+n_j-1)}\\] \\[\\approx \\prod_{j=1}^J\\frac{\\alpha^{y_j}\\beta^{n_j-y_j}}{(\\alpha+\\beta)^n_j}\\] $ \\(=\\prod_{j=1}^J(\\frac{\\alpha}{\\alpha+\\beta})^y_j(\\frac{\\beta}{\\alpha+\\beta})^{n_j-y_j}\\) $, which is constants, so the pior density determindes whether the posterior density has a finite integral in this limit. A uniform prior density on \\(log(\\alpha+\\beta)\\) has an infinite integral in this limit, and so the posterior density does also in this case. 5-9b \\[\\begin{equation*} \\begin{vmatrix} \\frac{\\beta}{(\\alpha+\\beta)^2} & -\\frac{\\alpha}{(\\alpha+\\beta)^2} \\\\ -\\frac{1}{2}(\\alpha+\\beta)^{-3/2} & -\\frac{1}{2}(\\alpha+\\beta)^{-3/2} \\\\ \\end{vmatrix} \\end{equation*} = constant \\cdot (\\alpha+\\beta)^{-5/2}\\] 5-12 Applying formula (2.7) and (2.8), \\[E(\\theta_j|\\tau,y) = E[E(\\theta_j|\\mu,\\tau,y)|\\tau,y] = E[\\frac{\\frac{1}{\\sigma_j^2}y_j+\\frac{1}{\\tau^2}\\mu}{\\frac{1}{\\sigma_j^2}+\\frac{1}{\\tau^2}}|\\tau,y]=\\frac{\\frac{1}{\\sigma_j^2}y_j+\\frac{1}{\\tau^2}\\hat\\mu}{\\frac{1}{\\sigma_j^2}+\\frac{1}{\\tau^2}}\\] $ \\(var(\\theta_j|\\tau,y) = E[var(\\theta_j|\\mu,\\tau,y)|\\tau,y]+var[E(\\theta_j|\\mu,\\tau,y)|\\tau,y]=\\frac{1}{\\frac{1}{\\sigma^2}+\\frac{1}{\\tau^2}}+(\\frac{\\frac{1}{\\tau^2}}{\\frac{1}{\\sigma^2}+\\frac{1}{\\tau^2}})^2V_\\mu\\) $, where expressions for \\(\\hat\\mu=E(\\mu|\\tau,y)\\) and \\(V_\\mu=var(\\mu|\\tau,y)\\) are given in (5.20). Ex: Hierarchical model for Rats experiment library(ggplot2) theme_set(theme_minimal()) library(gridExtra) library(tidyr) library(latex2exp) y <- c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2, 2,1,5,2,5,3,2,7,7,3,3,2,9,10,4,4,4,4,4,4,4,10,4,4,4,5,11,12, 5,5,6,5,6,6,6,6,16,15,15,9,4) n <- c(20,20,20,20,20,20,20,19,19,19,19,18,18,17,20,20,20,20,19,19,18,18,25,24, 23,20,20,20,20,20,20,10,49,19,46,27,17,49,47,20,20,13,48,50,20,20,20,20, 20,20,20,48,19,19,19,22,46,49,20,20,23,19,22,20,20,20,52,46,47,24,14) x <- seq(0.0001, 0.9999, length.out = 1000) bdens <- function(n, y, x) dbeta(x, y+1, n-y+1) df_sep <- mapply(bdens, n, y, MoreArgs = list(x = x)) %>% as.data.frame() %>% cbind(x) %>% gather(ind, p, -x) labs1 <- paste('posterior of', c('theta_j', 'theta_71')) plot_sep <- ggplot(data = df_sep) + geom_line(aes(x = x, y = p, color = (ind=='V71'), group = ind)) + labs(x = expression(theta), y = '', title = 'Separate model', color = '') + scale_y_continuous(breaks = NULL) + scale_color_manual(values = c('blue','red'), labels = labs1) + theme(legend.background = element_blank(), legend.position = c(0.8,0.9)) # The last one is for emphasize colored red plot_sep df_pool <- data.frame(x = x, p = dbeta(x, sum(y)+1, sum(n)-sum(y)+1)) plot_pool <- ggplot(data = df_pool) + geom_line(aes(x = x, y = p, color = '1')) + labs(x = expression(theta), y = '', title = 'Pooled model', color = '') + scale_y_continuous(breaks = NULL) + scale_color_manual(values = 'red', labels = 'Posterior of common theta') + theme(legend.background = element_blank(), legend.position = c(0.7,0.9)) grid.arrange(plot_sep, plot_pool) A <- seq(0.5, 6, length.out = 100) B <- seq(3, 33, length.out = 100) cA <- rep(A, each = length(B)) cB <- rep(B, length(A)) lpfun <- function(a, b, y, n) log(a+b)*(-5/2) + sum(lgamma(a+b)-lgamma(a)-lgamma(b)+lgamma(a+y)+lgamma(b+n-y)-lgamma(a+b+n)) lp <- mapply(lpfun, cA, cB, MoreArgs = list(y, n)) df_marg <- data.frame(x = cA, y = cB, p = exp(lp - max(lp))) title1 <- TeX('The marginal of $\\\\alpha$ and $\\\\beta$') ggplot(data = df_marg, aes(x = x, y = y)) + geom_raster(aes(fill = p, alpha = p), interpolate = T) + geom_contour(aes(z = p), colour = 'black', size = 0.2) + coord_cartesian(xlim = c(1,5), ylim = c(4, 26)) + labs(x = TeX('$\\\\alpha$'), y = TeX('$\\\\beta$'), title = title1) + scale_fill_gradient(low = 'yellow', high = 'red', guide = F) + scale_alpha(range = c(0, 1), guide = F) nsamp <- 100 samp_indices <- sample(length(df_marg$p), size = nsamp, replace = T, prob = df_marg$p/sum(df_marg$p)) samp_A <- cA[samp_indices[1:nsamp]] samp_B <- cB[samp_indices[1:nsamp]] df_psamp <- mapply(function(a, b, x) dbeta(x, a, b), samp_A, samp_B, MoreArgs = list(x = x)) %>% as.data.frame() %>% cbind(x) %>% gather(ind, p, -x) indtonum <- function(x) strtoi(substring(x,2)) title2 <- TeX('Beta($\\\\alpha,\\\\beta$) given posterior draws of $\\\\alpha$ and $\\\\beta$') plot_psamp <- ggplot(data = subset(df_psamp, indtonum(ind) <= 20)) + geom_line(aes(x = x, y = p, group = ind), color='forestgreen') + labs(x = expression(theta), y = '', title = title2) + scale_y_continuous(breaks = NULL) df_psampmean <- spread(df_psamp, ind, p) %>% subset(select = -x) %>% rowMeans() %>% data.frame(x = x, p = .) title3 <- TeX('Population distribution (prior) for $\\\\theta_j$') plot_psampmean <- ggplot(data = df_psampmean) + geom_line(aes(x = x, y = p), color='forestgreen') + labs(x = expression(theta), y = '', title = title3) + scale_y_continuous(breaks = NULL) grid.arrange(plot_psamp, plot_psampmean) plot_sep7 <- ggplot(data = subset(df_sep, indtonum(ind)%%7==0)) + geom_line(aes(x = x, y = p, color = (ind=='V49'), group = ind)) + labs(x = expression(theta), y = '', title = 'Separate model', color = '') + scale_y_continuous(breaks = NULL) + scale_color_manual(values = c('blue', 'red'), guide = F) + theme(legend.background = element_blank(), legend.position = c(0.8,0.9)) bdens2 <- function(n, y, a, b, x) rowMeans(mapply(dbeta, a + y, n - y + b, MoreArgs = list(x = x))) df_hier <- mapply(bdens2, n, y, MoreArgs = list(samp_A, samp_B, x)) %>% as.data.frame() %>% cbind(x) %>% gather(ind, p, -x) plot_hier7 <- ggplot(data = subset(df_hier, indtonum(ind)%%7==0)) + geom_line(aes(x = x, y = p, color = (ind=='V49'), group = ind)) + labs(x = expression(theta), y = '', title = 'Hierarchical model', color = '') + scale_color_manual(values = c('blue', 'red'), guide = F) + scale_y_continuous(breaks = NULL) + theme(legend.background = element_blank(), legend.position = c(0.8,0.9)) grid.arrange(plot_sep7, plot_hier7) qq_separate<-data.frame(id=1:length(n), n=n, y=y, q10=qbeta(0.1,y+1,n-y+1), q90=qbeta(0.9,y+1,n-y+1)) qh <- function(q, n, y) colMeans(mapply(function(q, n, y, a, b) mapply(qbeta, q, a + y, n - y + b), q, n, y, MoreArgs = list(samp_A, samp_B))) qq_hier <- data.frame(id=1:length(n), n=n, y=y, qh(0.05, n, y), qh(0.95, n, y)) ggplot(data=qq_separate[seq(1,length(n),by=3),], aes(x=jitter(n,amount=1),ymin=q10,ymax=q90)) + geom_linerange() + xlim(c(0,60))","title":"R Notebook"},{"location":"10%20Bayesian%20Statistics/HW5/#bayes-statistics-hw5","text":"","title":"Bayes Statistics HW5"},{"location":"10%20Bayesian%20Statistics/HW5/#5-4","text":"","title":"5-4."},{"location":"10%20Bayesian%20Statistics/HW5/#5-4a","text":"Yes. The joint distribution is $ \\(p(\\theta_1,..,\\theta_{2J}) = {2J \\choose J}(\\prod_{j=1}^JN(\\theta_{p(j)}|1,1)N(\\theta_{p(j)}|-1,1))\\) $, where the sum is over all permutations p of (1,...,2J). The density above is obviously invariant permutations of the indexex (1,...,2J).","title":"5-4a"},{"location":"10%20Bayesian%20Statistics/HW5/#5-4b","text":"The covariance of \\(\\theta_i, \\theta_j\\) is negative. If \\(\\theta_i\\) is large, then it probably comes from the N(1,1) distribution, which means that it is more likely than not that \\(\\theta_j\\) from the N(-1,1) distribution, which means that \\(\\theta_j\\) will probably be negative. Conversely, if \\(\\theta_i\\) is negative, \\(\\theta_j\\) is most likely positive. Then, \\((\\theta_1,..,\\theta_{2J})\\) can't be written as a mixture form.","title":"5-4b"},{"location":"10%20Bayesian%20Statistics/HW5/#5-4c","text":"As \\(J \\rightarrow \\infty\\) , the negative \\(Cor(\\theta_i,\\theta_j)\\) approaches zero, and the joint distribution approaches iid. To put it another way, the distinction disappears between independently assigning each \\(\\theta_j\\) to one of two groups, and picking exactly half of the \\(\\theta_j\\) 's each group.","title":"5-4c"},{"location":"10%20Bayesian%20Statistics/HW5/#5-5","text":"Let \\(\\mu(\\phi) =E(\\theta_j|\\phi)\\) . \\[cov(\\theta_i,\\theta_j) = E(cov(\\theta_i,\\theta_j|\\phi))+cov(E(\\theta_i|\\phi),E(\\theta_j|\\phi))\\] \\[0+cov(\\mu(\\phi)),\\mu(\\phi))\\] \\[=var(\\mu(\\phi)) \\geq 0.\\]","title":"5-5"},{"location":"10%20Bayesian%20Statistics/HW5/#5-7","text":"","title":"5-7"},{"location":"10%20Bayesian%20Statistics/HW5/#5-7a","text":"From formula (2.7), \\(E(y) = E(E(y|\\theta)) = \\theta = \\alpha/\\beta\\) . Because \\(y|\\theta \\sim Pois(\\theta), E(y|\\theta) = \\theta\\) . From formula (2.8), $ \\(var(y)=E(var(y|\\theta))+var(E(y|\\theta))=E(\\theta)+var(\\theta)=\\alpha/\\beta+\\alpha/\\beta^2 = \\alpha/\\beta^2(\\beta+1),\\ var(y|\\theta)=\\theta\\) $.","title":"5-7(a)"},{"location":"10%20Bayesian%20Statistics/HW5/#5-7b","text":"To solve this problems, \\(n,s, \\bar{y}\\) are essentially treated like constants. From (2.7) and (3.3), \\[E(\\sqrt{n}(\\mu-\\bar{y})/s|y)=E(E(\\sqrt{n}(\\mu-\\bar{y})/s|\\sigma,y)|y) =E((\\sqrt{n}/s)E(\\mu-\\bar{y}|\\sigma,y|y)=E((\\sqrt{n}/s\\cdot0|y) =E(0|y)=0.\\] We must have \\(n > 1\\) for \\(s\\) to be defined, but in order for the expectation to exist, we must have \\(n >2\\) . We can compute from (2.8), (3.3), and (3.5) that \\[var(\\sqrt{n}(\\mu-\\bar{y}/s|y)=var(E(\\sqrt{n}(\\mu-\\bar{y}/s|\\sigma,y|y)+E(var(\\sqrt{n}(\\mu-\\bar{y})/s|\\sigma,y)|y)\\] \\[=var(0|y)+E((n/s^2)var(\\mu|\\sigma,y)|y)\\] \\[E((n/s^2)\\sigma^2/n|y)\\] $ \\(E(\\sigma^2|y)/s^2 =\\frac{n-1}{n-3}\\) $. For this to work, we need \\(n > 3\\) .","title":"5-7(b)"},{"location":"10%20Bayesian%20Statistics/HW5/#5-8","text":"Let \\(p_m(\\theta|y)\\) denote the posterior density of \\(\\theta\\) corresponding to the prior density \\(p_m(\\theta)\\) .If \\(p(\\theta) = \\sum_n\\lambda_mp_m(\\theta),\\) the posterior density of \\(\\theta\\) is proportional to \\(\\sum_m\\lambda_mp_m(\\theta)p(y|\\theta)=\\sum_m\\lambda_mp_m(y)p_m(\\theta|y):\\) mixture of the posterior densities \\(p_m(\\theta|y)\\) with weights proportional to \\(\\lambda_mp_m(y)\\) . Since each \\(p_m(\\theta)\\) is conjugate for the model for y given \\(\\theta\\) , the preceding computation demonstrates that the class of finite mixture prior densities is also conjugate. Consider an example : \\(p_1(\\theta) \\sim N(1,0.5^2), p_2(\\theta) \\sim N(-1,0.5^2)\\) , and suppose \\(\\lambda_1=0.9, \\lambda_2=0.1\\) . We know that \\(p_1(\\theta|y) \\sim N(1.5/14,1/14)\\) and \\(p_2(\\theta|y) \\sim N(-6.5/14,1/14)\\) . We also know \\(p(\\theta|y)\\) will be a weighted sum of these conditional posterior densities with weights \\(\\lambda_mp_m(y)/\\sum_k\\lambda_kp_k(y)\\) for \\(m=1,2\\) . \\(p_1(y) = N(-0.25|1,0.5^2+1/10)=0.072\\) , and \\(p_2(y) = N(-0.25|-1,0.5^2+1/10)=0.302\\) . So the weights for \\(p_1(\\theta|y)\\) and \\(p_2(y)\\) are not 0.9 and 0.1 but are, \\(\\frac{0.9\\cdot0.072}{0.9\\cdot0.072+0.1\\cdot0.302}=0.68\\) and \\(\\frac{0.1\\cdot0.302}{0.9\\cdot0.072+0.1\\cdot0.302}=0.32\\) . theta <- seq(-3,3,.01) prior <- c (0.9, 0.1) dens <- prior[1]*dnorm(theta,1,0.5) + prior[2]*dnorm(theta,-1,0.5) plot (theta, dens, ylim=c(0,1.1*max(dens)), type=\"l\", xlab=\"theta\", ylab=\"\", xaxs=\"i\", yaxs=\"i\", yaxt=\"n\", bty=\"n\", cex=2) mtext (\"prior density\", cex=2, 3) marg <- dnorm(-.25,c(1,-1),sqrt(c(0.5,0.5)^2+1/10)) posterior <- prior*marg/sum(prior*marg) dens <- posterior[1]*dnorm(theta,1.5/14,sqrt(1/14)) + posterior[2]*dnorm(theta,-6.5/14,sqrt(1/14)) plot (theta, dens, ylim=c(0,1.1*max(dens)), type=\"l\", xlab=\"theta\", ylab=\"\", xaxs=\"i\", yaxs=\"i\", yaxt=\"n\", bty=\"n\", cex=2) mtext (\"posterior density\", cex=2, 3)","title":"5-8"},{"location":"10%20Bayesian%20Statistics/HW5/#5-9","text":"","title":"5-9"},{"location":"10%20Bayesian%20Statistics/HW5/#5-9a","text":"Consider the limit \\((\\alpha+\\beta) \\rightarrow \\infty\\) with \\(\\alpha/\\beta\\) fixed at any nonzero value. The likelihood is \\[p(y|\\alpha,\\beta) \\propto \\prod_{j=1}^J\\frac{[\\alpha\\cdots(\\alpha+y_i-1)][\\beta\\cdot n_j-y_j-1)]}{(\\alpha+\\beta)\\cdots(\\alpha+\\beta+n_j-1)}\\] \\[\\approx \\prod_{j=1}^J\\frac{\\alpha^{y_j}\\beta^{n_j-y_j}}{(\\alpha+\\beta)^n_j}\\] $ \\(=\\prod_{j=1}^J(\\frac{\\alpha}{\\alpha+\\beta})^y_j(\\frac{\\beta}{\\alpha+\\beta})^{n_j-y_j}\\) $, which is constants, so the pior density determindes whether the posterior density has a finite integral in this limit. A uniform prior density on \\(log(\\alpha+\\beta)\\) has an infinite integral in this limit, and so the posterior density does also in this case.","title":"5-9a"},{"location":"10%20Bayesian%20Statistics/HW5/#5-9b","text":"\\[\\begin{equation*} \\begin{vmatrix} \\frac{\\beta}{(\\alpha+\\beta)^2} & -\\frac{\\alpha}{(\\alpha+\\beta)^2} \\\\ -\\frac{1}{2}(\\alpha+\\beta)^{-3/2} & -\\frac{1}{2}(\\alpha+\\beta)^{-3/2} \\\\ \\end{vmatrix} \\end{equation*} = constant \\cdot (\\alpha+\\beta)^{-5/2}\\]","title":"5-9b"},{"location":"10%20Bayesian%20Statistics/HW5/#5-12","text":"Applying formula (2.7) and (2.8), \\[E(\\theta_j|\\tau,y) = E[E(\\theta_j|\\mu,\\tau,y)|\\tau,y] = E[\\frac{\\frac{1}{\\sigma_j^2}y_j+\\frac{1}{\\tau^2}\\mu}{\\frac{1}{\\sigma_j^2}+\\frac{1}{\\tau^2}}|\\tau,y]=\\frac{\\frac{1}{\\sigma_j^2}y_j+\\frac{1}{\\tau^2}\\hat\\mu}{\\frac{1}{\\sigma_j^2}+\\frac{1}{\\tau^2}}\\] $ \\(var(\\theta_j|\\tau,y) = E[var(\\theta_j|\\mu,\\tau,y)|\\tau,y]+var[E(\\theta_j|\\mu,\\tau,y)|\\tau,y]=\\frac{1}{\\frac{1}{\\sigma^2}+\\frac{1}{\\tau^2}}+(\\frac{\\frac{1}{\\tau^2}}{\\frac{1}{\\sigma^2}+\\frac{1}{\\tau^2}})^2V_\\mu\\) $, where expressions for \\(\\hat\\mu=E(\\mu|\\tau,y)\\) and \\(V_\\mu=var(\\mu|\\tau,y)\\) are given in (5.20).","title":"5-12"},{"location":"10%20Bayesian%20Statistics/HW5/#ex-hierarchical-model-for-rats-experiment","text":"library(ggplot2) theme_set(theme_minimal()) library(gridExtra) library(tidyr) library(latex2exp) y <- c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2, 2,1,5,2,5,3,2,7,7,3,3,2,9,10,4,4,4,4,4,4,4,10,4,4,4,5,11,12, 5,5,6,5,6,6,6,6,16,15,15,9,4) n <- c(20,20,20,20,20,20,20,19,19,19,19,18,18,17,20,20,20,20,19,19,18,18,25,24, 23,20,20,20,20,20,20,10,49,19,46,27,17,49,47,20,20,13,48,50,20,20,20,20, 20,20,20,48,19,19,19,22,46,49,20,20,23,19,22,20,20,20,52,46,47,24,14) x <- seq(0.0001, 0.9999, length.out = 1000) bdens <- function(n, y, x) dbeta(x, y+1, n-y+1) df_sep <- mapply(bdens, n, y, MoreArgs = list(x = x)) %>% as.data.frame() %>% cbind(x) %>% gather(ind, p, -x) labs1 <- paste('posterior of', c('theta_j', 'theta_71')) plot_sep <- ggplot(data = df_sep) + geom_line(aes(x = x, y = p, color = (ind=='V71'), group = ind)) + labs(x = expression(theta), y = '', title = 'Separate model', color = '') + scale_y_continuous(breaks = NULL) + scale_color_manual(values = c('blue','red'), labels = labs1) + theme(legend.background = element_blank(), legend.position = c(0.8,0.9)) # The last one is for emphasize colored red plot_sep df_pool <- data.frame(x = x, p = dbeta(x, sum(y)+1, sum(n)-sum(y)+1)) plot_pool <- ggplot(data = df_pool) + geom_line(aes(x = x, y = p, color = '1')) + labs(x = expression(theta), y = '', title = 'Pooled model', color = '') + scale_y_continuous(breaks = NULL) + scale_color_manual(values = 'red', labels = 'Posterior of common theta') + theme(legend.background = element_blank(), legend.position = c(0.7,0.9)) grid.arrange(plot_sep, plot_pool) A <- seq(0.5, 6, length.out = 100) B <- seq(3, 33, length.out = 100) cA <- rep(A, each = length(B)) cB <- rep(B, length(A)) lpfun <- function(a, b, y, n) log(a+b)*(-5/2) + sum(lgamma(a+b)-lgamma(a)-lgamma(b)+lgamma(a+y)+lgamma(b+n-y)-lgamma(a+b+n)) lp <- mapply(lpfun, cA, cB, MoreArgs = list(y, n)) df_marg <- data.frame(x = cA, y = cB, p = exp(lp - max(lp))) title1 <- TeX('The marginal of $\\\\alpha$ and $\\\\beta$') ggplot(data = df_marg, aes(x = x, y = y)) + geom_raster(aes(fill = p, alpha = p), interpolate = T) + geom_contour(aes(z = p), colour = 'black', size = 0.2) + coord_cartesian(xlim = c(1,5), ylim = c(4, 26)) + labs(x = TeX('$\\\\alpha$'), y = TeX('$\\\\beta$'), title = title1) + scale_fill_gradient(low = 'yellow', high = 'red', guide = F) + scale_alpha(range = c(0, 1), guide = F) nsamp <- 100 samp_indices <- sample(length(df_marg$p), size = nsamp, replace = T, prob = df_marg$p/sum(df_marg$p)) samp_A <- cA[samp_indices[1:nsamp]] samp_B <- cB[samp_indices[1:nsamp]] df_psamp <- mapply(function(a, b, x) dbeta(x, a, b), samp_A, samp_B, MoreArgs = list(x = x)) %>% as.data.frame() %>% cbind(x) %>% gather(ind, p, -x) indtonum <- function(x) strtoi(substring(x,2)) title2 <- TeX('Beta($\\\\alpha,\\\\beta$) given posterior draws of $\\\\alpha$ and $\\\\beta$') plot_psamp <- ggplot(data = subset(df_psamp, indtonum(ind) <= 20)) + geom_line(aes(x = x, y = p, group = ind), color='forestgreen') + labs(x = expression(theta), y = '', title = title2) + scale_y_continuous(breaks = NULL) df_psampmean <- spread(df_psamp, ind, p) %>% subset(select = -x) %>% rowMeans() %>% data.frame(x = x, p = .) title3 <- TeX('Population distribution (prior) for $\\\\theta_j$') plot_psampmean <- ggplot(data = df_psampmean) + geom_line(aes(x = x, y = p), color='forestgreen') + labs(x = expression(theta), y = '', title = title3) + scale_y_continuous(breaks = NULL) grid.arrange(plot_psamp, plot_psampmean) plot_sep7 <- ggplot(data = subset(df_sep, indtonum(ind)%%7==0)) + geom_line(aes(x = x, y = p, color = (ind=='V49'), group = ind)) + labs(x = expression(theta), y = '', title = 'Separate model', color = '') + scale_y_continuous(breaks = NULL) + scale_color_manual(values = c('blue', 'red'), guide = F) + theme(legend.background = element_blank(), legend.position = c(0.8,0.9)) bdens2 <- function(n, y, a, b, x) rowMeans(mapply(dbeta, a + y, n - y + b, MoreArgs = list(x = x))) df_hier <- mapply(bdens2, n, y, MoreArgs = list(samp_A, samp_B, x)) %>% as.data.frame() %>% cbind(x) %>% gather(ind, p, -x) plot_hier7 <- ggplot(data = subset(df_hier, indtonum(ind)%%7==0)) + geom_line(aes(x = x, y = p, color = (ind=='V49'), group = ind)) + labs(x = expression(theta), y = '', title = 'Hierarchical model', color = '') + scale_color_manual(values = c('blue', 'red'), guide = F) + scale_y_continuous(breaks = NULL) + theme(legend.background = element_blank(), legend.position = c(0.8,0.9)) grid.arrange(plot_sep7, plot_hier7) qq_separate<-data.frame(id=1:length(n), n=n, y=y, q10=qbeta(0.1,y+1,n-y+1), q90=qbeta(0.9,y+1,n-y+1)) qh <- function(q, n, y) colMeans(mapply(function(q, n, y, a, b) mapply(qbeta, q, a + y, n - y + b), q, n, y, MoreArgs = list(samp_A, samp_B))) qq_hier <- data.frame(id=1:length(n), n=n, y=y, qh(0.05, n, y), qh(0.95, n, y)) ggplot(data=qq_separate[seq(1,length(n),by=3),], aes(x=jitter(n,amount=1),ymin=q10,ymax=q90)) + geom_linerange() + xlim(c(0,60))","title":"Ex: Hierarchical model for Rats experiment"},{"location":"10%20Bayesian%20Statistics/HW6/","text":"Bayes Statistics HW6 10-4 10-4a Suppose that \\(\\theta\\) is drawn forme the density proportional to \\(g(\\theta)\\) , and U is a random Uniform(0,1) draw. Then we can express the cdf of draws accepted by rejection sampling as \\[Pr(\\theta \\leq \\theta^*|\\theta \\ is\\ accepted) = \\frac{Pr(\\theta \\leq \\theta^*\\ and \\ U \\leq \\frac{p(\\theta|y)}{Mg(\\theta)})}{Pr(U \\leq \\frac{p(\\theta|y)}{Mg(\\theta)})} \\] \\[\\frac{\\int_{-\\infty}^{\\theta^*}\\int_0^{p(\\theta|y)/(Mg(]\\theta))}g(\\theta)dud\\theta} {\\int_{-\\infty}^{\\infty}\\int_0^{p(\\theta|y)/(Mg(\\theta))}g(\\theta)dud\\theta}\\] \\[\\frac{\\frac{1}{M}\\int_{-\\infty}^{\\theta^*}p(\\theta |y)d\\theta }{\\frac{1}{M}}\\] which is the cdf for \\(p(\\theta|y)\\) . 10-4b Above proof requires that (i) \\(g(\\theta) > 0\\) wherever \\(p(\\theta) > 0\\) and that (ii) \\(p(\\theta)/Mg(\\theta) \\leq 1\\) always. If \\(p/g\\) is unbounded, than one or both of (i) and (ii) will be violated. 11-1 As described on 279, consider any two points \\(\\theta_a\\) and \\(\\theta_b\\) at iteration t labeled so that \\(p(\\theta_b|y)J_t(\\theta_a|\\theta_b) \\geq p(\\theta_a|y)J_t(\\theta_b|\\theta_a)\\) . To show that the posterior distribution is a stationary distribution, suppose that \\(\\theta^{t-1}\\) is a draw from the posterior distribution. Then teh unconditional probability density of a transition from \\(\\theta_a\\) to \\(\\theta_b\\) is $ \\(p(\\theta^{t-1}=\\theta_a,\\theta^t=\\theta_b) = p(\\theta_a|y)J_t(\\theta_b|\\theta_a)\\) $, where the acceptance probability is 1 cuz of our labeling of a and b so that the ratio of importance ratios is at least 1. The uncinditional probability density of a transition from \\(\\theta_b\\) to \\(\\theta_a\\) is $ \\(p(\\theta^t=\\theta_a,\\theta^{t-1}=\\theta_b)=p(\\theta_b|y)J_t(\\theta_a|\\theta_b)(\\frac{p(\\theta_a|y)/J_t(\\theta_)a|\\theta_b)}{p(\\theta_b|y)/J_t(\\theta_b|\\theta_a)})\\) $, whichi is the same as the unconditional probability in the other direction. Since their joint distribution is symmetric, \\(\\theta^t\\) and \\(\\theta^{t-1}\\) have the same marginal distributions, and so \\(p(\\theta|y)\\) is a stationary distribution of the Markov chain. As with the Metropolis algorithm, the stationary distribution is unique if the Markov chain is irreducible, aperiodic, and not transient. Importance sampling with normal distribution as a proposal for Bioassay model library(ggplot2) theme_set(theme_minimal()) library(gridExtra) library(grid) library(tidyr) library(MASS) library(loo) ## This is loo version 2.4.1 ## - Online documentation and vignettes at mc-stan.org/loo ## - As of v2.0.0 loo defaults to 1 core but we recommend using as many as possible. Use the 'cores' argument or set options(mc.cores = NUM_CORES) for an entire session. df1 <- data.frame(x = c(-0.86, -0.30, -0.05, 0.73),n = c(5, 5, 5, 5), y = c(0, 1, 3, 5)) A = seq(-1.5, 7, length.out = 100) B = seq(-5, 35, length.out = 100) # make vectors that contain all pairwise combinations of A and B cA <- rep(A, each = length(B)) cB <- rep(B, length(A)) logl <- function(df, a, b) df['y']*(a + b*df['x']) - df['n']*log1p(exp(a + b*df['x'])) # calculate likelihoods: apply logl function for each observation # ie. each row of data frame of x, n and y p <- apply(df1, 1, logl, cA, cB) %>% # sum the log likelihoods of observations # and exponentiate to get the joint likelihood rowSums() %>% exp() nsamp <- 1000 samp_indices <- sample(length(p), size = nsamp, replace = T, prob = p/sum(p)) samp_A <- cA[samp_indices[1:nsamp]] samp_B <- cB[samp_indices[1:nsamp]] # add random jitter, see BDA3 p. 76 samp_A <- samp_A + runif(nsamp, (A[1] - A[2])/2, (A[2] - A[1])/2) samp_B <- samp_B + runif(nsamp, (B[1] - B[2])/2, (B[2] - B[1])/2) samp_ld50 <- -samp_A/samp_B xl <- c(-2, 7) yl <- c(-2, 35) pos <- ggplot(data = data.frame(cA ,cB, p), aes(x = cA, y = cB)) + geom_raster(aes(fill = p, alpha = p), interpolate = T) + geom_contour(aes(z = p), colour = 'black', size = 0.2) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') + scale_fill_gradient(low = 'yellow', high = 'red', guide = F) + scale_alpha(range = c(0, 1), guide = F) pos sam <- ggplot(data = data.frame(samp_A, samp_B)) + geom_point(aes(samp_A, samp_B), color = 'blue', size = 0.3) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') sam his <- ggplot() + geom_histogram(aes(samp_ld50), binwidth = 0.05, fill = 'steelblue', color = 'black') + coord_cartesian(xlim = c(-0.8, 0.8)) + labs(x = 'LD50 = -alpha/beta') his Normal approximation for Bioassay model bioassayfun <- function(w, df) { z <- w[1] + w[2]*df$x -sum(df$y*(z) - df$n*log1p(exp(z))) } w0 <- c(0,0) optim_res <- optim(w0, bioassayfun, gr = NULL, df1, hessian = T) w <- optim_res$par S <- solve(optim_res$hessian) dmvnorm <- function(x, mu, sig) exp(-0.5*(length(x)*log(2*pi) + log(det(sig)) + (x-mu)%*%solve(sig, x-mu))) p <- apply(cbind(cA, cB), 1, dmvnorm, w, S) samp_norm <- mvrnorm(nsamp, w, S) bpi <- samp_norm[,2] > 0 samp_norm_ld50 <- -samp_norm[bpi,1]/samp_norm[bpi,2] pos_norm <- ggplot(data = data.frame(cA ,cB, p), aes(x = cA, y = cB)) + geom_raster(aes(fill = p, alpha = p), interpolate = T) + geom_contour(aes(z = p), colour = 'black', size = 0.2) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') + scale_fill_gradient(low = 'yellow', high = 'red', guide = F) + scale_alpha(range = c(0, 1), guide = F) pos_norm sam_norm <- ggplot(data = data.frame(samp_A=samp_norm[,1], samp_B=samp_norm[,2])) + geom_point(aes(samp_A, samp_B), color = 'blue', size = 0.3) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') sam_norm his_norm <- ggplot() + geom_histogram(aes(samp_norm_ld50), binwidth = 0.05, fill = 'steelblue', color = 'black') + coord_cartesian(xlim = c(-0.8, 0.8)) + labs(x = 'LD50 = -alpha/beta, beta > 0') his_norm Importance sampling for Bioassay model ldmvnorm <- function(x, mu, sig) (-0.5*(length(x)*log(2*pi) + log(det(sig)) + (x-mu)%*%solve(sig, x-mu))) lg <- apply(samp_norm, 1, ldmvnorm, w, S) lp <- apply(df1, 1, logl, samp_norm[,1], samp_norm[,2]) %>% rowSums() lw <- lp-lg psislw <- psis(lw, r_eff = 1) ## Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details. print(psislw$diagnostics$pareto_k, digits=2) ## [1] 0.74 print(psislw$diagnostics$n_eff, digits=2) ## [1] 385 psisw <- exp(psislw$log_weights) samp_indices <- sample(length(psisw), size = nsamp, replace = T, prob = psisw) rissamp_A <- samp_norm[samp_indices,1] rissamp_B <- samp_norm[samp_indices,2] # add random jitter, see BDA3 p. 76 rissamp_A <- rissamp_A + runif(nsamp, (A[1] - A[2])/2, (A[2] - A[1])/2) rissamp_B <- rissamp_B + runif(nsamp, (B[1] - B[2])/2, (B[2] - B[1])/2) # samples of LD50 rissamp_ld50 <- -rissamp_A/rissamp_B sam_ris <- ggplot(data = data.frame(rissamp_A, rissamp_B)) + geom_point(aes(rissamp_A, rissamp_B), color = 'blue', size = 0.3) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') sam_ris his_ris <- ggplot() + geom_histogram(aes(rissamp_ld50), binwidth = 0.05, fill = 'steelblue', color = 'black') + coord_cartesian(xlim = c(-0.8, 0.8)) + labs(x = 'LD50 = -alpha/beta') his_ris blank <- grid.rect(gp=gpar(col=\"white\")) grid.arrange(pos, sam, his, pos_norm, sam_norm, his_norm, blank, sam_ris, his_ris, ncol=3) Gibbs sampling Metropolis sampling + convergence illustration Metropolis sampling + convergence illustration","title":"R Notebook"},{"location":"10%20Bayesian%20Statistics/HW6/#bayes-statistics-hw6","text":"","title":"Bayes Statistics HW6"},{"location":"10%20Bayesian%20Statistics/HW6/#10-4","text":"","title":"10-4"},{"location":"10%20Bayesian%20Statistics/HW6/#10-4a","text":"Suppose that \\(\\theta\\) is drawn forme the density proportional to \\(g(\\theta)\\) , and U is a random Uniform(0,1) draw. Then we can express the cdf of draws accepted by rejection sampling as \\[Pr(\\theta \\leq \\theta^*|\\theta \\ is\\ accepted) = \\frac{Pr(\\theta \\leq \\theta^*\\ and \\ U \\leq \\frac{p(\\theta|y)}{Mg(\\theta)})}{Pr(U \\leq \\frac{p(\\theta|y)}{Mg(\\theta)})} \\] \\[\\frac{\\int_{-\\infty}^{\\theta^*}\\int_0^{p(\\theta|y)/(Mg(]\\theta))}g(\\theta)dud\\theta} {\\int_{-\\infty}^{\\infty}\\int_0^{p(\\theta|y)/(Mg(\\theta))}g(\\theta)dud\\theta}\\] \\[\\frac{\\frac{1}{M}\\int_{-\\infty}^{\\theta^*}p(\\theta |y)d\\theta }{\\frac{1}{M}}\\] which is the cdf for \\(p(\\theta|y)\\) .","title":"10-4a"},{"location":"10%20Bayesian%20Statistics/HW6/#10-4b","text":"Above proof requires that (i) \\(g(\\theta) > 0\\) wherever \\(p(\\theta) > 0\\) and that (ii) \\(p(\\theta)/Mg(\\theta) \\leq 1\\) always. If \\(p/g\\) is unbounded, than one or both of (i) and (ii) will be violated.","title":"10-4b"},{"location":"10%20Bayesian%20Statistics/HW6/#11-1","text":"As described on 279, consider any two points \\(\\theta_a\\) and \\(\\theta_b\\) at iteration t labeled so that \\(p(\\theta_b|y)J_t(\\theta_a|\\theta_b) \\geq p(\\theta_a|y)J_t(\\theta_b|\\theta_a)\\) . To show that the posterior distribution is a stationary distribution, suppose that \\(\\theta^{t-1}\\) is a draw from the posterior distribution. Then teh unconditional probability density of a transition from \\(\\theta_a\\) to \\(\\theta_b\\) is $ \\(p(\\theta^{t-1}=\\theta_a,\\theta^t=\\theta_b) = p(\\theta_a|y)J_t(\\theta_b|\\theta_a)\\) $, where the acceptance probability is 1 cuz of our labeling of a and b so that the ratio of importance ratios is at least 1. The uncinditional probability density of a transition from \\(\\theta_b\\) to \\(\\theta_a\\) is $ \\(p(\\theta^t=\\theta_a,\\theta^{t-1}=\\theta_b)=p(\\theta_b|y)J_t(\\theta_a|\\theta_b)(\\frac{p(\\theta_a|y)/J_t(\\theta_)a|\\theta_b)}{p(\\theta_b|y)/J_t(\\theta_b|\\theta_a)})\\) $, whichi is the same as the unconditional probability in the other direction. Since their joint distribution is symmetric, \\(\\theta^t\\) and \\(\\theta^{t-1}\\) have the same marginal distributions, and so \\(p(\\theta|y)\\) is a stationary distribution of the Markov chain. As with the Metropolis algorithm, the stationary distribution is unique if the Markov chain is irreducible, aperiodic, and not transient.","title":"11-1"},{"location":"10%20Bayesian%20Statistics/HW6/#importance-sampling-with-normal-distribution-as-a-proposal-for-bioassay-model","text":"library(ggplot2) theme_set(theme_minimal()) library(gridExtra) library(grid) library(tidyr) library(MASS) library(loo) ## This is loo version 2.4.1 ## - Online documentation and vignettes at mc-stan.org/loo ## - As of v2.0.0 loo defaults to 1 core but we recommend using as many as possible. Use the 'cores' argument or set options(mc.cores = NUM_CORES) for an entire session. df1 <- data.frame(x = c(-0.86, -0.30, -0.05, 0.73),n = c(5, 5, 5, 5), y = c(0, 1, 3, 5)) A = seq(-1.5, 7, length.out = 100) B = seq(-5, 35, length.out = 100) # make vectors that contain all pairwise combinations of A and B cA <- rep(A, each = length(B)) cB <- rep(B, length(A)) logl <- function(df, a, b) df['y']*(a + b*df['x']) - df['n']*log1p(exp(a + b*df['x'])) # calculate likelihoods: apply logl function for each observation # ie. each row of data frame of x, n and y p <- apply(df1, 1, logl, cA, cB) %>% # sum the log likelihoods of observations # and exponentiate to get the joint likelihood rowSums() %>% exp() nsamp <- 1000 samp_indices <- sample(length(p), size = nsamp, replace = T, prob = p/sum(p)) samp_A <- cA[samp_indices[1:nsamp]] samp_B <- cB[samp_indices[1:nsamp]] # add random jitter, see BDA3 p. 76 samp_A <- samp_A + runif(nsamp, (A[1] - A[2])/2, (A[2] - A[1])/2) samp_B <- samp_B + runif(nsamp, (B[1] - B[2])/2, (B[2] - B[1])/2) samp_ld50 <- -samp_A/samp_B xl <- c(-2, 7) yl <- c(-2, 35) pos <- ggplot(data = data.frame(cA ,cB, p), aes(x = cA, y = cB)) + geom_raster(aes(fill = p, alpha = p), interpolate = T) + geom_contour(aes(z = p), colour = 'black', size = 0.2) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') + scale_fill_gradient(low = 'yellow', high = 'red', guide = F) + scale_alpha(range = c(0, 1), guide = F) pos sam <- ggplot(data = data.frame(samp_A, samp_B)) + geom_point(aes(samp_A, samp_B), color = 'blue', size = 0.3) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') sam his <- ggplot() + geom_histogram(aes(samp_ld50), binwidth = 0.05, fill = 'steelblue', color = 'black') + coord_cartesian(xlim = c(-0.8, 0.8)) + labs(x = 'LD50 = -alpha/beta') his Normal approximation for Bioassay model bioassayfun <- function(w, df) { z <- w[1] + w[2]*df$x -sum(df$y*(z) - df$n*log1p(exp(z))) } w0 <- c(0,0) optim_res <- optim(w0, bioassayfun, gr = NULL, df1, hessian = T) w <- optim_res$par S <- solve(optim_res$hessian) dmvnorm <- function(x, mu, sig) exp(-0.5*(length(x)*log(2*pi) + log(det(sig)) + (x-mu)%*%solve(sig, x-mu))) p <- apply(cbind(cA, cB), 1, dmvnorm, w, S) samp_norm <- mvrnorm(nsamp, w, S) bpi <- samp_norm[,2] > 0 samp_norm_ld50 <- -samp_norm[bpi,1]/samp_norm[bpi,2] pos_norm <- ggplot(data = data.frame(cA ,cB, p), aes(x = cA, y = cB)) + geom_raster(aes(fill = p, alpha = p), interpolate = T) + geom_contour(aes(z = p), colour = 'black', size = 0.2) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') + scale_fill_gradient(low = 'yellow', high = 'red', guide = F) + scale_alpha(range = c(0, 1), guide = F) pos_norm sam_norm <- ggplot(data = data.frame(samp_A=samp_norm[,1], samp_B=samp_norm[,2])) + geom_point(aes(samp_A, samp_B), color = 'blue', size = 0.3) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') sam_norm his_norm <- ggplot() + geom_histogram(aes(samp_norm_ld50), binwidth = 0.05, fill = 'steelblue', color = 'black') + coord_cartesian(xlim = c(-0.8, 0.8)) + labs(x = 'LD50 = -alpha/beta, beta > 0') his_norm Importance sampling for Bioassay model ldmvnorm <- function(x, mu, sig) (-0.5*(length(x)*log(2*pi) + log(det(sig)) + (x-mu)%*%solve(sig, x-mu))) lg <- apply(samp_norm, 1, ldmvnorm, w, S) lp <- apply(df1, 1, logl, samp_norm[,1], samp_norm[,2]) %>% rowSums() lw <- lp-lg psislw <- psis(lw, r_eff = 1) ## Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details. print(psislw$diagnostics$pareto_k, digits=2) ## [1] 0.74 print(psislw$diagnostics$n_eff, digits=2) ## [1] 385 psisw <- exp(psislw$log_weights) samp_indices <- sample(length(psisw), size = nsamp, replace = T, prob = psisw) rissamp_A <- samp_norm[samp_indices,1] rissamp_B <- samp_norm[samp_indices,2] # add random jitter, see BDA3 p. 76 rissamp_A <- rissamp_A + runif(nsamp, (A[1] - A[2])/2, (A[2] - A[1])/2) rissamp_B <- rissamp_B + runif(nsamp, (B[1] - B[2])/2, (B[2] - B[1])/2) # samples of LD50 rissamp_ld50 <- -rissamp_A/rissamp_B sam_ris <- ggplot(data = data.frame(rissamp_A, rissamp_B)) + geom_point(aes(rissamp_A, rissamp_B), color = 'blue', size = 0.3) + coord_cartesian(xlim = xl, ylim = yl) + labs(x = 'alpha', y = 'beta') sam_ris his_ris <- ggplot() + geom_histogram(aes(rissamp_ld50), binwidth = 0.05, fill = 'steelblue', color = 'black') + coord_cartesian(xlim = c(-0.8, 0.8)) + labs(x = 'LD50 = -alpha/beta') his_ris blank <- grid.rect(gp=gpar(col=\"white\")) grid.arrange(pos, sam, his, pos_norm, sam_norm, his_norm, blank, sam_ris, his_ris, ncol=3)","title":"Importance sampling with normal distribution as a proposal for Bioassay model"},{"location":"10%20Bayesian%20Statistics/HW6/#gibbs-sampling","text":"","title":"Gibbs sampling"},{"location":"10%20Bayesian%20Statistics/HW6/#metropolis-sampling-convergence-illustration","text":"","title":"Metropolis sampling + convergence illustration"},{"location":"10%20Bayesian%20Statistics/HW6/#metropolis-sampling-convergence-illustration_1","text":"","title":"Metropolis sampling + convergence illustration"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/","text":"Ybigta \uad50\uc721\uc138\uc158 ML \uc2ec\ud654(1) Regression Simple Linear Regression \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\ub294 singele predictor variable\uc778 X\uc640 quantitative response Y\uc5d0 \ub300\ud55c \uad00\uacc4\ub97c \ub9e4\uc6b0 \uc9c1\uad00\uc801\uc73c\ub85c \uc124\uba85\ud558\ub294 \ubc29\ubc95\uc774\ub2e4. \uc774\ub294 \uc989 X\uc640 Y\uac04\uc5d0 \uc120\ud615\uc801\uc778 \uad00\uacc4\uac00 \uc788\ub2e4\ub294 \uac83\uc744 \uac00\uc815\ud558\uba70 \uc2dc\uc791\ud55c\ub2e4. \uc218\ud559\uc801\uc73c\ub85c \uc6b0\ub9ac\ub294 \uc774\ub97c \uc544\ub798\uc640 \uac19\uc774 \ud45c\uae30\ud55c\ub2e4. $ \\(Y\\approx {\\beta}_0 + {\\beta}_1X\\) $ \uc5ec\uae30\uc758 \\({\\beta}_0\\) \uc640 \\({\\beta}_1\\) \uc740 \ub450\uac1c\uc758 unknown constants\uc774\uba70 \uac01\uac01 \uc21c\uc11c\ub300\ub85c intercept \uc640 slope terms\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc774 \ub458\uc740 parameters \ud639\uc740 coefficients\ub85c\ub3c4 \ubd88\ub9b0\ub2e4. \uc6b0\ub9ac\ub294 \uc774\ub7ec\ud55c \ud568\uc218\uc758 \uad00\uacc4\ub97c \ucd94\uc815\ud558\uae30 \uc704\ud574\uc11c \ucd94\uc815\uce58\uc778 \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) \ub97c \uc6b0\ub9ac\uc758 training data\ub97c \ud1b5\ud574 \uc5bb\uc744 \uc218 \uc788\uace0, \uc774\ub97c \ud1b5\ud574 \ubbf8\ub798\uc758 \uc5b4\ub5a0\ud55c \uc815\ub7c9\uc801 \uac12\ub4e4\uc744 \uc608\uce21\ud558\ub294 \uac83\uc774\ub2e4. \uc774\ub97c \ub3c4\uc2dd\ud654 \ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\[\\hat{y}=\\hat{\\beta}_0 + \\hat{\\beta}_1x\\] \uc6b0\ub9ac\uc758 \ubaa9\ud45c\ub294 \uc2e4\uc81c X\uc640 Y\uc758 \uad00\uacc4\ub97c \ucd94\uc815\ud558\uc5ec \ucd94\ud6c4\uc5d0 \ub4e4\uc5b4\uc624\ub294 \ubbf8\ub798 \ub370\uc774\ud130\ub97c \ubbf8\ub9ac \uc608\uce21\ud558\ub294 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc \ubaa8\ub4e0 \uc77c\uc5d0\ub294 \uc608\uc0c1\uce58 \ubabb\ud558\ub294 \ubcc0\uc218\uac00 \uc788\uace0, \uc544\ubb34\ub9ac \ud6c8\ub828 \ub370\uc774\ud130\uac04\uc758 \uad00\uacc4\ub97c \uc798 \ucd94\uc815\ud558\uc600\ub2e4\ud558\ub354\ub77c\ub3c4 \uc774 \uac83\uc774 \ubbf8\ub798\uc5d0\ub3c4 \uc9c0\uae08 \uac16\uace0\uc788\ub294 \ub370\uc774\ud130\ucc98\ub7fc \uc77c\uc774 \ubc1c\uc0dd\ud560 \uac83\uc774\ub77c\ub294 \uac83\uc73c\ub85c \uc774\uc5b4\uc9c0\uc9c0 \uc54a\ub294\ub2e4. \uc55e\uc11c \uc704\uc5d0\uc11c \uc6b0\ub9ac\ub294 Y\uc640 X\uc758 \uad00\uacc4\ub97c \uadfc\uc0ac\uc801\uc73c\ub85c \ud45c\ud604\ud574\ubcf4\uc558\uc9c0\ub9cc, \uc774 \ub458\uac04\uc758 true relationship\uc740 \uc544\ub9c8\ub3c4 \uc774\ub7f0 \ubaa8\uc591\uc77c \uac83\uc774\ub2e4. \\[Y={\\beta}_0+{\\beta}_1X+{\\epsilon}\\] \uc704\uc5d0 \ub9e8\ucc98\uc74c\uc5d0 \ubcf4\uc5ec\uc900 \uc2dd\uacfc\ub294 \ub2e4\ub974\uac8c \ub4a4\uc5d0 \uc694\uc0c1\ud55c \uc5e1\uc2e4\ub860\uc774 \ud3ec\ud568\ub418\uc5c8\ub2e4. \uc9c0\ub3c4\ud559\uc2b5\uc758 \uad00\uc811\ud574\uc11c \uc6b0\ub9ac\ub294 Y\ub97c \ucd94\uc815\ud560\ub54c \\(Y=f(X)+{\\epsilon}\\) \uc758 \uc2dd\uc744 \ub450\uc5b4 \ud45c\uae30\ud558\ub294\ub370, \ud68c\uadc0\uc5d0\uc11c unknown function\uc778 \\(f(X)\\) \uac00 \\({\\beta}_0+{\\beta}_1X\\) \uc5d0 \ub300\uc751\ub41c\ub2e4. \uc5ec\uae30\uc11c error \\({\\epsilon}\\) \uc740 \uc2e4\uc81c Y\uc640 X\uc758 \uad00\uacc4\uac00 \uc120\ud615\uc801 \uad00\uacc4\uac00 \uc544\ub2d0\uc218\ub3c4 \uc788\ub2e4\ub294 \uac83\uc744 \ub098\ud0c0\ub0b4\uba70, \ud639\uc740 Y\uc758 \ubcc0\ub3d9\uc744 \uc57c\uae30\ud558\ub294 \ub2e4\ub978 \ubcc0\uc218\uc77c\uc218\ub3c4, \uc544\ub2c8\uba74 \uce21\uc815\uc5d0\uc11c\uc758 \uc624\ub958\uc77c\uc218\ub3c4 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc900\ub2e4. \uc6b0\ub9ac\ub294 \uadf8\ub798\uc11c \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\uc5d0\uc11c \uc774\ub7ec\ud55c error term\uc774 X\uc5d0 \ub300\ud574\uc11c \ub3c5\ub9bd\uc774\uba70 \\(N(0,{\\sigma}^2)\\) \uc5d0\uc11c \ub098\uc628 iid\ud55c \ub188\uc774\ub77c\uace0 \uac00\uc815\uc744 \ud55c\ub2e4. \uc774 \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\uc5d0\uc11c \\(X_{i}\\) \ub294 \ud655\ub960\ubcc0\uc218\uac00 \uc544\ub2cc \uc0c1\uc218\uc774\ub2e4. \uc774\ub294 \uc2e4\ud5d8\uc790\uc5d0 \uc758\ud574 \ud1b5\uc81c\ub41c \uac12\uc774\uba70 \ud1b5\uc81c\ub41c \ud658\uacbd\uc5d0\uc11c \uc124\uc815\ub418\uc5c8\uc74c\uc744 \ub098\ud0c0\ub0b8\ub2e4. \ubc18\ub300\ub85c \\(Y_{i}\\) \ub294 \uc774\ub7ec\ud55c \\(X_{i}\\) \ub4e4\uc5d0 \ub300\uc751\ud558\ub294 \ud655\ub960\ubcc0\uc218\uc774\ub2e4. \uc5ec\uae30\uc11c\ubd80\ud130 \uc0ac\uc2e4 X\ub97c \ud1b5\uc81c\ud55c\ub2e4\ub294 \uac83\uc774 \ube44\ud604\uc2e4\uc801\uc778 \uac00\uc815\uc774\uba70, prediction\uc758 \uad00\uc810\uc5d0\uc11c \uadf8 \uac00\uce58\uac00 \ub5a8\uc5b4\uc9c0\ub294 \uac00\uc815\uc774\ub77c\uace0 \ud560 \uc218\uc788\ub2e4. \uc6b0\ub9ac\uac00 \uc120\ud615\ud68c\uadc0\ub97c \ud558\uba74\uc11c \uc911\uc694\ud55c \uac83\uc740 X\uc640 Y\uc758 \uad00\uacc4\ub97c \uc798 \uc124\uba85\ud558\uba74\uc11c \uc608\uce21\ud55c \uac12\uacfc \uc2e4\uc81c\uac12\uac04\uc758 \ucc28\uc774\ub97c \ucd5c\uc18c\ub85c\ud558\ub294 \ubaa8\ub378\uc744 \ucc3e\ub294 \uac83\uc774 \ubaa9\ud45c\uc774\ub2e4. \uc774\ub97c \uac00\ub2a5\ud558\uac8c\ud558\ub294 \ubc29\ubc95\uc73c\ub85c\ub294 \uac00\uc7a5 \uc77c\ubc18\uc801\uc778 approach\uc778 RSS\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \uac83\uc774 \uc788\ub2e4. \uc704\uc758 \uadf8\ub9bc\uc744 \ubcf4\uc790. \ube68\uac04\uc0c9 \uc810\ub4e4\uc740 \uc6b0\ub9ac\uac00 \uac16\uace0\uc788\ub294 \\(y_{i}\\) \uac12\uc774\uba70 \uc9c1\uc120\uc774 \ud45c\ud604\ud558\uace0 \uc788\ub294 \uac83\uc740 \ucd94\uc815\ud55c \\(\\hat{y}_{i}\\) \ub4e4\uc758 \ub098\uc5f4\uc774\ub2e4. \uadf8\ub807\ub2e4\uba74 \uc794\ucc28 \\(e_{i}\\) \ub294 \\(y_{i}-\\hat{y}_{i}\\) \ub97c \uc758\ubbf8\ud558\uace0 \uc774\ub294 i \ubc88\uc9f8 \uad00\ucc30\ub41c response value\uc640 i \ubc88\uc9f8 \uc6b0\ub9ac\uac00 \ucd94\uc815\ud55c \uc120\ud615\ubaa8\ub378\uc758 response value\uac04\uc758 \ucc28\uc774\ub97c \ub098\ud0c0\ub0b8\ub2e4. \uc6b0\ub9ac\ub294 \uc774\ub97c RSS\ub77c\uace0 \uce6d\ud558\uba70 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\uac00\ub2a5\ud558\ub2e4. $ \\(RSS=e_1^2+e_2^2+...+e_{n}^2\\) $ \ub610\ub294 $ \\(RSS=\\sum\\limits_{k=1}^n(y_{k}-\\hat{\\beta}_0-\\hat{\\beta}_1x_{k})^2\\) $ \ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \ucd5c\uc18c\uc81c\uacf1\ubc95\uc740 \uc774\ub7ec\ud55c \uc794\ucc28\uc81c\uacf1\ud569 RSS\ub97c \ucd5c\uc18c\ud654 \ud558\ub294 \\(\\hat{\\beta}_0\\) \uacfc \\(\\hat{\\beta}_1\\) \ub97c \uad6c\ud558\ub294 \uac83\uc778\ub370, \uc774\ub7ec\ud55c \ud574\ub97c \uad6c\ud558\uba74 \ubca0\ud0c0\uc758 \ucd94\uc815\uce58\ub4e4\uc740 \uc544\ub798\uc640 \uac19\ub2e4. Interpretation of the Coefficient Estimates \ucd94\uc815\ud55c \ud68c\uadc0\uacc4\uc218\ub4e4\uc758 \uc758\ubbf8\ub97c \uc0b4\ud3b4\ubcf4\uba74 \uc544\ub798\uc640 \uac19\uc774 \uc694\uc57d\ud560 \uc218 \uc788\ub2e4. slope \\({\\beta}_1\\) \uc740 \ub3c5\ub9bd\ubcc0\uc218 X\uac00 \ud55c\ub2e8\uc704 \uc99d\uac00\ud560 \ub54c\uc758 \uc885\uc18d\ubcc0\uc218 Y\uc758 \ud3c9\uade0\uc801 \ubcc0\ud654\ub7c9\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc808\ud3b8\uacc4\uc218 \\({\\beta}_0\\) \uc740 \uc758\ubbf8\ub97c \uac16\uc9c0\uc54a\uace0, \uc120\ud615\ud68c\uadc0\uc2dd\uc744 \ubcf4\uc815\ud558\ub294 \uc5ed\ud560\uc774\ub2e4. \ud68c\uadc0\uacc4\uc218\uc758 \ucd94\uc815\uacfc\uc815\uc744 \uc0b4\ud3b4\ubcf4\uba74 \uc774\ub4e4\uc740 \uc8fc\uc5b4\uc9c4 X\uc640 Y\uc5d0 \ub300\ud574 \uc644\uc804\ud788 \uc758\uc874\ud558\uba70, \uc774\ub294 \uc989 \ub2e4\ub978 X\uac12\ub4e4\uc774 \ubc18\uc601\ub418\uba74 \ub2e4\ub978 \uacb0\uacfc\uac12\uc744 \ub3c4\ucd9c\ud568\uc744 \uc758\ubbf8\ud55c\ub2e4. Assessing the accuracy of the Coefficient Estimates \uc6b0\ub9ac\ub294 \ucd94\uc815\ud55c \ud68c\uadc0\uacc4\uc218\ub4e4\uc774 \uc2e4\uc81c\uc758 \ud68c\uadc0\uacc4\uc218\uac12\uacfc \uc5bc\ub9c8\ub098 \ube44\uc2b7\ud55c\uc9c0, \uc5bc\ub9c8\ub098 \uba40\ub9ac\ub5a8\uc5b4\uc838\uc788\uc744\uc9c0\uc5d0 \ub300\ud574\uc11c \uad81\uae08\uc99d\uc744 \uac00\uc9c8 \uac83\uc774\ub2e4. \uc774\ub54c\ub294 \ud68c\uadc0\uacc4\uc218 \\({\\beta}_0\\) , \\({\\beta}_1\\) \uc758 \ud45c\uc900\uc624\ucc28(Standard error)\ub97c \ud1b5\ud55c \uc2e0\ub8b0\uad6c\uac04 \ucd94\uc815\uc744 \ud1b5\ud574 \uc811\uadfc\ud574\ubcfc \uc218 \uc788\ub2e4. \\(\\hat{\\beta}_0\\) , \\(\\hat{\\beta}_1\\) \uc758 \ud45c\uc900\uc624\ucc28\ub97c \uad6c\ud558\uae30\uc804\uc5d0 \uc774\ub4e4\uc758 \ubd84\ud3ec\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. $ \\(\\hat{\\beta}_0 \\sim N({\\beta}_0,Var(\\hat{\\beta}_0)), \\hat{\\beta}_0 \\sim N({\\beta}_1,Var(\\hat{\\beta}_1))\\) $ \uc774\uc911 \\(\\hat{\\beta}_1\\) \uc5d0 \ub300\ud574\uc11c\ub9cc \\(Var(\\beta_1)\\) \uc744 \uad6c\ud574\ubcf4\uc790. $ \\({\\epsilon} \\sim N(0,{\\sigma}^2)\\) $ $ \\(\\sum\\limits_{i=1}^n(x_{i}-\\bar{x})(y_{i}-\\bar{y}) = \\sum\\limits_{i=1}^n(x_{i}-\\bar{x})y_{i}\\) $ \ub97c \uc774\uc6a9\ud558\uc790. \\(\\sum\\limits_{i=1}^n(x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) \ub294 \\(S_{xy}\\) \ub85c \ud45c\ud604\uac00\ub2a5. \uba3c\uc800 \\(k_{i}= \\frac{(x_{i}-\\bar{x})}{S_{xx}}\\) \ub85c \ub450\uc790. \uadf8\ub807\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc740 \uc2dd\uc804\uac1c\ub97c \ud1b5\ud574 \\(Var({\\beta}_1)\\) \ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4. $ \\(V(\\hat{\\beta}_1) = V(\\sum{k_{i}}y_{i})=\\sum{k_{i}}V(y_{i})=\\sum{k^2}{\\sigma}^2=\\frac{\\sum(x_{i}-\\bar{x})^2{\\sigma}^2}{S_{xx}}=\\frac{\\sigma^2}{S_{xx}}\\) $ \\(Var(y_{i})={\\sigma}^2\\) \uc778 \uc774\uc720\ub294, \\({\\epsilon} \\sim N(0,{\\sigma}^2)\\) \uc73c\ub85c \uc815\uc758\ud588\uace0, \\({\\epsilon}\\) \ub07c\ub9ac \uc11c\ub85c \ub3c5\ub9bd\uc774\ubbc0\ub85c \\(y_{i}\\) \ub3c4 \uc11c\ub85c \ub3c5\ub9bd\uc774\uace0 \ubd84\uc0b0\uc774 \\({\\sigma}^2\\) \uc774 \ub41c\ub2e4. \uc774\uc640 \uac19\uc774 \ube44\uc2b7\ud55c \ubc29\ubc95\uc744 \ud1b5\ud574 \\({\\beta}_0\\) \uc5d0 \ub300\ud574\uc11c\ub3c4 \ubd84\uc0b0\uc758 \ucd94\uc815\uce58\ub97c \uad6c\ud558\uc5ec \ud45c\uc900\uc624\ucc28\ub97c \uad6c\ud558\uba74 \ub450 \ud68c\uadc0\uacc4\uc218\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\(SE(\\hat{\\beta}_0)= \\sqrt{{\\sigma}^2[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum\\limits_{i=1}^n(x_{i}-\\bar{x})^2}]}\\) , \\(SE(\\hat{\\beta}_1)=\\sqrt{\\frac{\\sigma^2}{\\sum\\limits_{i=1}^n(x_{i}-\\bar{x})^2}}\\) \ud45c\uc900\uc624\ucc28 SE\ub97c \\({\\beta}_0\\) \uacfc \\({\\beta}_1\\) \uc5d0 \ub300\ud574 \uc2e0\ub8b0\uad6c\uac04\uc744 \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \uc4f8 \uc218 \uc788\ub2e4. $ \\([\\hat{\\beta}\\pm{t}_{n-2,{\\alpha}/2} SE(\\hat{\\beta})]\\) $ \uc2e0\ub8b0\uad6c\uac04\uc758 \uc758\ubbf8\ub294 \ud2b9\uc815\ud55c \uac12\uc744 \uac00\uc9c0\ub294 \uc54c \uc218 \uc5c6\ub294 parameter\uc758 \uac12\uc744 \ud2b9\uc815 %\uc758 \ud655\ub960\ub85c \ud3ec\ud568\ud558\ub294 \uad6c\uac04\uc744 \uc758\ubbf8\ud558\ub294\ub370, \uc774\ub294 \uc870\uae08 \ub354 \uc5c4\ubc00\ud788 \ub9d0\ud558\uba74 \uac19\uc740 \ubc29\ubc95\uc73c\ub85c \uc77c\uc815\ud55c \uac1c\uc218\ub9cc\ud07c \ubaa8\uc9d1\ub2e8\uc5d0\uc11c 100\uac1c\uc758 \ud45c\ubcf8 \ub370\uc774\ud130\uc14b\uc744 \ucd94\ucd9c\ud588\uc744 \ub54c \ud568\uaed8 \uacc4\uc0b0\ub418\ub294 100\uac1c\uc758 \uc2e0\ub8b0\uad6c\uac04\ub4e4 \uc911 parameter\ub97c \ud3ec\ud568\ud55c \uc2e0\ub8b0\uad6c\uac04\ub4e4\uc758 \uac1c\uc218\uac00 nn% x 100\uac1c \uc815\ub3c4\ub294 \ub41c\ub2e4 \uc758 \uc758\ubbf8\ub97c \uac16\ub294\ub2e4. \ud45c\uc900\uc624\ucc28\ub294 \ub610\ud55c \ud68c\uadc0\uacc4\uc218\uc5d0 \ub300\ud55c \uac00\uc124\uac80\uc815\uc744 \uc704\ud574 \uc0ac\uc6a9\ub418\uae30\ub3c4 \ud55c\ub2e4. \\(H_0\\) : There is no relationship between X and Y \\(H_{a}\\) : There is some relationship between X and Y \uc774 \uac00\uc7a5 \ud754\ud558\uac8c \uc0ac\uc6a9\ub418\ub294 \uac00\uc124\uac80\uc815\uc758 form\uc774\ub2e4. \uc218\ud558\uc801\uc73c\ub85c \ud45c\ud604\ud558\uba74 \uc774\ub294 \\[H_0 : {\\beta}_1=0\\] $ \\(H_{a} : {\\beta}_1 \\neq 0\\) $ \uc774\ub2e4. \ub9cc\uc57d \uc5ec\uae30\uc11c \\({\\beta}_1\\) =0\uc774\uba74, \uc774 \ubaa8\ub378\uc740 \\(Y={\\beta}_0+{\\epsilon}\\) \uc73c\ub85c reduced form\uc758 \ud615\ud0dc\ub97c \ub744\uba70, X\ub294 Y\uc640 \uad00\ub828\ub418\uc9c0 \uc54a\uc740 predictor\uac00 \ub41c\ub2e4. \uac00\uc124\uac80\uc815\uc744 \uc704\ud574\uc11c\ub294 t-\ud1b5\uacc4\ub7c9\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc774\ub294 \uc544\ub798\uc640 \uac19\uc740 \uc2dd\uc744 \uac16\uace0, $ \\(t=\\frac{{\\hat\\beta}_1-0}{SE(\\hat{\\beta}_1)} \\sim t(n-2)\\) $ \\(H_0\\) \ud558\uc5d0\uc11c \\(\\hat{\\beta}_1\\) \uc774 \ub530\ub974\ub294 \ubd84\ud3ec\ub294 \uc704\uc640 \uac19\uace0 \uc774\ub97c \ud1b5\ud574 \uc5bb\uc740 t \ud1b5\uacc4\ub7c9 \uac12\uc774 critical value\uc758 \ubc14\uae65, \uc989 \uae30\uac01\uc5ed\uc5d0 \uc18d\ud558\uba74 \uc6b0\ub9ac\ub294 \uadc0\ubb34\uac00\uc124 \\(H_0\\) \ub97c \uae30\uac01\ud55c\ub2e4. \uc774\ub294 \uc6b0\ub9ac\uac00 \\(\\hat{\\beta}_1\\) \uc774 0\uc774\ub77c\uace0 \uac00\uc815\ud558\uc5ec \ubb38\uc81c\ub97c \uc811\uadfc\ud55c \ud6c4, \uadf8 \uc911\uc2ec\uac12\ubcf4\ub2e4 \uc0c1\ub2f9\ud788 \uba3c \uacf3\uc5d0(\uae30\uac01\uc5ed\uc5d0 \uc18d\ud558\ub294 \uc601\uc5ed) \\(\\hat{\\beta}_1\\) \uc758 \ud1b5\uacc4\ub7c9\uc774 \uc704\uce58\ud558\ub294 \uac83\uc740 \uae30\uc874\uc5d0 \uc0dd\uac01\ud55c \\(H_0\\) \ub85c \uc5ec\uae34 \ubd84\ud3ec\ub97c \uac16\ub294\uac8c \uc544\ub2cc \uc544\uc608 \ub2e4\ub978 \ubd84\ud3ec\ub97c \uac16\ub294\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \uace7 \uc774\ub294 \\(\\hat{\\beta}_1\\) \uc774 0\uc774 \uc544\ub2cc \uc720\uc758\ud55c \uc5b4\ub5a0\ud55c \uac12\uc744 \uac00\uc9d0\uc744 \ubcf4\uc5ec\uc900\ub2e4. Assessing the Accuracy of the Model \uc55e\uc5d0\uc11c \uc6b0\ub9ac\uac00 \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\ubaa8\ud615\uc758 \uacc4\uc218\ub97c \ucd94\uc815\ud558\uace0 \uae30\uc6b8\uae30\uc758 \uacc4\uc218\uac00 \uc720\uc758\ud55c\uc9c0 test\ub97c \ud588\uc73c\ub2c8, \uc774\ubc88\uc5d0\ub294 \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \uc5bc\ub9c8\ub098 \uc815\ud655\ud55c\uc9c0\uc5d0 \ub300\ud574\uc11c \ud3c9\uac00\ub97c \ud574\ubcf4\ub294 \uc2dc\uac04\uc744 \uac00\uc838\ubcf4\uc790! \uc774 \ucc45\uc5d0\uc11c\ub294 \ub450\uac00\uc9c0\ub97c \ub300\ud45c\uc801\uc73c\ub85c \uc18c\uac1c\ud558\ub294\ub370, \uba4b\uc7c1\uc77412\uae30\ucd5c\uc5f0\uc218 \ub2d8\uc758 \uc790\ub8cc\ub97c \ucc38\uace0\ud558\uc5ec \ud55c\uac00\uc9c0\ub97c \ub354 \ub123\uc5b4\ubcf4\uc558\ub2e4. Residual Standard Error(RSE) RMSE(\ud3c9\uade0\uc81c\uacf1\uadfc\uc624\ucc28) \uba4b\uc7c1\uc774\ucd5c\uc5f0\uc218\ub2d8\uc758\uc544\uc774\ub514\uc5b4 R-squared \uba3c\uc800 RSE\ub97c \uc0b4\ud3b4\ubcf4\uc790. \uc6b0\ub9ac\uac00 true regression line\uc744 \ucd94\uc815\ud560 \uc218 \uc788\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uc790. \uadf8\ub807\ub2e4\uba74 \uc6b0\ub9ac\ub294 Y\ub97c \uc644\ubcbd\ud558\uac8c \uc608\uce21\ud560 \uc218 \uc788\uc744\uae4c? \uc544\uc27d\uac8c\ub3c4 \uc6b0\ub9ac\ub294 Y\uc758 \uad00\uacc4\uc2dd\uc5d0 random\ud55c error term\uc774 \uc788\ub2e4\ub294 \uac83\uc744 \uc78a\uc73c\uba74 \uc548\ub41c\ub2e4. \uc774 \\({\\epsilon} \\sim N(0,{\\sigma}^2)\\) \ub294 irreducible\ud558\uace0 \uc5b4\uca54 \uc218 \uc5c6\ub294 \uc624\ucc28\ub97c \uc218\ubc18\ud55c\ub2e4. RSE\ub294 \uacb0\uad6d \uc774\ub7ec\ud55c \\({\\epsilon}\\) \uc758 \ud45c\uc900\ud3b8\ucc28\uc5d0 \ub300\ud55c \ucd94\uc815\uce58\ub97c \uc758\ubbf8\ud558\uba70 \ud3c9\uade0\uc801\uc73c\ub85c \uc885\uc18d\ubcc0\uc218 Y\uac00 true regression line\uc5d0\uc11c \uc5bc\ub9c8\ub098 \ub5a8\uc5b4\uc838\uc788\ub294\uac00\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ucd94\uc815\uce58\uc774\ub2e4. \\[RSE=\\sqrt{\\frac{1}{n-2}RSS}=\\sqrt{\\frac{1}{n-2}\\sum\\limits_{i=1}^n(y_{i}-\\hat{y}_{i})^2}\\] \ud3c9\uade0\uc81c\uacf1\uadfc\uc624\ucc28\ub294 RSS\ub97c \uc790\uc720\ub3c4(n-2)\ub300\uc2e0 n\uc73c\ub85c \ub098\ub208 \uac83\uc774\ub2e4. n\uc774 \ub9e4\uc6b0\ud06c\ub2e4\uba74 RSE\uc640 RMSE\uc758 \ucc28\uc774\uac00 \ubbf8\ubbf8\ud558\uaca0\uc9c0\ub9cc, RMSE\ub294 prediction\uad00\uc810\uc5d0\uc11c \ub9ce\uc774 \uc0ac\uc6a9\ub41c\ub2e4. \\[RMSE=\\sqrt{\\frac{1}{n}RSS}\\] \uc774 \ub450 RSE\uc640 RMSE\ub294 \ubaa8\ub378 \uc798 fitting\uc774 \ub418\uc5c8\ub294\uc9c0\ub97c \uce21\uc815\ud558\ub294 \uac83\uc73c\ub85c \uc5ec\uaca8\uc9c4\ub2e4. R-squared\ub294 \ucd1d \ubcc0\ub3d9 \uc911\uc5d0 \uc124\uba85\ub41c \ubcc0\ub3d9\uc758 \ube44\uc728\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc9c0\ud45c\ub2e4. RSE \uc640 RMSE \uac00 \uc808\ub300\uc801\uc778 \uac12\uc73c\ub85c \ubaa8\ub378\uc758 \uc624\ucc28\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud55c\ub2e4\uba74, \\(R^2\\) \ub294\uc0c1\ub300\uc801\uc778 \uac12\uc73c\ub85c\uc368 \ud835\udc4c\uc758 \ub2e8\uc704\uc5d0 \uc0c1\uad00\uc5c6\uc774 \ubcc0\ub3d9\uc5d0 \ub300\ud55c \uc124\uba85 \ube44\uc728\ub85c \ubaa8\ub378\uc758 \uc124\uba85\ub825\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\ub294 \uac83\uc5d0 \uc704\uc758 \ub450\uac1c\uc640 \ucc28\uc774\uc810\uc744 \uac16\ub294\ub2e4. $ \\(R^2= \\frac{SST-SSE}{SST}=1-\\frac{SSE}{SST}\\) $ \uc704\uc758 \uc2dd\ucc98\ub7fc \ud45c\ud604\ub418\uace0 $ \\(SST = \\sum(y_{i}-\\bar{y}^2)\\) $ \uc740 Total sum of squares(\ucd1d \ubcc0\ub3d9), $ \\(SSE=\\sum(y_{i}-\\hat{y}^2)\\) $\uc740 Residual sum of squares(\uc124\uba85\ub418\uc9c0\uc54a\uc740 \ubcc0\ub3d9), $ \\(SSR=\\sum(\\hat{y}_{i}-\\bar{y}^2)\\) $\uc740 Explained sum of squares(\uc124\uba85\ub41c \ubcc0\ub3d9) \uc744 \uc758\ubbf8\ud55c\ub2e4. \uc0ac\uc9c4\uc73c\ub85c \uc27d\uac8c \ud45c\ud604\ud558\uba74 \uc704\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4.( \uc5f0\uc218\uc57c \uace0\ub9c8\uc6cc! ) Multiple Linear Regression \ub2e4\uc911\uc120\ud615\ud68c\uadc0\ub294 \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\uc640 \uc720\uc0ac\ud558\uc9c0\ub9cc \ub3c5\ub9bd\ubcc0\uc218 X\uc758 \uac1c\uc218 p\uac1c\ub77c\ub294 \uc810\ub9cc \ub2e4\ub974\ub2e4. \uc774 \ub610\ud55c \uc5ed\uc2dc \ub3c5\ub9bd\ubcc0\uc218 X\uc640 Y\uac04\uc5d0 \uadfc\uc0ac\uc801\uc778 \uc120\ud615\uad00\uacc4\uac00 \uc788\ub2e4\uace0 \uac00\uc815\ud55c \ubaa8\ub378\uc774\uba70, \uc120\ud615 \uc9c1\uc120 \ud639\uc740 \uc120\ud615 \ud3c9\uba74 \ub610\ub294 \uadf8 \uc774\uc0c1\uc758 \uc120\ud615\uad00\uacc4\ub97c \ub098\ud0c0\ub0b4\ub294 \ubaa8\ub378\uc774\ub2e4. $ \\(Y= {\\beta}_0+ {\\beta}_1X_1+...+{\\beta}_{p}X_{p}+{\\epsilon}\\) $ \uc704\uc758 \uc2dd\uc774 \ub2e4\uc911\uc120\ud615\ud68c\uadc0\uc758 \uae30\ubcf8 \ud615\ud0dc\uc774\uba70 \uc774\ub97c \ucd94\uc815\ud558\ub294 \ubaa8\ub378(\ud68c\uadc0\uc120)\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \\[\\hat{Y}={\\hat\\beta}_0+{\\hat\\beta}_1X_1+...+{\\hat\\beta}_{p}X_{p}\\] RSS\ub294 \ub2e8\uc21c\ud68c\uadc0\uc640 \uac19\uc740 \uc2dd\uc774\uc9c0\ub9cc \\(\\hat{y}_{i}\\) \ub9cc \ub2e4\ub974\ub2e4\ub294 \ucc28\uc774\uac00 \uc788\ub2e4. \uc704 \uadf8\ub9bc\uc740 3\ucc28\uc6d0\uc77c \ub54c\uc758 \ub450\uac1c\uc758 predictor\uc640 \ud558\ub098\uc758 response Y\ub97c \ubcf4\uc5ec\uc8fc\uba70 \uc5ec\uae30\uc11c \ucd5c\uc18c\uc81c\uacf1\ud68c\uadc0\uc120\uc740 \uc120\ud615\ud3c9\uba74\uc758 \ud615\ud0dc\ub97c \ub748\ub2e4. \ub2e4\uc911\uc120\ud615\ud68c\uadc0\uc5d0\uc11c \\({\\beta}\\) \ub97c \ucd94\uc815\ud558\ub294 \uac83\uc744 \uc120\ud615\ub300\uc218\ud559\uc801\uc73c\ub85c \uacc4\uc0b0\ud558\uba74 \uc544\ub798\uc640 \uac19\uc740 \uacfc\uc815\uc744 \uac70\uce5c\ub2e4. \ucd94\uc815\ud55c \ud68c\uadc0\uc120\uc5d0\uc11c \uae30\uc6b8\uae30 \uacc4\uc218 \\(\\hat{\\beta}_{i}\\) \uc758 \uc758\ubbf8\ub294 \ub2e4\ub978 \ub3c5\ub9bd\ubcc0\uc218\ub4e4\uc758 \uc120\ud615\uc131\uc5d0 \ub300\ud574 \uc870\uc808\ud588\uc744 \ub54c( Y\uc640 \uacf5\ubcc0\ud558\uc9c0 \uc54a\uc744 \ub54c ) \ub3c5\ub9bd\ubcc0\uc218 \\(X_{i}\\) \uac00 \ud55c \ub2e8\uc704 \uc99d\uac00\ud560 \ub54c \uc885\uc18d\ubcc0\uc218 Y\uc758 \ud3c9\uade0\uc801\uc778 \uc21c \ubcc0\ud654\ub7c9\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc774\ub294 \ud55c\uac1c\uc758 \ub3c5\ub9bd\ubcc0\uc218 X\uac00 \uc6c0\uc9c1\uc77c \ub54c \ub098\uba38\uc9c0 \ub3c5\ub9bd\ubcc0\uc218\ub4e4\uc740 \ud1b5\uc81c\ub97c \ud55c \uc0c1\ud0dc\uc5d0\uc11c Y\uc640 \ud558\ub098\uc758 \ub3c5\ub9bd\ubcc0\uc218 X\uac04\uc758 \ubcc0\ud654\ub7c9 \uad00\uacc4\ub97c \uc124\uba85\ud558\ub294\ub370, \uc5ec\uae30\uc11c \uc870\uc2ec\ud574\uc57c\ud560 \uac83\uc740 \ub098\uba38\uc9c0 \ubcc0\uc218\ub97c \ud1b5\uc81c\uc2dc\ud0a8\ub2e4\ub294 \uc758\ubbf8\uac00 \uadf8 \ubcc0\uc218\ub4e4\uc744 \ubb34\uc2dc\ud558\uace0 \uc0dd\uac01\ud55c\ub2e4\ub294 \ub73b\uacfc \ub2e4\ub974\ub2e4\ub294 \uac83 \uc774\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 \ub2e4\uc911\uc120\ud615\ud68c\uadc0\ub97c \ud588\uc744 \ub54c\uc758 \uacc4\uc218 \\({\\hat\\beta}_{i}\\) \ub294 \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\ub97c \ud588\uc744 \ub54c\uc758 \\({\\hat\\beta}_{i}\\) \uc640 \ub2e4\ub974\ub2e4.(\ubb34\uc2dc\uc640 \ud1b5\uc81c\uc758 \ucc28\uc774) F-test \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\uc640\ub294 \ub2e4\ub974\uac8c \ub2e4\uc911\uc120\ud615\ud68c\uadc0\uc5d0\uc11c\ub294 \ub3c5\ub9bd\ubcc0\uc218\uc640 \uc885\uc18d\ubcc0\uc218 \uac04\uc758 \uc120\ud615\uad00\uacc4 \uc720\ubb34\ub97c \ud310\ubcc4\ud560 \ub54c\uc5d0\ub294 F-test\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac80\uc815\ud55c\ub2e4. \uc774\ub7ec\ud55c \uc774\uc720\ub294 \ub3c5\ub9bd\ubcc0\uc218\uc640 \uc885\uc18d\ubcc0\uc218\uac00 jointly \uc720\uc758\ud55c \uc120\ud615 \uad00\uacc4\ub97c \uac00\uc9c0\ub294\uc9c0\uc5d0 \ub300\ud55c \ubb38\uc81c\ub97c \ub17c\ud558\uae30 \ub54c\ubb38\uc778\ub370, \uc5ec\ub7ec \ud68c\uadc0\uacc4\uc218\uac12\ub4e4\uc5d0 \ub300\ud574\uc11c \uac01\uac01 \ub3c5\ub9bd\uc801 T-test\ub97c \uc2e4\uc2dc\ud55c\ub2e4\uace0 \ud574\ub3c4 \ub9cc\ub4e4\uc5b4\uc9c4 \uc11c\ub85c \ub2e4\ub978 \uadc0\ubb34\uac00\uc124\ub4e4\uc774 \ub3c5\ub9bd\uc774 \uc544\ub2c8\ub780 \ubcf4\uc7a5\uc774 \uc5c6\uae30 \ub54c\ubb38\uc774\ub2e4. \ub610\ud55c \uc5ec\ub7ec\uac1c\uc758 \uadc0\ubb34\uac00\uc124\ub4e4\uc774 \uac01\uac01 \uac80\uc815\ub418\ub294 \uacbd\uc6b0 \uc774 \uc911 \ud558\ub098\ub77c\ub3c4 \uae30\uac01\ub420 \ud655\ub960\uc774 \ucee4\uc9c0\uace0, \uadc0\ubb34\uac00\uc124\uc744 \uacfc\uc18c\ud3c9\uac00\ud560 \uac00\ub2a5\uc131\uc774 \ub192\uae30 \ub54c\ubb38\uc774\ub2e4. \\[H_{0}:{\\beta}_1={\\beta}_2=...={\\beta}_{p}\\] \\(H_{a}:\\) at least one \\({\\beta}_{j}\\) is non-zero \\[F=\\frac{(SST-SSE)/p}{SSE/(n-p-1)}\\] Deciding on Important Variables Forward selection(\uc804\uc9c4\uc120\ud0dd\ubc95) : \uc808\ud3b8\uacc4\uc218\ub9cc \uc874\uc7ac\ud558\ub294 \ubaa8\ub378\uc5d0\uc11c \uc2dc\uc791\ud558\uc5ec( \\(Y={\\beta}_0+{\\epsilon}\\) ) RSS\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ubcc0\uc218\ub97c \ucd94\uac00\ud558\ub294 \ubc29\ubc95, \ud2b9\uc815 \ubaa9\uc801\uc5d0 \ub3c4\ub2ec\ud560 \ub54c \uae4c\uc9c0 \uacc4\uc18d\ud55c\ub2e4. Backward selection(\ud6c4\uc9c4\uc81c\uac70\ubc95) : \ubaa8\ub4e0 \ub3c5\ub9bd\ubcc0\uc218 p\uac1c\uac00 \uc874\uc7ac\ud558\ub294 \ubaa8\ud615\uc5d0\uc11c \ucd9c\ubc1c. \ud1b5\uacc4\uc801\uc73c\ub85c \uac00\uc7a5 \uc720\uc758\ud558\uc9c0 \uc54a\uc740 \ubcc0\uc218\ub97c \uc81c\uac70\ud574\ub098\uac10. \ubaa8\ub4e0 \uacc4\uc218\ub4e4\uc774 \uc720\uc758\ud560\ub54c\uae4c\uc9c0 \ubc18\ubcf5. Mixed selection : \uc808\ud3b8 \uacc4\uc218\ub9cc \uc874\uc7ac\ud558\ub294 \ubaa8\ud615\uc5d0\uc11c \uc2dc\uc791\ud558\uc5ec RSS\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ubcc0\uc218 \ucd94\uac00\ud574\ub098\uac10. \ud2b9\uc815\ubcc0\uc218\ub97c \ucd94\uac00\ud588\uc744\ub54c \ud1b5\uacc4\uc801\uc73c\ub85c \uc720\uc758\ud558\uc9c0\uc54a\uc740 \uacc4\uc218\uac00 \ubc1c\uc0dd\uc2dc \uadf8\ubcc0\uc218\ub97c \uc81c\uac70. \ucd5c\uc885\ubaa8\ub378\uc758 \ubaa8\ub4e0\ubcc0\uc218\ub4e4\uc774 \uc720\uc758\ud558\uace0 \ub2e4\ub978\ubcc0\uc218\ub97c \ucd94\uac00\ud558\uba74 \uc720\uc758\ud558\uc9c0\uc54a\uc544\uc9c8 \ub54c\uae4c\uc9c0\ubc18\ubcf5 model fit\uc5d0 \ub300\ud55c \uc815\ud655\uc131\uc744 \ud310\ub2e8\ud560 \uc218 \uc788\ub294 \ucc99\ub3c4\ub85c \ub2e4\uc911\ud68c\uadc0\uc5d0\uc11c\ub294 \\(adjusted R^2\\) \ub97c \uc0ac\uc6a9\ud55c\ub2e4. \ucd5c\uc18c\uc81c\uacf1\ubc95\uc740 \uc885\uc18d\ubcc0\uc218\uc640 \uc544\ubb34\ub9ac \uad00\ub828\uc5c6\ub294 \ubcc0\uc218\ub77c\ub3c4 \uc5b4\ub5a4\ud615\ud0dc\ub85c\ub4e0 RSS\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ud68c\uadc0\uc9c1\uc120\uc744 \ucd94\uc815\ud558\ub294\ub370, \ubcc0\uc218\uc758 \uac1c\uc218\uac00 \uc99d\uac00\ud560\uc218\ub85d RSS\ub294 \uac10\uc18c\ud558\uace0 \\(R^2\\) \ub294 \uc99d\uac00\ud560 \uc218\ubc16\uc5d0 \uc5c6\ub2e4. \ud558\uc9c0\ub9cc \uc2e4\uc81c\ub85c \ubcc0\uc218\ub97c \ucd94\uac00\ud574\uc11c RSS\uac00 \uac10\uc18c\ud558\uace0 \\(R^2\\) \uac00 \uc99d\uac00\ud558\ub294 \uac83\uc774 \uc2e4\uc81c\ub85c \ucd94\uac00\ub41c \ub3c5\ub9bd\ubcc0\uc218\uac00 \uc885\uc18d\ubcc0\uc218\uc5d0 \ud070 \uc601\ud5a5\uc744 \uc8fc\ub294 \ubcc0\uc218\uc5ec\uc11c\uc778\uc9c0\ub294 \ud655\uc2e4\ud558\uac8c \uc54c \uc218\uac00\uc5c6\ub2e4. \uadf8\ub9ac\ud558\uc5ec \uc6b0\ub9ac\ub294 \\(R^2\\) \uc758 \ub300\uc548\uc73c\ub85c \\(adjusted R^2\\) \ub97c \uc4f4\ub2e4. \\[Adjusted-R^2 =1-\\frac{SSE/n-K}{SST/n-1}\\] \ub2e4\uc911\uc120\ud615\ud68c\uadc0\uc5d0\uc11c\ub3c4 \uc55e\uc5d0\uc11c \ubc30\uc6b4 \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\uc640 \ub9c8\ucc2c\uac00\uc9c0\ub85c \ucd94\uc815\ub41c \ud68c\uadc0\uacc4\uc218\ub294 \ucd94\uc815\uce58\uc77c \ubfd0, \uc6b0\ub9ac\ub294 true relationship\uc744 \uc808\ub300 \uc54c \uc218\uac00 \uc5c6\ub2e4. \ud568\uc218f\ufffd \uc5d0\ub300\ud574 \uc120\ud615\ubaa8\ud615\uc744 \uac00\uc815\ud558\ub294 \uac83\uc740 \ub2e8\uc9c0 \uac00\uc815\uc77c \ubfd0\uc774\uace0,true regression line\uc744 \uc548\ub2e4\uace0 \ud558\ub354\ub77c\ub3c4 random \ud558\uac8c \ubc1c\uc0dd\ud558\ub294 \\({\\epsilon}\\) \ufffd \uae4c\uc9c0\ub294 \ud1b5\uc81c \ubd88\uac00\ub2a5\ud558\ub2e4. Other Considerations in the Regression Qualitative Predictors \ub9ce\uc740 \uacbd\uc6b0\uc5d0 \ub3c5\ub9bd\ubcc0\uc218 \uc911\uc5d0 \uc9c8\uc801\ubcc0\uc218\uac00 \uc874\uc7ac\ud55c\ub2e4. \ud68c\uadc0\uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c \ubcc0\uc218\ub97c \ub9cc\ub098\uba74 \uc5b4\ub5bb\uac8c \ucc98\ub9ac\ub97c \ud574\uc57c\ud560\uae4c? \uc774\ub54c\ub294 Dummy variable\uc744 \ub9cc\ub4e4\uba74 \ub41c\ub2e4. (\ub3c5\ub9bd\ubcc0\uc218\ub97c 0,1\uc758 \uac12\uc73c\ub85c \ubcc0\ud658\ud55c \ubcc0\uc218) \uc774\ub294 \\(x_{i}\\) \ub97c i\ubc88\uc9f8 \uc0ac\ub78c\uc774 female\uc774\uba74 x\uac00 1, male\uc774\uba74 0\uc73c\ub85c \ubcc0\ud658\uc744 \ud574\uc900\ub2e4. \ub9cc\uc57d \uc9c8\uc801\ub3c5\ub9bd\ubcc0\uc218\uac00 3\uac1c \uc774\uc0c1\uc758 \ud074\ub798\uc2a4\ub97c \uac00\uc9c8\uacbd\uc6b0\uc5d0\ub294 \ub2e8\uc77c\ub354\ubbf8\ubcc0\uc218\uac00 \ubaa8\ub4e0 \uac00\ub2a5\ud55c \ud074\ub798\uc2a4\ub97c \ub098\ud0c0\ub0bc \uc218\uac00 \uc5c6\ub2e4. \uc774\ub54c \ud544\uc694\ud55c \ub354\ubbf8\ubcc0\uc218\uc758 \uac2f\uc218\ub294 (\ubc94\uc8fc\uc758 \uac1c\uc218-1)\uac1c \ub9cc\ud07c \ub9cc\ub4e4\uc5b4 \uc918\uc57c\ud55c\ub2e4. \uc9c0\uae08\uae4c\uc9c0 \ub2e4\ub8ec \uc120\ud615\ud68c\uadc0\ub294 \ub2e8\uc21c\ud558\uba74\uc11c\ub3c4 \uacb0\uacfc \ud574\uc11d\uc774 \uac00\ub2a5\ud558\uace0 \ube44\uad50\uc801 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc900\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub7f0 \uc120\ud615\ud68c\uadc0\ub294\uc2e4\uc81c\uc5d0\uc11c \uc885\uc885 \uc9c0\ucf1c\uc9c0\uae30 \ud798\ub4e0 \uc5c4\uaca9\ud55c \uac00\uc815\ub4e4\uc744 \ud488\uace0\uc788\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \uc544\ub798\uc640 \uac19\uc740 \uac83\ub4e4\uc774 \uc788\ub2e4. Addictive \uac00\uc815 : \ub3c5\ub9bd\ubcc0\uc218 \\(X_{j}\\) \uc5d0 \ub530\ub978 \uc885\uc18d\ubcc0\uc218 Y\uc758 \ubcc0\ud654\uac00 \ub2e4\ub978 \ub3c5\ub9bd\ubcc0\uc218\ub4e4\uc758 \uac12\uc5d0 \uc0c1\uad00\uc5c6\uc774 \ub3c5\ub9bd\uc774\ub77c\ub294 \uac83 Linear \uac00\uc815 : \ub3c5\ub9bd\ubcc0\uc218 \\(X_{j}\\) \uac00 \ud55c \ub2e8\uc704 \ubcc0\ud654\ud560 \ub54c \uc885\uc18d\ubcc0\uc218 Y\uc758 \ubcc0\ud654\ub7c9\uc774 \\(X_{j}\\) \uc758 \uac12\uacfc \uc0c1\uad00\uc5c6\uc774 \uc0c1\uc218\ub77c\ub294 \uac83. \ud558\uc9c0\ub9cc \uc2e4\uc81c\ub85c \ud55c \ub3c5\ub9bd\ubcc0\uc218\uac00 \ub2e4\ub978 \ub3c5\ub9bd\ubcc0\uc218\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uc77c\uc740 \uc885\uc885 \uc77c\uc5b4\ub09c\ub2e4. \ub9e4\uccb4\uad11\uace0\ub85c \uc778\ud55c sale\uc744 \uc608\uce21\ud558\ub294\ub370\uc5d0 \ubcc0\uc218\ub85c tv\uc640 radio\uac00 \uc788\ub2e4\uace0 \ud574\ubcf4\uc790. \ub9cc\uc57d radio\uad11\uace0\uc5d0 \ud22c\uc790\ud560\uc218\ub85d tv\uc758 \uad11\uace0 \ub610\ud55c \ucee4\uc9c4\ub2e4\uace0 \ud55c\ub2e4\uba74, \ud55c \ubcc0\uc218\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ub2e4\ub978 \ubcc0\uc218\uc758 \uae30\uc6b8\uae30\uacc4\uc218 \uc5ed\uc2dc \ubcc0\ud654\ud558\uac8c \ub420 \uac83\uc774\ub2e4. \uc774\ub97c \ub9c8\ucf00\ud305\uc5d0\uc11c\ub294 Synergy Effect , \ud1b5\uacc4\ud559\uc5d0\uc11c\ub294 Interaction Effect \ub77c \ud55c\ub2e4. Interaction term\uc744 \ud1b5\ud574 \uc6b0\ub9ac\ub294 \uc774\ub7ec\ud55c \ud6a8\uacfc\ub97c \ubaa8\ud615\uc5d0\uc11c \uace0\ub824\ud560 \uc218 \uc788\uc73c\uba70, \uc704\uc758 \uc608\uc2dc\ub97c \uc2dd\uc73c\ub85c \uc801\ub294\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc740 \uaf34\uc77c \uac83\uc774\ub2e4. \\[Y={\\beta}_0+{\\beta}_1X_1+{\\beta}_2X_2+{\\beta}_3X_1X_2+{\\epsilon}\\] $ \\(={\\beta}_0+({\\beta}_1+{\\beta}_3X_2)X_1+{\\beta}_2X_2+{\\epsilon}\\) $ \uc774\ub807\uac8c \ub41c\ub2e4\uba74 \uc885\uc18d\ubcc0\uc218 Y\uc5d0 \ub300\ud55c \ud6a8\uacfc\ub294 \ub354\uc774\uc0c1 \uc0c1\uc218\uac00 \uc544\ub2c8\ub2e4. Non-linear relationship \uc5b4\ub5a4 \ub3c5\ub9bd\ubcc0\uc218\ub294 \uc885\uc18d\ubcc0\uc218\uc640\uc758 \uad00\uacc4\ub97c plot\uc73c\ub85c \uadf8\ub824\ubcf4\uc558\uc744 \ub54c \ube44\uc120\ud615\uc801\uc778 \uad00\uacc4\ub97c \uac00\uc9c8 \uc218\ub3c4\uc788\ub2e4.(\uc0ac\uc2e4 \ub9e4\uc6b0\ub9e4\uc6b0 \ub300\ubd80\ubd84\uc758 \ubcc0\uc218\uad00\uacc4\ub4e4\uc774 \uadf8\ub7f4 \uac83\uc774\ub2e4.) \uc774\ub7f4 \ub54c\ub294 \ub2e8\uc21c\ud558\uac8c \ud68c\uadc0\ubd84\uc11d\uc744 \ud568\uc5d0 \uc788\uc5b4\uc11c \ube44\uc120\ud615\ud568\uc218\ub97c \ud68c\uadc0\uc2dd\uc5d0 \ud3ec\ud568\uc2dc\ucf1c\uc8fc\uba74 \ub41c\ub2e4. \ud558\uc9c0\ub9cc \uadf8\ub9cc\ud07c \uc2dd\uc774 \ubcf5\uc7a1\ud574\uc9c0\uace0 \uc124\uba85\uc744 \ud558\uae30\uc5d4 \uc120\ud615\ud68c\uadc0\ubcf4\ub2e4\ub294 \uc5b4\ub824\uc6b8 \uac83\uc774\ub2e4. \uc544\ub2c8\uba74 \ub2e4\ub978 \ubc29\ubc95\uc73c\ub85c\ub294 \uc801\uc808\ud55c \ubcc0\ud658\uc744 \ud1b5\ud574 X\uc640 Y\uc758 \ube44\uc120\ud615\uad00\uacc4\ub97c \uc120\ud615\ud654\ud558\uac70\ub098 \uc815\uaddc\ubd84\ud3ec\ud654 \uc2dc\ud0a4\ub294 \ubc29\ubc95 \ub4f1\uc744 \ud1b5\ud574 \uc785\ub825\ubcc0\uc218 X\ub97c \uae54\ub054\ud558\uac8c \ubcc0\ud658\ud558\uc5ec \ubaa8\ub378\uc5d0 \uc9d1\uc5b4\ub123\ub294 \uac83\ub3c4 \ud558\ub098\uc758 \ud574\uacb0\ucc45\uc774\ub2e4.(ex. skewed\ub41c \ub370\uc774\ud130\ub97c \uc801\uc808\ud55c transformation\uc744 \ud1b5\ud574 \ub300\uce6d\ud615\ud0dc\ub85c \ub9cc\ub4e0\ub2e4\ub294 \ub4f1) Non-constant Variance of Error Terms \uc6b0\ub9ac\ub294 error term\uc5d0 \ub300\ud574\uc11c \uba87\uac00\uc9c0 \uac00\uc815\uc744 \ub123\uc5b4\ub193\uc558\ub2e4. Normality(\uc815\uaddc\uc131) Homoskedasticity(\ub4f1\ubd84\uc0b0\uc131) Independent(\ub3c5\ub9bd\uc131) \ud558\uc9c0\ub9cc \uc885\uc885 \uc794\ucc28\ub3c4 plot\uc744 \ud655\uc778\ud574\ubcf4\uba74 \uc774\ub7ec\ud55c \uac00\uc815\uc744 \ub9cc\uc871\ud558\uc9c0 \uc54a\ub294 \uacbd\uc6b0\ub3c4 \ub9ce\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \uc624\ucc28\uc758 \ubd84\uc0b0\uc774 constant term\uc774 \uc544\ub2d0 \uacbd\uc6b0\uc5d0\ub294 heteroskedasticity(\uc774\ubd84\uc0b0\uc131)\uc744 \ub744\uac8c \ub418\ub294\ub370, \uc774\ub7f4 \ub54c\uc5d0\ub294 response Y\uc5d0 \ub300\ud55c \uc801\uc808\ud55c \ubcc0\ud658\uc744 \ud1b5\ud574 \ub4f1\ubd84\uc0b0\uac00\uc815\uc744 \ub9cc\uc871\ud558\uac8c \ubcc0\ud658\uc744 \ud574\uc904 \uc218 \uc788\ub2e4. \uc704\uc758 \uadf8\ub9bc\uc740 ISL\uc758 \uc0ac\uc9c4\uc744 \ucca8\ubd80\ud55c \uac83\uc778\ub370, Y\uc5d0 \ub85c\uadf8\ubcc0\ud658\uc744 \ud1b5\ud574 \uc801\uc808\ud558\uac8c \uc774\ubd84\uc0b0\uc131\uc744 \uc5c6\uc564 \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc900\ub2e4. Outliers Outlier\ub780 \uc885\uc18d\ubcc0\uc218 \\(y_{i}\\) \uac00 \uad00\uce21\ub41c \ub370\uc774\ud130 \ubc94\uc704 \ub0b4\uc5d0\uc11c \uba40\ub9ac \ub5a8\uc5b4\uc9c4 \uc544\uc8fc \uc791\uc740 \uac12\uc774\ub098 \ud070 \uac12\uc744 \ub9d0\ud55c\ub2e4. \uc774\ub294 \uce21\uc815\uc744 \ud558\ub294\ub370 \uc0dd\uae34 \uc624\ub958\uc77c\uc218\ub3c4 \uc788\uace0 \uc785\ub825\uc624\ub958\uc77c\uc218\ub3c4 \uc788\uc73c\uba70 \uc544\ub2c8\uba74 \uc815\ub9d0 \ud2b9\uc774\ud55c \uac12\uc774 \ub370\uc774\ud130 \uc548\uc5d0 \uc874\uc7ac\ud558\ub294 \uacbd\uc6b0 \uc77c\uc218\ub3c4 \uc788\ub2e4. \ud639\uc740 \ubaa8\ub378\uc758 \ub2e4\ub978 \ub3c5\ub9bd\ubcc0\uc218\uac00 \uace0\ub824\ub418\uc9c0 \uc54a\uc544 \ubc1c\uc0dd\ud558\ub294 \uc2e0\ud638\uc77c\uc218\ub3c4 \uc788\uace0 \uc774\uc0c1\uce58\uac00 \uc0dd\uae30\ub294 \uc774\uc720\ub294 \uad49\uc7a5\ud788 \ub2e4\uc591\ud558\uae30\uc5d0 \uc774\ub97c \ucc98\ub9ac\ud568\uc5d0 \uc788\uc5b4\uc11c\ub294 \uc2e0\uc911\ud558\uac8c \ucc98\ub9ac\ud574\uc57c\ud55c\ub2e4. Collinearity \uacf5\uc120\uc131\uc740 \ub450 \uac1c\uc774\uc0c1\uc758 \ub3c5\ub9bd\ubcc0\uc218\ub4e4\uc774 \uc11c\ub85c \ub192\uc740 \uc120\ud615\uad00\uacc4\ub97c \uac00\uc9c0\ub294 \uac83\uc774\ub2e4. \uc774\ub294 \ucd94\ub860\uad00\uc810\uc5d0\uc11c \uac1c\ubcc4\uacc4\uc218\ub4e4\uc744 \ud574\uc11d\ud558\ub294\ub370 \uc7a5\uc560\ubb3c\uc774 \ub418\uae30\ub3c4 \ud55c\ub2e4. \ub610\ud55c \uc608\uce21\uad00\uc810\uc5d0\uc11c\ub3c4 \ube44\uc2b7\ud55c \uac12\uc744 \uac16\ub294 RSS\uac00 \ub098\uc62c \uc218 \uc788\ub294 \ud68c\uadc0 \uacc4\uc218\ub4e4\uc758 \uc870\ud569\uc774 \ub9ce\uc544\uc9c4\ub2e4. \uc774\ub294 \ucd94\uc815\ud55c \ud68c\uadc0\uacc4\uc218\uc758 \ubd88\ud655\uc2e4\uc131\uc744 \ub192\ud788\uba70 \uacc4\uc218\uac00 \uc720\uc758\ud558\uc9c0 \uc54a\uc744 \ud655\ub960\uc774 \uc62c\ub77c\uac00\uac8c \ub41c\ub2e4. \uacf5\uc120\uc131\uc744 detecting\ud558\ub294 \uac04\ub2e8\ud55c \ubc29\ubc95\uc73c\ub85c\ub294 \uc785\ub825\ubcc0\uc218\ub4e4\uac04\uc758 \uc0c1\uad00\uacc4\uc218 \ud589\ub82c\uc744 \uccb4\ud06c\ud574\ubcf4\ub294 \uac83\uc774\ub2e4. \uc0c1\uad00\uacc4\uc218\uc758 \uc808\ub300\uac12\uc774 \ud070 \uc6d0\uc18c\uac00 \uc788\ub2e4\uba74 \uc774\ub294 \ub450 \ubcc0\uc218\uac04\uc5d0 corelation\uc774 \uc0c1\ub2f9\ud788 \ub192\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud558\uace0 \uc774\ub294 \uacf5\uc120\uc131 \ubb38\uc81c\uac00 \ub370\uc774\ud130 \ub0b4\uc5d0 \uc874\uc7ac\ud55c\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \ud558\uc9c0\ub9cc \uc0c1\uad00\uacc4\uc218 \ud589\ub82c\ub9cc\uc73c\ub85c\ub294 \ud56d\uc0c1 \uc774\ub97c detecting\ud560 \uc218\ub294 \uc5c6\ub294\ub370, \uc774\ub294 \uc138\uac1c \uc774\uc0c1\uc758 \ubcc0\uc218\uac00 \ud589\ub82c\uc758 \uc6d0\uc18c\uac12\uc774 \ud2b9\ubcc4\ud788 \ub192\uc9c0 \uc54a\ub354\ub77c\ub3c4 \uc11c\ub85c \uacf5\uc120\uc131\uc774 \uc874\uc7ac\ud560 \uc218\uac00 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. \uc6b0\ub9ac\ub294 \uc774\ub7ec\ud55c \uc0c1\ud669\uc744 \ub2e4\uc911\uacf5\uc120\uc131\uc774 \uc874\uc7ac\ud55c\ub2e4\uace0 \ud45c\ud604\ud55c\ub2e4. \ub2e4\uc911\uacf5\uc120\uc131 \ubb38\uc81c\ub97c \ub9de\ub531\ub4e4\uc600\uc744 \ub54c\ub294 \uc0c1\uad00\uacc4\uc218 \ud589\ub82c\ubcf4\ub2e4\ub294 variance inflation factor (VIF)\ub97c \uacc4\uc0b0\ud558\ub294 \uac83\uc774 \ub354 \ub098\uc740 \ubc29\ubc95\uc774\ub2e4. $ \\(VIF(\\hat{\\beta}_{j})=\\frac{1}{1-R^2_{X_{j}|X_{-j}}}\\) $ VIF\uc758 \uc2dd\uc740 \uc704\uc640 \uac19\uc73c\uba70 \\(R^2_{X_{j}|X_{-j}}\\) \uc774 1\uc5d0 \uac00\uae4c\uc6cc \uc9c0\uba74 \uacf5\uc120\uc131\uc774 \uc874\uc7ac\ud55c\ub2e4\ub294 \ub73b\uc774\uace0 VIF\uac00 \ucee4\uc9c4\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c VIF\uc758 \uac12\uc774 5\ub098 10\uc744 \ucd08\uacfc\ud558\uba74 \uacf5\uc120\uc131\uc758 \ubb38\uc81c\uac00 \uc874\uc7ac\ud55c\ub2e4\ub294 \uac83\uc744 \ub098\ud0c0\ub0b8\ub2e4. \uacf5\uc120\uc131\uc744 \ud574\uacb0\ud558\ub294 \ubc29\ubc95\uc73c\ub85c \ucc28\uc6d0\ucd95\uc18c\ub97c \ud1b5\ud574 \ubcc0\uc218\ub97c \uc870\uc808\ud558\ub294 \ubc29\ubc95\uc774 \uc788\ub2e4. \ucc28\uc6d0\ucd95\uc18c\ub97c \ud558\uae30 \uc704\ud574 \uc544\ub798\uc640 \uac19\uc740 \ub450\uac00\uc9c0 \ubc29\ubc95\uc774 \uc885\uc885 \uc0ac\uc6a9\ub41c\ub2e4. variable selection PCA \uba3c\uc800 \ubcc0\uc218\uc120\ud0dd\uc740 \uacf5\uc120\uc131\uc774 \uc874\uc7ac\ud558\ub294 \ub450 \uac00\uc9c0\uc758 \ubcc0\uc218 \uc911, \ud55c\uac00\uc9c0\ub9cc \ucc44\ud0dd\ud558\uc5ec \ucd94\uc815\ubaa8\ub378\uc5d0 \ub123\ub294 \uac83\uc774\ub2e4. \uc774\ub294 \uac04\ud3b8\ud558\uace0 \ud574\uc11d\uc774 \uc6a9\uc774\ud558\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\uc9c0\ub9cc, \ud558\ub098\uc758 \uc785\ub825\ubcc0\uc218\ub97c \uc644\uc804\ud788 \uc0ad\uc81c\ud574\ubc84\ub9ac\ub294 \uac83\uc774\uae30 \ub54c\ubb38\uc5d0 \uc6b0\ub9ac\uac00 \uac00\uc9c0\uace0 \uc788\ub294 \uae30\ubcf8 \uc815\ubcf4\ub97c \uc190\uc2e4\ud55c\ub2e4\ub294 \ub2e8\uc810 \ub610\ud55c \uc874\uc7ac\ud55c\ub2e4. \uac04\ub2e8\ud55c \ubaa8\ub378\ub85c \uc785\ub825\ubcc0\uc218\uac00 2\uac1c\ub9cc \uc874\uc7ac\ud558\ub294 \\(Y={\\beta}_0+{\\beta}_1X_1+{\\beta}_2X_2+{\\epsilon}\\) \ubaa8\ub378\uc744 \uc0dd\uac01\ud574\ubcfc \ub54c PCA\ub294 \uc785\ub825\ubcc0\uc218 X1\uacfc X2\uac04\uc758 Linear comnination\uc744 \ud1b5\ud574 \ucd95\uc744 \ud68c\uc804\uc2dc\ucf1c \ub450 \ucd95(PC1,PC2)\uc744 independent\ud55c \uad00\uacc4\ub85c \ubc14\uafb8\uc5b4 \ud55c\uac1c\uc758 \ucd95\ub9cc \ucc44\ud0dd\ud558\uc5ec \ucc28\uc6d0\uc744 \ucd95\uc18c\ud558\ub294 \ubc29\ubc95\uc774\ub2e4. \uc774\ub294 PC1\ub9cc\uc744 \ucc44\ud0dd\ud558\uc5ec \ud558\ub098\uc758 \ubcc0\uc218\ub97c \ubc84\ub9ac\ub294 \uac83\uacfc \ube44\uc2b7\ud558\uac8c \ubcf4\uc5ec\uc9c0\uc9c0\ub9cc, PC1\uc740 \uacb0\uad6d \\({\\alpha}X_1+{\\beta}X_2\\) \uaf34\uc758 \uc120\ud615\uacb0\ud569\uc774\uae30 \ub54c\ubb38\uc5d0, \uc815\ubcf4\ub97c \uc190\uc2e4\ud558\uc9c0 \uc54a\uace0\ub3c4 \ucc28\uc6d0\uc744 \ucd95\uc18c\ud558\uc5ec \uacf5\uc120\uc131\uc744 \ud574\uacb0\ud560 \uc218 \uc788\ub294 \ubc29\ubc95\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub7ec\ud55c \uc120\ud615\uacb0\ud569\uc73c\ub85c \uc778\ud574 \ubaa8\ub378 \ud574\uc11d\uc774 \uc5b4\ub824\uc6cc\uc9c4\ub2e4\ub294 \ub2e8\uc810 \ub610\ud55c \uc874\uc7ac\ud55c\ub2e4. The Basics of Decision Trees Tree-based model\uc740 \ub2e8\uc21c\ud558\uba70 \ud574\uc11d\uc5d0 \uc6a9\uc774\ud55c \uc9c0\ub3c4\ud559\uc2b5\uc758 \ubc29\ubc95 \uc911 \ud558\ub098\uc774\ub2e4. \uc21c\uc11c\ub294 Decision tree\ub97c Regression\uacfc classification \ub450 \uac00\uc9c0\ub85c \ub098\ub204\uc5b4 \uc124\uba85\ud558\uba70, \uc758\uc0ac\uacb0\uc815\ub098\ubb34\ubaa8\ud615 \uc774\ud6c4\uc5d0 Bagging\uacfc random forests, boosting\uc744 \uac04\ub2e8\ud558\uac8c \uc124\uba85\ud558\ub3c4\ub85d \ud560 \uac83\uc774\ub2e4. Regression Trees \uba3c\uc800 Regression Tree model\ub85c \uac04\ub2e8\ud55c \uc608\uc2dc\ub97c An Introduction to Statistical Learning with R(ISLR) \uad50\uc7ac\uc5d0\uc11c \uac00\uc838\uc640 \ubcf4\uc774\uaca0\ub2e4. \uc704\ub294 Hitters \ub370\uc774\ud130 \uc14b\uc744 \uc0ac\uc6a9\ud558\uc600\uc73c\uba70, \ub18d\uad6c\uc120\uc218 \ub4e4\uc758 Salary\ub97c \uc120\uc218\ub4e4\uc774 \ud504\ub85c\ub9ac\uadf8\uc5d0\uc11c \uacbd\uae30\ud55c \ud587\uc218\uc640 \uc29b\ud305\ud69f\uc218\ub97c \ud1b5\ud574 \ub370\uc774\ud130\ub97c \ubd84\ub958\ud55c \uac04\ub2e8\ud55c \uc0ac\uc9c4\uc774\ub2e4. \ub9e8 \uc717\uc904\uc758 \ubd84\ud560 \uaddc\uce59\uc744 \ubcf4\uba74(top split) \uba3c\uc800 \uc120\uc218\ub4e4\uc758 \uc9d1\ub2e8\uc744 \ud504\ub85c\ub9ac\uadf8\uc5d0\uc11c \ub6f4 \ud587\uc218\uac00 4.5\ub144 \uc774\uc0c1\uc778\uc9c0 \uc774\ud558\uc778\uc9c0\ub85c \uad6c\ubd84\uc744 \ud568\uc744 \uc54c \uc218 \uc788\ub2e4. \uadf8 \uc544\ub798 node\ub294 \ud587\uc218\ub85c \uc120\uc218\ub4e4\uc744 \ub450 \uc9d1\ub2e8\uc73c\ub85c \uad6c\ubd84\ud55c \ub4a4 , \uc29b\ud305\ud69f\uc218 117.5\uac1c\ub97c \uae30\uc900\uc73c\ub85c \ub610 \uadf8\ub8f9 split\uc744 \ud568\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc774 \uadf8\ub9bc\uc744 2\ucc28\uc6d0\uc758 \ud3c9\uba74 \uc704\uc5d0\uc11c \uadf8\ub9bc\uc73c\ub85c \ud45c\ud604\ud55c\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc774 \ub098\ud0c0\ub09c\ub2e4. \uc704\uc640 \uac19\uc740 \uc138\uac00\uc9c0 regions\uc5d0\uc11c \uac01\uac01 region\uc5d0 \uc18d\ud574\uc788\ub294 player\ub4e4\uc758 mean response value\ub97c \uad6c\ud558\ub294 \uac83\uc744 predicted Y\ub85c \uc124\uc815\ud558\ub294 \uac83\uc774 \uae30\ubcf8\uc801\uc778 regression decision tree method\uc758 \ub9e4\ucee4\ub2c8\uc998\uc774\ub2e4. \uc704\uc758 \uadf8\ub9bc\uc5d0\uc11c \\(R_1,R_2, R_3\\) \uc758 region\uc744 \uc6b0\ub9ac\ub294 \ud2b8\ub9ac\uc758 terminal nodes , \ud639\uc740 leaves \ub77c\uace0 \uce6d\ud55c\ub2e4. \uc704\uc758 \uac04\ub2e8\ud55c \uc608\uc2dc\ub97c \ud574\uc11d\ud55c\ub2e4\uba74 \uc6b0\ub9ac\ub294 Salary\ub97c \uacb0\uc815\ud558\ub294 \uc694\uc18c\uc911 \uac00\uc7a5 \uc911\uc694\ud55c factor\ub294 Years\ub77c\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uace0, \ub9ac\uadf8\uc5d0\uc11c \ub6f4\uc9c0 4.5\ub144\uc774\uc0c1\uc774 \ub41c \ud50c\ub808\uc774\ub4e4 \uc911\uc5d0\uc11c\ub294 \uc774\uc804 \ub144\ub3c4\uc758 number of hits\uc774 salary\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce5c\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \uc704 \uc608\uc2dc\uc758 \ud68c\uadc0 \ub098\ubb34 \ubaa8\ud615\uc740 Hits, Years, Salary\uac04\uc758 true relationship\uc744 \uc9c0\ub098\uce58\uac8c \ub2e8\uc21c\ud654 \ud558\uc5ec \ud45c\ud604\ud55c \uac00\ub2a5\uc131\uc774 \ub192\uc9c0\ub9cc, \uc774\ub294 \ud68c\uadc0\ubaa8\ud615\uc758 \uc7a5\uc810\uc774\uba70, \ud574\uc11d\uc5d0 \uc6a9\uc774\ud558\uace0 \uc2dc\uac01\ud654\ud558\uae30 \ud3b8\ud558\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\ub2e4. Process of building regression tree Rough\ud558\uac8c \ub9d0\ud574\uc11c \uc6b0\ub9ac\ub294 \ud68c\uadc0\ud2b8\ub9ac\ub97c \ub9cc\ub4dc\ub294 \uac83\uc744 \ub450\uac00\uc9c0 \ub2e8\uacc4\ub85c \uc124\uba85\ud560 \uc218 \uc788\ub2e4. predictor space\ub97c set of possible values\uc778 \\(X_1,X_2,...,X_{p}\\) \uac00 \\(\\mathbf{J}\\) \uac1c\uc758 distinct\ud55c non-overlapping \ud55c \\(R_1,R_2,...,R_{J}\\) \uad6c\uc5ed\uc5d0 \uc18d\ud558\uac8c \uc815\uc758\ud55c\ub2e4. \ubaa8\ub4e0 \uad00\uce21\uce58\ub294 region \\(R_{j}\\) \uc5d0 \uc18d\ud558\uba70, \\(R_{j}\\) \uc5d0 \uc18d\ud558\ub294 training observations\ub4e4\uc758 mean of response value\ub85c \uc608\uce21\uc744 \uc2dc\ud589\ud55c\ub2e4. \uccab\ubc88\uc9f8 Step\uc5d0\uc11c predictor Space\uac00 two regions\uc73c\ub85c\ub9cc \ub098\ub258\uc5b4\uc838 \uc788\ub2e4\uace0 \uac00\uc815\uc744 \ud574\ubcf4\uace0, \\(R_1\\) \uad6c\uc5ed\uc758 response mean\uc774 10, \\(R_2\\) \uad6c\uc5ed\uc758 response mean\uc774 20\uc774\ub77c\uace0 \uac00\uc815\ud574\ubcf4\uc790. \uc5ec\uae30\uc11c Given observation\uc774 \\(X=x\\) , if \\(x\\in{R_1}\\) \uc774\uba74 \uc6b0\ub9ac\ub294 10\uc73c\ub85c \uc608\uce21\uc744 \ud558\ub294 \uac83\uc774\ub2e4. \ub450\ubc88\uc9f8 \ub2e8\uacc4\uc5d0\uc11c \uc6b0\ub9ac\ub294 regions\uc744 \uc5b4\ub5bb\uac8c \ubaa8\uc591\uc744 \uc815\uc758\ud560 \uc218 \uc788\uc744\uae4c? \uc2e4\uc81c\ub85c region\uc740 \uc5b4\ub290 \ubaa8\uc591\uc73c\ub85c\ub3c4 \ud615\uc131\ub420 \uc218 \uc788\uc73c\uba70, \uace0\ucc28\uc6d0\uc5d0\uc11c\ub294 \uc6b0\ub9ac\uac00 \uc0dd\uac01\ud558\ub294 \uac83 \ucc98\ub7fc \uc608\uc05c \uc9c1\uc120\uc73c\ub85c\ub9cc \uacbd\uacc4\ub97c \ub098\ub204\uc9c0\ub294 \uc54a\uc744 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc \uacb0\uad6d region\ub4e4\uc758 \ubaa8\uc591\uc774 \uc5b4\ub5bb\uac8c \ub418\ub358\uac04\uc758 \uc6b0\ub9ac\uc758 \ubaa9\ud45c\ub294 RSS, \uc989 \uc794\ucc28\uc81c\uacf1\ud569\uc744 \ucd5c\uc18c\ud654\ud558\ub294 \\(R_1,..,R_{J}\\) \ub97c \ucc3e\ub294 \uac83\uc774\ub2e4. $ \\(\\sum\\limits_{j=1}^J\\sum\\limits_{i\\in{R_j}}(y_{i}-\\hat{y}_{R_{j}})^2\\) $ RSS\ub294 \uc704\ucc98\ub7fc \ud45c\ud604\ub418\uba70, \uc5ec\uae30\uc11c \\(\\hat{y}_{R_{j}}\\) \ub294 mean response for the training observations within the j th box\uc774\ub2e4. \ud558\uc9c0\ub9cc \ubd88\ud589\ud788\ub3c4 J box\uc5d0 \ub300\ud574\uc11c \uac00\ub2a5\ud55c \ubaa8\ub4e0 partition\uc744 \uace0\ub824\ud558\ub294 \uac83\uc740 \ub9e4\uc6b0\ud798\ub4e4\ub2e4. \uc774\ub7ec\ud55c \uc774\uc720\ub85c \uc6b0\ub9ac\ub294 top-down, greedy \uc811\uadfc\ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\ub294\ub370 \uc774\ub294 recursive binary splitting \uc73c\ub85c \ubd88\ub9b0\ub2e4. Recursive binary splitting\uc744 \uc218\ud589\ud558\uae30 \uc704\ud574 \uc6b0\ub9ac\ub294 \uccab\ub2e8\uacc4\ub85c predictor \\(X_{j}\\) \uc640 cutpoint s \ub97c \uc9c0\uc815\ud574\uc8fc\uace0 \uc774\ub294 predictor space\ub97c \\(\\{X|X_{j}<s\\}\\) \uc640 \\(\\{X|X_{j}\\ge{s}\\}\\) \ub85c \ub098\ub294\ub370, \uc774\ub54c predictor\ub098 cutpoint\uc124\uc815\uc740 RSS\uc758 \uac10\uc18c\ub97c \ucd5c\ub300\ud654\ud558\ub294 \uac83\uc73c\ub85c \ub098\ub204\ub294 \uac83\uc774\ub2e4. \uc774\ub97c \uc218\uc2dd\uc73c\ub85c \uc880 \ub354 \uc790\uc138\ud558\uac8c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\uac00\ub2a5\ud558\uba70, \uc774\ub54c\uc758 j \uc640 s \ub97c \ucc3e\ub294 \uac83\uc740 \uadf8 \uc544\ub798\uc758 eqation\uc744 \ucd5c\uc18c\ud654\ud558\ub294 j\uc640 s\ub97c \ucc3e\ub294 \uac83\uacfc \uac19\ub2e4. \\(R_1(j,s) = \\{X|X_{j} <s\\}\\) and \\(R_2(j,s) = \\{X|X_{j} \\geq s\\}\\) , \\(\\sum\\limits_{i:x_{i}\\in{R_{1}(j,s)}}(y_{i}-\\hat{y}_{R_{1}})^2\\) + \\(\\sum\\limits_{i:x_{i}\\in{R_{2}(j,s)}}(y_{i}-\\hat{y}_{R_{2}})^2\\) Input variables\ub4e4\uc758 \uc0ac\uc774\uc988\uac00 \ud06c\uc9c0\uc54a\ub2e4\uba74 \uc704\uc758 \uc2dd\uc744 \uad6c\ud558\ub294 \uac83\uc740 \uadf8\ub807\uac8c \uc624\ub798 \uac78\ub9ac\uc9c0\ub294 \uc54a\uc744 \uac83\uc774\ub2e4. \uadf8 \ub2e4\uc74c\uc73c\ub85c \uc6b0\ub9ac\ub294 \uc774\ub7ec\ud55c \uacfc\uc815\uc744 \uacc4\uc18d\ud574\uc11c \ubc18\ubcf5\ud558\uc5ec \ubaa8\ub4e0 resulting regions\ub0b4\uc5d0\uc11c RSS\ub97c \ucd5c\uc18c\ud654\ud558\uac8c \ub370\uc774\ud130\ub97c \ubd84\ud560\ud558\ub294 \ucd5c\uc801\uc758 prediction \uc640 cutpoint\ub97c \ucc3e\uc73c\uba74 \ub41c\ub2e4. \ub9cc\uc57d \ubaa8\ub4e0 regions\uc774 \uc815\uc758\uac00 \ub418\uc5c8\ub2e4\uba74, \uc6b0\ub9ac\ub294 \uc0c8\ub85c \ub4e4\uc5b4\uc624\ub294 test \uad00\uce21\uce58\uc5d0 \ub300\ud574\uc11c \uadf8 test \uad00\uce21\uce58\uac00 \uc18d\ud558\ub294 regions\uc758 train \uad00\uce21\uce58\uc758 \ud3c9\uade0\uc744 \uc608\uce21\uac12\uc73c\ub85c \uc0ac\uc6a9\ud558\uba74 \ub41c\ub2e4. Tree Pruning \uc55e\uc11c \uc18c\uac1c\ud55c \ubc29\uc2dd\uc740 training set\uc5d0 \ub300\ud574 \uc88b\uc740 \uc608\uce21\uac12\uc744 \uac16\ub294 \uac83 \uac19\uc9c0\ub9cc, \uc790\ub8cc\uc5d0 \ub300\ud574 \uacfc\uc801\ud569\ud560 \uac00\ub2a5\uc131\uc774 \ub9e4\uc6b0\ud06c\uace0, test set\uc5d0 \ub300\ud574 \ud615\ud3b8\uc5c6\ub294 \uacb0\uacfc\ub97c \uac00\uc838\uc624\uae30\ub3c4 \ud55c\ub2e4. \uc774\ub294 \uc9e0 \ud2b8\ub9ac\ubaa8\ub378\uc774 \ub108\ubb34 \ubcf5\uc7a1\ud558\uae30 \ub54c\ubb38\uc778\ub370, \uadf8\ub798\uc11c \uc801\uc740 \ubd84\ud560\uc744 \ud55c \uc791\uc740 \ud2b8\ub9ac\ub4e4\uc740 \ud3b8\ud5a5\uc744 \uc904\uc774\ub294 \uac83\uc744 \ud76c\uc0dd\ud558\uba70 \ub0ae\uc740 variance\uc640 \ub354 \ub098\uc740 \uc6a9\uc774\ud55c \ud574\uc11d\uc744 \uc774\ub048\ub2e4. \uc704\uc640 \uac19\uc740 \ubc29\uc2dd\uc758 \ud55c\uac00\uc9c0 \ub300\uc548\uc73c\ub85c\ub294, \uac01 \ubd84\ud560\uc5d0\uc11c RSS\uc758 \uac10\uc18c\uac00 \ud2b9\uc815 threshold\ub97c \ub118\ub294 split\ub9cc \ucc44\ud0dd\uc744 \ud558\uc5ec tree\ub97c \uc904\uc774\ub294 \uac83\uc744 \uc608\ub85c \ub4e4 \uc218 \uc788\ub2e4. \uc774\ub7f0 \ub300\uc548\uc740 \uc870\uae08 \ub354 \uc791\uc740 tree\ub97c \ub9cc\ub4e4\uc9c0\ub9cc, \uc774\ub294 \uadfc\uc2dc\uc548\uc801\uc778 \ubc29\ubc95\uc774\ub2e4. \uadf8 \uc774\uc720\ub85c\ub294 \ud2b8\ub9ac\ud615\uc131\uc758 \uac01 \ucd08\uae30\ub2e8\uacc4\uc5d0\uc11c\ub294 \ud615\ud3b8\uc5c6\uc5b4 \ubcf4\uc774\ub294 split\uc774 \ub098\uc911\uc5d0 tree\uc804\uccb4\ub97c \ub9cc\ub4e4\uc5b4 \ub193\uc740 \uc774\ud6c4 \ubcf4\uc558\uc744 \ub54c\ub294 large reduction in RSS\ub97c \uac00\uc838\uc62c \uc218\ub3c4 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. \uadf8\ub7ec\ubbc0\ub85c, \ud2b8\ub9ac\ubaa8\ud615\uc744 \uc124\uacc4\ub97c \ud558\uba74\uc11c \uc5bb\uc744 \uc218 \uc788\ub294 \ub354 \ub098\uc740 \uc804\ub7b5\uc740, \uba3c\uc800 \uac00\ub2a5\ud55c \ub9e4\uc6b0 \ud070 \ub098\ubb34 \\(T_0\\) \uc744 \ub9cc\ub4e0 \uc774\ud6c4\uc5d0, subtree\ub97c \uc5bb\uae30 \uc704\ud574 \ub9c8\uc9c0\ub9c9\uc5d0\uc11c \uac00\uc9c0\ub97c \uccd0\ub0b4\ub294 \uac83\uc774\ub2e4. \uc5b4\ub5bb\uac8c \ub098\ubb34\uc5d0 \uac00\uc9c0\uce58\uae30\ub97c \ud558\ub294 \uac83\uc774 \uac00\uc7a5 \uc88b\uc740 \ubc29\ubc95\uc77c\uae4c? \uc9c1\uad00\uc801\uc73c\ub85c \uc6b0\ub9ac\ub294 \uac00\uc7a5 \ub0ae\uc740 test error rate\ub97c \uac16\ub294 subtree\ub97c \ucc3e\ub294 \uac83\uc774\ub2e4. subtree\ub97c \uc5bb\uc740 \ud6c4\uc5d0, \uc6b0\ub9ac\ub294 \uad50\ucc28\uac80\uc99d \ub610\ub294 validation set \uc811\uadfc\uc744 \ud1b5\ud574\uc11c test error\ub97c \ucd94\uc815\ud560\uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc \ubaa8\ub4e0 subtree\uc5d0 \ub300\ud574 \ub108\ubb34\ub098 \ub9ce\uc740 \uc870\ud569\uc758 \uc218\uac00 \uc874\uc7ac\ud558\uae30\uc5d0, \uad50\ucc28\uac80\uc99d error\ub97c \ucd94\uc815\ud558\ub294 \uac83\uc740 \ub9e4\uc6b0 \uc18c\ubaa8\uc801\uc774\uace0 \ube44\ud604\uc2e4\uc801\uc774\ub2e4. \uc774\ub97c \uc704\ud574 \uc6b0\ub9ac\ub294 Cost complexity pruning \ub610\ub294 weakest link pruning method\ub85c \uc54c\ub824\uc9c4 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. \ubaa8\ub4e0 subtree\uc758 \uacbd\uc6b0\uc758 \uc218\ub97c \ucc3e\ub294 \uac83\uc774 \uc544\ub2c8\uace0 \uc74c\uc218\uac00 \uc544\ub2cc \ud29c\ub2dd \ud30c\ub77c\ubbf8\ud130 \\(\\alpha\\) \ub85c \uc778\ub371\uc2f1\ub41c \ud2b8\ub9ac\uc758 \uc2dc\ud000\uc2a4\ub97c \uace0\ub824\ud574\ubcf4\ub294 \uac83\uc774\ub2e4. \uc54c\uace0\ub9ac\uc998\uc744 \uac04\ub2e8\ud558\uac8c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\uc740 \ub2e8\uacc4\ub85c \uc774\ub8e8\uc5b4\uc9c4\ub2e4. Recursive binary splitting\uc744 \ud1b5\ud574 traninig data\ub85c \ud070 \ud2b8\ub9ac\ub97c \ub9cc\ub4e4\uba74\uc11c, \uac01 terminal node\uac00 \uc77c\uc815 \uc218\uc900\uc758 minimum level\uc758 \uad00\uce21\uce58 \uac2f\uc218\ub97c \uac16\uac8c \ub41c\ub2e4\uba74 \ud2b8\ub9ac\ud615\uc131\uc744 \uba48\ucd98\ub2e4. \ucd5c\uc801\uc758 subtree \uc2dc\ud000\uc2a4\ub97c \uc5bb\uae30\uc704\ud574 \\(\\alpha\\) \uc758 \ud568\uc218\ub97c \uc774\uc6a9\ud558\uc5ec cost complecity pruning\uc744 \uc801\uc6a9\ud55c\ub2e4. K-\uad50\ucc28\uac80\uc99d\uc744 \uc774\uc6a9\ud558\uc5ec \\(\\alpha\\) \ub97c \uacb0\uc815\ud55c\ub2e4. \uac01 \\(\\alpha\\) \uac12\ub4e4\uc758 \uacb0\uacfc\ub97c \ud3c9\uade0\ub0b4\uc5b4 average error\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \\(\\alpha\\) \ub97c \ucc44\ud0dd\ud55c\ub2e4. \\(\\alpha\\) \ub97c \uc774\uc6a9\ud558\uc5ec RSS\uc5d0 penalty\ub97c \ubd80\uacfc\ud558\uc5ec \uac00\uc9c0\uce58\uae30\ub97c \ud558\ub294 \uac83\uc744 \uc2dd\uc73c\ub85c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \ub098\ud0c0\ub0bc \uc218 \uc788\ub2e4. $ \\(\\sum\\limits_{m=1}^{|T|}\\sum\\limits_{x_{i}\\in{R_{m}}}(y_{i}-\\hat{y}_{R_{m}})^2 + {\\alpha}|T|\\) $ \uc55e\uc11c \uc77c\ubc18\uc801\uc778 \uc758\uc0ac\uacb0\uc815\ub098\ubb34\ubaa8\ud615\uc5d0\uc11c RSS\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ub9e4\ucee4\ub2c8\uc998\uc740 \ub611\uac19\uc9c0\ub9cc \\(\\alpha\\) \uc5d0 \ub300\ud55c \ud56d\uc774 \ub354\ud558\uae30\ub85c \ucd94\uac00\ub418\uc5c8\ub2e4. \uc5ec\uae30\uc11c \\(T\\) \ub294 terminal nodes\uc758 \uac2f\uc218\uc774\uba70, \\(\\alpha\\) \ub294 \ud29c\ub2dd \ud30c\ub77c\ubbf8\ud130\uc774\ub2e4. \uc774 \\(\\alpha\\) \ub294 subtree\uc758 \ubaa8\ub378 complexity\uc640 training data\uc5d0 fitting\ud558\ub294 \uac83\uacfc trade-off\ud55c \uad00\uacc4\ub97c \uac16\ub294\ub2e4. \uc27d\uac8c \uc124\uba85\uc744 \ud558\uba74 \\(\\alpha\\) \uac00 0\uc774\ub77c\uba74 \uc704\uc758 \uc2dd\uc740\uc77c\ubc18\uc801\uc73c\ub85c \uc6b0\ub9ac\uac00 RSS\ub97c \uad6c\ud558\ub294 \uc2dd\uacfc \uac19\uc73c\uba70 \uadf8\ub7f4\ub54c\uc758 RSS\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \uc2dd\uc740 \ud2b8\ub9ac\uc758 \uae4a\uc774\ub97c \ucd5c\ub300\ud55c\uc73c\ub85c \uae4a\uac8c \ub9cc\ub4e4\uc5b4 \ubaa8\ub4e0 \uad00\uce21\uce58\ub4e4\uc744 \ud558\ub098\ud558\ub098 terminal node\ub85c \uc0bc\ub294 \uacbd\uc6b0\uc77c \uac83\uc774\ub2e4. \ub9cc\uc57d \ubaa8\ub4e0 \uac83\uc774 \uac19\uace0 \uc5ec\uae30\uc11c \\(\\alpha\\) \uac00 0\uc774 \uc544\ub2c8\ub77c\uba74, tree\ub97c \uac00\uc7a5 \uae4a\uac8c \ubed7\uc5c8\uc744 \ub54c\ub294 \uc794\ucc28\uc81c\uacf1\ud569\uc5d0 \ucd94\uac00\ub85c \\(\\alpha\\) \ud56d\uc774 \ucd94\uac00\ub418\uc5c8\uae30 \ub54c\ubb38\uc5d0, RSS\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \uacbd\uc6b0\uc640 \uc77c\uce58\ud558\uc9c0 \uc54a\uac8c \ub41c\ub2e4. \ub367\ubd99\ud600 \\(\\alpha\\) \uac00 0\uc5d0 \uadfc\uc0ac\ud55c \uc791\uc740 \uac12\uc774 \uc544\ub2c8\uace0 \uc801\ub2f9\ud788 \ud070 \uc22b\uc790\ub97c \uac16\ub294\ub2e4\uba74 \uc794\ucc28\uc81c\uacf1\uc744 \ub098\ud0c0\ub0b4\ub294 \\(\\sum\\) \uc548\uc758 \uc67c\ucabd \ud56d\ubcf4\ub2e4 \uc624\ub978\ucabd\ud56d\uc774 \uc804\uccb4\uc2dd\uc5d0 \uc601\ud5a5\uc744 \ub354 \ud06c\uac8c \uc8fc\uae30\ub54c\ubb38\uc5d0 \uc774\ub7f4 \uacbd\uc6b0\ub77c\uba74 terminal node\uc758 \uc218\ub97c \uc904\uc774\ub294 \uac83\uc774(=make smaller subtree) \uc704 \uc2dd\uc758 quantity\ub97c minimize \uc2dc\ud0ac \uac83\uc774\ub2e4. Classification Trees Classification tree\ub294 regression tree\uc640 \uc591\uc801 response\uac00 \uc544\ub2cc \uc9c8\uc801 response\ub97c \uc608\uce21\ud55c\ub2e4\ub294 \uac83\uc744 \uc81c\uc678\ud558\uace0\ub294 \ub9e4\uc6b0 \uc720\uc0ac\ud558\ub2e4. \ud68c\uadc0\uc758 \uacbd\uc6b0 \uad00\uce21\uce58\uc5d0 \ub300\ud55c response\ub294 \uac19\uc740 terminal node\uc5d0 \uc18d\ud55c training \uad00\uce21\uce58\uc758 \ud3c9\uade0 response\ub85c \uc608\uce21\uc744 \ud558\ub294\ub370 \ubc18\ud574, \ubd84\ub958\uc758 \uacbd\uc6b0\uc5d0\ub294 training \uad00\uce21\uce58\uac00 \uc18d\ud55c region\uc5d0\uc11c most commonly occuring class \uc5d0 \uc18d\ud55c \uac1c\ubcc4 \uad00\uce21\uce58\ub97c \uc608\uce21\ud55c\ub2e4. \ubd84\ub958\ub098\ubb34\uc758 \uacb0\uacfc\ub97c \ud574\uc11d\ud560 \ub54c\ub294 \ud2b9\uc815 terminal node region\uc5d0 \uc0c1\uc751\ud558\ub294 class prediction \ubfd0\ub9cc \uc544\ub2c8\ub77c, \uac01 region\uc5d0 \ub4e4\uc5b4\uc788\ub294 training \uad00\uce21\uce58 \uc0ac\uc774\uc5d0\uc11c class proportions \ub610\ud55c \ud3ec\ud568\ud55c\ub2e4. \ud2b8\ub9ac\ub97c \ub9cc\ub4e4 \ub54c\uc5d0\ub294 \ud68c\uadc0\uc758 \uacbd\uc6b0\uc640 \ube44\uc2b7\ud558\uac8c Recursive binary splitting \uc744 \uc0ac\uc6a9\ud558\uc9c0\ub9cc, binary split\uc758 \uae30\uc900\uc774 RSS\uac00 \uc544\ub2cc classification error rate \ub97c \uc0ac\uc6a9\ud55c\ub2e4. classification error rate\ub294 \ub2e8\uc21c\ud558\uac8c \uac00\uc7a5 \uacf5\ud1b5\uc801\uc778 class\uc5d0 \uc18d\ud558\uc9c0 \uc54a\uc740 region\uc5d0 \ud3ec\ud568\ub41c training \uad00\uce21\uce58\uc758 \uc77c\ubd80\ubd84\uc73c\ub85c \uc0dd\uac01\ud558\uba74 \ub41c\ub2e4. $ \\(E= 1-max_{k}(\\hat{p}_{mk})\\) $ \uc544\ub798\uc758 \uc2dd\uc5d0\uc11c \\(\\hat{p}_{mk}\\) \ub294 k th class\uc778 m \ubc88\uc9f8 region\uc548\uc5d0 \uc788\ub294 training \uad00\uce21\uce58\uc758 \ube44\uc728\uc744 \ub098\ud0c0\ub0b8\ub2e4. \ud558\uc9c0\ub9cc \uae30\uaecf \uc124\uba85\ud588\uc9c0\ub9cc \uc774 \ubd84\ub958\uc5d0\ub7ec\ube44\uc728\uc740 tree-growing\uc5d0 \uc788\uc5b4\uc11c \ucda9\ubd84\ud558\uac8c sensitive\ud558\uc9c0 \uc54a\uc544\uc11c \ub2e4\ub978 \ub450 \uac00\uc9c0 \ubc29\ubc95\uc774 \ub354 \uc120\ud638\ub418\ub294\ub370 \uc774\ub294 \uc6b0\ub9ac\uac00 \uc798 \uc54c\uace0 \uc788\ub294 Gini index \uc640 entropy \uac00 \uc788\ub2e4. \uc9c0\ub2c8\uc9c0\uc218\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ub418\ub294\ub370, $ \\(G=\\sum\\limits_{k=1}^K\\hat{p}_{mk}(1-\\hat{p}_{mk})\\) $ \uc27d\uac8c \ub9d0\ud574\uc11c \uc774 \uc778\ub371\uc2a4\ub294 node\uc758 purity\ub97c \uce21\uc815\ud558\ub294 \uac83\uc774\uba70, \uc774 \uac12\uc774 \uc791\uc73c\uba74 \uc774\ub294 node\uac00 \ub300\uac1c \ub2e8\uc77c \ud074\ub798\uc2a4\ub85c\ubd80\ud130 \ub098\uc628 \uad00\uce21\uce58\ub85c \uc774\ub8e8\uc5b4\uc838\uc788\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. $ \\(D=-\\sum\\limits_{k=1}^K\\hat{p}_{mk}log\\hat{p}_{mk}\\) $ \uc704\uc758 \uc2dd\uc740 Entropy\uc5d0 \ub300\ud55c \uc124\uba85\uc778\ub370, \uc774 \ub610\ud55c \uc9c0\ub2c8\uacc4\uc218\uc640 \ube44\uc2b7\ud558\uac8c \uc218\uce58\uac00 \uc791\uc744 \uc218\ub85d m \ubc88\uc9f8 \ub178\ub4dc\uac00 pure\ud558\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \ubd84\ub958\uc5d0\ub7ec\ube44\uc728\uacfc \uc9c0\ub2c8\uacc4\uc218, \uc5d4\ud2b8\ub85c\ud53c\ub294 \ubaa8\ub450 \ub098\ubb34\uc5d0 \uac00\uc9c0\uce58\uae30\ub97c \ud560\ub54c \uc774\uc6a9\ud558\uc9c0\ub9cc \ubcf4\ud1b5 \uc77c\ubc18\uc801\uc73c\ub85c \ucd5c\uc885 \uac00\uc9c0\uce58\uae30\ud55c \ub098\ubb34\uc758 \uc608\uce21 \uc815\ud655\uc131\uc774 main goal\uc774\ub77c\uba74 \ubd84\ub958\uc5d0\ub7ec\ube44\uc728\uc744 criterion\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc120\ud638\ub41c\ub2e4. \uac04\ub2e8\ud558\uac8c \uc694\uc57d\ud558\uba74 Tree\ubaa8\ud615\uc740 \uc544\ub798\uc640 \uac19\uc740 \uc7a5\ub2e8\uc810\uc744 \uac16\ub294\ub2e4. \uc124\uba85\uc774 \ub9e4\uc6b0 \uc6a9\uc774\ud558\uace0, \uc5b4\ub5a8\ub54c\ub294 \uc120\ud615\ud68c\uadc0\ubcf4\ub2e4 \uc27d\uac8c \uc124\uba85\ud560 \uc218 \uc788\ub2e4. \ube44\uc804\ubb38\uac00\ub77c\ub3c4 tree\uc758 \uc0ac\uc774\uc988\uac00 \ub108\ubb34 \ud06c\uc9c0\ub9cc \uc54a\ub2e4\uba74 \ud574\uc11d\uc774 \uc27d\uace0 \uc2dc\uac01\ud654\ud558\uc5ec \ubcfc \uc218\uc788\ub2e4. dummy variable\ub4e4\uc744 \ub530\ub85c \uc0dd\uc131\ud558\uc9c0 \uc54a\uc544\ub3c4 \uc9c8\uc801 predictors\ub97c \uc870\uc791\ud558\uae30\uc5d0 \ub9e4\uc6b0 \uc27d\ub2e4. \ud558\uc9c0\ub9cc \uc774 \ucc45\uc5d0\uc11c \ub2e4\ub8e8\ub294 \ub2e4\ub978 \ud68c\uadc0\ub098 \ubd84\ub958\ub97c \ub2e4\ub8e8\ub294 \ubc29\ubc95\ub4e4\uacfc \ub3d9\uc77c\ud55c \uc608\uce21 \uc815\ud655\uc131\uc744 \uac16\uc9c0\ub294 \uc54a\ub294\ub2e4 \uac00\uc7a5 \ud070 \ub2e8\uc810\uc73c\ub85c\ub294 \ud2b8\ub9ac\ubaa8\ud615\uc740 \uc544\uc8fc\uc544\uc8fc non-robust \ud558\ub2e4. \ub2ec\ub9ac\ub9d0\ud574, \ub370\uc774\ud130\uac00 \uc870\uae08\ub9cc \ubc14\ub00c\uc5b4\ub3c4 \ucd5c\uc885 \uc608\uce21\ub418\ub294 \ud2b8\ub9ac\uc5d0 \uc544\uc8fc \ud070 \ubcc0\ud654\ub97c \uc57c\uae30\ud55c\ub2e4. \ud558\uc9c0\ub9cc \uc6b0\ub9ac\ub294 bagging, random forest, boosting \ub4f1\uc758 \ubc29\ubc95\uc73c\ub85c \ud2b8\ub9ac\ubaa8\ud615\uc758 \uc131\ub2a5\uc744 \ud5a5\uc0c1 \uc2dc\ud0ac \uc218 \uc788\ub2e4! Bagging, Random Forests, Boosting \uc55e\uc11c \ub9d0\ud55c \uac83\ucc98\ub7fc \ud2b8\ub9ac\ubaa8\ub378\uc740 \ub370\uc774\ud130\uc14b\uc774 \uc870\uae08\ub9cc \ub2ec\ub77c\ub3c4 \uc544\uc8fc \ub2e4\ub978 \uacb0\uacfc\uac12\uc744 \ub0b3\uc744 \uc218\ub3c4 \uc788\ub2e4\uace0 \uc5b8\uae09\ud588\ub2e4. \uc774\ub294 \ud3b8\ud5a5\uc740 \uc791\uc9c0\ub9cc \ub192\uc740 \ubd84\uc0b0\uc744 \uac16\ub294\ub2e4. \uc774\ub7ec\ud55c high-variance\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud55c \ubc29\ubc95\uc73c\ub85c\ub294 bagging , \uc989 bootstrap aggregation \uc744 \ud1b5\ud574 \ud1b5\uacc4\ud559\uc2b5\ubc29\ubc95\uc758 \ubd84\uc0b0\uac10\uc18c\ub97c \uc5bb\uc744 \uc218\uc788\ub2e4. \\[\\hat{f}_{avg}(x) = \\frac{1}{B}\\sum\\limits_{b=1}^B\\hat{f}^b(x)\\] \ubd80\ud2b8\uc2a4\ud2b8\ub7a9\uc744 \uc774\uc6a9\ud558\ub294 \uac83\uc740 \uc704\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \ud480\uc5b4\uc11c \uc124\uba85\ud558\uba74, \uc6b0\ub9ac\uac00 \uac00\uc9c0\uace0 \uc788\ub294 training data set\uc744 \uc5ec\ub7ec\ubc88 (\ub9ce\uc774 \ud639\uc740 \uc544\uc8fc \ub9ce\uc774) \ubcf5\uc6d0\ucd94\ucd9c\uc744 \ud1b5\ud574 resampling\ud558\uc5ec \uc5ec\ub7ec\uac1c\uc758 \ud45c\ubcf8 sample\uc744 \uc5bb\uc740 \ud6c4, \uc774\ub4e4\uc744 \uac01\uac01 \ud559\uc2b5 \uc54c\uace0\ub9ac\uc998\uc5d0 \ub123\uc5b4 \ubd84\ub958 \ud639\uc740 \ud68c\uadc0\ub97c \uc2dc\ud589\ud55c \ud6c4 \uadf8 \uacb0\uacfc\uac12(MSE\ub4f1)\uc758 \ud3c9\uade0\uc744 \ud1b5\ud558\uc5ec prediction\uc744 \ud558\ub294 \ubc29\ubc95\uc774\ub2e4. B\uac1c \ub9cc\ud07c\uc758 \ud2b8\ub9ac\uac00 \uc0dd\uae30\ub294 \uac70\uace0 \uc774\ub97c averaging\ud558\uba74 \ubd84\uc0b0\uc744 \uac10\uc18c\uc2dc\ud0a4\ub294 \ud6a8\uacfc\ub97c \ubcfc \uc218 \uc788\ub2e4. \uc5ec\uae30\uc11c number of trees B (resampling\uc744 \ud1b5\ud574 \uadf8\ub9cc\ud07c \ud558\ub098\uc758 training set\uc5d0\uc11c \uc5ec\ub7ec\uac1c\uc758 data set\uc744 \ub9cc\ub4e4\uae30\uc5d0 \uadf8\ub9cc\ud07c \ud2b8\ub9ac\uac00 \uc0dd\uae30\ub294 \uac83) \ub294 bagging\uc5d0\uc11c \uc5c4\uccad \uc911\uc694\uc2dc \uc5ec\uaca8\uc9c0\ub294 \ud30c\ub77c\ubbf8\ud130\ub294 \uc544\ub2cc\ub370, \uadf8 \uc774\uc720\ub294 B\uac00 \ub9e4\uc6b0\ucee4\uc838\ub3c4 \uc774\uac83\uc774 \uacfc\uc801\ud569\uacfc \uc9c1\uacb0\ub418\uc9c0\ub294 \uc54a\uae30 \ub54c\ubb38\uc774\ub2e4. B\ub97c \ud06c\uac8c \ub298\ub9b0\ub2e4\ub294 \uac83\uc740 \ub098\ubb34\uc758 \uae4a\uc774\ub97c \uae4a\uac8c\ud574 \ub9e4\uc6b0 \ub9ce\uc740 split\uc744 \ud1b5\ud574 terminal node\ub97c \ub298\ub9ac\ub294 \uac83\uc758 \uc758\ubbf8\uac00 \uc544\ub2c8\uba70 \uc624\ud788\ub824 error\ub97c \ub354 \uc548\uc815\ud654\uc2dc\ud0a4\uac8c \ub9cc\ub4e0\ub2e4. Out-of-Bag Error Estimation \ud3c9\uade0\uc801\uc73c\ub85c bootstrap\uc744 \uc9c4\ud589\ud558\uba74 \uad00\uce21\uce58\uc758 2/3\uc815\ub3c4\ub9cc \uc0ac\uc6a9\ub418\uc5b4\uc9c4\ub2e4. \uc774\ub860\uc801\uc73c\ub85c N\uac1c\uc758 \uad00\uce21\uce58\uc5d0 data\uc5d0\uc11c N\uac1c\uc758 \ud45c\ubcf8\uc744 \ubcf5\uc6d0\ucd94\ucd9c\ud558\uac8c \ub420 \uacbd\uc6b0 \uac01 \ub370\uc774\ud130\uac00 \ubf51\ud790 \ud655\ub960\uc740 \uc544\ub798\uc640 \uac19\uace0, \\[1-(1-\\frac{1}{N})^N\\] N\uc774 \ub9e4\uc6b0 \ucee4\uc9c0\uba74 \uc704 \uc2dd\uc740 \\(1-\\frac{1}{e}\\) \ub85c \uc218\ub834\ud558\ub294\ub370 \uc774\ub294 \uc57d 0.63\uc815\ub3c4\uc774\uae30\uc5d0 \uad00\uce21\uce58\uc758 2/3\uc815\ub3c4\ub9cc \ud655\ub960\uc801\uc73c\ub85c \uc0ac\uc6a9\ub418\uc5b4\uc9c4\ub2e4\ub294 \ub73b\uc774\ub2e4. \uadf8\ub807\ub2e4\uba74 \ub0a8\uc740 \uad00\uce21\uce58\uc758 1/3\uc740 bagged tree\uc5d0\uc11c fitting\ub418\ub294\ub370 \uc0ac\uc6a9\ub418\uc9c0 \ubabb\ud55c\ub2e4\ub294 \ub73b\uc744 \uc758\ubbf8\ud558\ub294\ub370, \uc774\ub97c out-of-bag (OOB)\uad00\uce21\uce58\ub77c\uace0 \uce6d\ud55c\ub2e4. \uc6b0\ub9ac\ub294 \uc774\ub7ec\ud55c OOB\uc0d8\ud50c\ub4e4\uc744 \ud65c\uc6a9\ud558\uc5ec \ud2b8\ub9ac\ubaa8\ud615\uc5d0\uc11c\uc758 decision\uc5d0 \uac00\uc911\uce58\ub97c \uc870\uc815\ud560\uc218\ub3c4 \uc788\uace0, \ubd84\ub958\ubaa8\ud615\uc5d0\uc11c\ub294 \uc624\ubd84\ub958\uc728\uc744 \ucd94\uc815\ud558\ub294 \ub4f1 \ub2e4\uc591\ud55c \uc6a9\ub3c4\ub85c \uc0ac\uc6a9\ud560 \uc218\uc788\ub2e4. \ud2b9\ud788 OOB error\uc758 \uacb0\uacfc\ub294 bagged model\uc5d0\uc11c test error\ub97c \ucd94\uc815\ud558\ub294\ub370 \uc788\uc5b4\uc11c \ud55c\ubc88\ub3c4 fittingd\u3154 \uc0ac\uc6a9\ub418\uc9c0 \uc54a\uc740 response\ub4e4\uc744 \uac16\uace0 \ucd94\uc815\uc744 \ud558\uae30\ub54c\ubb38\uc5d0 \ud0c0\ub2f9\ud558\ub2e4. \uc774\ub7ec\ud55c Bagging\uae30\ubc95\uc740 \uc758\uc0ac\uacb0\uc815\ub098\ubb34\uc5d0 \uc801\uc6a9\uc744 \ud558\uba74 \uac00\uc9c0\uce58\uae30 \uc791\uc5c5\uc744 \uc0dd\ub7b5\ud560\uc218\ub3c4 \uc788\uac8c \ud574\uc8fc\uace0, error\uc758 variance\ub97c \uc904\uc5ec\uc900\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\uc9c0\ub9cc, single decision tree\uc640\ub294 \ub2e4\ub974\uac8c \uc5b4\ub5a0\ud55c \ubcc0\uc218\uac00 procedure\uc5d0\uc11c \uc5bc\ub9c8\ub098 \uc911\uc694\ud55c \uc601\ud5a5\ub825\uc744 \uac16\ub294\uc9c0\ub97c \uccb4\ud06c\ud560 \uc218 \uc5c6\ub2e4. \uc989 Bagging\uc740 \ud574\uc11d\uc744 \ud76c\uc0dd\ud558\uc5ec \uc608\uce21\uc758 \uc815\ud655\ub3c4\ub97c \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ubc29\ubc95 \uc774\ub77c\uace0 \ub9d0\ud560 \uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc \ube44\ub85d \ub2e8\uc77c \uc758\uc0ac\uacb0\uc815\ub098\ubb34\ubcf4\ub2e4\ub294 \ud574\uc11d\uc5d0 \uc6a9\uc774\ud558\uc9c0 \uc54a\ub354\ub77c\ub3c4, \ud68c\uadc0\ud2b8\ub9ac\uc5d0\uc11c\uc758 RSS\uc640 \ubd84\ub958\ud2b8\ub9ac\uc5d0\uc11c\uc758 Gini index\ub4f1\uc744 \ud1b5\ud574 \uc804\uccb4\uc801\uc73c\ub85c \uac1c\ub7b5\uc801\uc778 \uac01 \ubcc0\uc218\uc758 \uc911\uc694\ub3c4\ub294 \uac00\ub2a0\ud560 \uc218 \uc788\ub2e4. \uc6b0\ub9ac\ub294 bagging\uc5d0\uc11c \ubd80\ud2b8\uc2a4\ud2b8\ub7a9\uc744 \ud1b5\ud574\uc5bb\uc740 B\uac1c\uc758 \ud2b8\ub9ac\ub4e4\uc744 \ud3c9\uade0\ud568\uc73c\ub85c\uc368 RSS\ub098 gini index\uac00 \uc5bc\ub9c8\ub9cc\ud07c \uac10\uc18c\ud588\ub294\uc9c0\ub97c\uc5d0 \ub300\ud574 total amount\ub97c \uccb4\ud06c\ud560 \uc218 \uc788\ub294\ub370, \uc5ec\uae30\uc11c \uac00\uc7a5 \ud070 \uac12\ub4e4\uc744 \ubcf4\uc774\ub294 \ubcc0\uc218\ub4e4\uc774 \uc911\uc694\ud55c \ubcc0\uc218\ub4e4\uc774 \ub41c\ub2e4. \uc704\uc758 \uadf8\ub9bc\uc740 \ubd84\ub958\ud2b8\ub9ac\uc758 \uc608\uc2dc\uc778\ub370, value\uac12\uc774 \uc81c\uc77c \ud070 Thai, Ca, ChestPain\uc774 largest mean decrease in Gini index\ub97c \uac16\ub294 \ubcc0\uc218\uc774\uace0 \uac1c\uc911\uc5d0 \uc81c\uc77c \uc911\uc694\ud55c \ubcc0\uc218\ub77c\ub294 \ub73b\uc744 \uc758\ubbf8\ud55c\ub2e4. Random Forest \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8 \uae30\ubc95\uc740 \ud2b8\ub9ac\uc5d0 bagging\uc744 \uc801\uc6a9\ud55c \ubc29\ubc95\uc744 \ud2b8\ub9ac\uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uc81c\uac70\ud558\ub294 \ub9e4\ucee4\ub2c8\uc998\uc744 \uc774\uc6a9\ud558\uc5ec \uc608\uce21\uc815\ud655\ub3c4\ub97c \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uae30\ubc95\uc774\ub2e4. \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\ub97c \uc9e4 \ub54c, \ud2b8\ub9ac\uc758 \uac01 split\uc5d0\uc11c \uc54c\uace0\ub9ac\uc998\uc740 \uac00\ub2a5\ud55c \ub300\ubd80\ubd84\uc758 predictors\ub4e4\uc744 \uace0\ub824\ud558\ub294 \uac83\uc744 \ud5c8\uc6a9\ud558\uc9c0\ub294 \uc54a\ub294\ub2e4. p\uac1c\uc758 predictors\ub4e4\uc774 \uc788\ub2e4\uba74 \uc774\ub4e4\uc744 \uc804\ubd80\uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 \uc77c\ubd80\ub9cc \uc0ac\uc6a9\ud558\uc5ec \ud2b8\ub9ac\ub97c \uc9dc\ub294 \uac83\uc774\ub2e4.(\ubcf4\ud1b5 total number of predictors\uc758 \uac2f\uc218\uc5d0 \uc81c\uacf1\uadfc\uc744 \ucde8\ud55c \uac2f\uc218\ub9cc\ud07c\uc744 \uc0ac\uc6a9\ud55c\ub2e4.) \uc774\ub294 \uc815\ud655\ub3c4\ub97c \uc704\ud574 \ud68c\uadc0\uc640 \ubd84\ub958\ub97c \uc218\ud589\ud568\uc5d0 \uc788\uc5b4\uc11c \uc774\uc0c1\ud55c \uc18c\ub9ac\ucc98\ub7fc \ub4e4\ub9ac\uaca0\uc9c0\ub9cc, \uc0ac\uc2e4\uc740 \uc0c1\ub2f9\ud788 \ud569\ub2f9\ud558\ub2e4. \ub9cc\uc57d data set\uc5d0\uc11c \ub9e4\uc6b0 \uac15\ub825\ud55c \uc124\uba85\ubcc0\uc218\uac00 \uc788\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uc790. \uc774\ub7ec\ud55c data set\uc744 \ud1b5\ud574 bagging\uc744 \uc218\ud589\ud558\uba74 \ub300\ubd80\ubd84 \ub610\ub294 \ubaa8\ub4e0 \ud2b8\ub9ac\uac00 \uc774 \uac15\ud55c \uc124\uba85\ubcc0\uc218\ub97c top split\uc5d0 \ub193\uc544 \uc0ac\uc6a9\ud560 \uac83\uc774\uace0, \uc774\ub294 \uacb0\uad6d \ub300\ubd80\ubd84\uc758 bagged tree\uac00 \ube44\uc2b7\ud55c \ubaa8\uc591\uc744 \uac16\uac8c\ub418\ub294 \uacb0\uacfc\ub97c \ub0b3\ub294\ub2e4. \ub354\uad70\ub2e4\ub098 \uc774\ub7f4 \uacbd\uc6b0\uc5d0 bagged tree\uc5d0\uc11c \ub098\uc624\ub294 predictions\uc740 \ub9e4\uc6b0 highly correlated \ub418\uc5b4\uc788\ub2e4. \ubd88\ud589\ud558\uac8c\ub3c4 \ub9ce\uc740 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 quantities \ub4e4\uc744 averaging \ud558\ub294 \uac83\uc740 \uadf8\ub807\uc9c0 \uc54a\uc744 \ub54c\uc758 \uacb0\uacfc\ubcf4\ub2e4 \ud615\ud3b8\uc5c6\ub294 reduction in variance\ub97c \uac16\ub294\ub2e4. \uc774\ub294 \uc989 bagging \uae30\ubc95\uc740 \ub2e8\uc77c \ud2b8\ub9ac setting \uc5d0\uc11c \uc0c1\ub2f9\ud55c \ubd84\uc0b0\uac10\uc18c\ud6a8\uacfc\ub97c \uac00\uc838\uc624\uc9c0 \uc54a\ub294\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\ub294 \uac01 split\uc5d0 \uc0ac\uc6a9\ud558\ub294 \ubcc0\uc218\ub97c predictor space \uc804\uccb4\ub85c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc544\ub2cc, predictor\uc758 \ubd80\ubd84\uc9d1\ud569\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \uadf9\ubcf5\ud55c\ub2e4. \uc774\ub7ec\ud55c \uacfc\uc815\uc744 \uc6b0\ub9ac\ub294 tree\ub97c decorrelationg \ud55c\ub2e4\uace0 \uc0dd\uac01\ud560 \uc218 \uc788\ub294\ub370, \uba3c\uc800 \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\ub3c4 bagging\uacfc \uc720\uc0ac\ud558\uac8c bootstrap\uc744 \ud1b5\ud574 \uc0d8\ud50c\uc744 \ucd94\ucd9c\ud558\uace0, \uc0ac\uc804\uc5d0 \ubaa8\ud615\uc801\ud569\uc5d0 \uc774\uc6a9\ud560 \ubcc0\uc218\uc758 \uc218 m\ub9cc\ud07c\uc744 p\uac1c\uc758 \ubcc0\uc218\ub4e4 \uc911\uc5d0\uc11c random\ud558\uac8c \ubf51\uc544 \ud2b8\ub9ac\uc0dd\uc131\uc5d0 \uc0ac\uc6a9\ud55c\ub2e4. \ud558\ub098\uc758 \ud2b8\ub9ac\uac00 \ub9cc\ub4e4\uc5b4\uc9c0\uba74 \ub610 \ub2e4\ub978 \ud2b8\ub9ac\ub97c \ud615\uc131\ud560 \ub54c \ub9cc\uc57d \uc5b4\ub5a0\ud55c \uac15\ub825\ud55c \uc124\uba85\ubcc0\uc218\uac00 \uc788\ub2e4\uace0 \ud574\ub3c4 random\ud558\uac8c m\uac1c\uc758 \ud45c\ubcf8\ubcc0\uc218\ub97c \ubf51\uc544 \uc218\ud589\ud558\uae30 \ub54c\ubb38\uc5d0, \ub2e4\ub978 \ubcc0\uc218\ub4e4\uc774 split\ud615\uc131\uc5d0 \ub9ce\uc740 \uae30\ud68c\ub97c \uac16\uac8c \ub418\ub294 \uac83\uc774\ub2e4. bagging\uacfc \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\uc758 \uac00\uc7a5 \ud070 \ucc28\uc774\uc810\uc740 predictor \ubd80\ubd84\uc9d1\ud569\uc758 \uc0ac\uc774\uc988\uc5d0 \uad00\ud558\uc5ec \uac00\uc7a5 \ud070 \ucc28\uc774\uc810\uc744 \uac16\ub294\ub2e4. \ub9cc\uc57d \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\uc5d0\uc11c m=p \ub85c \ub193\ub294\ub2e4\uba74, \ud2b8\ub9ac \ubd84\ud560\uc5d0 \ubaa8\ub4e0 predictor\uc758 \uc804\uccb4\uc9d1\ud569\uc744 \uc0ac\uc6a9\ud55c\ub2e4\ub294 \uc758\ubbf8\uc774\ubbc0\ub85c \uc774\ub54c\ub294 bagging\uacfc \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\uac00 \uac19\uc740 \uacb0\uacfc\ub97c \ub0b3\ub294\ub2e4. \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \\(m=\\sqrt{p}\\) \uac1c\uc758 predictors\ub97c \uc0ac\uc6a9\ud558\uba70, \uc774\ub294 test error\uc640 OOB error\uc5d0 \ub300\ud574 bagging\ubcf4\ub2e4 \ub098\uc740 \uacb0\uacfc\ub97c \uac16\ub294\ub2e4. Boosting \uc55e\uc5d0\uc11c\ub294 bagging\uacfc random forest\uc5d0 \ub300\ud574\uc11c \uc0b4\ud3b4\ubcf4\uc558\ub294\ub370, \uc774\ubc88\uc5d0\ub294 prediction\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub610 \ub2e4\ub978 \ubc29\ubc95\uc778 boosting\uc5d0 \ub300\ud574\uc11c \ub17c\uc758\ud574\ubcf4\uc790. bagging\uc744 \uc0dd\uac01\ud574\ubcf4\uba74 \uc6b0\ub9ac\ub294 original training set\uc744 bootstrap sampling\uc744 \ud1b5\ud574 \uc5ec\ub7ec\uac1c\ub85c \uce74\ud53c\ud558\uace0, \uac01\uac01\uc758 \ubcf5\uc0ac\ubcf8\uc740 single predictive model\uc744 \ub9cc\ub4e4\uae30 \uc704\ud574 \uac01\uac01 \ud2b8\ub9ac\ub97c \ud615\uc131\ud55c \ud6c4, \uc774\uc5d0 \ub300\ud55c \uacb0\uacfc\uac12(MSE, Gini index\ub4f1..)\uc744 \uacb0\ud569\ud558\uc5ec \uc608\uce21\uc744 \uc218\ud589\ud558\uc600\ub2e4. \uc774\ub860\uc801\uc73c\ub85c bootstrap\uc744 \ud1b5\ud574 \uc0dd\uc131\ub41c \ud2b8\ub9ac\ub4e4\uc740 \ub2e4\ub978 \ud2b8\ub9ac\ub4e4\uacfc \ub3c5\ub9bd\uc77c \uac83\uc774\ub2e4. \ubd80\uc2a4\ud305\uc740 \uc774\uc640 \ube44\uc2b7\ud558\uc9c0\ub9cc, \ud2b8\ub9ac\ub4e4\uc774 \ub3c5\ub9bd\uc801\uc774\uc9c0 \uc54a\uace0 \uc11c\ub85c sequentially\ud558\uac8c \uc5f0\uacb0\ub418\uc5b4\uc788\ub294 \uac83\uc774 \uc55e\uc758 \ubc29\ubc95\uacfc \uac00\uc7a5 \ud070 \ucc28\uc774\uc810\uc774\ub2e4. \uc704\uc758 \uadf8\ub9bc\uc774 \ubd80\uc2a4\ud305\uc54c\uace0\ub9ac\uc998\uc744 \ub3c4\uc2dd\ud654 \ud55c \uac83\uc774\ub2e4. \ud480\uc5b4\uc11c \uc124\uba85\ud558\uba74 \uba3c\uc800 original data\ub97c \uc774\uc6a9\ud558\uc5ec d\uac1c\uc758 terminal node\ub97c \uac16\ub294 tree model\uc744 fitting\ud55c \ud6c4, \uadf8 \uc608\uce21 \uacb0\uacfc\uc640 \uc2e4\uc81c \uac12\uc758 \ucc28\uc774\ub97c \uc0b0\ucd9c\ud558\ub294\ub370 \uc774\ub54c learning rate \\(\\lambda\\) \ub97c \uc120\ud0dd\ud574 \uc608\uce21\uac12\uc5d0 learning rate\ub97c \uacf1\ud55c \uac12\ub9cc\ud07c\uc744 \uc81c\uc678\ud55c\ub2e4. \uadf8\ub9ac\uace0 \ub2e4\uc74c \ud2b8\ub9ac\ub97c \ub9cc\ub4dc\ub294 round\uc5d0\uc11c \uae30\uc874 outcome \\(Y\\) \uac00 \uc544\ub2cc \ubaa8\ud615\uc758 \uc794\ucc28\ub97c \uc774\uc6a9\ud558\uc5ec \uc0c8\ub85c\uc6b4 \ud2b8\ub9ac\ub97c \uc0dd\uc131\ud55c\ub2e4. \uadf8\ub9ac\uace0 \uc774\ub7ec\ud55c \uacfc\uc815\uc744 B\ubc88 \ubc18\ubcf5\ud558\ub294 \uac83\uc774\ub2e4. \uc704\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ubd80\uc2a4\ud305\uc5d0\ub294 \uc138\uac00\uc9c0 \ud30c\ub77c\ubbf8\ud130\uac00 \uc788\uace0 \uc774\ub294 \uc544\ub798\uc640 \uac19\ub2e4. The number of tree : \\(B\\) \ubc30\uae45\uacfc \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\uc640\ub294 \ub2e4\ub974\uac8c \ubd80\uc2a4\ud305\uc740 B\uac00 \ub9e4\uc6b0 \ucee4\uc9c8 \uc218\ub85d \uacfc\uc801\ud569\uc774 \ub420 \uac00\ub2a5\uc131\uc774 \uc874\uc7ac\ud558\ub294\ub370, \uace0\ub85c B\uac12\uc740 \uad50\ucc28\uac80\uc99d\uc744 \ud1b5\ud574 \uacb0\uc815\ud55c\ub2e4. learning rate(=shrinkage parameter) : \\(\\lambda\\) \uc774\ub294 \ubaa8\ud615\uc758 \ud559\uc2b5\uc18d\ub3c4\ub97c \uc870\uc808\ud558\ub294 \uac83\uc73c\ub85c \ubcf4\ud1b5 0.01, 0.001\uc77c \uc4f0\uc9c0\ub9cc \ub370\uc774\ud130\uc5d0 \ub530\ub77c \uadf8 \uac12\uc744 \ub2ec\ub9ac\ud55c\ub2e4. \ub9cc\uc57d \ub78c\ub2e4 \uac12\uc774 \ub9e4\uc6b0 \uc791\uc744 \uacbd\uc6b0\uc5d0\ub294 \ubaa8\ud615\uc774 \uc801\uc808\ud558\uac8c \ud559\uc2b5\ub418\uae30 \uc704\ud574 B\uac00 \ub9e4\uc6b0 \ucee4\uc57c\ud55c\ub2e4. tree\uc758 terminal node\uc218 : \\(d\\) \ubcf4\ud1b5 d=1\uc77c \ub54c \ud559\uc2b5\uc774 \uc88b\uc740 \uc608\uce21\uc744 \uc774\ub04c\uba70 \uc774\ub7f4 \uacbd\uc6b0 \ud2b8\ub9ac\ub97c stump model \uc774\ub77c \uce6d\ud55c\ub2e4. d\ub294 \uac00\ubc95\ubaa8\ud615\uc5d0\uc11cinteraction effect\ub97c \ubc18\uc601\ud558\ub294 \uac83\uacfc \uc720\uc0ac\ud558\uae30 \ub54c\ubb38\uc5d0 interaction depth\ub85c \ubd88\ub9ac\uae30\ub3c4 \ud55c\ub2e4. \ubd80\uc2a4\ud305\uc5d0\uc11c\ub294 \uae30\uc874\uc758 \ud2b8\ub9ac\ubaa8\ub378\uc774 \ub2e4\uc74c \ud2b8\ub9ac\uc5d0 sequential\ud558\uac8c \uc601\ud5a5\uc744 \uc8fc\uae30 \ub54c\ubb38\uc5d0 \uc77c\ubc18\uc801\uc73c\ub85c \uc801\uc740\uc218\uc758 terminal node\ub97c \uc0ac\uc6a9\ud558\ub354\ub77c\ub3c4 \uc88b\uc740 \uacb0\uacfc\ub97c \ub098\ud0c0\ub0bc \uc218 \uc788\ub2e4. \uace0\uc0dd\ud558\uc168\uc2b5\ub2c8\ub2f9","title":"99 Ybigta ML  \u1109\u1175\u11b7\u1112\u116a"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#ybigta-ml-1","text":"","title":"Ybigta \uad50\uc721\uc138\uc158 ML \uc2ec\ud654(1)"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#regression","text":"","title":"Regression"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#simple-linear-regression","text":"\ub2e8\uc21c\uc120\ud615\ud68c\uadc0\ub294 singele predictor variable\uc778 X\uc640 quantitative response Y\uc5d0 \ub300\ud55c \uad00\uacc4\ub97c \ub9e4\uc6b0 \uc9c1\uad00\uc801\uc73c\ub85c \uc124\uba85\ud558\ub294 \ubc29\ubc95\uc774\ub2e4. \uc774\ub294 \uc989 X\uc640 Y\uac04\uc5d0 \uc120\ud615\uc801\uc778 \uad00\uacc4\uac00 \uc788\ub2e4\ub294 \uac83\uc744 \uac00\uc815\ud558\uba70 \uc2dc\uc791\ud55c\ub2e4. \uc218\ud559\uc801\uc73c\ub85c \uc6b0\ub9ac\ub294 \uc774\ub97c \uc544\ub798\uc640 \uac19\uc774 \ud45c\uae30\ud55c\ub2e4. $ \\(Y\\approx {\\beta}_0 + {\\beta}_1X\\) $ \uc5ec\uae30\uc758 \\({\\beta}_0\\) \uc640 \\({\\beta}_1\\) \uc740 \ub450\uac1c\uc758 unknown constants\uc774\uba70 \uac01\uac01 \uc21c\uc11c\ub300\ub85c intercept \uc640 slope terms\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc774 \ub458\uc740 parameters \ud639\uc740 coefficients\ub85c\ub3c4 \ubd88\ub9b0\ub2e4. \uc6b0\ub9ac\ub294 \uc774\ub7ec\ud55c \ud568\uc218\uc758 \uad00\uacc4\ub97c \ucd94\uc815\ud558\uae30 \uc704\ud574\uc11c \ucd94\uc815\uce58\uc778 \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1\\) \ub97c \uc6b0\ub9ac\uc758 training data\ub97c \ud1b5\ud574 \uc5bb\uc744 \uc218 \uc788\uace0, \uc774\ub97c \ud1b5\ud574 \ubbf8\ub798\uc758 \uc5b4\ub5a0\ud55c \uc815\ub7c9\uc801 \uac12\ub4e4\uc744 \uc608\uce21\ud558\ub294 \uac83\uc774\ub2e4. \uc774\ub97c \ub3c4\uc2dd\ud654 \ud558\uba74 \uc544\ub798\uc640 \uac19\ub2e4. \\[\\hat{y}=\\hat{\\beta}_0 + \\hat{\\beta}_1x\\] \uc6b0\ub9ac\uc758 \ubaa9\ud45c\ub294 \uc2e4\uc81c X\uc640 Y\uc758 \uad00\uacc4\ub97c \ucd94\uc815\ud558\uc5ec \ucd94\ud6c4\uc5d0 \ub4e4\uc5b4\uc624\ub294 \ubbf8\ub798 \ub370\uc774\ud130\ub97c \ubbf8\ub9ac \uc608\uce21\ud558\ub294 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc \ubaa8\ub4e0 \uc77c\uc5d0\ub294 \uc608\uc0c1\uce58 \ubabb\ud558\ub294 \ubcc0\uc218\uac00 \uc788\uace0, \uc544\ubb34\ub9ac \ud6c8\ub828 \ub370\uc774\ud130\uac04\uc758 \uad00\uacc4\ub97c \uc798 \ucd94\uc815\ud558\uc600\ub2e4\ud558\ub354\ub77c\ub3c4 \uc774 \uac83\uc774 \ubbf8\ub798\uc5d0\ub3c4 \uc9c0\uae08 \uac16\uace0\uc788\ub294 \ub370\uc774\ud130\ucc98\ub7fc \uc77c\uc774 \ubc1c\uc0dd\ud560 \uac83\uc774\ub77c\ub294 \uac83\uc73c\ub85c \uc774\uc5b4\uc9c0\uc9c0 \uc54a\ub294\ub2e4. \uc55e\uc11c \uc704\uc5d0\uc11c \uc6b0\ub9ac\ub294 Y\uc640 X\uc758 \uad00\uacc4\ub97c \uadfc\uc0ac\uc801\uc73c\ub85c \ud45c\ud604\ud574\ubcf4\uc558\uc9c0\ub9cc, \uc774 \ub458\uac04\uc758 true relationship\uc740 \uc544\ub9c8\ub3c4 \uc774\ub7f0 \ubaa8\uc591\uc77c \uac83\uc774\ub2e4. \\[Y={\\beta}_0+{\\beta}_1X+{\\epsilon}\\] \uc704\uc5d0 \ub9e8\ucc98\uc74c\uc5d0 \ubcf4\uc5ec\uc900 \uc2dd\uacfc\ub294 \ub2e4\ub974\uac8c \ub4a4\uc5d0 \uc694\uc0c1\ud55c \uc5e1\uc2e4\ub860\uc774 \ud3ec\ud568\ub418\uc5c8\ub2e4. \uc9c0\ub3c4\ud559\uc2b5\uc758 \uad00\uc811\ud574\uc11c \uc6b0\ub9ac\ub294 Y\ub97c \ucd94\uc815\ud560\ub54c \\(Y=f(X)+{\\epsilon}\\) \uc758 \uc2dd\uc744 \ub450\uc5b4 \ud45c\uae30\ud558\ub294\ub370, \ud68c\uadc0\uc5d0\uc11c unknown function\uc778 \\(f(X)\\) \uac00 \\({\\beta}_0+{\\beta}_1X\\) \uc5d0 \ub300\uc751\ub41c\ub2e4. \uc5ec\uae30\uc11c error \\({\\epsilon}\\) \uc740 \uc2e4\uc81c Y\uc640 X\uc758 \uad00\uacc4\uac00 \uc120\ud615\uc801 \uad00\uacc4\uac00 \uc544\ub2d0\uc218\ub3c4 \uc788\ub2e4\ub294 \uac83\uc744 \ub098\ud0c0\ub0b4\uba70, \ud639\uc740 Y\uc758 \ubcc0\ub3d9\uc744 \uc57c\uae30\ud558\ub294 \ub2e4\ub978 \ubcc0\uc218\uc77c\uc218\ub3c4, \uc544\ub2c8\uba74 \uce21\uc815\uc5d0\uc11c\uc758 \uc624\ub958\uc77c\uc218\ub3c4 \uc788\uc74c\uc744 \ubcf4\uc5ec\uc900\ub2e4. \uc6b0\ub9ac\ub294 \uadf8\ub798\uc11c \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\uc5d0\uc11c \uc774\ub7ec\ud55c error term\uc774 X\uc5d0 \ub300\ud574\uc11c \ub3c5\ub9bd\uc774\uba70 \\(N(0,{\\sigma}^2)\\) \uc5d0\uc11c \ub098\uc628 iid\ud55c \ub188\uc774\ub77c\uace0 \uac00\uc815\uc744 \ud55c\ub2e4. \uc774 \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\uc5d0\uc11c \\(X_{i}\\) \ub294 \ud655\ub960\ubcc0\uc218\uac00 \uc544\ub2cc \uc0c1\uc218\uc774\ub2e4. \uc774\ub294 \uc2e4\ud5d8\uc790\uc5d0 \uc758\ud574 \ud1b5\uc81c\ub41c \uac12\uc774\uba70 \ud1b5\uc81c\ub41c \ud658\uacbd\uc5d0\uc11c \uc124\uc815\ub418\uc5c8\uc74c\uc744 \ub098\ud0c0\ub0b8\ub2e4. \ubc18\ub300\ub85c \\(Y_{i}\\) \ub294 \uc774\ub7ec\ud55c \\(X_{i}\\) \ub4e4\uc5d0 \ub300\uc751\ud558\ub294 \ud655\ub960\ubcc0\uc218\uc774\ub2e4. \uc5ec\uae30\uc11c\ubd80\ud130 \uc0ac\uc2e4 X\ub97c \ud1b5\uc81c\ud55c\ub2e4\ub294 \uac83\uc774 \ube44\ud604\uc2e4\uc801\uc778 \uac00\uc815\uc774\uba70, prediction\uc758 \uad00\uc810\uc5d0\uc11c \uadf8 \uac00\uce58\uac00 \ub5a8\uc5b4\uc9c0\ub294 \uac00\uc815\uc774\ub77c\uace0 \ud560 \uc218\uc788\ub2e4. \uc6b0\ub9ac\uac00 \uc120\ud615\ud68c\uadc0\ub97c \ud558\uba74\uc11c \uc911\uc694\ud55c \uac83\uc740 X\uc640 Y\uc758 \uad00\uacc4\ub97c \uc798 \uc124\uba85\ud558\uba74\uc11c \uc608\uce21\ud55c \uac12\uacfc \uc2e4\uc81c\uac12\uac04\uc758 \ucc28\uc774\ub97c \ucd5c\uc18c\ub85c\ud558\ub294 \ubaa8\ub378\uc744 \ucc3e\ub294 \uac83\uc774 \ubaa9\ud45c\uc774\ub2e4. \uc774\ub97c \uac00\ub2a5\ud558\uac8c\ud558\ub294 \ubc29\ubc95\uc73c\ub85c\ub294 \uac00\uc7a5 \uc77c\ubc18\uc801\uc778 approach\uc778 RSS\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \uac83\uc774 \uc788\ub2e4. \uc704\uc758 \uadf8\ub9bc\uc744 \ubcf4\uc790. \ube68\uac04\uc0c9 \uc810\ub4e4\uc740 \uc6b0\ub9ac\uac00 \uac16\uace0\uc788\ub294 \\(y_{i}\\) \uac12\uc774\uba70 \uc9c1\uc120\uc774 \ud45c\ud604\ud558\uace0 \uc788\ub294 \uac83\uc740 \ucd94\uc815\ud55c \\(\\hat{y}_{i}\\) \ub4e4\uc758 \ub098\uc5f4\uc774\ub2e4. \uadf8\ub807\ub2e4\uba74 \uc794\ucc28 \\(e_{i}\\) \ub294 \\(y_{i}-\\hat{y}_{i}\\) \ub97c \uc758\ubbf8\ud558\uace0 \uc774\ub294 i \ubc88\uc9f8 \uad00\ucc30\ub41c response value\uc640 i \ubc88\uc9f8 \uc6b0\ub9ac\uac00 \ucd94\uc815\ud55c \uc120\ud615\ubaa8\ub378\uc758 response value\uac04\uc758 \ucc28\uc774\ub97c \ub098\ud0c0\ub0b8\ub2e4. \uc6b0\ub9ac\ub294 \uc774\ub97c RSS\ub77c\uace0 \uce6d\ud558\uba70 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\uac00\ub2a5\ud558\ub2e4. $ \\(RSS=e_1^2+e_2^2+...+e_{n}^2\\) $ \ub610\ub294 $ \\(RSS=\\sum\\limits_{k=1}^n(y_{k}-\\hat{\\beta}_0-\\hat{\\beta}_1x_{k})^2\\) $ \ub85c \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \ucd5c\uc18c\uc81c\uacf1\ubc95\uc740 \uc774\ub7ec\ud55c \uc794\ucc28\uc81c\uacf1\ud569 RSS\ub97c \ucd5c\uc18c\ud654 \ud558\ub294 \\(\\hat{\\beta}_0\\) \uacfc \\(\\hat{\\beta}_1\\) \ub97c \uad6c\ud558\ub294 \uac83\uc778\ub370, \uc774\ub7ec\ud55c \ud574\ub97c \uad6c\ud558\uba74 \ubca0\ud0c0\uc758 \ucd94\uc815\uce58\ub4e4\uc740 \uc544\ub798\uc640 \uac19\ub2e4.","title":"Simple Linear Regression"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#interpretation-of-the-coefficient-estimates","text":"\ucd94\uc815\ud55c \ud68c\uadc0\uacc4\uc218\ub4e4\uc758 \uc758\ubbf8\ub97c \uc0b4\ud3b4\ubcf4\uba74 \uc544\ub798\uc640 \uac19\uc774 \uc694\uc57d\ud560 \uc218 \uc788\ub2e4. slope \\({\\beta}_1\\) \uc740 \ub3c5\ub9bd\ubcc0\uc218 X\uac00 \ud55c\ub2e8\uc704 \uc99d\uac00\ud560 \ub54c\uc758 \uc885\uc18d\ubcc0\uc218 Y\uc758 \ud3c9\uade0\uc801 \ubcc0\ud654\ub7c9\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc808\ud3b8\uacc4\uc218 \\({\\beta}_0\\) \uc740 \uc758\ubbf8\ub97c \uac16\uc9c0\uc54a\uace0, \uc120\ud615\ud68c\uadc0\uc2dd\uc744 \ubcf4\uc815\ud558\ub294 \uc5ed\ud560\uc774\ub2e4. \ud68c\uadc0\uacc4\uc218\uc758 \ucd94\uc815\uacfc\uc815\uc744 \uc0b4\ud3b4\ubcf4\uba74 \uc774\ub4e4\uc740 \uc8fc\uc5b4\uc9c4 X\uc640 Y\uc5d0 \ub300\ud574 \uc644\uc804\ud788 \uc758\uc874\ud558\uba70, \uc774\ub294 \uc989 \ub2e4\ub978 X\uac12\ub4e4\uc774 \ubc18\uc601\ub418\uba74 \ub2e4\ub978 \uacb0\uacfc\uac12\uc744 \ub3c4\ucd9c\ud568\uc744 \uc758\ubbf8\ud55c\ub2e4.","title":"Interpretation of the Coefficient Estimates"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#assessing-the-accuracy-of-the-coefficient-estimates","text":"\uc6b0\ub9ac\ub294 \ucd94\uc815\ud55c \ud68c\uadc0\uacc4\uc218\ub4e4\uc774 \uc2e4\uc81c\uc758 \ud68c\uadc0\uacc4\uc218\uac12\uacfc \uc5bc\ub9c8\ub098 \ube44\uc2b7\ud55c\uc9c0, \uc5bc\ub9c8\ub098 \uba40\ub9ac\ub5a8\uc5b4\uc838\uc788\uc744\uc9c0\uc5d0 \ub300\ud574\uc11c \uad81\uae08\uc99d\uc744 \uac00\uc9c8 \uac83\uc774\ub2e4. \uc774\ub54c\ub294 \ud68c\uadc0\uacc4\uc218 \\({\\beta}_0\\) , \\({\\beta}_1\\) \uc758 \ud45c\uc900\uc624\ucc28(Standard error)\ub97c \ud1b5\ud55c \uc2e0\ub8b0\uad6c\uac04 \ucd94\uc815\uc744 \ud1b5\ud574 \uc811\uadfc\ud574\ubcfc \uc218 \uc788\ub2e4. \\(\\hat{\\beta}_0\\) , \\(\\hat{\\beta}_1\\) \uc758 \ud45c\uc900\uc624\ucc28\ub97c \uad6c\ud558\uae30\uc804\uc5d0 \uc774\ub4e4\uc758 \ubd84\ud3ec\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. $ \\(\\hat{\\beta}_0 \\sim N({\\beta}_0,Var(\\hat{\\beta}_0)), \\hat{\\beta}_0 \\sim N({\\beta}_1,Var(\\hat{\\beta}_1))\\) $ \uc774\uc911 \\(\\hat{\\beta}_1\\) \uc5d0 \ub300\ud574\uc11c\ub9cc \\(Var(\\beta_1)\\) \uc744 \uad6c\ud574\ubcf4\uc790. $ \\({\\epsilon} \\sim N(0,{\\sigma}^2)\\) $ $ \\(\\sum\\limits_{i=1}^n(x_{i}-\\bar{x})(y_{i}-\\bar{y}) = \\sum\\limits_{i=1}^n(x_{i}-\\bar{x})y_{i}\\) $ \ub97c \uc774\uc6a9\ud558\uc790. \\(\\sum\\limits_{i=1}^n(x_{i}-\\bar{x})(y_{i}-\\bar{y})\\) \ub294 \\(S_{xy}\\) \ub85c \ud45c\ud604\uac00\ub2a5. \uba3c\uc800 \\(k_{i}= \\frac{(x_{i}-\\bar{x})}{S_{xx}}\\) \ub85c \ub450\uc790. \uadf8\ub807\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc740 \uc2dd\uc804\uac1c\ub97c \ud1b5\ud574 \\(Var({\\beta}_1)\\) \ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4. $ \\(V(\\hat{\\beta}_1) = V(\\sum{k_{i}}y_{i})=\\sum{k_{i}}V(y_{i})=\\sum{k^2}{\\sigma}^2=\\frac{\\sum(x_{i}-\\bar{x})^2{\\sigma}^2}{S_{xx}}=\\frac{\\sigma^2}{S_{xx}}\\) $ \\(Var(y_{i})={\\sigma}^2\\) \uc778 \uc774\uc720\ub294, \\({\\epsilon} \\sim N(0,{\\sigma}^2)\\) \uc73c\ub85c \uc815\uc758\ud588\uace0, \\({\\epsilon}\\) \ub07c\ub9ac \uc11c\ub85c \ub3c5\ub9bd\uc774\ubbc0\ub85c \\(y_{i}\\) \ub3c4 \uc11c\ub85c \ub3c5\ub9bd\uc774\uace0 \ubd84\uc0b0\uc774 \\({\\sigma}^2\\) \uc774 \ub41c\ub2e4. \uc774\uc640 \uac19\uc774 \ube44\uc2b7\ud55c \ubc29\ubc95\uc744 \ud1b5\ud574 \\({\\beta}_0\\) \uc5d0 \ub300\ud574\uc11c\ub3c4 \ubd84\uc0b0\uc758 \ucd94\uc815\uce58\ub97c \uad6c\ud558\uc5ec \ud45c\uc900\uc624\ucc28\ub97c \uad6c\ud558\uba74 \ub450 \ud68c\uadc0\uacc4\uc218\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \\(SE(\\hat{\\beta}_0)= \\sqrt{{\\sigma}^2[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum\\limits_{i=1}^n(x_{i}-\\bar{x})^2}]}\\) , \\(SE(\\hat{\\beta}_1)=\\sqrt{\\frac{\\sigma^2}{\\sum\\limits_{i=1}^n(x_{i}-\\bar{x})^2}}\\) \ud45c\uc900\uc624\ucc28 SE\ub97c \\({\\beta}_0\\) \uacfc \\({\\beta}_1\\) \uc5d0 \ub300\ud574 \uc2e0\ub8b0\uad6c\uac04\uc744 \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \uc4f8 \uc218 \uc788\ub2e4. $ \\([\\hat{\\beta}\\pm{t}_{n-2,{\\alpha}/2} SE(\\hat{\\beta})]\\) $ \uc2e0\ub8b0\uad6c\uac04\uc758 \uc758\ubbf8\ub294 \ud2b9\uc815\ud55c \uac12\uc744 \uac00\uc9c0\ub294 \uc54c \uc218 \uc5c6\ub294 parameter\uc758 \uac12\uc744 \ud2b9\uc815 %\uc758 \ud655\ub960\ub85c \ud3ec\ud568\ud558\ub294 \uad6c\uac04\uc744 \uc758\ubbf8\ud558\ub294\ub370, \uc774\ub294 \uc870\uae08 \ub354 \uc5c4\ubc00\ud788 \ub9d0\ud558\uba74 \uac19\uc740 \ubc29\ubc95\uc73c\ub85c \uc77c\uc815\ud55c \uac1c\uc218\ub9cc\ud07c \ubaa8\uc9d1\ub2e8\uc5d0\uc11c 100\uac1c\uc758 \ud45c\ubcf8 \ub370\uc774\ud130\uc14b\uc744 \ucd94\ucd9c\ud588\uc744 \ub54c \ud568\uaed8 \uacc4\uc0b0\ub418\ub294 100\uac1c\uc758 \uc2e0\ub8b0\uad6c\uac04\ub4e4 \uc911 parameter\ub97c \ud3ec\ud568\ud55c \uc2e0\ub8b0\uad6c\uac04\ub4e4\uc758 \uac1c\uc218\uac00 nn% x 100\uac1c \uc815\ub3c4\ub294 \ub41c\ub2e4 \uc758 \uc758\ubbf8\ub97c \uac16\ub294\ub2e4. \ud45c\uc900\uc624\ucc28\ub294 \ub610\ud55c \ud68c\uadc0\uacc4\uc218\uc5d0 \ub300\ud55c \uac00\uc124\uac80\uc815\uc744 \uc704\ud574 \uc0ac\uc6a9\ub418\uae30\ub3c4 \ud55c\ub2e4. \\(H_0\\) : There is no relationship between X and Y \\(H_{a}\\) : There is some relationship between X and Y \uc774 \uac00\uc7a5 \ud754\ud558\uac8c \uc0ac\uc6a9\ub418\ub294 \uac00\uc124\uac80\uc815\uc758 form\uc774\ub2e4. \uc218\ud558\uc801\uc73c\ub85c \ud45c\ud604\ud558\uba74 \uc774\ub294 \\[H_0 : {\\beta}_1=0\\] $ \\(H_{a} : {\\beta}_1 \\neq 0\\) $ \uc774\ub2e4. \ub9cc\uc57d \uc5ec\uae30\uc11c \\({\\beta}_1\\) =0\uc774\uba74, \uc774 \ubaa8\ub378\uc740 \\(Y={\\beta}_0+{\\epsilon}\\) \uc73c\ub85c reduced form\uc758 \ud615\ud0dc\ub97c \ub744\uba70, X\ub294 Y\uc640 \uad00\ub828\ub418\uc9c0 \uc54a\uc740 predictor\uac00 \ub41c\ub2e4. \uac00\uc124\uac80\uc815\uc744 \uc704\ud574\uc11c\ub294 t-\ud1b5\uacc4\ub7c9\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc774\ub294 \uc544\ub798\uc640 \uac19\uc740 \uc2dd\uc744 \uac16\uace0, $ \\(t=\\frac{{\\hat\\beta}_1-0}{SE(\\hat{\\beta}_1)} \\sim t(n-2)\\) $ \\(H_0\\) \ud558\uc5d0\uc11c \\(\\hat{\\beta}_1\\) \uc774 \ub530\ub974\ub294 \ubd84\ud3ec\ub294 \uc704\uc640 \uac19\uace0 \uc774\ub97c \ud1b5\ud574 \uc5bb\uc740 t \ud1b5\uacc4\ub7c9 \uac12\uc774 critical value\uc758 \ubc14\uae65, \uc989 \uae30\uac01\uc5ed\uc5d0 \uc18d\ud558\uba74 \uc6b0\ub9ac\ub294 \uadc0\ubb34\uac00\uc124 \\(H_0\\) \ub97c \uae30\uac01\ud55c\ub2e4. \uc774\ub294 \uc6b0\ub9ac\uac00 \\(\\hat{\\beta}_1\\) \uc774 0\uc774\ub77c\uace0 \uac00\uc815\ud558\uc5ec \ubb38\uc81c\ub97c \uc811\uadfc\ud55c \ud6c4, \uadf8 \uc911\uc2ec\uac12\ubcf4\ub2e4 \uc0c1\ub2f9\ud788 \uba3c \uacf3\uc5d0(\uae30\uac01\uc5ed\uc5d0 \uc18d\ud558\ub294 \uc601\uc5ed) \\(\\hat{\\beta}_1\\) \uc758 \ud1b5\uacc4\ub7c9\uc774 \uc704\uce58\ud558\ub294 \uac83\uc740 \uae30\uc874\uc5d0 \uc0dd\uac01\ud55c \\(H_0\\) \ub85c \uc5ec\uae34 \ubd84\ud3ec\ub97c \uac16\ub294\uac8c \uc544\ub2cc \uc544\uc608 \ub2e4\ub978 \ubd84\ud3ec\ub97c \uac16\ub294\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \uace7 \uc774\ub294 \\(\\hat{\\beta}_1\\) \uc774 0\uc774 \uc544\ub2cc \uc720\uc758\ud55c \uc5b4\ub5a0\ud55c \uac12\uc744 \uac00\uc9d0\uc744 \ubcf4\uc5ec\uc900\ub2e4.","title":"Assessing the accuracy of the Coefficient Estimates"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#assessing-the-accuracy-of-the-model","text":"\uc55e\uc5d0\uc11c \uc6b0\ub9ac\uac00 \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\ubaa8\ud615\uc758 \uacc4\uc218\ub97c \ucd94\uc815\ud558\uace0 \uae30\uc6b8\uae30\uc758 \uacc4\uc218\uac00 \uc720\uc758\ud55c\uc9c0 test\ub97c \ud588\uc73c\ub2c8, \uc774\ubc88\uc5d0\ub294 \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \uc5bc\ub9c8\ub098 \uc815\ud655\ud55c\uc9c0\uc5d0 \ub300\ud574\uc11c \ud3c9\uac00\ub97c \ud574\ubcf4\ub294 \uc2dc\uac04\uc744 \uac00\uc838\ubcf4\uc790! \uc774 \ucc45\uc5d0\uc11c\ub294 \ub450\uac00\uc9c0\ub97c \ub300\ud45c\uc801\uc73c\ub85c \uc18c\uac1c\ud558\ub294\ub370, \uba4b\uc7c1\uc77412\uae30\ucd5c\uc5f0\uc218 \ub2d8\uc758 \uc790\ub8cc\ub97c \ucc38\uace0\ud558\uc5ec \ud55c\uac00\uc9c0\ub97c \ub354 \ub123\uc5b4\ubcf4\uc558\ub2e4. Residual Standard Error(RSE) RMSE(\ud3c9\uade0\uc81c\uacf1\uadfc\uc624\ucc28) \uba4b\uc7c1\uc774\ucd5c\uc5f0\uc218\ub2d8\uc758\uc544\uc774\ub514\uc5b4 R-squared \uba3c\uc800 RSE\ub97c \uc0b4\ud3b4\ubcf4\uc790. \uc6b0\ub9ac\uac00 true regression line\uc744 \ucd94\uc815\ud560 \uc218 \uc788\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uc790. \uadf8\ub807\ub2e4\uba74 \uc6b0\ub9ac\ub294 Y\ub97c \uc644\ubcbd\ud558\uac8c \uc608\uce21\ud560 \uc218 \uc788\uc744\uae4c? \uc544\uc27d\uac8c\ub3c4 \uc6b0\ub9ac\ub294 Y\uc758 \uad00\uacc4\uc2dd\uc5d0 random\ud55c error term\uc774 \uc788\ub2e4\ub294 \uac83\uc744 \uc78a\uc73c\uba74 \uc548\ub41c\ub2e4. \uc774 \\({\\epsilon} \\sim N(0,{\\sigma}^2)\\) \ub294 irreducible\ud558\uace0 \uc5b4\uca54 \uc218 \uc5c6\ub294 \uc624\ucc28\ub97c \uc218\ubc18\ud55c\ub2e4. RSE\ub294 \uacb0\uad6d \uc774\ub7ec\ud55c \\({\\epsilon}\\) \uc758 \ud45c\uc900\ud3b8\ucc28\uc5d0 \ub300\ud55c \ucd94\uc815\uce58\ub97c \uc758\ubbf8\ud558\uba70 \ud3c9\uade0\uc801\uc73c\ub85c \uc885\uc18d\ubcc0\uc218 Y\uac00 true regression line\uc5d0\uc11c \uc5bc\ub9c8\ub098 \ub5a8\uc5b4\uc838\uc788\ub294\uac00\ub97c \ubcf4\uc5ec\uc8fc\ub294 \ucd94\uc815\uce58\uc774\ub2e4. \\[RSE=\\sqrt{\\frac{1}{n-2}RSS}=\\sqrt{\\frac{1}{n-2}\\sum\\limits_{i=1}^n(y_{i}-\\hat{y}_{i})^2}\\] \ud3c9\uade0\uc81c\uacf1\uadfc\uc624\ucc28\ub294 RSS\ub97c \uc790\uc720\ub3c4(n-2)\ub300\uc2e0 n\uc73c\ub85c \ub098\ub208 \uac83\uc774\ub2e4. n\uc774 \ub9e4\uc6b0\ud06c\ub2e4\uba74 RSE\uc640 RMSE\uc758 \ucc28\uc774\uac00 \ubbf8\ubbf8\ud558\uaca0\uc9c0\ub9cc, RMSE\ub294 prediction\uad00\uc810\uc5d0\uc11c \ub9ce\uc774 \uc0ac\uc6a9\ub41c\ub2e4. \\[RMSE=\\sqrt{\\frac{1}{n}RSS}\\] \uc774 \ub450 RSE\uc640 RMSE\ub294 \ubaa8\ub378 \uc798 fitting\uc774 \ub418\uc5c8\ub294\uc9c0\ub97c \uce21\uc815\ud558\ub294 \uac83\uc73c\ub85c \uc5ec\uaca8\uc9c4\ub2e4. R-squared\ub294 \ucd1d \ubcc0\ub3d9 \uc911\uc5d0 \uc124\uba85\ub41c \ubcc0\ub3d9\uc758 \ube44\uc728\uc744 \ubcf4\uc5ec\uc8fc\ub294 \uc9c0\ud45c\ub2e4. RSE \uc640 RMSE \uac00 \uc808\ub300\uc801\uc778 \uac12\uc73c\ub85c \ubaa8\ub378\uc758 \uc624\ucc28\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud55c\ub2e4\uba74, \\(R^2\\) \ub294\uc0c1\ub300\uc801\uc778 \uac12\uc73c\ub85c\uc368 \ud835\udc4c\uc758 \ub2e8\uc704\uc5d0 \uc0c1\uad00\uc5c6\uc774 \ubcc0\ub3d9\uc5d0 \ub300\ud55c \uc124\uba85 \ube44\uc728\ub85c \ubaa8\ub378\uc758 \uc124\uba85\ub825\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uc81c\uacf5\ud558\ub294 \uac83\uc5d0 \uc704\uc758 \ub450\uac1c\uc640 \ucc28\uc774\uc810\uc744 \uac16\ub294\ub2e4. $ \\(R^2= \\frac{SST-SSE}{SST}=1-\\frac{SSE}{SST}\\) $ \uc704\uc758 \uc2dd\ucc98\ub7fc \ud45c\ud604\ub418\uace0 $ \\(SST = \\sum(y_{i}-\\bar{y}^2)\\) $ \uc740 Total sum of squares(\ucd1d \ubcc0\ub3d9), $ \\(SSE=\\sum(y_{i}-\\hat{y}^2)\\) $\uc740 Residual sum of squares(\uc124\uba85\ub418\uc9c0\uc54a\uc740 \ubcc0\ub3d9), $ \\(SSR=\\sum(\\hat{y}_{i}-\\bar{y}^2)\\) $\uc740 Explained sum of squares(\uc124\uba85\ub41c \ubcc0\ub3d9) \uc744 \uc758\ubbf8\ud55c\ub2e4. \uc0ac\uc9c4\uc73c\ub85c \uc27d\uac8c \ud45c\ud604\ud558\uba74 \uc704\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4.( \uc5f0\uc218\uc57c \uace0\ub9c8\uc6cc! )","title":"Assessing the Accuracy of the Model"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#multiple-linear-regression","text":"\ub2e4\uc911\uc120\ud615\ud68c\uadc0\ub294 \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\uc640 \uc720\uc0ac\ud558\uc9c0\ub9cc \ub3c5\ub9bd\ubcc0\uc218 X\uc758 \uac1c\uc218 p\uac1c\ub77c\ub294 \uc810\ub9cc \ub2e4\ub974\ub2e4. \uc774 \ub610\ud55c \uc5ed\uc2dc \ub3c5\ub9bd\ubcc0\uc218 X\uc640 Y\uac04\uc5d0 \uadfc\uc0ac\uc801\uc778 \uc120\ud615\uad00\uacc4\uac00 \uc788\ub2e4\uace0 \uac00\uc815\ud55c \ubaa8\ub378\uc774\uba70, \uc120\ud615 \uc9c1\uc120 \ud639\uc740 \uc120\ud615 \ud3c9\uba74 \ub610\ub294 \uadf8 \uc774\uc0c1\uc758 \uc120\ud615\uad00\uacc4\ub97c \ub098\ud0c0\ub0b4\ub294 \ubaa8\ub378\uc774\ub2e4. $ \\(Y= {\\beta}_0+ {\\beta}_1X_1+...+{\\beta}_{p}X_{p}+{\\epsilon}\\) $ \uc704\uc758 \uc2dd\uc774 \ub2e4\uc911\uc120\ud615\ud68c\uadc0\uc758 \uae30\ubcf8 \ud615\ud0dc\uc774\uba70 \uc774\ub97c \ucd94\uc815\ud558\ub294 \ubaa8\ub378(\ud68c\uadc0\uc120)\uc740 \uc544\ub798\uc640 \uac19\ub2e4. \\[\\hat{Y}={\\hat\\beta}_0+{\\hat\\beta}_1X_1+...+{\\hat\\beta}_{p}X_{p}\\] RSS\ub294 \ub2e8\uc21c\ud68c\uadc0\uc640 \uac19\uc740 \uc2dd\uc774\uc9c0\ub9cc \\(\\hat{y}_{i}\\) \ub9cc \ub2e4\ub974\ub2e4\ub294 \ucc28\uc774\uac00 \uc788\ub2e4. \uc704 \uadf8\ub9bc\uc740 3\ucc28\uc6d0\uc77c \ub54c\uc758 \ub450\uac1c\uc758 predictor\uc640 \ud558\ub098\uc758 response Y\ub97c \ubcf4\uc5ec\uc8fc\uba70 \uc5ec\uae30\uc11c \ucd5c\uc18c\uc81c\uacf1\ud68c\uadc0\uc120\uc740 \uc120\ud615\ud3c9\uba74\uc758 \ud615\ud0dc\ub97c \ub748\ub2e4. \ub2e4\uc911\uc120\ud615\ud68c\uadc0\uc5d0\uc11c \\({\\beta}\\) \ub97c \ucd94\uc815\ud558\ub294 \uac83\uc744 \uc120\ud615\ub300\uc218\ud559\uc801\uc73c\ub85c \uacc4\uc0b0\ud558\uba74 \uc544\ub798\uc640 \uac19\uc740 \uacfc\uc815\uc744 \uac70\uce5c\ub2e4. \ucd94\uc815\ud55c \ud68c\uadc0\uc120\uc5d0\uc11c \uae30\uc6b8\uae30 \uacc4\uc218 \\(\\hat{\\beta}_{i}\\) \uc758 \uc758\ubbf8\ub294 \ub2e4\ub978 \ub3c5\ub9bd\ubcc0\uc218\ub4e4\uc758 \uc120\ud615\uc131\uc5d0 \ub300\ud574 \uc870\uc808\ud588\uc744 \ub54c( Y\uc640 \uacf5\ubcc0\ud558\uc9c0 \uc54a\uc744 \ub54c ) \ub3c5\ub9bd\ubcc0\uc218 \\(X_{i}\\) \uac00 \ud55c \ub2e8\uc704 \uc99d\uac00\ud560 \ub54c \uc885\uc18d\ubcc0\uc218 Y\uc758 \ud3c9\uade0\uc801\uc778 \uc21c \ubcc0\ud654\ub7c9\uc744 \uc758\ubbf8\ud55c\ub2e4. \uc774\ub294 \ud55c\uac1c\uc758 \ub3c5\ub9bd\ubcc0\uc218 X\uac00 \uc6c0\uc9c1\uc77c \ub54c \ub098\uba38\uc9c0 \ub3c5\ub9bd\ubcc0\uc218\ub4e4\uc740 \ud1b5\uc81c\ub97c \ud55c \uc0c1\ud0dc\uc5d0\uc11c Y\uc640 \ud558\ub098\uc758 \ub3c5\ub9bd\ubcc0\uc218 X\uac04\uc758 \ubcc0\ud654\ub7c9 \uad00\uacc4\ub97c \uc124\uba85\ud558\ub294\ub370, \uc5ec\uae30\uc11c \uc870\uc2ec\ud574\uc57c\ud560 \uac83\uc740 \ub098\uba38\uc9c0 \ubcc0\uc218\ub97c \ud1b5\uc81c\uc2dc\ud0a8\ub2e4\ub294 \uc758\ubbf8\uac00 \uadf8 \ubcc0\uc218\ub4e4\uc744 \ubb34\uc2dc\ud558\uace0 \uc0dd\uac01\ud55c\ub2e4\ub294 \ub73b\uacfc \ub2e4\ub974\ub2e4\ub294 \uac83 \uc774\ub2e4. \uadf8\ub807\uae30 \ub54c\ubb38\uc5d0 \ub2e4\uc911\uc120\ud615\ud68c\uadc0\ub97c \ud588\uc744 \ub54c\uc758 \uacc4\uc218 \\({\\hat\\beta}_{i}\\) \ub294 \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\ub97c \ud588\uc744 \ub54c\uc758 \\({\\hat\\beta}_{i}\\) \uc640 \ub2e4\ub974\ub2e4.(\ubb34\uc2dc\uc640 \ud1b5\uc81c\uc758 \ucc28\uc774)","title":"Multiple Linear Regression"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#f-test","text":"\ub2e8\uc21c\uc120\ud615\ud68c\uadc0\uc640\ub294 \ub2e4\ub974\uac8c \ub2e4\uc911\uc120\ud615\ud68c\uadc0\uc5d0\uc11c\ub294 \ub3c5\ub9bd\ubcc0\uc218\uc640 \uc885\uc18d\ubcc0\uc218 \uac04\uc758 \uc120\ud615\uad00\uacc4 \uc720\ubb34\ub97c \ud310\ubcc4\ud560 \ub54c\uc5d0\ub294 F-test\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac80\uc815\ud55c\ub2e4. \uc774\ub7ec\ud55c \uc774\uc720\ub294 \ub3c5\ub9bd\ubcc0\uc218\uc640 \uc885\uc18d\ubcc0\uc218\uac00 jointly \uc720\uc758\ud55c \uc120\ud615 \uad00\uacc4\ub97c \uac00\uc9c0\ub294\uc9c0\uc5d0 \ub300\ud55c \ubb38\uc81c\ub97c \ub17c\ud558\uae30 \ub54c\ubb38\uc778\ub370, \uc5ec\ub7ec \ud68c\uadc0\uacc4\uc218\uac12\ub4e4\uc5d0 \ub300\ud574\uc11c \uac01\uac01 \ub3c5\ub9bd\uc801 T-test\ub97c \uc2e4\uc2dc\ud55c\ub2e4\uace0 \ud574\ub3c4 \ub9cc\ub4e4\uc5b4\uc9c4 \uc11c\ub85c \ub2e4\ub978 \uadc0\ubb34\uac00\uc124\ub4e4\uc774 \ub3c5\ub9bd\uc774 \uc544\ub2c8\ub780 \ubcf4\uc7a5\uc774 \uc5c6\uae30 \ub54c\ubb38\uc774\ub2e4. \ub610\ud55c \uc5ec\ub7ec\uac1c\uc758 \uadc0\ubb34\uac00\uc124\ub4e4\uc774 \uac01\uac01 \uac80\uc815\ub418\ub294 \uacbd\uc6b0 \uc774 \uc911 \ud558\ub098\ub77c\ub3c4 \uae30\uac01\ub420 \ud655\ub960\uc774 \ucee4\uc9c0\uace0, \uadc0\ubb34\uac00\uc124\uc744 \uacfc\uc18c\ud3c9\uac00\ud560 \uac00\ub2a5\uc131\uc774 \ub192\uae30 \ub54c\ubb38\uc774\ub2e4. \\[H_{0}:{\\beta}_1={\\beta}_2=...={\\beta}_{p}\\] \\(H_{a}:\\) at least one \\({\\beta}_{j}\\) is non-zero \\[F=\\frac{(SST-SSE)/p}{SSE/(n-p-1)}\\]","title":"F-test"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#deciding-on-important-variables","text":"Forward selection(\uc804\uc9c4\uc120\ud0dd\ubc95) : \uc808\ud3b8\uacc4\uc218\ub9cc \uc874\uc7ac\ud558\ub294 \ubaa8\ub378\uc5d0\uc11c \uc2dc\uc791\ud558\uc5ec( \\(Y={\\beta}_0+{\\epsilon}\\) ) RSS\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ubcc0\uc218\ub97c \ucd94\uac00\ud558\ub294 \ubc29\ubc95, \ud2b9\uc815 \ubaa9\uc801\uc5d0 \ub3c4\ub2ec\ud560 \ub54c \uae4c\uc9c0 \uacc4\uc18d\ud55c\ub2e4. Backward selection(\ud6c4\uc9c4\uc81c\uac70\ubc95) : \ubaa8\ub4e0 \ub3c5\ub9bd\ubcc0\uc218 p\uac1c\uac00 \uc874\uc7ac\ud558\ub294 \ubaa8\ud615\uc5d0\uc11c \ucd9c\ubc1c. \ud1b5\uacc4\uc801\uc73c\ub85c \uac00\uc7a5 \uc720\uc758\ud558\uc9c0 \uc54a\uc740 \ubcc0\uc218\ub97c \uc81c\uac70\ud574\ub098\uac10. \ubaa8\ub4e0 \uacc4\uc218\ub4e4\uc774 \uc720\uc758\ud560\ub54c\uae4c\uc9c0 \ubc18\ubcf5. Mixed selection : \uc808\ud3b8 \uacc4\uc218\ub9cc \uc874\uc7ac\ud558\ub294 \ubaa8\ud615\uc5d0\uc11c \uc2dc\uc791\ud558\uc5ec RSS\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ubcc0\uc218 \ucd94\uac00\ud574\ub098\uac10. \ud2b9\uc815\ubcc0\uc218\ub97c \ucd94\uac00\ud588\uc744\ub54c \ud1b5\uacc4\uc801\uc73c\ub85c \uc720\uc758\ud558\uc9c0\uc54a\uc740 \uacc4\uc218\uac00 \ubc1c\uc0dd\uc2dc \uadf8\ubcc0\uc218\ub97c \uc81c\uac70. \ucd5c\uc885\ubaa8\ub378\uc758 \ubaa8\ub4e0\ubcc0\uc218\ub4e4\uc774 \uc720\uc758\ud558\uace0 \ub2e4\ub978\ubcc0\uc218\ub97c \ucd94\uac00\ud558\uba74 \uc720\uc758\ud558\uc9c0\uc54a\uc544\uc9c8 \ub54c\uae4c\uc9c0\ubc18\ubcf5 model fit\uc5d0 \ub300\ud55c \uc815\ud655\uc131\uc744 \ud310\ub2e8\ud560 \uc218 \uc788\ub294 \ucc99\ub3c4\ub85c \ub2e4\uc911\ud68c\uadc0\uc5d0\uc11c\ub294 \\(adjusted R^2\\) \ub97c \uc0ac\uc6a9\ud55c\ub2e4. \ucd5c\uc18c\uc81c\uacf1\ubc95\uc740 \uc885\uc18d\ubcc0\uc218\uc640 \uc544\ubb34\ub9ac \uad00\ub828\uc5c6\ub294 \ubcc0\uc218\ub77c\ub3c4 \uc5b4\ub5a4\ud615\ud0dc\ub85c\ub4e0 RSS\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ud68c\uadc0\uc9c1\uc120\uc744 \ucd94\uc815\ud558\ub294\ub370, \ubcc0\uc218\uc758 \uac1c\uc218\uac00 \uc99d\uac00\ud560\uc218\ub85d RSS\ub294 \uac10\uc18c\ud558\uace0 \\(R^2\\) \ub294 \uc99d\uac00\ud560 \uc218\ubc16\uc5d0 \uc5c6\ub2e4. \ud558\uc9c0\ub9cc \uc2e4\uc81c\ub85c \ubcc0\uc218\ub97c \ucd94\uac00\ud574\uc11c RSS\uac00 \uac10\uc18c\ud558\uace0 \\(R^2\\) \uac00 \uc99d\uac00\ud558\ub294 \uac83\uc774 \uc2e4\uc81c\ub85c \ucd94\uac00\ub41c \ub3c5\ub9bd\ubcc0\uc218\uac00 \uc885\uc18d\ubcc0\uc218\uc5d0 \ud070 \uc601\ud5a5\uc744 \uc8fc\ub294 \ubcc0\uc218\uc5ec\uc11c\uc778\uc9c0\ub294 \ud655\uc2e4\ud558\uac8c \uc54c \uc218\uac00\uc5c6\ub2e4. \uadf8\ub9ac\ud558\uc5ec \uc6b0\ub9ac\ub294 \\(R^2\\) \uc758 \ub300\uc548\uc73c\ub85c \\(adjusted R^2\\) \ub97c \uc4f4\ub2e4. \\[Adjusted-R^2 =1-\\frac{SSE/n-K}{SST/n-1}\\] \ub2e4\uc911\uc120\ud615\ud68c\uadc0\uc5d0\uc11c\ub3c4 \uc55e\uc5d0\uc11c \ubc30\uc6b4 \ub2e8\uc21c\uc120\ud615\ud68c\uadc0\uc640 \ub9c8\ucc2c\uac00\uc9c0\ub85c \ucd94\uc815\ub41c \ud68c\uadc0\uacc4\uc218\ub294 \ucd94\uc815\uce58\uc77c \ubfd0, \uc6b0\ub9ac\ub294 true relationship\uc744 \uc808\ub300 \uc54c \uc218\uac00 \uc5c6\ub2e4. \ud568\uc218f\ufffd \uc5d0\ub300\ud574 \uc120\ud615\ubaa8\ud615\uc744 \uac00\uc815\ud558\ub294 \uac83\uc740 \ub2e8\uc9c0 \uac00\uc815\uc77c \ubfd0\uc774\uace0,true regression line\uc744 \uc548\ub2e4\uace0 \ud558\ub354\ub77c\ub3c4 random \ud558\uac8c \ubc1c\uc0dd\ud558\ub294 \\({\\epsilon}\\) \ufffd \uae4c\uc9c0\ub294 \ud1b5\uc81c \ubd88\uac00\ub2a5\ud558\ub2e4.","title":"Deciding on Important Variables"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#other-considerations-in-the-regression","text":"","title":"Other Considerations in the Regression"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#qualitative-predictors","text":"\ub9ce\uc740 \uacbd\uc6b0\uc5d0 \ub3c5\ub9bd\ubcc0\uc218 \uc911\uc5d0 \uc9c8\uc801\ubcc0\uc218\uac00 \uc874\uc7ac\ud55c\ub2e4. \ud68c\uadc0\uc5d0\uc11c\ub294 \uc774\ub7ec\ud55c \ubcc0\uc218\ub97c \ub9cc\ub098\uba74 \uc5b4\ub5bb\uac8c \ucc98\ub9ac\ub97c \ud574\uc57c\ud560\uae4c? \uc774\ub54c\ub294 Dummy variable\uc744 \ub9cc\ub4e4\uba74 \ub41c\ub2e4. (\ub3c5\ub9bd\ubcc0\uc218\ub97c 0,1\uc758 \uac12\uc73c\ub85c \ubcc0\ud658\ud55c \ubcc0\uc218) \uc774\ub294 \\(x_{i}\\) \ub97c i\ubc88\uc9f8 \uc0ac\ub78c\uc774 female\uc774\uba74 x\uac00 1, male\uc774\uba74 0\uc73c\ub85c \ubcc0\ud658\uc744 \ud574\uc900\ub2e4. \ub9cc\uc57d \uc9c8\uc801\ub3c5\ub9bd\ubcc0\uc218\uac00 3\uac1c \uc774\uc0c1\uc758 \ud074\ub798\uc2a4\ub97c \uac00\uc9c8\uacbd\uc6b0\uc5d0\ub294 \ub2e8\uc77c\ub354\ubbf8\ubcc0\uc218\uac00 \ubaa8\ub4e0 \uac00\ub2a5\ud55c \ud074\ub798\uc2a4\ub97c \ub098\ud0c0\ub0bc \uc218\uac00 \uc5c6\ub2e4. \uc774\ub54c \ud544\uc694\ud55c \ub354\ubbf8\ubcc0\uc218\uc758 \uac2f\uc218\ub294 (\ubc94\uc8fc\uc758 \uac1c\uc218-1)\uac1c \ub9cc\ud07c \ub9cc\ub4e4\uc5b4 \uc918\uc57c\ud55c\ub2e4. \uc9c0\uae08\uae4c\uc9c0 \ub2e4\ub8ec \uc120\ud615\ud68c\uadc0\ub294 \ub2e8\uc21c\ud558\uba74\uc11c\ub3c4 \uacb0\uacfc \ud574\uc11d\uc774 \uac00\ub2a5\ud558\uace0 \ube44\uad50\uc801 \ub192\uc740 \uc815\ud655\ub3c4\ub97c \ubcf4\uc5ec\uc900\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub7f0 \uc120\ud615\ud68c\uadc0\ub294\uc2e4\uc81c\uc5d0\uc11c \uc885\uc885 \uc9c0\ucf1c\uc9c0\uae30 \ud798\ub4e0 \uc5c4\uaca9\ud55c \uac00\uc815\ub4e4\uc744 \ud488\uace0\uc788\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \uc544\ub798\uc640 \uac19\uc740 \uac83\ub4e4\uc774 \uc788\ub2e4. Addictive \uac00\uc815 : \ub3c5\ub9bd\ubcc0\uc218 \\(X_{j}\\) \uc5d0 \ub530\ub978 \uc885\uc18d\ubcc0\uc218 Y\uc758 \ubcc0\ud654\uac00 \ub2e4\ub978 \ub3c5\ub9bd\ubcc0\uc218\ub4e4\uc758 \uac12\uc5d0 \uc0c1\uad00\uc5c6\uc774 \ub3c5\ub9bd\uc774\ub77c\ub294 \uac83 Linear \uac00\uc815 : \ub3c5\ub9bd\ubcc0\uc218 \\(X_{j}\\) \uac00 \ud55c \ub2e8\uc704 \ubcc0\ud654\ud560 \ub54c \uc885\uc18d\ubcc0\uc218 Y\uc758 \ubcc0\ud654\ub7c9\uc774 \\(X_{j}\\) \uc758 \uac12\uacfc \uc0c1\uad00\uc5c6\uc774 \uc0c1\uc218\ub77c\ub294 \uac83. \ud558\uc9c0\ub9cc \uc2e4\uc81c\ub85c \ud55c \ub3c5\ub9bd\ubcc0\uc218\uac00 \ub2e4\ub978 \ub3c5\ub9bd\ubcc0\uc218\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \uc77c\uc740 \uc885\uc885 \uc77c\uc5b4\ub09c\ub2e4. \ub9e4\uccb4\uad11\uace0\ub85c \uc778\ud55c sale\uc744 \uc608\uce21\ud558\ub294\ub370\uc5d0 \ubcc0\uc218\ub85c tv\uc640 radio\uac00 \uc788\ub2e4\uace0 \ud574\ubcf4\uc790. \ub9cc\uc57d radio\uad11\uace0\uc5d0 \ud22c\uc790\ud560\uc218\ub85d tv\uc758 \uad11\uace0 \ub610\ud55c \ucee4\uc9c4\ub2e4\uace0 \ud55c\ub2e4\uba74, \ud55c \ubcc0\uc218\uac00 \uc99d\uac00\ud568\uc5d0 \ub530\ub77c \ub2e4\ub978 \ubcc0\uc218\uc758 \uae30\uc6b8\uae30\uacc4\uc218 \uc5ed\uc2dc \ubcc0\ud654\ud558\uac8c \ub420 \uac83\uc774\ub2e4. \uc774\ub97c \ub9c8\ucf00\ud305\uc5d0\uc11c\ub294 Synergy Effect , \ud1b5\uacc4\ud559\uc5d0\uc11c\ub294 Interaction Effect \ub77c \ud55c\ub2e4. Interaction term\uc744 \ud1b5\ud574 \uc6b0\ub9ac\ub294 \uc774\ub7ec\ud55c \ud6a8\uacfc\ub97c \ubaa8\ud615\uc5d0\uc11c \uace0\ub824\ud560 \uc218 \uc788\uc73c\uba70, \uc704\uc758 \uc608\uc2dc\ub97c \uc2dd\uc73c\ub85c \uc801\ub294\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc740 \uaf34\uc77c \uac83\uc774\ub2e4. \\[Y={\\beta}_0+{\\beta}_1X_1+{\\beta}_2X_2+{\\beta}_3X_1X_2+{\\epsilon}\\] $ \\(={\\beta}_0+({\\beta}_1+{\\beta}_3X_2)X_1+{\\beta}_2X_2+{\\epsilon}\\) $ \uc774\ub807\uac8c \ub41c\ub2e4\uba74 \uc885\uc18d\ubcc0\uc218 Y\uc5d0 \ub300\ud55c \ud6a8\uacfc\ub294 \ub354\uc774\uc0c1 \uc0c1\uc218\uac00 \uc544\ub2c8\ub2e4.","title":"Qualitative Predictors"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#non-linear-relationship","text":"\uc5b4\ub5a4 \ub3c5\ub9bd\ubcc0\uc218\ub294 \uc885\uc18d\ubcc0\uc218\uc640\uc758 \uad00\uacc4\ub97c plot\uc73c\ub85c \uadf8\ub824\ubcf4\uc558\uc744 \ub54c \ube44\uc120\ud615\uc801\uc778 \uad00\uacc4\ub97c \uac00\uc9c8 \uc218\ub3c4\uc788\ub2e4.(\uc0ac\uc2e4 \ub9e4\uc6b0\ub9e4\uc6b0 \ub300\ubd80\ubd84\uc758 \ubcc0\uc218\uad00\uacc4\ub4e4\uc774 \uadf8\ub7f4 \uac83\uc774\ub2e4.) \uc774\ub7f4 \ub54c\ub294 \ub2e8\uc21c\ud558\uac8c \ud68c\uadc0\ubd84\uc11d\uc744 \ud568\uc5d0 \uc788\uc5b4\uc11c \ube44\uc120\ud615\ud568\uc218\ub97c \ud68c\uadc0\uc2dd\uc5d0 \ud3ec\ud568\uc2dc\ucf1c\uc8fc\uba74 \ub41c\ub2e4. \ud558\uc9c0\ub9cc \uadf8\ub9cc\ud07c \uc2dd\uc774 \ubcf5\uc7a1\ud574\uc9c0\uace0 \uc124\uba85\uc744 \ud558\uae30\uc5d4 \uc120\ud615\ud68c\uadc0\ubcf4\ub2e4\ub294 \uc5b4\ub824\uc6b8 \uac83\uc774\ub2e4. \uc544\ub2c8\uba74 \ub2e4\ub978 \ubc29\ubc95\uc73c\ub85c\ub294 \uc801\uc808\ud55c \ubcc0\ud658\uc744 \ud1b5\ud574 X\uc640 Y\uc758 \ube44\uc120\ud615\uad00\uacc4\ub97c \uc120\ud615\ud654\ud558\uac70\ub098 \uc815\uaddc\ubd84\ud3ec\ud654 \uc2dc\ud0a4\ub294 \ubc29\ubc95 \ub4f1\uc744 \ud1b5\ud574 \uc785\ub825\ubcc0\uc218 X\ub97c \uae54\ub054\ud558\uac8c \ubcc0\ud658\ud558\uc5ec \ubaa8\ub378\uc5d0 \uc9d1\uc5b4\ub123\ub294 \uac83\ub3c4 \ud558\ub098\uc758 \ud574\uacb0\ucc45\uc774\ub2e4.(ex. skewed\ub41c \ub370\uc774\ud130\ub97c \uc801\uc808\ud55c transformation\uc744 \ud1b5\ud574 \ub300\uce6d\ud615\ud0dc\ub85c \ub9cc\ub4e0\ub2e4\ub294 \ub4f1)","title":"Non-linear relationship"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#non-constant-variance-of-error-terms","text":"\uc6b0\ub9ac\ub294 error term\uc5d0 \ub300\ud574\uc11c \uba87\uac00\uc9c0 \uac00\uc815\uc744 \ub123\uc5b4\ub193\uc558\ub2e4. Normality(\uc815\uaddc\uc131) Homoskedasticity(\ub4f1\ubd84\uc0b0\uc131) Independent(\ub3c5\ub9bd\uc131) \ud558\uc9c0\ub9cc \uc885\uc885 \uc794\ucc28\ub3c4 plot\uc744 \ud655\uc778\ud574\ubcf4\uba74 \uc774\ub7ec\ud55c \uac00\uc815\uc744 \ub9cc\uc871\ud558\uc9c0 \uc54a\ub294 \uacbd\uc6b0\ub3c4 \ub9ce\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \uc624\ucc28\uc758 \ubd84\uc0b0\uc774 constant term\uc774 \uc544\ub2d0 \uacbd\uc6b0\uc5d0\ub294 heteroskedasticity(\uc774\ubd84\uc0b0\uc131)\uc744 \ub744\uac8c \ub418\ub294\ub370, \uc774\ub7f4 \ub54c\uc5d0\ub294 response Y\uc5d0 \ub300\ud55c \uc801\uc808\ud55c \ubcc0\ud658\uc744 \ud1b5\ud574 \ub4f1\ubd84\uc0b0\uac00\uc815\uc744 \ub9cc\uc871\ud558\uac8c \ubcc0\ud658\uc744 \ud574\uc904 \uc218 \uc788\ub2e4. \uc704\uc758 \uadf8\ub9bc\uc740 ISL\uc758 \uc0ac\uc9c4\uc744 \ucca8\ubd80\ud55c \uac83\uc778\ub370, Y\uc5d0 \ub85c\uadf8\ubcc0\ud658\uc744 \ud1b5\ud574 \uc801\uc808\ud558\uac8c \uc774\ubd84\uc0b0\uc131\uc744 \uc5c6\uc564 \ubaa8\uc2b5\uc744 \ubcf4\uc5ec\uc900\ub2e4.","title":"Non-constant Variance of Error Terms"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#outliers","text":"Outlier\ub780 \uc885\uc18d\ubcc0\uc218 \\(y_{i}\\) \uac00 \uad00\uce21\ub41c \ub370\uc774\ud130 \ubc94\uc704 \ub0b4\uc5d0\uc11c \uba40\ub9ac \ub5a8\uc5b4\uc9c4 \uc544\uc8fc \uc791\uc740 \uac12\uc774\ub098 \ud070 \uac12\uc744 \ub9d0\ud55c\ub2e4. \uc774\ub294 \uce21\uc815\uc744 \ud558\ub294\ub370 \uc0dd\uae34 \uc624\ub958\uc77c\uc218\ub3c4 \uc788\uace0 \uc785\ub825\uc624\ub958\uc77c\uc218\ub3c4 \uc788\uc73c\uba70 \uc544\ub2c8\uba74 \uc815\ub9d0 \ud2b9\uc774\ud55c \uac12\uc774 \ub370\uc774\ud130 \uc548\uc5d0 \uc874\uc7ac\ud558\ub294 \uacbd\uc6b0 \uc77c\uc218\ub3c4 \uc788\ub2e4. \ud639\uc740 \ubaa8\ub378\uc758 \ub2e4\ub978 \ub3c5\ub9bd\ubcc0\uc218\uac00 \uace0\ub824\ub418\uc9c0 \uc54a\uc544 \ubc1c\uc0dd\ud558\ub294 \uc2e0\ud638\uc77c\uc218\ub3c4 \uc788\uace0 \uc774\uc0c1\uce58\uac00 \uc0dd\uae30\ub294 \uc774\uc720\ub294 \uad49\uc7a5\ud788 \ub2e4\uc591\ud558\uae30\uc5d0 \uc774\ub97c \ucc98\ub9ac\ud568\uc5d0 \uc788\uc5b4\uc11c\ub294 \uc2e0\uc911\ud558\uac8c \ucc98\ub9ac\ud574\uc57c\ud55c\ub2e4.","title":"Outliers"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#collinearity","text":"\uacf5\uc120\uc131\uc740 \ub450 \uac1c\uc774\uc0c1\uc758 \ub3c5\ub9bd\ubcc0\uc218\ub4e4\uc774 \uc11c\ub85c \ub192\uc740 \uc120\ud615\uad00\uacc4\ub97c \uac00\uc9c0\ub294 \uac83\uc774\ub2e4. \uc774\ub294 \ucd94\ub860\uad00\uc810\uc5d0\uc11c \uac1c\ubcc4\uacc4\uc218\ub4e4\uc744 \ud574\uc11d\ud558\ub294\ub370 \uc7a5\uc560\ubb3c\uc774 \ub418\uae30\ub3c4 \ud55c\ub2e4. \ub610\ud55c \uc608\uce21\uad00\uc810\uc5d0\uc11c\ub3c4 \ube44\uc2b7\ud55c \uac12\uc744 \uac16\ub294 RSS\uac00 \ub098\uc62c \uc218 \uc788\ub294 \ud68c\uadc0 \uacc4\uc218\ub4e4\uc758 \uc870\ud569\uc774 \ub9ce\uc544\uc9c4\ub2e4. \uc774\ub294 \ucd94\uc815\ud55c \ud68c\uadc0\uacc4\uc218\uc758 \ubd88\ud655\uc2e4\uc131\uc744 \ub192\ud788\uba70 \uacc4\uc218\uac00 \uc720\uc758\ud558\uc9c0 \uc54a\uc744 \ud655\ub960\uc774 \uc62c\ub77c\uac00\uac8c \ub41c\ub2e4. \uacf5\uc120\uc131\uc744 detecting\ud558\ub294 \uac04\ub2e8\ud55c \ubc29\ubc95\uc73c\ub85c\ub294 \uc785\ub825\ubcc0\uc218\ub4e4\uac04\uc758 \uc0c1\uad00\uacc4\uc218 \ud589\ub82c\uc744 \uccb4\ud06c\ud574\ubcf4\ub294 \uac83\uc774\ub2e4. \uc0c1\uad00\uacc4\uc218\uc758 \uc808\ub300\uac12\uc774 \ud070 \uc6d0\uc18c\uac00 \uc788\ub2e4\uba74 \uc774\ub294 \ub450 \ubcc0\uc218\uac04\uc5d0 corelation\uc774 \uc0c1\ub2f9\ud788 \ub192\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud558\uace0 \uc774\ub294 \uacf5\uc120\uc131 \ubb38\uc81c\uac00 \ub370\uc774\ud130 \ub0b4\uc5d0 \uc874\uc7ac\ud55c\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \ud558\uc9c0\ub9cc \uc0c1\uad00\uacc4\uc218 \ud589\ub82c\ub9cc\uc73c\ub85c\ub294 \ud56d\uc0c1 \uc774\ub97c detecting\ud560 \uc218\ub294 \uc5c6\ub294\ub370, \uc774\ub294 \uc138\uac1c \uc774\uc0c1\uc758 \ubcc0\uc218\uac00 \ud589\ub82c\uc758 \uc6d0\uc18c\uac12\uc774 \ud2b9\ubcc4\ud788 \ub192\uc9c0 \uc54a\ub354\ub77c\ub3c4 \uc11c\ub85c \uacf5\uc120\uc131\uc774 \uc874\uc7ac\ud560 \uc218\uac00 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. \uc6b0\ub9ac\ub294 \uc774\ub7ec\ud55c \uc0c1\ud669\uc744 \ub2e4\uc911\uacf5\uc120\uc131\uc774 \uc874\uc7ac\ud55c\ub2e4\uace0 \ud45c\ud604\ud55c\ub2e4. \ub2e4\uc911\uacf5\uc120\uc131 \ubb38\uc81c\ub97c \ub9de\ub531\ub4e4\uc600\uc744 \ub54c\ub294 \uc0c1\uad00\uacc4\uc218 \ud589\ub82c\ubcf4\ub2e4\ub294 variance inflation factor (VIF)\ub97c \uacc4\uc0b0\ud558\ub294 \uac83\uc774 \ub354 \ub098\uc740 \ubc29\ubc95\uc774\ub2e4. $ \\(VIF(\\hat{\\beta}_{j})=\\frac{1}{1-R^2_{X_{j}|X_{-j}}}\\) $ VIF\uc758 \uc2dd\uc740 \uc704\uc640 \uac19\uc73c\uba70 \\(R^2_{X_{j}|X_{-j}}\\) \uc774 1\uc5d0 \uac00\uae4c\uc6cc \uc9c0\uba74 \uacf5\uc120\uc131\uc774 \uc874\uc7ac\ud55c\ub2e4\ub294 \ub73b\uc774\uace0 VIF\uac00 \ucee4\uc9c4\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c VIF\uc758 \uac12\uc774 5\ub098 10\uc744 \ucd08\uacfc\ud558\uba74 \uacf5\uc120\uc131\uc758 \ubb38\uc81c\uac00 \uc874\uc7ac\ud55c\ub2e4\ub294 \uac83\uc744 \ub098\ud0c0\ub0b8\ub2e4. \uacf5\uc120\uc131\uc744 \ud574\uacb0\ud558\ub294 \ubc29\ubc95\uc73c\ub85c \ucc28\uc6d0\ucd95\uc18c\ub97c \ud1b5\ud574 \ubcc0\uc218\ub97c \uc870\uc808\ud558\ub294 \ubc29\ubc95\uc774 \uc788\ub2e4. \ucc28\uc6d0\ucd95\uc18c\ub97c \ud558\uae30 \uc704\ud574 \uc544\ub798\uc640 \uac19\uc740 \ub450\uac00\uc9c0 \ubc29\ubc95\uc774 \uc885\uc885 \uc0ac\uc6a9\ub41c\ub2e4. variable selection PCA \uba3c\uc800 \ubcc0\uc218\uc120\ud0dd\uc740 \uacf5\uc120\uc131\uc774 \uc874\uc7ac\ud558\ub294 \ub450 \uac00\uc9c0\uc758 \ubcc0\uc218 \uc911, \ud55c\uac00\uc9c0\ub9cc \ucc44\ud0dd\ud558\uc5ec \ucd94\uc815\ubaa8\ub378\uc5d0 \ub123\ub294 \uac83\uc774\ub2e4. \uc774\ub294 \uac04\ud3b8\ud558\uace0 \ud574\uc11d\uc774 \uc6a9\uc774\ud558\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\uc9c0\ub9cc, \ud558\ub098\uc758 \uc785\ub825\ubcc0\uc218\ub97c \uc644\uc804\ud788 \uc0ad\uc81c\ud574\ubc84\ub9ac\ub294 \uac83\uc774\uae30 \ub54c\ubb38\uc5d0 \uc6b0\ub9ac\uac00 \uac00\uc9c0\uace0 \uc788\ub294 \uae30\ubcf8 \uc815\ubcf4\ub97c \uc190\uc2e4\ud55c\ub2e4\ub294 \ub2e8\uc810 \ub610\ud55c \uc874\uc7ac\ud55c\ub2e4. \uac04\ub2e8\ud55c \ubaa8\ub378\ub85c \uc785\ub825\ubcc0\uc218\uac00 2\uac1c\ub9cc \uc874\uc7ac\ud558\ub294 \\(Y={\\beta}_0+{\\beta}_1X_1+{\\beta}_2X_2+{\\epsilon}\\) \ubaa8\ub378\uc744 \uc0dd\uac01\ud574\ubcfc \ub54c PCA\ub294 \uc785\ub825\ubcc0\uc218 X1\uacfc X2\uac04\uc758 Linear comnination\uc744 \ud1b5\ud574 \ucd95\uc744 \ud68c\uc804\uc2dc\ucf1c \ub450 \ucd95(PC1,PC2)\uc744 independent\ud55c \uad00\uacc4\ub85c \ubc14\uafb8\uc5b4 \ud55c\uac1c\uc758 \ucd95\ub9cc \ucc44\ud0dd\ud558\uc5ec \ucc28\uc6d0\uc744 \ucd95\uc18c\ud558\ub294 \ubc29\ubc95\uc774\ub2e4. \uc774\ub294 PC1\ub9cc\uc744 \ucc44\ud0dd\ud558\uc5ec \ud558\ub098\uc758 \ubcc0\uc218\ub97c \ubc84\ub9ac\ub294 \uac83\uacfc \ube44\uc2b7\ud558\uac8c \ubcf4\uc5ec\uc9c0\uc9c0\ub9cc, PC1\uc740 \uacb0\uad6d \\({\\alpha}X_1+{\\beta}X_2\\) \uaf34\uc758 \uc120\ud615\uacb0\ud569\uc774\uae30 \ub54c\ubb38\uc5d0, \uc815\ubcf4\ub97c \uc190\uc2e4\ud558\uc9c0 \uc54a\uace0\ub3c4 \ucc28\uc6d0\uc744 \ucd95\uc18c\ud558\uc5ec \uacf5\uc120\uc131\uc744 \ud574\uacb0\ud560 \uc218 \uc788\ub294 \ubc29\ubc95\uc774\ub2e4. \ud558\uc9c0\ub9cc \uc774\ub7ec\ud55c \uc120\ud615\uacb0\ud569\uc73c\ub85c \uc778\ud574 \ubaa8\ub378 \ud574\uc11d\uc774 \uc5b4\ub824\uc6cc\uc9c4\ub2e4\ub294 \ub2e8\uc810 \ub610\ud55c \uc874\uc7ac\ud55c\ub2e4.","title":"Collinearity"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#the-basics-of-decision-trees","text":"Tree-based model\uc740 \ub2e8\uc21c\ud558\uba70 \ud574\uc11d\uc5d0 \uc6a9\uc774\ud55c \uc9c0\ub3c4\ud559\uc2b5\uc758 \ubc29\ubc95 \uc911 \ud558\ub098\uc774\ub2e4. \uc21c\uc11c\ub294 Decision tree\ub97c Regression\uacfc classification \ub450 \uac00\uc9c0\ub85c \ub098\ub204\uc5b4 \uc124\uba85\ud558\uba70, \uc758\uc0ac\uacb0\uc815\ub098\ubb34\ubaa8\ud615 \uc774\ud6c4\uc5d0 Bagging\uacfc random forests, boosting\uc744 \uac04\ub2e8\ud558\uac8c \uc124\uba85\ud558\ub3c4\ub85d \ud560 \uac83\uc774\ub2e4.","title":"The Basics of Decision Trees"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#regression-trees","text":"\uba3c\uc800 Regression Tree model\ub85c \uac04\ub2e8\ud55c \uc608\uc2dc\ub97c An Introduction to Statistical Learning with R(ISLR) \uad50\uc7ac\uc5d0\uc11c \uac00\uc838\uc640 \ubcf4\uc774\uaca0\ub2e4. \uc704\ub294 Hitters \ub370\uc774\ud130 \uc14b\uc744 \uc0ac\uc6a9\ud558\uc600\uc73c\uba70, \ub18d\uad6c\uc120\uc218 \ub4e4\uc758 Salary\ub97c \uc120\uc218\ub4e4\uc774 \ud504\ub85c\ub9ac\uadf8\uc5d0\uc11c \uacbd\uae30\ud55c \ud587\uc218\uc640 \uc29b\ud305\ud69f\uc218\ub97c \ud1b5\ud574 \ub370\uc774\ud130\ub97c \ubd84\ub958\ud55c \uac04\ub2e8\ud55c \uc0ac\uc9c4\uc774\ub2e4. \ub9e8 \uc717\uc904\uc758 \ubd84\ud560 \uaddc\uce59\uc744 \ubcf4\uba74(top split) \uba3c\uc800 \uc120\uc218\ub4e4\uc758 \uc9d1\ub2e8\uc744 \ud504\ub85c\ub9ac\uadf8\uc5d0\uc11c \ub6f4 \ud587\uc218\uac00 4.5\ub144 \uc774\uc0c1\uc778\uc9c0 \uc774\ud558\uc778\uc9c0\ub85c \uad6c\ubd84\uc744 \ud568\uc744 \uc54c \uc218 \uc788\ub2e4. \uadf8 \uc544\ub798 node\ub294 \ud587\uc218\ub85c \uc120\uc218\ub4e4\uc744 \ub450 \uc9d1\ub2e8\uc73c\ub85c \uad6c\ubd84\ud55c \ub4a4 , \uc29b\ud305\ud69f\uc218 117.5\uac1c\ub97c \uae30\uc900\uc73c\ub85c \ub610 \uadf8\ub8f9 split\uc744 \ud568\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uc774 \uadf8\ub9bc\uc744 2\ucc28\uc6d0\uc758 \ud3c9\uba74 \uc704\uc5d0\uc11c \uadf8\ub9bc\uc73c\ub85c \ud45c\ud604\ud55c\ub2e4\uba74 \uc544\ub798\uc640 \uac19\uc774 \ub098\ud0c0\ub09c\ub2e4. \uc704\uc640 \uac19\uc740 \uc138\uac00\uc9c0 regions\uc5d0\uc11c \uac01\uac01 region\uc5d0 \uc18d\ud574\uc788\ub294 player\ub4e4\uc758 mean response value\ub97c \uad6c\ud558\ub294 \uac83\uc744 predicted Y\ub85c \uc124\uc815\ud558\ub294 \uac83\uc774 \uae30\ubcf8\uc801\uc778 regression decision tree method\uc758 \ub9e4\ucee4\ub2c8\uc998\uc774\ub2e4. \uc704\uc758 \uadf8\ub9bc\uc5d0\uc11c \\(R_1,R_2, R_3\\) \uc758 region\uc744 \uc6b0\ub9ac\ub294 \ud2b8\ub9ac\uc758 terminal nodes , \ud639\uc740 leaves \ub77c\uace0 \uce6d\ud55c\ub2e4. \uc704\uc758 \uac04\ub2e8\ud55c \uc608\uc2dc\ub97c \ud574\uc11d\ud55c\ub2e4\uba74 \uc6b0\ub9ac\ub294 Salary\ub97c \uacb0\uc815\ud558\ub294 \uc694\uc18c\uc911 \uac00\uc7a5 \uc911\uc694\ud55c factor\ub294 Years\ub77c\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uace0, \ub9ac\uadf8\uc5d0\uc11c \ub6f4\uc9c0 4.5\ub144\uc774\uc0c1\uc774 \ub41c \ud50c\ub808\uc774\ub4e4 \uc911\uc5d0\uc11c\ub294 \uc774\uc804 \ub144\ub3c4\uc758 number of hits\uc774 salary\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce5c\ub2e4\ub294 \uac83\uc744 \uc54c \uc218 \uc788\ub2e4. \uc704 \uc608\uc2dc\uc758 \ud68c\uadc0 \ub098\ubb34 \ubaa8\ud615\uc740 Hits, Years, Salary\uac04\uc758 true relationship\uc744 \uc9c0\ub098\uce58\uac8c \ub2e8\uc21c\ud654 \ud558\uc5ec \ud45c\ud604\ud55c \uac00\ub2a5\uc131\uc774 \ub192\uc9c0\ub9cc, \uc774\ub294 \ud68c\uadc0\ubaa8\ud615\uc758 \uc7a5\uc810\uc774\uba70, \ud574\uc11d\uc5d0 \uc6a9\uc774\ud558\uace0 \uc2dc\uac01\ud654\ud558\uae30 \ud3b8\ud558\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\ub2e4.","title":"Regression Trees"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#process-of-building-regression-tree","text":"Rough\ud558\uac8c \ub9d0\ud574\uc11c \uc6b0\ub9ac\ub294 \ud68c\uadc0\ud2b8\ub9ac\ub97c \ub9cc\ub4dc\ub294 \uac83\uc744 \ub450\uac00\uc9c0 \ub2e8\uacc4\ub85c \uc124\uba85\ud560 \uc218 \uc788\ub2e4. predictor space\ub97c set of possible values\uc778 \\(X_1,X_2,...,X_{p}\\) \uac00 \\(\\mathbf{J}\\) \uac1c\uc758 distinct\ud55c non-overlapping \ud55c \\(R_1,R_2,...,R_{J}\\) \uad6c\uc5ed\uc5d0 \uc18d\ud558\uac8c \uc815\uc758\ud55c\ub2e4. \ubaa8\ub4e0 \uad00\uce21\uce58\ub294 region \\(R_{j}\\) \uc5d0 \uc18d\ud558\uba70, \\(R_{j}\\) \uc5d0 \uc18d\ud558\ub294 training observations\ub4e4\uc758 mean of response value\ub85c \uc608\uce21\uc744 \uc2dc\ud589\ud55c\ub2e4. \uccab\ubc88\uc9f8 Step\uc5d0\uc11c predictor Space\uac00 two regions\uc73c\ub85c\ub9cc \ub098\ub258\uc5b4\uc838 \uc788\ub2e4\uace0 \uac00\uc815\uc744 \ud574\ubcf4\uace0, \\(R_1\\) \uad6c\uc5ed\uc758 response mean\uc774 10, \\(R_2\\) \uad6c\uc5ed\uc758 response mean\uc774 20\uc774\ub77c\uace0 \uac00\uc815\ud574\ubcf4\uc790. \uc5ec\uae30\uc11c Given observation\uc774 \\(X=x\\) , if \\(x\\in{R_1}\\) \uc774\uba74 \uc6b0\ub9ac\ub294 10\uc73c\ub85c \uc608\uce21\uc744 \ud558\ub294 \uac83\uc774\ub2e4. \ub450\ubc88\uc9f8 \ub2e8\uacc4\uc5d0\uc11c \uc6b0\ub9ac\ub294 regions\uc744 \uc5b4\ub5bb\uac8c \ubaa8\uc591\uc744 \uc815\uc758\ud560 \uc218 \uc788\uc744\uae4c? \uc2e4\uc81c\ub85c region\uc740 \uc5b4\ub290 \ubaa8\uc591\uc73c\ub85c\ub3c4 \ud615\uc131\ub420 \uc218 \uc788\uc73c\uba70, \uace0\ucc28\uc6d0\uc5d0\uc11c\ub294 \uc6b0\ub9ac\uac00 \uc0dd\uac01\ud558\ub294 \uac83 \ucc98\ub7fc \uc608\uc05c \uc9c1\uc120\uc73c\ub85c\ub9cc \uacbd\uacc4\ub97c \ub098\ub204\uc9c0\ub294 \uc54a\uc744 \uac83\uc774\ub2e4. \ud558\uc9c0\ub9cc \uacb0\uad6d region\ub4e4\uc758 \ubaa8\uc591\uc774 \uc5b4\ub5bb\uac8c \ub418\ub358\uac04\uc758 \uc6b0\ub9ac\uc758 \ubaa9\ud45c\ub294 RSS, \uc989 \uc794\ucc28\uc81c\uacf1\ud569\uc744 \ucd5c\uc18c\ud654\ud558\ub294 \\(R_1,..,R_{J}\\) \ub97c \ucc3e\ub294 \uac83\uc774\ub2e4. $ \\(\\sum\\limits_{j=1}^J\\sum\\limits_{i\\in{R_j}}(y_{i}-\\hat{y}_{R_{j}})^2\\) $ RSS\ub294 \uc704\ucc98\ub7fc \ud45c\ud604\ub418\uba70, \uc5ec\uae30\uc11c \\(\\hat{y}_{R_{j}}\\) \ub294 mean response for the training observations within the j th box\uc774\ub2e4. \ud558\uc9c0\ub9cc \ubd88\ud589\ud788\ub3c4 J box\uc5d0 \ub300\ud574\uc11c \uac00\ub2a5\ud55c \ubaa8\ub4e0 partition\uc744 \uace0\ub824\ud558\ub294 \uac83\uc740 \ub9e4\uc6b0\ud798\ub4e4\ub2e4. \uc774\ub7ec\ud55c \uc774\uc720\ub85c \uc6b0\ub9ac\ub294 top-down, greedy \uc811\uadfc\ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\ub294\ub370 \uc774\ub294 recursive binary splitting \uc73c\ub85c \ubd88\ub9b0\ub2e4. Recursive binary splitting\uc744 \uc218\ud589\ud558\uae30 \uc704\ud574 \uc6b0\ub9ac\ub294 \uccab\ub2e8\uacc4\ub85c predictor \\(X_{j}\\) \uc640 cutpoint s \ub97c \uc9c0\uc815\ud574\uc8fc\uace0 \uc774\ub294 predictor space\ub97c \\(\\{X|X_{j}<s\\}\\) \uc640 \\(\\{X|X_{j}\\ge{s}\\}\\) \ub85c \ub098\ub294\ub370, \uc774\ub54c predictor\ub098 cutpoint\uc124\uc815\uc740 RSS\uc758 \uac10\uc18c\ub97c \ucd5c\ub300\ud654\ud558\ub294 \uac83\uc73c\ub85c \ub098\ub204\ub294 \uac83\uc774\ub2e4. \uc774\ub97c \uc218\uc2dd\uc73c\ub85c \uc880 \ub354 \uc790\uc138\ud558\uac8c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\uac00\ub2a5\ud558\uba70, \uc774\ub54c\uc758 j \uc640 s \ub97c \ucc3e\ub294 \uac83\uc740 \uadf8 \uc544\ub798\uc758 eqation\uc744 \ucd5c\uc18c\ud654\ud558\ub294 j\uc640 s\ub97c \ucc3e\ub294 \uac83\uacfc \uac19\ub2e4. \\(R_1(j,s) = \\{X|X_{j} <s\\}\\) and \\(R_2(j,s) = \\{X|X_{j} \\geq s\\}\\) , \\(\\sum\\limits_{i:x_{i}\\in{R_{1}(j,s)}}(y_{i}-\\hat{y}_{R_{1}})^2\\) + \\(\\sum\\limits_{i:x_{i}\\in{R_{2}(j,s)}}(y_{i}-\\hat{y}_{R_{2}})^2\\) Input variables\ub4e4\uc758 \uc0ac\uc774\uc988\uac00 \ud06c\uc9c0\uc54a\ub2e4\uba74 \uc704\uc758 \uc2dd\uc744 \uad6c\ud558\ub294 \uac83\uc740 \uadf8\ub807\uac8c \uc624\ub798 \uac78\ub9ac\uc9c0\ub294 \uc54a\uc744 \uac83\uc774\ub2e4. \uadf8 \ub2e4\uc74c\uc73c\ub85c \uc6b0\ub9ac\ub294 \uc774\ub7ec\ud55c \uacfc\uc815\uc744 \uacc4\uc18d\ud574\uc11c \ubc18\ubcf5\ud558\uc5ec \ubaa8\ub4e0 resulting regions\ub0b4\uc5d0\uc11c RSS\ub97c \ucd5c\uc18c\ud654\ud558\uac8c \ub370\uc774\ud130\ub97c \ubd84\ud560\ud558\ub294 \ucd5c\uc801\uc758 prediction \uc640 cutpoint\ub97c \ucc3e\uc73c\uba74 \ub41c\ub2e4. \ub9cc\uc57d \ubaa8\ub4e0 regions\uc774 \uc815\uc758\uac00 \ub418\uc5c8\ub2e4\uba74, \uc6b0\ub9ac\ub294 \uc0c8\ub85c \ub4e4\uc5b4\uc624\ub294 test \uad00\uce21\uce58\uc5d0 \ub300\ud574\uc11c \uadf8 test \uad00\uce21\uce58\uac00 \uc18d\ud558\ub294 regions\uc758 train \uad00\uce21\uce58\uc758 \ud3c9\uade0\uc744 \uc608\uce21\uac12\uc73c\ub85c \uc0ac\uc6a9\ud558\uba74 \ub41c\ub2e4.","title":"Process of building regression tree"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#tree-pruning","text":"\uc55e\uc11c \uc18c\uac1c\ud55c \ubc29\uc2dd\uc740 training set\uc5d0 \ub300\ud574 \uc88b\uc740 \uc608\uce21\uac12\uc744 \uac16\ub294 \uac83 \uac19\uc9c0\ub9cc, \uc790\ub8cc\uc5d0 \ub300\ud574 \uacfc\uc801\ud569\ud560 \uac00\ub2a5\uc131\uc774 \ub9e4\uc6b0\ud06c\uace0, test set\uc5d0 \ub300\ud574 \ud615\ud3b8\uc5c6\ub294 \uacb0\uacfc\ub97c \uac00\uc838\uc624\uae30\ub3c4 \ud55c\ub2e4. \uc774\ub294 \uc9e0 \ud2b8\ub9ac\ubaa8\ub378\uc774 \ub108\ubb34 \ubcf5\uc7a1\ud558\uae30 \ub54c\ubb38\uc778\ub370, \uadf8\ub798\uc11c \uc801\uc740 \ubd84\ud560\uc744 \ud55c \uc791\uc740 \ud2b8\ub9ac\ub4e4\uc740 \ud3b8\ud5a5\uc744 \uc904\uc774\ub294 \uac83\uc744 \ud76c\uc0dd\ud558\uba70 \ub0ae\uc740 variance\uc640 \ub354 \ub098\uc740 \uc6a9\uc774\ud55c \ud574\uc11d\uc744 \uc774\ub048\ub2e4. \uc704\uc640 \uac19\uc740 \ubc29\uc2dd\uc758 \ud55c\uac00\uc9c0 \ub300\uc548\uc73c\ub85c\ub294, \uac01 \ubd84\ud560\uc5d0\uc11c RSS\uc758 \uac10\uc18c\uac00 \ud2b9\uc815 threshold\ub97c \ub118\ub294 split\ub9cc \ucc44\ud0dd\uc744 \ud558\uc5ec tree\ub97c \uc904\uc774\ub294 \uac83\uc744 \uc608\ub85c \ub4e4 \uc218 \uc788\ub2e4. \uc774\ub7f0 \ub300\uc548\uc740 \uc870\uae08 \ub354 \uc791\uc740 tree\ub97c \ub9cc\ub4e4\uc9c0\ub9cc, \uc774\ub294 \uadfc\uc2dc\uc548\uc801\uc778 \ubc29\ubc95\uc774\ub2e4. \uadf8 \uc774\uc720\ub85c\ub294 \ud2b8\ub9ac\ud615\uc131\uc758 \uac01 \ucd08\uae30\ub2e8\uacc4\uc5d0\uc11c\ub294 \ud615\ud3b8\uc5c6\uc5b4 \ubcf4\uc774\ub294 split\uc774 \ub098\uc911\uc5d0 tree\uc804\uccb4\ub97c \ub9cc\ub4e4\uc5b4 \ub193\uc740 \uc774\ud6c4 \ubcf4\uc558\uc744 \ub54c\ub294 large reduction in RSS\ub97c \uac00\uc838\uc62c \uc218\ub3c4 \uc788\uae30 \ub54c\ubb38\uc774\ub2e4. \uadf8\ub7ec\ubbc0\ub85c, \ud2b8\ub9ac\ubaa8\ud615\uc744 \uc124\uacc4\ub97c \ud558\uba74\uc11c \uc5bb\uc744 \uc218 \uc788\ub294 \ub354 \ub098\uc740 \uc804\ub7b5\uc740, \uba3c\uc800 \uac00\ub2a5\ud55c \ub9e4\uc6b0 \ud070 \ub098\ubb34 \\(T_0\\) \uc744 \ub9cc\ub4e0 \uc774\ud6c4\uc5d0, subtree\ub97c \uc5bb\uae30 \uc704\ud574 \ub9c8\uc9c0\ub9c9\uc5d0\uc11c \uac00\uc9c0\ub97c \uccd0\ub0b4\ub294 \uac83\uc774\ub2e4. \uc5b4\ub5bb\uac8c \ub098\ubb34\uc5d0 \uac00\uc9c0\uce58\uae30\ub97c \ud558\ub294 \uac83\uc774 \uac00\uc7a5 \uc88b\uc740 \ubc29\ubc95\uc77c\uae4c? \uc9c1\uad00\uc801\uc73c\ub85c \uc6b0\ub9ac\ub294 \uac00\uc7a5 \ub0ae\uc740 test error rate\ub97c \uac16\ub294 subtree\ub97c \ucc3e\ub294 \uac83\uc774\ub2e4. subtree\ub97c \uc5bb\uc740 \ud6c4\uc5d0, \uc6b0\ub9ac\ub294 \uad50\ucc28\uac80\uc99d \ub610\ub294 validation set \uc811\uadfc\uc744 \ud1b5\ud574\uc11c test error\ub97c \ucd94\uc815\ud560\uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc \ubaa8\ub4e0 subtree\uc5d0 \ub300\ud574 \ub108\ubb34\ub098 \ub9ce\uc740 \uc870\ud569\uc758 \uc218\uac00 \uc874\uc7ac\ud558\uae30\uc5d0, \uad50\ucc28\uac80\uc99d error\ub97c \ucd94\uc815\ud558\ub294 \uac83\uc740 \ub9e4\uc6b0 \uc18c\ubaa8\uc801\uc774\uace0 \ube44\ud604\uc2e4\uc801\uc774\ub2e4. \uc774\ub97c \uc704\ud574 \uc6b0\ub9ac\ub294 Cost complexity pruning \ub610\ub294 weakest link pruning method\ub85c \uc54c\ub824\uc9c4 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4. \ubaa8\ub4e0 subtree\uc758 \uacbd\uc6b0\uc758 \uc218\ub97c \ucc3e\ub294 \uac83\uc774 \uc544\ub2c8\uace0 \uc74c\uc218\uac00 \uc544\ub2cc \ud29c\ub2dd \ud30c\ub77c\ubbf8\ud130 \\(\\alpha\\) \ub85c \uc778\ub371\uc2f1\ub41c \ud2b8\ub9ac\uc758 \uc2dc\ud000\uc2a4\ub97c \uace0\ub824\ud574\ubcf4\ub294 \uac83\uc774\ub2e4. \uc54c\uace0\ub9ac\uc998\uc744 \uac04\ub2e8\ud558\uac8c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\uc740 \ub2e8\uacc4\ub85c \uc774\ub8e8\uc5b4\uc9c4\ub2e4. Recursive binary splitting\uc744 \ud1b5\ud574 traninig data\ub85c \ud070 \ud2b8\ub9ac\ub97c \ub9cc\ub4e4\uba74\uc11c, \uac01 terminal node\uac00 \uc77c\uc815 \uc218\uc900\uc758 minimum level\uc758 \uad00\uce21\uce58 \uac2f\uc218\ub97c \uac16\uac8c \ub41c\ub2e4\uba74 \ud2b8\ub9ac\ud615\uc131\uc744 \uba48\ucd98\ub2e4. \ucd5c\uc801\uc758 subtree \uc2dc\ud000\uc2a4\ub97c \uc5bb\uae30\uc704\ud574 \\(\\alpha\\) \uc758 \ud568\uc218\ub97c \uc774\uc6a9\ud558\uc5ec cost complecity pruning\uc744 \uc801\uc6a9\ud55c\ub2e4. K-\uad50\ucc28\uac80\uc99d\uc744 \uc774\uc6a9\ud558\uc5ec \\(\\alpha\\) \ub97c \uacb0\uc815\ud55c\ub2e4. \uac01 \\(\\alpha\\) \uac12\ub4e4\uc758 \uacb0\uacfc\ub97c \ud3c9\uade0\ub0b4\uc5b4 average error\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \\(\\alpha\\) \ub97c \ucc44\ud0dd\ud55c\ub2e4. \\(\\alpha\\) \ub97c \uc774\uc6a9\ud558\uc5ec RSS\uc5d0 penalty\ub97c \ubd80\uacfc\ud558\uc5ec \uac00\uc9c0\uce58\uae30\ub97c \ud558\ub294 \uac83\uc744 \uc2dd\uc73c\ub85c \ud45c\ud604\ud558\uba74 \uc544\ub798\uc640 \uac19\uc774 \ub098\ud0c0\ub0bc \uc218 \uc788\ub2e4. $ \\(\\sum\\limits_{m=1}^{|T|}\\sum\\limits_{x_{i}\\in{R_{m}}}(y_{i}-\\hat{y}_{R_{m}})^2 + {\\alpha}|T|\\) $ \uc55e\uc11c \uc77c\ubc18\uc801\uc778 \uc758\uc0ac\uacb0\uc815\ub098\ubb34\ubaa8\ud615\uc5d0\uc11c RSS\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ub9e4\ucee4\ub2c8\uc998\uc740 \ub611\uac19\uc9c0\ub9cc \\(\\alpha\\) \uc5d0 \ub300\ud55c \ud56d\uc774 \ub354\ud558\uae30\ub85c \ucd94\uac00\ub418\uc5c8\ub2e4. \uc5ec\uae30\uc11c \\(T\\) \ub294 terminal nodes\uc758 \uac2f\uc218\uc774\uba70, \\(\\alpha\\) \ub294 \ud29c\ub2dd \ud30c\ub77c\ubbf8\ud130\uc774\ub2e4. \uc774 \\(\\alpha\\) \ub294 subtree\uc758 \ubaa8\ub378 complexity\uc640 training data\uc5d0 fitting\ud558\ub294 \uac83\uacfc trade-off\ud55c \uad00\uacc4\ub97c \uac16\ub294\ub2e4. \uc27d\uac8c \uc124\uba85\uc744 \ud558\uba74 \\(\\alpha\\) \uac00 0\uc774\ub77c\uba74 \uc704\uc758 \uc2dd\uc740\uc77c\ubc18\uc801\uc73c\ub85c \uc6b0\ub9ac\uac00 RSS\ub97c \uad6c\ud558\ub294 \uc2dd\uacfc \uac19\uc73c\uba70 \uadf8\ub7f4\ub54c\uc758 RSS\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \uc2dd\uc740 \ud2b8\ub9ac\uc758 \uae4a\uc774\ub97c \ucd5c\ub300\ud55c\uc73c\ub85c \uae4a\uac8c \ub9cc\ub4e4\uc5b4 \ubaa8\ub4e0 \uad00\uce21\uce58\ub4e4\uc744 \ud558\ub098\ud558\ub098 terminal node\ub85c \uc0bc\ub294 \uacbd\uc6b0\uc77c \uac83\uc774\ub2e4. \ub9cc\uc57d \ubaa8\ub4e0 \uac83\uc774 \uac19\uace0 \uc5ec\uae30\uc11c \\(\\alpha\\) \uac00 0\uc774 \uc544\ub2c8\ub77c\uba74, tree\ub97c \uac00\uc7a5 \uae4a\uac8c \ubed7\uc5c8\uc744 \ub54c\ub294 \uc794\ucc28\uc81c\uacf1\ud569\uc5d0 \ucd94\uac00\ub85c \\(\\alpha\\) \ud56d\uc774 \ucd94\uac00\ub418\uc5c8\uae30 \ub54c\ubb38\uc5d0, RSS\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \uacbd\uc6b0\uc640 \uc77c\uce58\ud558\uc9c0 \uc54a\uac8c \ub41c\ub2e4. \ub367\ubd99\ud600 \\(\\alpha\\) \uac00 0\uc5d0 \uadfc\uc0ac\ud55c \uc791\uc740 \uac12\uc774 \uc544\ub2c8\uace0 \uc801\ub2f9\ud788 \ud070 \uc22b\uc790\ub97c \uac16\ub294\ub2e4\uba74 \uc794\ucc28\uc81c\uacf1\uc744 \ub098\ud0c0\ub0b4\ub294 \\(\\sum\\) \uc548\uc758 \uc67c\ucabd \ud56d\ubcf4\ub2e4 \uc624\ub978\ucabd\ud56d\uc774 \uc804\uccb4\uc2dd\uc5d0 \uc601\ud5a5\uc744 \ub354 \ud06c\uac8c \uc8fc\uae30\ub54c\ubb38\uc5d0 \uc774\ub7f4 \uacbd\uc6b0\ub77c\uba74 terminal node\uc758 \uc218\ub97c \uc904\uc774\ub294 \uac83\uc774(=make smaller subtree) \uc704 \uc2dd\uc758 quantity\ub97c minimize \uc2dc\ud0ac \uac83\uc774\ub2e4.","title":"Tree Pruning"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#classification-trees","text":"Classification tree\ub294 regression tree\uc640 \uc591\uc801 response\uac00 \uc544\ub2cc \uc9c8\uc801 response\ub97c \uc608\uce21\ud55c\ub2e4\ub294 \uac83\uc744 \uc81c\uc678\ud558\uace0\ub294 \ub9e4\uc6b0 \uc720\uc0ac\ud558\ub2e4. \ud68c\uadc0\uc758 \uacbd\uc6b0 \uad00\uce21\uce58\uc5d0 \ub300\ud55c response\ub294 \uac19\uc740 terminal node\uc5d0 \uc18d\ud55c training \uad00\uce21\uce58\uc758 \ud3c9\uade0 response\ub85c \uc608\uce21\uc744 \ud558\ub294\ub370 \ubc18\ud574, \ubd84\ub958\uc758 \uacbd\uc6b0\uc5d0\ub294 training \uad00\uce21\uce58\uac00 \uc18d\ud55c region\uc5d0\uc11c most commonly occuring class \uc5d0 \uc18d\ud55c \uac1c\ubcc4 \uad00\uce21\uce58\ub97c \uc608\uce21\ud55c\ub2e4. \ubd84\ub958\ub098\ubb34\uc758 \uacb0\uacfc\ub97c \ud574\uc11d\ud560 \ub54c\ub294 \ud2b9\uc815 terminal node region\uc5d0 \uc0c1\uc751\ud558\ub294 class prediction \ubfd0\ub9cc \uc544\ub2c8\ub77c, \uac01 region\uc5d0 \ub4e4\uc5b4\uc788\ub294 training \uad00\uce21\uce58 \uc0ac\uc774\uc5d0\uc11c class proportions \ub610\ud55c \ud3ec\ud568\ud55c\ub2e4. \ud2b8\ub9ac\ub97c \ub9cc\ub4e4 \ub54c\uc5d0\ub294 \ud68c\uadc0\uc758 \uacbd\uc6b0\uc640 \ube44\uc2b7\ud558\uac8c Recursive binary splitting \uc744 \uc0ac\uc6a9\ud558\uc9c0\ub9cc, binary split\uc758 \uae30\uc900\uc774 RSS\uac00 \uc544\ub2cc classification error rate \ub97c \uc0ac\uc6a9\ud55c\ub2e4. classification error rate\ub294 \ub2e8\uc21c\ud558\uac8c \uac00\uc7a5 \uacf5\ud1b5\uc801\uc778 class\uc5d0 \uc18d\ud558\uc9c0 \uc54a\uc740 region\uc5d0 \ud3ec\ud568\ub41c training \uad00\uce21\uce58\uc758 \uc77c\ubd80\ubd84\uc73c\ub85c \uc0dd\uac01\ud558\uba74 \ub41c\ub2e4. $ \\(E= 1-max_{k}(\\hat{p}_{mk})\\) $ \uc544\ub798\uc758 \uc2dd\uc5d0\uc11c \\(\\hat{p}_{mk}\\) \ub294 k th class\uc778 m \ubc88\uc9f8 region\uc548\uc5d0 \uc788\ub294 training \uad00\uce21\uce58\uc758 \ube44\uc728\uc744 \ub098\ud0c0\ub0b8\ub2e4. \ud558\uc9c0\ub9cc \uae30\uaecf \uc124\uba85\ud588\uc9c0\ub9cc \uc774 \ubd84\ub958\uc5d0\ub7ec\ube44\uc728\uc740 tree-growing\uc5d0 \uc788\uc5b4\uc11c \ucda9\ubd84\ud558\uac8c sensitive\ud558\uc9c0 \uc54a\uc544\uc11c \ub2e4\ub978 \ub450 \uac00\uc9c0 \ubc29\ubc95\uc774 \ub354 \uc120\ud638\ub418\ub294\ub370 \uc774\ub294 \uc6b0\ub9ac\uac00 \uc798 \uc54c\uace0 \uc788\ub294 Gini index \uc640 entropy \uac00 \uc788\ub2e4. \uc9c0\ub2c8\uc9c0\uc218\ub294 \uc544\ub798\uc640 \uac19\uc774 \ud45c\ud604\ub418\ub294\ub370, $ \\(G=\\sum\\limits_{k=1}^K\\hat{p}_{mk}(1-\\hat{p}_{mk})\\) $ \uc27d\uac8c \ub9d0\ud574\uc11c \uc774 \uc778\ub371\uc2a4\ub294 node\uc758 purity\ub97c \uce21\uc815\ud558\ub294 \uac83\uc774\uba70, \uc774 \uac12\uc774 \uc791\uc73c\uba74 \uc774\ub294 node\uac00 \ub300\uac1c \ub2e8\uc77c \ud074\ub798\uc2a4\ub85c\ubd80\ud130 \ub098\uc628 \uad00\uce21\uce58\ub85c \uc774\ub8e8\uc5b4\uc838\uc788\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. $ \\(D=-\\sum\\limits_{k=1}^K\\hat{p}_{mk}log\\hat{p}_{mk}\\) $ \uc704\uc758 \uc2dd\uc740 Entropy\uc5d0 \ub300\ud55c \uc124\uba85\uc778\ub370, \uc774 \ub610\ud55c \uc9c0\ub2c8\uacc4\uc218\uc640 \ube44\uc2b7\ud558\uac8c \uc218\uce58\uac00 \uc791\uc744 \uc218\ub85d m \ubc88\uc9f8 \ub178\ub4dc\uac00 pure\ud558\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \ubd84\ub958\uc5d0\ub7ec\ube44\uc728\uacfc \uc9c0\ub2c8\uacc4\uc218, \uc5d4\ud2b8\ub85c\ud53c\ub294 \ubaa8\ub450 \ub098\ubb34\uc5d0 \uac00\uc9c0\uce58\uae30\ub97c \ud560\ub54c \uc774\uc6a9\ud558\uc9c0\ub9cc \ubcf4\ud1b5 \uc77c\ubc18\uc801\uc73c\ub85c \ucd5c\uc885 \uac00\uc9c0\uce58\uae30\ud55c \ub098\ubb34\uc758 \uc608\uce21 \uc815\ud655\uc131\uc774 main goal\uc774\ub77c\uba74 \ubd84\ub958\uc5d0\ub7ec\ube44\uc728\uc744 criterion\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc120\ud638\ub41c\ub2e4. \uac04\ub2e8\ud558\uac8c \uc694\uc57d\ud558\uba74 Tree\ubaa8\ud615\uc740 \uc544\ub798\uc640 \uac19\uc740 \uc7a5\ub2e8\uc810\uc744 \uac16\ub294\ub2e4. \uc124\uba85\uc774 \ub9e4\uc6b0 \uc6a9\uc774\ud558\uace0, \uc5b4\ub5a8\ub54c\ub294 \uc120\ud615\ud68c\uadc0\ubcf4\ub2e4 \uc27d\uac8c \uc124\uba85\ud560 \uc218 \uc788\ub2e4. \ube44\uc804\ubb38\uac00\ub77c\ub3c4 tree\uc758 \uc0ac\uc774\uc988\uac00 \ub108\ubb34 \ud06c\uc9c0\ub9cc \uc54a\ub2e4\uba74 \ud574\uc11d\uc774 \uc27d\uace0 \uc2dc\uac01\ud654\ud558\uc5ec \ubcfc \uc218\uc788\ub2e4. dummy variable\ub4e4\uc744 \ub530\ub85c \uc0dd\uc131\ud558\uc9c0 \uc54a\uc544\ub3c4 \uc9c8\uc801 predictors\ub97c \uc870\uc791\ud558\uae30\uc5d0 \ub9e4\uc6b0 \uc27d\ub2e4. \ud558\uc9c0\ub9cc \uc774 \ucc45\uc5d0\uc11c \ub2e4\ub8e8\ub294 \ub2e4\ub978 \ud68c\uadc0\ub098 \ubd84\ub958\ub97c \ub2e4\ub8e8\ub294 \ubc29\ubc95\ub4e4\uacfc \ub3d9\uc77c\ud55c \uc608\uce21 \uc815\ud655\uc131\uc744 \uac16\uc9c0\ub294 \uc54a\ub294\ub2e4 \uac00\uc7a5 \ud070 \ub2e8\uc810\uc73c\ub85c\ub294 \ud2b8\ub9ac\ubaa8\ud615\uc740 \uc544\uc8fc\uc544\uc8fc non-robust \ud558\ub2e4. \ub2ec\ub9ac\ub9d0\ud574, \ub370\uc774\ud130\uac00 \uc870\uae08\ub9cc \ubc14\ub00c\uc5b4\ub3c4 \ucd5c\uc885 \uc608\uce21\ub418\ub294 \ud2b8\ub9ac\uc5d0 \uc544\uc8fc \ud070 \ubcc0\ud654\ub97c \uc57c\uae30\ud55c\ub2e4. \ud558\uc9c0\ub9cc \uc6b0\ub9ac\ub294 bagging, random forest, boosting \ub4f1\uc758 \ubc29\ubc95\uc73c\ub85c \ud2b8\ub9ac\ubaa8\ud615\uc758 \uc131\ub2a5\uc744 \ud5a5\uc0c1 \uc2dc\ud0ac \uc218 \uc788\ub2e4!","title":"Classification Trees"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#bagging-random-forests-boosting","text":"\uc55e\uc11c \ub9d0\ud55c \uac83\ucc98\ub7fc \ud2b8\ub9ac\ubaa8\ub378\uc740 \ub370\uc774\ud130\uc14b\uc774 \uc870\uae08\ub9cc \ub2ec\ub77c\ub3c4 \uc544\uc8fc \ub2e4\ub978 \uacb0\uacfc\uac12\uc744 \ub0b3\uc744 \uc218\ub3c4 \uc788\ub2e4\uace0 \uc5b8\uae09\ud588\ub2e4. \uc774\ub294 \ud3b8\ud5a5\uc740 \uc791\uc9c0\ub9cc \ub192\uc740 \ubd84\uc0b0\uc744 \uac16\ub294\ub2e4. \uc774\ub7ec\ud55c high-variance\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud55c \ubc29\ubc95\uc73c\ub85c\ub294 bagging , \uc989 bootstrap aggregation \uc744 \ud1b5\ud574 \ud1b5\uacc4\ud559\uc2b5\ubc29\ubc95\uc758 \ubd84\uc0b0\uac10\uc18c\ub97c \uc5bb\uc744 \uc218\uc788\ub2e4. \\[\\hat{f}_{avg}(x) = \\frac{1}{B}\\sum\\limits_{b=1}^B\\hat{f}^b(x)\\] \ubd80\ud2b8\uc2a4\ud2b8\ub7a9\uc744 \uc774\uc6a9\ud558\ub294 \uac83\uc740 \uc704\uc640 \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\ub2e4. \ud480\uc5b4\uc11c \uc124\uba85\ud558\uba74, \uc6b0\ub9ac\uac00 \uac00\uc9c0\uace0 \uc788\ub294 training data set\uc744 \uc5ec\ub7ec\ubc88 (\ub9ce\uc774 \ud639\uc740 \uc544\uc8fc \ub9ce\uc774) \ubcf5\uc6d0\ucd94\ucd9c\uc744 \ud1b5\ud574 resampling\ud558\uc5ec \uc5ec\ub7ec\uac1c\uc758 \ud45c\ubcf8 sample\uc744 \uc5bb\uc740 \ud6c4, \uc774\ub4e4\uc744 \uac01\uac01 \ud559\uc2b5 \uc54c\uace0\ub9ac\uc998\uc5d0 \ub123\uc5b4 \ubd84\ub958 \ud639\uc740 \ud68c\uadc0\ub97c \uc2dc\ud589\ud55c \ud6c4 \uadf8 \uacb0\uacfc\uac12(MSE\ub4f1)\uc758 \ud3c9\uade0\uc744 \ud1b5\ud558\uc5ec prediction\uc744 \ud558\ub294 \ubc29\ubc95\uc774\ub2e4. B\uac1c \ub9cc\ud07c\uc758 \ud2b8\ub9ac\uac00 \uc0dd\uae30\ub294 \uac70\uace0 \uc774\ub97c averaging\ud558\uba74 \ubd84\uc0b0\uc744 \uac10\uc18c\uc2dc\ud0a4\ub294 \ud6a8\uacfc\ub97c \ubcfc \uc218 \uc788\ub2e4. \uc5ec\uae30\uc11c number of trees B (resampling\uc744 \ud1b5\ud574 \uadf8\ub9cc\ud07c \ud558\ub098\uc758 training set\uc5d0\uc11c \uc5ec\ub7ec\uac1c\uc758 data set\uc744 \ub9cc\ub4e4\uae30\uc5d0 \uadf8\ub9cc\ud07c \ud2b8\ub9ac\uac00 \uc0dd\uae30\ub294 \uac83) \ub294 bagging\uc5d0\uc11c \uc5c4\uccad \uc911\uc694\uc2dc \uc5ec\uaca8\uc9c0\ub294 \ud30c\ub77c\ubbf8\ud130\ub294 \uc544\ub2cc\ub370, \uadf8 \uc774\uc720\ub294 B\uac00 \ub9e4\uc6b0\ucee4\uc838\ub3c4 \uc774\uac83\uc774 \uacfc\uc801\ud569\uacfc \uc9c1\uacb0\ub418\uc9c0\ub294 \uc54a\uae30 \ub54c\ubb38\uc774\ub2e4. B\ub97c \ud06c\uac8c \ub298\ub9b0\ub2e4\ub294 \uac83\uc740 \ub098\ubb34\uc758 \uae4a\uc774\ub97c \uae4a\uac8c\ud574 \ub9e4\uc6b0 \ub9ce\uc740 split\uc744 \ud1b5\ud574 terminal node\ub97c \ub298\ub9ac\ub294 \uac83\uc758 \uc758\ubbf8\uac00 \uc544\ub2c8\uba70 \uc624\ud788\ub824 error\ub97c \ub354 \uc548\uc815\ud654\uc2dc\ud0a4\uac8c \ub9cc\ub4e0\ub2e4.","title":"Bagging, Random Forests, Boosting"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#out-of-bag-error-estimation","text":"\ud3c9\uade0\uc801\uc73c\ub85c bootstrap\uc744 \uc9c4\ud589\ud558\uba74 \uad00\uce21\uce58\uc758 2/3\uc815\ub3c4\ub9cc \uc0ac\uc6a9\ub418\uc5b4\uc9c4\ub2e4. \uc774\ub860\uc801\uc73c\ub85c N\uac1c\uc758 \uad00\uce21\uce58\uc5d0 data\uc5d0\uc11c N\uac1c\uc758 \ud45c\ubcf8\uc744 \ubcf5\uc6d0\ucd94\ucd9c\ud558\uac8c \ub420 \uacbd\uc6b0 \uac01 \ub370\uc774\ud130\uac00 \ubf51\ud790 \ud655\ub960\uc740 \uc544\ub798\uc640 \uac19\uace0, \\[1-(1-\\frac{1}{N})^N\\] N\uc774 \ub9e4\uc6b0 \ucee4\uc9c0\uba74 \uc704 \uc2dd\uc740 \\(1-\\frac{1}{e}\\) \ub85c \uc218\ub834\ud558\ub294\ub370 \uc774\ub294 \uc57d 0.63\uc815\ub3c4\uc774\uae30\uc5d0 \uad00\uce21\uce58\uc758 2/3\uc815\ub3c4\ub9cc \ud655\ub960\uc801\uc73c\ub85c \uc0ac\uc6a9\ub418\uc5b4\uc9c4\ub2e4\ub294 \ub73b\uc774\ub2e4. \uadf8\ub807\ub2e4\uba74 \ub0a8\uc740 \uad00\uce21\uce58\uc758 1/3\uc740 bagged tree\uc5d0\uc11c fitting\ub418\ub294\ub370 \uc0ac\uc6a9\ub418\uc9c0 \ubabb\ud55c\ub2e4\ub294 \ub73b\uc744 \uc758\ubbf8\ud558\ub294\ub370, \uc774\ub97c out-of-bag (OOB)\uad00\uce21\uce58\ub77c\uace0 \uce6d\ud55c\ub2e4. \uc6b0\ub9ac\ub294 \uc774\ub7ec\ud55c OOB\uc0d8\ud50c\ub4e4\uc744 \ud65c\uc6a9\ud558\uc5ec \ud2b8\ub9ac\ubaa8\ud615\uc5d0\uc11c\uc758 decision\uc5d0 \uac00\uc911\uce58\ub97c \uc870\uc815\ud560\uc218\ub3c4 \uc788\uace0, \ubd84\ub958\ubaa8\ud615\uc5d0\uc11c\ub294 \uc624\ubd84\ub958\uc728\uc744 \ucd94\uc815\ud558\ub294 \ub4f1 \ub2e4\uc591\ud55c \uc6a9\ub3c4\ub85c \uc0ac\uc6a9\ud560 \uc218\uc788\ub2e4. \ud2b9\ud788 OOB error\uc758 \uacb0\uacfc\ub294 bagged model\uc5d0\uc11c test error\ub97c \ucd94\uc815\ud558\ub294\ub370 \uc788\uc5b4\uc11c \ud55c\ubc88\ub3c4 fittingd\u3154 \uc0ac\uc6a9\ub418\uc9c0 \uc54a\uc740 response\ub4e4\uc744 \uac16\uace0 \ucd94\uc815\uc744 \ud558\uae30\ub54c\ubb38\uc5d0 \ud0c0\ub2f9\ud558\ub2e4. \uc774\ub7ec\ud55c Bagging\uae30\ubc95\uc740 \uc758\uc0ac\uacb0\uc815\ub098\ubb34\uc5d0 \uc801\uc6a9\uc744 \ud558\uba74 \uac00\uc9c0\uce58\uae30 \uc791\uc5c5\uc744 \uc0dd\ub7b5\ud560\uc218\ub3c4 \uc788\uac8c \ud574\uc8fc\uace0, error\uc758 variance\ub97c \uc904\uc5ec\uc900\ub2e4\ub294 \uc7a5\uc810\uc774 \uc788\uc9c0\ub9cc, single decision tree\uc640\ub294 \ub2e4\ub974\uac8c \uc5b4\ub5a0\ud55c \ubcc0\uc218\uac00 procedure\uc5d0\uc11c \uc5bc\ub9c8\ub098 \uc911\uc694\ud55c \uc601\ud5a5\ub825\uc744 \uac16\ub294\uc9c0\ub97c \uccb4\ud06c\ud560 \uc218 \uc5c6\ub2e4. \uc989 Bagging\uc740 \ud574\uc11d\uc744 \ud76c\uc0dd\ud558\uc5ec \uc608\uce21\uc758 \uc815\ud655\ub3c4\ub97c \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ubc29\ubc95 \uc774\ub77c\uace0 \ub9d0\ud560 \uc218 \uc788\ub2e4. \ud558\uc9c0\ub9cc \ube44\ub85d \ub2e8\uc77c \uc758\uc0ac\uacb0\uc815\ub098\ubb34\ubcf4\ub2e4\ub294 \ud574\uc11d\uc5d0 \uc6a9\uc774\ud558\uc9c0 \uc54a\ub354\ub77c\ub3c4, \ud68c\uadc0\ud2b8\ub9ac\uc5d0\uc11c\uc758 RSS\uc640 \ubd84\ub958\ud2b8\ub9ac\uc5d0\uc11c\uc758 Gini index\ub4f1\uc744 \ud1b5\ud574 \uc804\uccb4\uc801\uc73c\ub85c \uac1c\ub7b5\uc801\uc778 \uac01 \ubcc0\uc218\uc758 \uc911\uc694\ub3c4\ub294 \uac00\ub2a0\ud560 \uc218 \uc788\ub2e4. \uc6b0\ub9ac\ub294 bagging\uc5d0\uc11c \ubd80\ud2b8\uc2a4\ud2b8\ub7a9\uc744 \ud1b5\ud574\uc5bb\uc740 B\uac1c\uc758 \ud2b8\ub9ac\ub4e4\uc744 \ud3c9\uade0\ud568\uc73c\ub85c\uc368 RSS\ub098 gini index\uac00 \uc5bc\ub9c8\ub9cc\ud07c \uac10\uc18c\ud588\ub294\uc9c0\ub97c\uc5d0 \ub300\ud574 total amount\ub97c \uccb4\ud06c\ud560 \uc218 \uc788\ub294\ub370, \uc5ec\uae30\uc11c \uac00\uc7a5 \ud070 \uac12\ub4e4\uc744 \ubcf4\uc774\ub294 \ubcc0\uc218\ub4e4\uc774 \uc911\uc694\ud55c \ubcc0\uc218\ub4e4\uc774 \ub41c\ub2e4. \uc704\uc758 \uadf8\ub9bc\uc740 \ubd84\ub958\ud2b8\ub9ac\uc758 \uc608\uc2dc\uc778\ub370, value\uac12\uc774 \uc81c\uc77c \ud070 Thai, Ca, ChestPain\uc774 largest mean decrease in Gini index\ub97c \uac16\ub294 \ubcc0\uc218\uc774\uace0 \uac1c\uc911\uc5d0 \uc81c\uc77c \uc911\uc694\ud55c \ubcc0\uc218\ub77c\ub294 \ub73b\uc744 \uc758\ubbf8\ud55c\ub2e4.","title":"Out-of-Bag Error Estimation"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#random-forest","text":"\ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8 \uae30\ubc95\uc740 \ud2b8\ub9ac\uc5d0 bagging\uc744 \uc801\uc6a9\ud55c \ubc29\ubc95\uc744 \ud2b8\ub9ac\uac04\uc758 \uc0c1\uad00\uad00\uacc4\ub97c \uc81c\uac70\ud558\ub294 \ub9e4\ucee4\ub2c8\uc998\uc744 \uc774\uc6a9\ud558\uc5ec \uc608\uce21\uc815\ud655\ub3c4\ub97c \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uae30\ubc95\uc774\ub2e4. \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\ub97c \uc9e4 \ub54c, \ud2b8\ub9ac\uc758 \uac01 split\uc5d0\uc11c \uc54c\uace0\ub9ac\uc998\uc740 \uac00\ub2a5\ud55c \ub300\ubd80\ubd84\uc758 predictors\ub4e4\uc744 \uace0\ub824\ud558\ub294 \uac83\uc744 \ud5c8\uc6a9\ud558\uc9c0\ub294 \uc54a\ub294\ub2e4. p\uac1c\uc758 predictors\ub4e4\uc774 \uc788\ub2e4\uba74 \uc774\ub4e4\uc744 \uc804\ubd80\uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 \uc77c\ubd80\ub9cc \uc0ac\uc6a9\ud558\uc5ec \ud2b8\ub9ac\ub97c \uc9dc\ub294 \uac83\uc774\ub2e4.(\ubcf4\ud1b5 total number of predictors\uc758 \uac2f\uc218\uc5d0 \uc81c\uacf1\uadfc\uc744 \ucde8\ud55c \uac2f\uc218\ub9cc\ud07c\uc744 \uc0ac\uc6a9\ud55c\ub2e4.) \uc774\ub294 \uc815\ud655\ub3c4\ub97c \uc704\ud574 \ud68c\uadc0\uc640 \ubd84\ub958\ub97c \uc218\ud589\ud568\uc5d0 \uc788\uc5b4\uc11c \uc774\uc0c1\ud55c \uc18c\ub9ac\ucc98\ub7fc \ub4e4\ub9ac\uaca0\uc9c0\ub9cc, \uc0ac\uc2e4\uc740 \uc0c1\ub2f9\ud788 \ud569\ub2f9\ud558\ub2e4. \ub9cc\uc57d data set\uc5d0\uc11c \ub9e4\uc6b0 \uac15\ub825\ud55c \uc124\uba85\ubcc0\uc218\uac00 \uc788\ub2e4\uace0 \uac00\uc815\ud574\ubcf4\uc790. \uc774\ub7ec\ud55c data set\uc744 \ud1b5\ud574 bagging\uc744 \uc218\ud589\ud558\uba74 \ub300\ubd80\ubd84 \ub610\ub294 \ubaa8\ub4e0 \ud2b8\ub9ac\uac00 \uc774 \uac15\ud55c \uc124\uba85\ubcc0\uc218\ub97c top split\uc5d0 \ub193\uc544 \uc0ac\uc6a9\ud560 \uac83\uc774\uace0, \uc774\ub294 \uacb0\uad6d \ub300\ubd80\ubd84\uc758 bagged tree\uac00 \ube44\uc2b7\ud55c \ubaa8\uc591\uc744 \uac16\uac8c\ub418\ub294 \uacb0\uacfc\ub97c \ub0b3\ub294\ub2e4. \ub354\uad70\ub2e4\ub098 \uc774\ub7f4 \uacbd\uc6b0\uc5d0 bagged tree\uc5d0\uc11c \ub098\uc624\ub294 predictions\uc740 \ub9e4\uc6b0 highly correlated \ub418\uc5b4\uc788\ub2e4. \ubd88\ud589\ud558\uac8c\ub3c4 \ub9ce\uc740 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 quantities \ub4e4\uc744 averaging \ud558\ub294 \uac83\uc740 \uadf8\ub807\uc9c0 \uc54a\uc744 \ub54c\uc758 \uacb0\uacfc\ubcf4\ub2e4 \ud615\ud3b8\uc5c6\ub294 reduction in variance\ub97c \uac16\ub294\ub2e4. \uc774\ub294 \uc989 bagging \uae30\ubc95\uc740 \ub2e8\uc77c \ud2b8\ub9ac setting \uc5d0\uc11c \uc0c1\ub2f9\ud55c \ubd84\uc0b0\uac10\uc18c\ud6a8\uacfc\ub97c \uac00\uc838\uc624\uc9c0 \uc54a\ub294\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud55c\ub2e4. \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\ub294 \uac01 split\uc5d0 \uc0ac\uc6a9\ud558\ub294 \ubcc0\uc218\ub97c predictor space \uc804\uccb4\ub85c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc544\ub2cc, predictor\uc758 \ubd80\ubd84\uc9d1\ud569\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc774\ub7ec\ud55c \ubb38\uc81c\ub97c \uadf9\ubcf5\ud55c\ub2e4. \uc774\ub7ec\ud55c \uacfc\uc815\uc744 \uc6b0\ub9ac\ub294 tree\ub97c decorrelationg \ud55c\ub2e4\uace0 \uc0dd\uac01\ud560 \uc218 \uc788\ub294\ub370, \uba3c\uc800 \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\ub3c4 bagging\uacfc \uc720\uc0ac\ud558\uac8c bootstrap\uc744 \ud1b5\ud574 \uc0d8\ud50c\uc744 \ucd94\ucd9c\ud558\uace0, \uc0ac\uc804\uc5d0 \ubaa8\ud615\uc801\ud569\uc5d0 \uc774\uc6a9\ud560 \ubcc0\uc218\uc758 \uc218 m\ub9cc\ud07c\uc744 p\uac1c\uc758 \ubcc0\uc218\ub4e4 \uc911\uc5d0\uc11c random\ud558\uac8c \ubf51\uc544 \ud2b8\ub9ac\uc0dd\uc131\uc5d0 \uc0ac\uc6a9\ud55c\ub2e4. \ud558\ub098\uc758 \ud2b8\ub9ac\uac00 \ub9cc\ub4e4\uc5b4\uc9c0\uba74 \ub610 \ub2e4\ub978 \ud2b8\ub9ac\ub97c \ud615\uc131\ud560 \ub54c \ub9cc\uc57d \uc5b4\ub5a0\ud55c \uac15\ub825\ud55c \uc124\uba85\ubcc0\uc218\uac00 \uc788\ub2e4\uace0 \ud574\ub3c4 random\ud558\uac8c m\uac1c\uc758 \ud45c\ubcf8\ubcc0\uc218\ub97c \ubf51\uc544 \uc218\ud589\ud558\uae30 \ub54c\ubb38\uc5d0, \ub2e4\ub978 \ubcc0\uc218\ub4e4\uc774 split\ud615\uc131\uc5d0 \ub9ce\uc740 \uae30\ud68c\ub97c \uac16\uac8c \ub418\ub294 \uac83\uc774\ub2e4. bagging\uacfc \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\uc758 \uac00\uc7a5 \ud070 \ucc28\uc774\uc810\uc740 predictor \ubd80\ubd84\uc9d1\ud569\uc758 \uc0ac\uc774\uc988\uc5d0 \uad00\ud558\uc5ec \uac00\uc7a5 \ud070 \ucc28\uc774\uc810\uc744 \uac16\ub294\ub2e4. \ub9cc\uc57d \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\uc5d0\uc11c m=p \ub85c \ub193\ub294\ub2e4\uba74, \ud2b8\ub9ac \ubd84\ud560\uc5d0 \ubaa8\ub4e0 predictor\uc758 \uc804\uccb4\uc9d1\ud569\uc744 \uc0ac\uc6a9\ud55c\ub2e4\ub294 \uc758\ubbf8\uc774\ubbc0\ub85c \uc774\ub54c\ub294 bagging\uacfc \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\uac00 \uac19\uc740 \uacb0\uacfc\ub97c \ub0b3\ub294\ub2e4. \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \\(m=\\sqrt{p}\\) \uac1c\uc758 predictors\ub97c \uc0ac\uc6a9\ud558\uba70, \uc774\ub294 test error\uc640 OOB error\uc5d0 \ub300\ud574 bagging\ubcf4\ub2e4 \ub098\uc740 \uacb0\uacfc\ub97c \uac16\ub294\ub2e4.","title":"Random Forest"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#boosting","text":"\uc55e\uc5d0\uc11c\ub294 bagging\uacfc random forest\uc5d0 \ub300\ud574\uc11c \uc0b4\ud3b4\ubcf4\uc558\ub294\ub370, \uc774\ubc88\uc5d0\ub294 prediction\uc744 \ud5a5\uc0c1\uc2dc\ud0a4\ub294 \ub610 \ub2e4\ub978 \ubc29\ubc95\uc778 boosting\uc5d0 \ub300\ud574\uc11c \ub17c\uc758\ud574\ubcf4\uc790. bagging\uc744 \uc0dd\uac01\ud574\ubcf4\uba74 \uc6b0\ub9ac\ub294 original training set\uc744 bootstrap sampling\uc744 \ud1b5\ud574 \uc5ec\ub7ec\uac1c\ub85c \uce74\ud53c\ud558\uace0, \uac01\uac01\uc758 \ubcf5\uc0ac\ubcf8\uc740 single predictive model\uc744 \ub9cc\ub4e4\uae30 \uc704\ud574 \uac01\uac01 \ud2b8\ub9ac\ub97c \ud615\uc131\ud55c \ud6c4, \uc774\uc5d0 \ub300\ud55c \uacb0\uacfc\uac12(MSE, Gini index\ub4f1..)\uc744 \uacb0\ud569\ud558\uc5ec \uc608\uce21\uc744 \uc218\ud589\ud558\uc600\ub2e4. \uc774\ub860\uc801\uc73c\ub85c bootstrap\uc744 \ud1b5\ud574 \uc0dd\uc131\ub41c \ud2b8\ub9ac\ub4e4\uc740 \ub2e4\ub978 \ud2b8\ub9ac\ub4e4\uacfc \ub3c5\ub9bd\uc77c \uac83\uc774\ub2e4. \ubd80\uc2a4\ud305\uc740 \uc774\uc640 \ube44\uc2b7\ud558\uc9c0\ub9cc, \ud2b8\ub9ac\ub4e4\uc774 \ub3c5\ub9bd\uc801\uc774\uc9c0 \uc54a\uace0 \uc11c\ub85c sequentially\ud558\uac8c \uc5f0\uacb0\ub418\uc5b4\uc788\ub294 \uac83\uc774 \uc55e\uc758 \ubc29\ubc95\uacfc \uac00\uc7a5 \ud070 \ucc28\uc774\uc810\uc774\ub2e4. \uc704\uc758 \uadf8\ub9bc\uc774 \ubd80\uc2a4\ud305\uc54c\uace0\ub9ac\uc998\uc744 \ub3c4\uc2dd\ud654 \ud55c \uac83\uc774\ub2e4. \ud480\uc5b4\uc11c \uc124\uba85\ud558\uba74 \uba3c\uc800 original data\ub97c \uc774\uc6a9\ud558\uc5ec d\uac1c\uc758 terminal node\ub97c \uac16\ub294 tree model\uc744 fitting\ud55c \ud6c4, \uadf8 \uc608\uce21 \uacb0\uacfc\uc640 \uc2e4\uc81c \uac12\uc758 \ucc28\uc774\ub97c \uc0b0\ucd9c\ud558\ub294\ub370 \uc774\ub54c learning rate \\(\\lambda\\) \ub97c \uc120\ud0dd\ud574 \uc608\uce21\uac12\uc5d0 learning rate\ub97c \uacf1\ud55c \uac12\ub9cc\ud07c\uc744 \uc81c\uc678\ud55c\ub2e4. \uadf8\ub9ac\uace0 \ub2e4\uc74c \ud2b8\ub9ac\ub97c \ub9cc\ub4dc\ub294 round\uc5d0\uc11c \uae30\uc874 outcome \\(Y\\) \uac00 \uc544\ub2cc \ubaa8\ud615\uc758 \uc794\ucc28\ub97c \uc774\uc6a9\ud558\uc5ec \uc0c8\ub85c\uc6b4 \ud2b8\ub9ac\ub97c \uc0dd\uc131\ud55c\ub2e4. \uadf8\ub9ac\uace0 \uc774\ub7ec\ud55c \uacfc\uc815\uc744 B\ubc88 \ubc18\ubcf5\ud558\ub294 \uac83\uc774\ub2e4. \uc704\uc5d0\uc11c \uc0ac\uc6a9\ub41c \ubd80\uc2a4\ud305\uc5d0\ub294 \uc138\uac00\uc9c0 \ud30c\ub77c\ubbf8\ud130\uac00 \uc788\uace0 \uc774\ub294 \uc544\ub798\uc640 \uac19\ub2e4. The number of tree : \\(B\\) \ubc30\uae45\uacfc \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\uc640\ub294 \ub2e4\ub974\uac8c \ubd80\uc2a4\ud305\uc740 B\uac00 \ub9e4\uc6b0 \ucee4\uc9c8 \uc218\ub85d \uacfc\uc801\ud569\uc774 \ub420 \uac00\ub2a5\uc131\uc774 \uc874\uc7ac\ud558\ub294\ub370, \uace0\ub85c B\uac12\uc740 \uad50\ucc28\uac80\uc99d\uc744 \ud1b5\ud574 \uacb0\uc815\ud55c\ub2e4. learning rate(=shrinkage parameter) : \\(\\lambda\\) \uc774\ub294 \ubaa8\ud615\uc758 \ud559\uc2b5\uc18d\ub3c4\ub97c \uc870\uc808\ud558\ub294 \uac83\uc73c\ub85c \ubcf4\ud1b5 0.01, 0.001\uc77c \uc4f0\uc9c0\ub9cc \ub370\uc774\ud130\uc5d0 \ub530\ub77c \uadf8 \uac12\uc744 \ub2ec\ub9ac\ud55c\ub2e4. \ub9cc\uc57d \ub78c\ub2e4 \uac12\uc774 \ub9e4\uc6b0 \uc791\uc744 \uacbd\uc6b0\uc5d0\ub294 \ubaa8\ud615\uc774 \uc801\uc808\ud558\uac8c \ud559\uc2b5\ub418\uae30 \uc704\ud574 B\uac00 \ub9e4\uc6b0 \ucee4\uc57c\ud55c\ub2e4. tree\uc758 terminal node\uc218 : \\(d\\) \ubcf4\ud1b5 d=1\uc77c \ub54c \ud559\uc2b5\uc774 \uc88b\uc740 \uc608\uce21\uc744 \uc774\ub04c\uba70 \uc774\ub7f4 \uacbd\uc6b0 \ud2b8\ub9ac\ub97c stump model \uc774\ub77c \uce6d\ud55c\ub2e4. d\ub294 \uac00\ubc95\ubaa8\ud615\uc5d0\uc11cinteraction effect\ub97c \ubc18\uc601\ud558\ub294 \uac83\uacfc \uc720\uc0ac\ud558\uae30 \ub54c\ubb38\uc5d0 interaction depth\ub85c \ubd88\ub9ac\uae30\ub3c4 \ud55c\ub2e4. \ubd80\uc2a4\ud305\uc5d0\uc11c\ub294 \uae30\uc874\uc758 \ud2b8\ub9ac\ubaa8\ub378\uc774 \ub2e4\uc74c \ud2b8\ub9ac\uc5d0 sequential\ud558\uac8c \uc601\ud5a5\uc744 \uc8fc\uae30 \ub54c\ubb38\uc5d0 \uc77c\ubc18\uc801\uc73c\ub85c \uc801\uc740\uc218\uc758 terminal node\ub97c \uc0ac\uc6a9\ud558\ub354\ub77c\ub3c4 \uc88b\uc740 \uacb0\uacfc\ub97c \ub098\ud0c0\ub0bc \uc218 \uc788\ub2e4.","title":"Boosting"},{"location":"99%20Ybigta%20ML/99_Ybigta_ML__%E1%84%89%E1%85%B5%E1%86%B7%E1%84%92%E1%85%AA/#_1","text":"","title":"\uace0\uc0dd\ud558\uc168\uc2b5\ub2c8\ub2f9"}]}