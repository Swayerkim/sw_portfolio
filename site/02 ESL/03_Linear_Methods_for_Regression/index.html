<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>03 Linear Methods for Regression - Swayer</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "03 Linear Methods for Regression";
        var mkdocs_page_input_path = "02 ESL/03_Linear_Methods_for_Regression.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Swayer
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">CPA Data Analysis</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../000%20CPA%20Data%20Analysis/likelihood/">Likelihood</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Swayer</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">03 Linear Methods for Regression</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <hr />
<h1 id="linear-methods-for-regression">Linear Methods for Regression</h1>
<h2 id="33-subset-selection">3.3 Subset Selection</h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp; 최소제곱법을 통한 추정이 종종 만족스럽지 않은 이유는 두가지가 있다.</p>
<ul>
<li>
<p>prediction accuracy</p>
</li>
<li>
<p>interpretation</p>
</li>
</ul>
<p>최소제곱법은 낮은 bias를 갖지만 높은 분산을 갖기도 한다.</p>
<p><strong>prediction accuracy</strong>는 때때로 shrinking 이나 몇 회귀계수들을 0으로 축소시키는 방법을 통해 개선되기도 하는데, 이는 낮은 bias를 포기하는대신 predicted value의 분산을 줄임으로써 전체적인 모델정확성을 향상시킨다.</p>
<p>만약 많은 predictors가 존재한다면 우리는 종종 예측에 좀 더 강한 영향을 주는 smaller subset으로 모델을 꾸리려 할 것이다. </p>
<p>큰 그림을 위해 우리는 기꺼이 몇몇 영향력이 적은 detail한 부분들을 버리기도 한다.</p>
<p>이번 section에서는 몇가지의 variable subset selection 방법을 살펴보도록 할 것이다. </p>
<p>이러한 여러 방법들은 결국 model selection의 한 과정이며 이는 선형 모델에만 국한되어있지않고, 추후에 다른 모델에서의 적용을 다룰 것이다. </p>
<p>이제부터 몇가지 choosing subset method를 살펴보자.</p>
<h4 id="1-best-subset-selection">1. Best-Subset Selection</h4>
<p>Best subset regression은 가능한 모든 모델을 고려하여 그 중 가장 좋은 모델이 무엇인지를 subset size k를 통해 찾아주는 방법이다. <span class="arithmatex">\(k \in  \{0,1,2,..,p\}\)</span></p>
<p>간단한 예시를 통해 위의 subset selection 방법을 확인해보자.</p>
<pre><code class="language-r">library(ISLR)
names(Hitters)
</code></pre>
<pre><code>##  [1] &quot;AtBat&quot;     &quot;Hits&quot;      &quot;HmRun&quot;     &quot;Runs&quot;      &quot;RBI&quot;      
##  [6] &quot;Walks&quot;     &quot;Years&quot;     &quot;CAtBat&quot;    &quot;CHits&quot;     &quot;CHmRun&quot;   
## [11] &quot;CRuns&quot;     &quot;CRBI&quot;      &quot;CWalks&quot;    &quot;League&quot;    &quot;Division&quot; 
## [16] &quot;PutOuts&quot;   &quot;Assists&quot;   &quot;Errors&quot;    &quot;Salary&quot;    &quot;NewLeague&quot;
</code></pre>
<pre><code class="language-r">Hitters &lt;- na.omit(Hitters)

#Best Subset Selection method!
#Best Subset Selection이 search하는 predictor 갯수의 디폴트값은 8
#변수의 갯수가 많지 않으니 max로 탐색해보자


library(leaps)
bss &lt;- regsubsets(Salary~.,Hitters,nvmax=length(names(Hitters))-1)
summary(bss)
</code></pre>
<pre><code>## Subset selection object
## Call: regsubsets.formula(Salary ~ ., Hitters, nvmax = length(names(Hitters)) - 
##     1)
## 19 Variables  (and intercept)
##            Forced in Forced out
## AtBat          FALSE      FALSE
## Hits           FALSE      FALSE
## HmRun          FALSE      FALSE
## Runs           FALSE      FALSE
## RBI            FALSE      FALSE
## Walks          FALSE      FALSE
## Years          FALSE      FALSE
## CAtBat         FALSE      FALSE
## CHits          FALSE      FALSE
## CHmRun         FALSE      FALSE
## CRuns          FALSE      FALSE
## CRBI           FALSE      FALSE
## CWalks         FALSE      FALSE
## LeagueN        FALSE      FALSE
## DivisionW      FALSE      FALSE
## PutOuts        FALSE      FALSE
## Assists        FALSE      FALSE
## Errors         FALSE      FALSE
## NewLeagueN     FALSE      FALSE
## 1 subsets of each size up to 19
## Selection Algorithm: exhaustive
##           AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns
## 1  ( 1 )  &quot; &quot;   &quot; &quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 2  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 3  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 4  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 5  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 6  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot; &quot;    &quot; &quot;  
## 7  ( 1 )  &quot; &quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot;    &quot; &quot;  
## 8  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot; &quot;    &quot; &quot;   &quot;*&quot;    &quot;*&quot;  
## 9  ( 1 )  &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;  
## 10  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;  
## 11  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;  
## 12  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;  
## 13  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot; &quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;  
## 14  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot; &quot;   &quot; &quot;    &quot;*&quot;  
## 15  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot; &quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;  
## 16  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;  
## 17  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot; &quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;  
## 18  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot; &quot;    &quot;*&quot;  
## 19  ( 1 ) &quot;*&quot;   &quot;*&quot;  &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot;*&quot;   &quot;*&quot;    &quot;*&quot;   &quot;*&quot;    &quot;*&quot;  
##           CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN
## 1  ( 1 )  &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 2  ( 1 )  &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot; &quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 3  ( 1 )  &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot; &quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 4  ( 1 )  &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 5  ( 1 )  &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 6  ( 1 )  &quot;*&quot;  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 7  ( 1 )  &quot; &quot;  &quot; &quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 8  ( 1 )  &quot; &quot;  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 9  ( 1 )  &quot;*&quot;  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot; &quot;     &quot; &quot;    &quot; &quot;       
## 10  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot; &quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 11  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 12  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot; &quot;    &quot; &quot;       
## 13  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 14  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 15  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 16  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot; &quot;       
## 17  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;       
## 18  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;       
## 19  ( 1 ) &quot;*&quot;  &quot;*&quot;    &quot;*&quot;     &quot;*&quot;       &quot;*&quot;     &quot;*&quot;     &quot;*&quot;    &quot;*&quot;
</code></pre>
<pre><code class="language-r">bss.table &lt;- summary(bss)
names(bss.table)
</code></pre>
<pre><code>## [1] &quot;which&quot;  &quot;rsq&quot;    &quot;rss&quot;    &quot;adjr2&quot;  &quot;cp&quot;     &quot;bic&quot;    &quot;outmat&quot; &quot;obj&quot;
</code></pre>
<pre><code class="language-r">par(mfrow=c(2,2))
plot(bss.table$rss,xlab=&quot;# of Variables&quot;,ylab=&quot;RSS&quot;,type=&quot;l&quot;)
plot(bss.table$adjr2,xlab=&quot;# of Variables&quot;, ylab =&quot;Adjusted R-square&quot;,type=&quot;l&quot;)
points(which.max(bss.table$adjr2),bss.table$adjr2[which.max(bss.table$adjr2)],col=&quot;red&quot;,cex=2,pch=20)
plot(bss.table$cp,xlab=&quot;# of Variables&quot;, ylab =&quot;Cp&quot;,type=&quot;l&quot;)
points(which.min(bss.table$cp),bss.table$cp[which.min(bss.table$cp)],col=&quot;red&quot;,cex=2,pch=20)
plot(bss.table$bic,xlab=&quot;# of Variables&quot;, ylab =&quot;BIC&quot;,type=&quot;l&quot;)
points(which.min(bss.table$bic),bss.table$bic[which.min(bss.table$bic)],col=&quot;red&quot;,cex=2,pch=20)
</code></pre>
<p><img alt="" src="../03_Linear_Methods_for_Regression_files/figure-html/unnamed-chunk-1-1.png" /><!-- --></p>
<p>위는 ISLR 패키지의 Hitters 데이터를 가져왔으며 best subset selection의 방법을 사용하기 위해 leaps 패키지의 regsubsets함수를 사용하였다. </p>
<p>첫번째 플롯을 확인해보면 RSS는 subset k의 사이즈가 커질수록 감소하는 모습을 보이는데, 이는 우리가 무작정 변수의 갯수를 늘리는 것이 가장 좋은 subset selection이라는 것을 의미하지 않는다. </p>
<p>training RSS는 full model을 사용하여 회귀를 돌렸을 때 가장 낮은 값을 갖기 때문에 RSS만으로는 좋은 모델이라 결론짓기 어렵다. </p>
<p>또한 이는 과적합으로 연결되어 test RSS는 정반대의 결과를 가져올 수 있다.</p>
<p>그리하여 나머지 수정결정계수와 mallow's Cp, BIC 지표를 추가로 확인하여 결정계수의 최고점과 Cp, BIC의 최소점을 찍어 그때의 size k를 빨간 점으로 표시하였다.</p>
<pre><code class="language-r">coef(bss,5)
</code></pre>
<pre><code>##  (Intercept)        AtBat         Hits         CRBI    DivisionW 
##   97.7684116   -1.4401428    7.1753197    0.6882079 -129.7319386 
##      PutOuts 
##    0.2905164
</code></pre>
<p>위와 같이 특정 size k에서의 회귀계수 또한 확인할 수 있다.</p>
<h3 id="23-forward-backward-stepwise-selection">2&amp;3. Forward, backward Stepwise Selection</h3>
<p>Best Subset Selection기법은 predictors의 갯수가 40개 이상이 넘어가면 매우 비효율적이며 계산이 불가능하다는 단점이 있다. </p>
<p>그리하여 이에 대한 대안으로 사용되는 것이 전진선택법과 후진제거법 등이 있다. </p>
<p>전진선택법은 intercept만 갖고 있는 reduced model에서 시작하여 model fit을 개선시킬 수 있는 predictor들을 추가하는 방법이며, 후진제거법은 full model에서 시작하여 전진선택법과 반대의 방법으로 모델을 형성한다.</p>
<pre><code class="language-r">library(ggplot2)
library(tidyverse)
</code></pre>
<pre><code>## ── Attaching packages ──────────────────────────────────────────────── tidyverse 1.2.1 ──
</code></pre>
<pre><code>## ✔ tibble  2.0.1     ✔ purrr   0.3.0
## ✔ tidyr   0.8.2     ✔ dplyr   0.7.8
## ✔ readr   1.3.1     ✔ stringr 1.4.0
## ✔ tibble  2.0.1     ✔ forcats 0.3.0
</code></pre>
<pre><code>## ── Conflicts ─────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
</code></pre>
<pre><code class="language-r">bss.f &lt;- regsubsets(Salary~.,Hitters,nvmax=length(names(Hitters))-1,method=&quot;forward&quot;)
bss.b &lt;- regsubsets(Salary~.,Hitters,nvmax=length(names(Hitters))-1,method=&quot;backward&quot;)


bss.f.table &lt;- summary(bss.f)
bss.b.table &lt;- summary(bss.b)
names(bss.f.table)
</code></pre>
<pre><code>## [1] &quot;which&quot;  &quot;rsq&quot;    &quot;rss&quot;    &quot;adjr2&quot;  &quot;cp&quot;     &quot;bic&quot;    &quot;outmat&quot; &quot;obj&quot;
</code></pre>
<pre><code class="language-r">x &lt;- seq(1,19,1)
y1 &lt;- bss.table$bic
y2 &lt;- bss.f.table$bic
y3 &lt;- (bss.b.table$bic)
df &lt;- data.frame(x,y1,y2,y3)

df2=gather(df,key,value,-x)
g&lt;- ggplot(df2,aes(x=x,y=value,color=key))+ geom_jitter()
g&lt;-g+ scale_colour_hue(labels=c('bss','forward','backward'))
g &lt;- g+ylab(&quot;BIC&quot;)+xlab(&quot;Number of Variables&quot;)
g
</code></pre>
<p><img alt="" src="../03_Linear_Methods_for_Regression_files/figure-html/unnamed-chunk-3-1.png" /><!-- --></p>
<h3 id="4-forward-stagewise-regression">4 Forward-Stagewise Regression</h3>
<p>이 방법은 위에서 소개한 Forward stepwise보다는 좀 더 제약이 많다. </p>
<p>Forward stepwise와 비슷하게 이 또한 intercept만 있는 모델에서 시작하지만 이때의 intercept 가 <span class="arithmatex">\(\bar{Y}\)</span>이다. Centered된 predictors의 계수를 시작이 0이도록 하는 것이다. </p>
<p>각 단계에서 알고리즘은 지금의 잔차와 가장 상관관계가 높은 변수를 확인해 이에 대한 잔차와 단순선형회귀를 하여 얻은 계수를 사용한다.</p>
<p>이 작업은 잔차가 어느 변수와도 유의한 관계를 가지지 않을때 까지 계속된다.</p>
<p>이 방법은 Stepwise 방법과는 달리 한번 모델에 포함된 변수들은 이후에 제거되지 않는다.</p>
<h2 id="shrinkage-methods">Shrinkage Methods</h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;가끔은 predictors들의 부분집합을 남기고 나머지 부분들을 버림으로써 subset selection 방법은 full model을 사용했을 때 보다 더 낮은 prediction error를 갖고 용이한 해석을 이끌기도 한다. </p>
<p>하지만 이는 <strong>discrete process</strong>인데, (변수를 버리거나 남기거나 하는 것은) 이는 종종 high variance를 갖고, prediction error of the full model을 줄이지 못하기도 한다.</p>
<p>Shrinkage methods는 좀더 continuous하며, 높은 변동성에 크게 제약을 받지 않는 방법이기에, 이번 소단원에서는 이에 대해서 다뤄보겠다.</p>
<h3 id="ridge-regression">Ridge Regression</h3>
<p>&nbsp;&nbsp;&nbsp;&nbsp; Ridge regression은 coeeicients의 size에 <strong>penalty</strong>를 걸어 이들을 축소시키는 방법이다.</p>
<p>Ridge 회귀는 고로 penalized된 residual sum of squares를 최소화 하는 방향으로 모델을 만들게 되는데 이를 수식으로 표현하면 아래와 같다.</p>
<div class="arithmatex">\[{\hat{\beta}}^{ridge}=argmin_{\beta}\{\sum_{i=1}^N(y_{i}-{\beta}_0-\sum_{j=1}^px_{ij}{\beta}_{j})^2+{\lambda}\sum_{j=1}^p{\beta}_{j}^2\}\]</div>
<p>여기서 <span class="arithmatex">\({\lambda} \geq 0\)</span>은 축소의 정도를 결정하는 complexity parameter인데, 이 람다의 값이 커질수록 계수를 0으로 보내는 축소가 더 많이 이루어진다.</p>
<p>위 식은 아래와 같이도 표현할 수 있다.</p>
<div class="arithmatex">\[{\hat{\beta}}^{ridge}=argmin_{\beta}\sum_{i=1}^N(y_{i}-{\beta}_0-\sum_{j=1}^px_{ij}{\beta}_{j})^2,\]</div>
<div class="arithmatex">\[subject\ to\ \sum_{j=1}^p{\beta}_{j}^2 \leq t \]</div>
<p>각 식에서 <span class="arithmatex">\({\lambda}\)</span>와 <span class="arithmatex">\(t\)</span>의 값은 1:1대응이 성립한다.</p>
<p>식을 확인해보면 우리는 intercept 항에 대해서는 penalization이 진행되지 않았다는
 것을 확인할 수 있는데, 이는 target <span class="arithmatex">\(y_{i}\)</span>에 c만큼의 상수를 더해주는 것이 예측값이 c만큼 이동한다는 의미와 다르기 때문이다.</p>
<p><span class="arithmatex">\({\lambda}\)</span>를 통해 얻은 penalized RSS의 해는 centered된 predictors를 사용한 재모수화로 두 부분으로 쪼개지는데, 이는 <span class="arithmatex">\(x_{ij}\)</span>가 <span class="arithmatex">\(x_{ij}-\bar{x}_{j}\)</span>로 대체됨을 의미힌다.</p>
<p>여기서 intercept <span class="arithmatex">\({\beta}_0\)</span>를 <span class="arithmatex">\(\bar{Y}\)</span>로 추정하며, 나머지 계수들은 intercept가 없는 ridge regression 으로 centered된  <span class="arithmatex">\(x_{ij}\)</span>들을 추정하게 된다.</p>
<p>이러한 중심화가 수행되었다고 가정했을때, input matrix <span class="arithmatex">\(\mathbf{X}\)</span>는 <span class="arithmatex">\(p+1\)</span>개가 아닌 <span class="arithmatex">\(p\)</span>개의 열을 갖는다.</p>
<p>이러한 식과 그때의 해를 행렬로 표현하면 아래와 같다.</p>
<p>$<span class="arithmatex">\(RSS({\lambda})=(\mathbf{y}-\mathbf{X}{\beta})^T(\mathbf{y}-\mathbf{X}{\beta})+{\lambda}{\beta}^T{\beta}\)</span>$</p>
<p>$<span class="arithmatex">\({\hat{\beta}}^{ridge}=(\mathbf{X}^T{\mathbf{X}}+{\lambda}\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\)</span>$</p>
<p><span class="arithmatex">\(\mathbf{I}\)</span>는 <span class="arithmatex">\(p\times{p}\)</span>의 항등행렬이다. </p>
<p>우리는 quadratic penalty <span class="arithmatex">\({\beta}^T{\beta}\)</span>를 사용하였기 때문에, ridge regression 의 해는 <span class="arithmatex">\(\mathbf{y}\)</span>에 대한 선형 함수가 된다.</p>
<p>이 해는 역행렬을 구하기 전에 <span class="arithmatex">\(\mathbf{X}^T\mathbf{X}\)</span>의 대각원소에 양의 상수를 더한다.</p>
<p>이는 문제를 <strong>nonsingular</strong>하게 해주며 <span class="arithmatex">\(\mathbf{X}^T\mathbf{X}\)</span>가 full rank가 아니어도 괜찮게 해준다.</p>
<p>아래에 첨부한 그림은 prostate cancer 예제를 통해 얻은 그림인데, 이는<br />
<span class="arithmatex">\(df({\lambda})\)</span>에 따라 ridge 회귀계수의 추정치가 변화하는 것을 보여준다.</p>
<p><span class="arithmatex">\(df({\lambda})\)</span>는 penalty <span class="arithmatex">\({\lambda}\)</span>에 대한 함수로  <em>effective degress of freedom</em>을 보여준다.</p>
<p><img alt="" src="../03_photo1.png" /></p>
<p>만약 입력변수들이 orthonormal 하다면, ridge의 추정치는 최소제곱법 추정치의 scaled version이 된다.</p>
<p><span class="arithmatex">\({\hat{\beta}}^{ridge}=\frac{\hat{\beta}}{1+{\lambda}}\)</span></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;중심화된 입력행렬 <span class="arithmatex">\(\mathbf{X}\)</span>의 고유값분해(<em>singular value decomposition</em>, SVD)를 통해 우리는 ridge regression의 특성에 대해 추가적인 insight를 얻을 수 있다.</p>
<p>이러한 분해는 다양한 통계적 방법에 대한 분석에 매우 용이한 방법이다.</p>
<p><span class="arithmatex">\(N\times{p}\)</span>  <span class="arithmatex">\(\mathbf{X}\)</span>행렬의 고유값분해는 아래와 같은 form을 갖는다.</p>
<div class="arithmatex">\[\mathbf{X}=\mathbf{U}\mathbf{D}\mathbf{V}^T\]</div>
<p>여기서 <span class="arithmatex">\(\mathbf{U}\)</span>와 <span class="arithmatex">\(\mathbf{V}\)</span>는 <span class="arithmatex">\(N\times{p}\)</span>, <span class="arithmatex">\(p\times{p}\)</span>의 직교행렬이고, <span class="arithmatex">\(\mathbf{U}\)</span>의 열은 <span class="arithmatex">\(\mathbf{X}\)</span>의 열공간을 span하며, <span class="arithmatex">\(\mathbf{V}\)</span>의 열들이 행공간을 span한다.</p>
<p><span class="arithmatex">\(\mathbf{D}\)</span>는 <span class="arithmatex">\(p\times{p}\)</span>의 대각행렬이며, 대각원소들은 <span class="arithmatex">\(d_1\geq{d_2}\geq...\geq{d_{p}}\geq0\)</span>인 <span class="arithmatex">\(\mathbf{X}\)</span>의 singular value들이다.</p>
<p>만약 한개라도 <span class="arithmatex">\(d_j=0\)</span>이라면, <span class="arithmatex">\(\mathbf{X}\)</span>는 singular이다.</p>
<p>SVD를 통해 우리는 least squares fitted vector를 아래와 같이 표현할 수 있다.</p>
<div class="arithmatex">\[\mathbf{X}{\hat{\beta}^{ls}}=\mathbf{X}{(\mathbf{X}^T\mathbf{X})}^{-1}\mathbf{X}^T\mathbf{y}\]</div>
<div class="arithmatex">\[=\mathbf{X}(\mathbf{V}\mathbf{D}^2\mathbf{V}^T)^{-1}\mathbf{X}^T\mathbf{y}\]</div>
<div class="arithmatex">\[=\mathbf{X}\mathbf{V}\mathbf{D}^{-2}\mathbf{V}^T\mathbf{X}^T\mathbf{y}\]</div>
<div class="arithmatex">\[=\mathbf{U}\mathbf{D}\mathbf{V}^T\mathbf{V}\mathbf{D}^{-2}\mathbf{V}^T\mathbf{X}^T\mathbf{y}\]</div>
<div class="arithmatex">\[=\mathbf{U}\mathbf{D}^{-1}\mathbf{V}^T\mathbf{X}^T\mathbf{y}\]</div>
<div class="arithmatex">\[=\mathbf{U}\mathbf{D}^{-1}\mathbf{V}^T\mathbf{V}\mathbf{D}\mathbf{U}^T\mathbf{y}=\mathbf{U}\mathbf{U}^T\mathbf{y}\]</div>
<p>로 표현가능하다.</p>
<p>여기서 <span class="arithmatex">\(\mathbf{U}^T\mathbf{y}\)</span>는 orthonormal basis <span class="arithmatex">\(\mathbf{U}\)</span>에 대한 <span class="arithmatex">\(\mathbf{y}\)</span>의 좌표들이다.</p>
<p>이 증명은 <span class="arithmatex">\(\mathbf{U}\)</span>와 <span class="arithmatex">\(\mathbf{V}\)</span>의 열들이 orthogonal하다는 성질과 각 열들이 <span class="arithmatex">\(\mathbf{X}\mathbf{X}^T\)</span>와 <span class="arithmatex">\(\mathbf{X}^T\mathbf{X}\)</span>의 eigenvector라는 성질을 이용하여 식전개를 한 것이다.</p>
<p>그렇다면 이제 ridge solutions은 아래와 같이 구할 수 있다.</p>
<div class="arithmatex">\[\mathbf{X}{\hat{\beta}}^{ridge}=\mathbf{X}(\mathbf{X}^T\mathbf{X}+{\lambda}\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\]</div>
<div class="arithmatex">\[=\mathbf{U}\mathbf{D}\mathbf{V}^T(\mathbf{V}\mathbf{D}^{-2}\mathbf{V}^T+\frac{{1}}{\lambda}\mathbf{I})\mathbf{V}\mathbf{D}\mathbf{U}^T\mathbf{y}\]</div>
<div class="arithmatex">\[=\mathbf{U}\mathbf{U}^T\mathbf{y}+\frac{1}{\lambda}\mathbf{U}\mathbf{D}^2\mathbf{U}^T\mathbf{y}\]</div>
<div class="arithmatex">\[=\mathbf{U}(\mathbf{I}+\frac{1}{\lambda}\mathbf{D}^2)\mathbf{U}^T\mathbf{y}\]</div>
<div class="arithmatex">\[=\mathbf{U}\mathbf{D}(\mathbf{D}^{-2}+\frac{1}{\lambda})\mathbf{D}\mathbf{U}^T\mathbf{y}\]</div>
<div class="arithmatex">\[=\mathbf{U}\mathbf{D}(\mathbf{D}^2+\lambda\mathbf{I})^{-1}\mathbf{D}\mathbf{U}^T\mathbf{y}\]</div>
<div class="arithmatex">\[=\sum_{j=1}^p\mathbf{u}_{j}\frac{d_j^2}{d_j^2+\lambda}\mathbf{u}_j^T\mathbf{y}\]</div>
<p>여기서 <span class="arithmatex">\(\mathbf{u}_j\)</span>들은 <span class="arithmatex">\(\mathbf{U}\)</span>의 열들이다. </p>
<p><span class="arithmatex">\(\lambda \geq 0\)</span> 일때 우리는 <span class="arithmatex">\(d_j^2/(d_j^2+\lambda) \leq 0\)</span>임을 알 수 있다.</p>
<p>선형 회귀 처럼 ridge회귀는 <span class="arithmatex">\(\mathbf{y}\)</span>의 좌표들을 정규직교기저 <span class="arithmatex">\(\mathbf{U}\)</span>에 대해 계산하게 된다.</p>
<p>이후 이는 이러한 좌표들을 <span class="arithmatex">\(d_j^2/(d_j^2+\lambda)\)</span>를 곱함으로써 축소시키는 것이다.</p>
<p>이는 즉 더 작은 <span class="arithmatex">\(d_j^2\)</span>를 갖는 기저 벡터들의 좌표에 더 큰 양의 축소가 적용되는 것이다.</p>
<p><span class="arithmatex">\(d_j^2\)</span>가 작다는 말은 무슨 말인가? 중심화된 행렬 <span class="arithmatex">\(\mathbf{X}\)</span>의 <span class="arithmatex">\(\mathbf{SVD}\)</span>는 <span class="arithmatex">\(\mathbf{X}\)</span>내의 변수들의 <strong>principal components</strong>를 표현하는 또 다른 방법이다.</p>
<p>sample covariance matrix는 <span class="arithmatex">\(\mathbf{S}=\mathbf{X^TX}/n\)</span>이며, SVD에 의해 <span class="arithmatex">\(\mathbf{X^TX}=\mathbf{VD^2V^T}\)</span>를 얻게 되며, 이는 <span class="arithmatex">\(\mathbf{X^TX}\)</span>의 eigen decomposition이며, eigenvector들의 <span class="arithmatex">\(v_j\)</span>는 <span class="arithmatex">\(\mathbf{X}\)</span>의 principal components directions이다. 첫번째 주성분 방향 <span class="arithmatex">\(v_1\)</span>은 <span class="arithmatex">\(\mathbf{z}_1=\mathbf{X}v_1\)</span>이 모든 <span class="arithmatex">\(\mathbf{X}\)</span>의 열들의 선형 조합들을 normalize한 것들 중 가장 큰 sample variance를 갖는 성질을 갖는다. 이러한 sample variance는  <span class="arithmatex">\(Var(\mathbf{Z}_1)=Var(\mathbf{X}v_1)=\frac{d_1^2}{n}\)</span>를 만족하고, 실제로 <span class="arithmatex">\(\mathbf{z}_1=\mathbf{X}v_1=\mathbf{u}_1d_1\)</span>임 또한 보일 수 있다.</p>
<p>도출된 변수 <span class="arithmatex">\(\mathbf{z}_1\)</span>는 <span class="arithmatex">\(\mathbf{X}\)</span>의 첫번째 주성분으로 불리며, 따라서 <span class="arithmatex">\(\mathbf{u}_1\)</span>는 normalize된 첫번째 주성분이라고 볼 수 있다.</p>
<p>고로 작은 singular value <span class="arithmatex">\(d_j\)</span>는 <span class="arithmatex">\(\mathbf{X}\)</span>의 열공간의 방향들 중 작은 분산을 갖는 방향과 대응되고 ridge regression은 이러한 방향들을 더욱 축소시킨다.</p>
<p>결국 작은 singular value값일 수록 계수가 더욱 shrink되는 것이다.</p>
<p>즉 ridge regression은 작은 분산을 갖는 축에 대해 계수를 축소시킴으로써 작은 분산을 갖는 방향에서 높은 분산을 갖는 기울기가 추정되지 않게끔 보호하는 역할을 해주는 것이다.</p>
<p>이러한 해석에 필요한 숨은 가정은 output변수 또한 입력값의 변동성을 가장 많이 설명하는 (분산이 제일 큰) 축에서 그 변동이 가장 클 것이라는 가정이다. 이는 필수적이지는 않지만 충분히 합리적인 가정이라고 한다.</p>
<pre><code class="language-r">#Do SVD!

library(MASS)
</code></pre>
<pre><code>## 
## Attaching package: 'MASS'
</code></pre>
<pre><code>## The following object is masked from 'package:dplyr':
## 
##     select
</code></pre>
<pre><code class="language-r">X &lt;- matrix(c(3,7,0,1,9,3,5,0,0,7),ncol = 2, nrow = 5)
XtX &lt;- t(X)%*%X
eigen(XtX)
</code></pre>
<pre><code>## eigen() decomposition
## $values
## [1] 222.2305288   0.7694712
## 
## $vectors
##            [,1]       [,2]
## [1,] -0.7929002  0.6093515
## [2,] -0.6093515 -0.7929002
</code></pre>
<pre><code class="language-r">V &lt;- eigen(XtX)$vectors
Vt &lt;- t(V)
singular_values &lt;- sqrt(eigen(XtX)$values)
singular_values
</code></pre>
<pre><code>## [1] 14.9073985  0.8771951
</code></pre>
<pre><code class="language-r">D &lt;- diag(singular_values)
U = vector()       
for(i in 1:length(V[1,])){
    U = append(U,1/singular_values[i]*X %*% V[,i])
  }
U = matrix(U,ncol = length(V[1,]))

SVD &lt;- function(X){  
      XtX = t(X)%*%X
      V =eigen(XtX)$vectors
      Vt = t(V)

      singular_values =sqrt(eigen(XtX)$values)
      D=diag(singular_values)

      U=vector() 
    for(i in 1:length(V[1,])){
    U = append(U,1/singular_values[i]*X %*% V[,i])
}
      U=matrix(U,ncol = length(V[1,]))      
      return(list(U=U,D=D,Vt=Vt))
}

U &lt;- SVD(X)$U
D &lt;- SVD(X)$D
Vt&lt;- SVD(X)$Vt

U%*%D%*%Vt
</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    3    3
## [2,]    7    5
## [3,]    0    0
## [4,]    1    0
## [5,]    9    7
</code></pre>
<pre><code class="language-r">X
</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    3    3
## [2,]    7    5
## [3,]    0    0
## [4,]    1    0
## [5,]    9    7
</code></pre>
<pre><code class="language-r">Y &lt;- c(1,0,5,3,2)

# \mathbf{X}{\hat{\beta}}^{ls} = UU^Ty check

X%*%solve(XtX)%*%t(X)%*%Y
</code></pre>
<pre><code>##            [,1]
## [1,] -0.2631579
## [2,]  1.5847953
## [3,]  0.0000000
## [4,]  1.0116959
## [5,]  1.4093567
</code></pre>
<pre><code class="language-r">U%*%t(U)%*%Y
</code></pre>
<pre><code>##            [,1]
## [1,] -0.2631579
## [2,]  1.5847953
## [3,]  0.0000000
## [4,]  1.0116959
## [5,]  1.4093567
</code></pre>
<pre><code class="language-r">#ridge case
lambda &lt;- 500*diag(2)

U%*%D%*%solve(D^2+lambda)%*%D%*%t(U)%*%Y
</code></pre>
<pre><code>##            [,1]
## [1,] 0.16991962
## [2,] 0.35051469
## [3,] 0.00000000
## [4,] 0.03365766
## [5,] 0.46379444
</code></pre>
<pre><code class="language-r">Xbeta_ridge &lt;- matrix(0,5,2)

for (j in 1:ncol(X)){
    Xbeta_ridge[,j] &lt;- ((diag(D)[j]^2)/(diag(D)[j]^2+500))*U[,j]%*%t(U[,j])%*%Y
    }

as.matrix(rowSums(Xbeta_ridge))
</code></pre>
<pre><code>##            [,1]
## [1,] 0.16991962
## [2,] 0.35051469
## [3,] 0.00000000
## [4,] 0.03365766
## [5,] 0.46379444
</code></pre>
<pre><code class="language-r">U%*%D%*%solve(D^2+lambda)%*%D%*%t(U)%*%Y    
</code></pre>
<pre><code>##            [,1]
## [1,] 0.16991962
## [2,] 0.35051469
## [3,] 0.00000000
## [4,] 0.03365766
## [5,] 0.46379444
</code></pre>
<h2 id="lasso-regression">Lasso Regression</h2>
<p>Lasso Regression 또한 Ridge와 같은 shrinkage method이다.</p>
<p>차이가 있다면 Lasso는 <span class="arithmatex">\(L_2\)</span>가 아닌 <span class="arithmatex">\(L_1\)</span> Regularization을 수행한 것이다.</p>
<p>Lasso를 통해 얻은 베타의 추정치를 식으로 표현하면 아래와 같이 표현할 수 있다.</p>
<div class="arithmatex">\[{\hat{\beta}}^{lasso}=argmin_{\beta}\sum_{i=1}^N(y_{i}-{\beta}_0-\sum_{j=1}^px_{ij}{\beta}_j)^2\]</div>
<div class="arithmatex">\[subject\ to\ \sum_{j=1}^p|{\beta}_j|\leq t\]</div>
<p>또 Lasso에서는 Ridge에서처럼 input predictors들을 표준화하여 <span class="arithmatex">\({\beta}_0\)</span>에 대한 추정치를 <span class="arithmatex">\(\bar{y}\)</span>로 얻어 절편 계수없이 모델적합을 시행할 수 있다.</p>
<p>위의 식을 우리는 <strong>Lagrangian form</strong>의 형태로도 표현가능한데, 이 식이 좀 더 이해가 수월한듯하다.</p>
<div class="arithmatex">\[{\hat{\beta}}^{lasso}=argmin_{\beta}\{\sum_{i=1}^N(y_{i}-{\beta}_0-\sum_{j=1}^px_{ij}{\beta}_j)^2+{\lambda}\sum_{j=1}^p|{\beta}_j|\}\]</div>
<p>대체된 <span class="arithmatex">\(L_1\)</span> penalty는 해가 <span class="arithmatex">\(y_i\)</span>에 대해 비선형인 식을 만들며, 고로 ridge와 다르게 closed form의 형태가 존재하지 않는다.</p>
<p>이러한 제약식으로 인해 t를 충분히 작게 줄이는 것은 몇몇 coefficients들을 정확하게 0으로 만들 것이다.</p>
<p><img alt="" src="../03_photo2.png" /></p>
<p>위 그림을 통해 우리는 타원모양의 능선을 갖는 RSS와 그 중심에서 Full LSE를 얻는 것을 알 수 있다.</p>
<p>ridge의 제약식은 constraint region을 원모양을 만들고(Predictor가 두개인 case),</p>
<p>lasso일때는 마름모 형태를 띈다. 이는 모양을 띄는 이유는 두 방법의 regularization form이 다르기 떄문이다.</p>
<div class="arithmatex">\[|{\beta}_1|+|{\beta}_2| \leq t \ , {\beta}_1^2+{\beta}_2^2 \leq t^2\]</div>
<p>lasso의 경우는 각져있는 모서리가 있기에 해가 그 곳에서 발생하면 하나의 parameter는 0이 되는 것을 확인할 수 있다. 차원이 다차원으로 확장된다면 더 많은 모서리와 면들이 존재하기 때문에 더 많은 접점이 만들어지고 더 많은 계수를 0으로 보낼 기회가 늘어난다.</p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
