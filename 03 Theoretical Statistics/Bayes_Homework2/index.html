<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Bayes Homework2 - Swayer</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Bayes Homework2";
        var mkdocs_page_input_path = "03 Theoretical Statistics/Bayes_Homework2.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Swayer
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">CPA Data Analysis</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../000%20CPA%20Data%20Analysis/likelihood/">Likelihood</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Swayer</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Bayes Homework2</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h3 id="1-1-14-theta_3-conjugate-prior-gammaalphabeta">1-1. 강의노트 14페이지 참고, 왜 <span class="arithmatex">\(\theta_3\)</span>에 대한 conjugate prior가 Gamma<span class="arithmatex">\((\alpha,\beta)\)</span>인지 설명하시오.</h3>
<p>정규분포의 케이스에서 충분통계량은 <span class="arithmatex">\(\bar{X}\)</span>와 <span class="arithmatex">\(S^2\)</span>이다. 이들을 이용하여 likelihood를 살짝 변형하여 아래와 같이 표현할 수 있다.</p>
<p><span class="arithmatex">\(L(\theta_1,\theta_3|\mathbf{x}) \propto (\frac{\theta_3}{2\pi})^{n/2}exp[-{\frac{1}{2}} \{(n-1)s^2+n(\bar{x}-\theta_1)^2\}\theta_3]\)</span></p>
<p>이는 다시 보면 감마분포인 <span class="arithmatex">\(\frac{1}{\Gamma{(\alpha})\beta^{\alpha}}x^{\alpha-1}exp(-\frac{x}{\beta})\)</span>의 꼴로 표현이 가능함을 확인할 수 있고, 모수 <span class="arithmatex">\(theta_3\)</span>에 대해 likelihood가 감마분포의 kernel로 이루어짐을 알 수 있다. </p>
<p>고로 <span class="arithmatex">\(\theta_3\)</span>에 대한 conjugate prior가 Gamma<span class="arithmatex">\((\alpha,\beta)\)</span>이다.</p>
<h3 id="1-2-theta_3-theta_1-prior-ntheta_0frac1n_0theta_3">1-2. <span class="arithmatex">\(\theta_3\)</span>가 주어진 경우, <span class="arithmatex">\(\theta_1\)</span>에 대한 prior로 <span class="arithmatex">\(N(\theta_0,\frac{1}{n_0\theta_3})\)</span>를 사용하면 어떤 점이 편리해지는지 설명하시오.</h3>
<p><span class="arithmatex">\(\theta_3\)</span>이 주어진 경우 <span class="arithmatex">\(\theta_1\)</span>에 대한 prior를 위와 같이 정하면,</p>
<p>posterior joint pdf를 아래와 같은 꼴로 표현할 수 있다.</p>
<p><span class="arithmatex">\(k(\theta_1,\theta_3|\bar{x},s^2) \propto L(\theta_1,\theta_3|\mathbf{x})h(\theta_3)h(\theta_1|\theta_3)\)</span></p>
<p>또한 이는 이어서 쓰면 </p>
<p><span class="arithmatex">\(\propto \theta_3^{\alpha+\frac{n}{2}+\frac{1}{2}-1}exp[-\frac{1}{2}Q(\theta_1)\theta_3]\)</span></p>
<p><span class="arithmatex">\(where\ Q(\theta_1)=\frac{2}{\beta}+n_0(\theta_1-\theta_0)^2+[(n-1)s^2+n(\bar{x}-\theta_1)^2]\)</span></p>
<p>posterior joint pdf가 위처럼 나올 시, 이는 <span class="arithmatex">\(\theta_3\)</span>에 대한 Gamma kernel 꼴이므로, </p>
<p><span class="arithmatex">\(\theta_1\)</span>에 대한 prior를 <span class="arithmatex">\(N(\theta_0,\frac{1}{n_0\theta_3})\)</span>로 주는 것이 편리한 이유는</p>
<p>우리가 알고자하는 모수 <span class="arithmatex">\(\theta_1\)</span>의 marginal pdf를 매우 편하게 구할 수 있기 때문이다.</p>
<h3 id="1-3-theta_1-theta_3-posterior-joint-pdf-14">1-3. <span class="arithmatex">\(\theta_1\)</span>과 <span class="arithmatex">\(\theta_3\)</span>에 대한 posterior joint pdf가 14페이지 맨 아래의 식처럼 주어짐을 보이시오.</h3>
<p>위에서 우리는 <span class="arithmatex">\(h(\theta_1,\theta_3)=h(\theta_3)h_1(\theta_1|\theta_3)\)</span></p>
<p><span class="arithmatex">\(\propto \theta_3^{\alpha-1}exp(-\frac{1}{\beta}\theta_3)(n_0\theta_3)^{1/2}exp(-\frac{n_0\theta_3}{2}(\theta_1-\theta_0)^2)\)</span>로 표현하였고,</p>
<p>이에 <span class="arithmatex">\(g(\bar{x},s^2|\theta_1,\theta_3)\)</span>을 곱하여 joint posterior pdf를 다음처럼 표현할 수 있다.</p>
<p><span class="arithmatex">\(k(\theta_1,\theta_3|\bar{x},s^2) \propto g(\bar{x},s^2|\theta_1,\theta_3)h(\theta_1,\theta_3)\)</span></p>
<p><span class="arithmatex">\(\propto (\theta_3)^{\frac{n}{2}}exp[-\frac{\theta_3}{2}\{(n-1)s^2+n(\bar{x}-\theta_1)^2\}] \theta_3^{\alpha-1}exp(-\frac{1}{\beta}\theta_3)(n_0\theta_3)^{1/2}exp(-\frac{n_0\theta_3}{2}(\theta_1-\theta_0)^2)\)</span></p>
<p><span class="arithmatex">\(\propto (\theta_3)^{\alpha+\frac{n}{2}+\frac{1}{2}-1}exp[-\frac{\theta_3}{2}\{\{(n-1)s^2+n(\bar{x}-\theta_1)^2\}+n_0(\theta_1-\theta_0)^2+\frac{2}{\beta}\}]\)</span></p>
<p><span class="arithmatex">\(\propto (\theta_3)^{\alpha+\frac{n}{2}+\frac{1}{2}-1}exp[-\frac{\theta_3}{2}Q(\theta_1)]\)</span></p>
<p><span class="arithmatex">\(where\ Q(\theta_1)=\frac{2}{\beta}+n_0(\theta_1-\theta_0)^2+[(n-1)s^2+n(\bar{x}-\theta_1)^2]\)</span></p>
<h3 id="2-prior-logistic-distribution-squared-error-loss-function-theta-bayes-estimator-closed-form">2. Prior로 logistic distribution을 사용하고 있다. 만일 squared-error loss function을 사용한다면, <span class="arithmatex">\(\theta\)</span>에 대한 Bayes estimator를 구하기 위해서는 적분을 두번이나 해야한다고 강의노트에 적혀있다. 왜 그런지 설명하고, 그 적분이 closed form으로 구할 수 없음도 설명하시오.</h3>
<p>책의 예제는 다음과 같다.</p>
<div class="arithmatex">\[X_1,...,X_n \stackrel{iid}{\sim} N(\theta_0,\sigma^2),\ \sigma^2\ is\ known\]</div>
<div class="arithmatex">\[Y=\bar{X},\ a\ sufficient\ statistic\ for\ \theta\]</div>
<p><span class="arithmatex">\(Y|\theta \sim N(\theta, \frac{\sigma^2}{n})\)</span></p>
<p><span class="arithmatex">\(\Theta \sim h(\theta)=\frac{1}{b}\frac{exp\{-(\theta-a)/b\}}{[1+exp\{-(\theta-a)/b\}]^2}\)</span></p>
<p>where <span class="arithmatex">\(-\infty&lt;\theta&lt;\infty,\ a\ and\ b &gt;0\ are\ known\)</span></p>
<p>이는 prior의 분포를 logistic분포로 가정한 것인데, 베이즈 정리를 통해 사후분포를 나타내면 아래와 같다.</p>
<div class="arithmatex">\[k(\theta|y)=\frac{g(\theta|y)h(\theta)}{g_1(y)}=\frac{\frac{\sqrt{n}}{\sqrt{2\pi}\sigma}exp\{-\frac{1}{2}\frac{(y-\theta)^2}{\sigma^2/n}\}\frac{b^{-1}exp\{-(\theta-a)/b\}}{[1+exp\{-(\theta-a)/b\}]^2}}{\int_{-\infty}^{\infty}\frac{\sqrt{n}}{\sqrt{2\pi}\sigma}exp\{-\frac{1}{2}\frac{(y-\theta)^2}{\sigma^2/n}\}\frac{b^{-1}exp\{-(\theta-a)/b\}}{[1+exp\{-(\theta-a)/b\}]^2}d\theta}\]</div>
<p>즉 posterior pdf를 구하는 과정에서 분자의 Y와 <span class="arithmatex">\(\Theta\)</span>의 joint pdf에서 Y의 marginal로 나눠주기 위해 <span class="arithmatex">\(\theta\)</span>를 marginalize하는 과정의 적분이 포함된다.</p>
<p>이후 squared-error loss function으로 계산되는 <span class="arithmatex">\(\theta\)</span>에 대한 Bayes estimate는 사후분포의 평균이기에, 이 평균을 계산하는 과정에서 필요한 적분과정이 하나 추가 되어 최종적으로 두번의 적분과정이 포함되게 되는 것이다.</p>
<p><span class="arithmatex">\(\delta(y)=E[\Theta|y]\)</span></p>
<p><span class="arithmatex">\(={\int_{-\infty}^\infty}\theta k(\theta|y)d\theta\)</span></p>
<p><span class="arithmatex">\(={\int_{-\infty}^\infty}\theta \frac{\frac{\sqrt{n}}{\sqrt{2\pi}\sigma}exp\{-\frac{1}{2}\frac{(y-\theta)^2}{\sigma^2/n}\}\frac{b^{-1}exp\{-(\theta-a)/b\}}{[1+exp\{-(\theta-a)/b\}]^2}}{\int_{-\infty}^{\infty}\frac{\sqrt{n}}{\sqrt{2\pi}\sigma}exp\{-\frac{1}{2}\frac{(y-\theta)^2}{\sigma^2/n}\}\frac{b^{-1}exp\{-(\theta-a)/b\}}{[1+exp\{-(\theta-a)/b\}]^2}d\theta}\)</span></p>
<p>그리고 켤례사전분포는 이로 인해 연산되는 사후분포가 같은 family에 속하여 계산이 closed form으로 표현될 수 있는 것인데,</p>
<p>logistic distribution의 pdf는 normal과 conjugate를 이루지 않기 때문에 적분계산이 closed form으로 얻어지지 않는다.</p>
<h3 id="3-18-inverse-of-the-logistic-cdf">3. 강의 노트 18페이지에는 "inverse of the logistic cdf"를 구하였다. 왜 그것을 구해야 하는지 설명하시오.</h3>
<p>inverse of the logistic cdf를 구한 이유는 <span class="arithmatex">\(Unif(0,1)\)</span>를 만들어 샘플링을 하기 위해서이다. </p>
<p>이를 구한 이유는, 위의 문제에서 Bayes estimate를 계산하는데 적분을 closed form으로 얻을 수 없으므로,</p>
<p>이를 해결하기 위해 <span class="arithmatex">\(w(\theta)=f(y|\theta)=\frac{\sqrt{n}}{\sqrt{2\pi}{\sigma}}exp\{-\frac{1}{2}\frac{(y-\theta)^2}{\sigma^2/n}\}\)</span>으로 정의하여 Bayes estimate를 다음와 같이 표현한다.</p>
<p><span class="arithmatex">\(\delta(y)=\frac{E[\Theta w(\Theta)]}{E[w(\Theta)]}\)</span></p>
<p>Monte Carlo 기법으로 위의 <span class="arithmatex">\(\frac{E[\Theta w(\Theta)]}{E[w(\Theta)]}\)</span> 로 확률수렴하는 </p>
<p><span class="arithmatex">\(\frac{m^{-1}\sum_{i=1}^m\Theta w(\Theta_i)}{m^{-1}\sum_{i=1}^mw(\Theta_i)}\)</span>를 계산하기 위해 <span class="arithmatex">\(\Theta_i\)</span>들을 로지스틱 분포에서 추출해야하며, 이러한 과정을 위해 아래의 기법이 필요하다.</p>
<p>continuous random variable과 그것의 cdf 또한 연속인인 <span class="arithmatex">\(X\)</span>를 생각해볼 때, 우리는 이것의 cdf <span class="arithmatex">\(F(X)\)</span>의 <strong>분포</strong>가 <span class="arithmatex">\([0,1]\)</span>을 범위로 갖는 Uniform Distribution이라는 것을 알 수있는데, 간단히 수식으로 표현하면 아래와 같다.</p>
<p>위에서 설명한 확률변수의 누적확률분포를 <span class="arithmatex">\(F(X)\)</span>라고 칭하자.</p>
<p>그렇다면 <span class="arithmatex">\(0&lt;x&lt;1\)</span>을 만족하는 임의의 <span class="arithmatex">\(x\)</span>에 대하여 아래와 같은 식전개가 가능하다.</p>
<p>$<span class="arithmatex">\(P(F(X) \leq x)\)</span>$</p>
<p>$<span class="arithmatex">\(=P(X \leq F^{-1}(x))\)</span>$</p>
<p>$<span class="arithmatex">\(=F(F^{-1}(x))\)</span>$</p>
<p>$<span class="arithmatex">\(=x\)</span>$</p>
<p>여기서 <span class="arithmatex">\(X=F^{-1}(U)\  has\ distribution\ function\ F,\ where U~ Unif(0,1)\)</span>이다.</p>
<h3 id="4-wlln-wlln-bayes-estimator">4. WLLN가 뭔지 기술하고, WLLN가 어떻게 Bayes estimator를 시뮬레이션을 이용하여 구하는데 사용되었는지 설명하시오.</h3>
<p>Let <span class="arithmatex">\({X_n}\)</span> be a sequence of iid random variables having common mean <span class="arithmatex">\(\mu\)</span> and variance <span class="arithmatex">\(\sigma^2&lt;\infty\)</span>. Let <span class="arithmatex">\(\bar{X_n}=\frac{\sum_{i=1}^nX_i}{n}\)</span>. Then </p>
<div class="arithmatex">\[\bar{X_n}\xrightarrow{p} \mu\]</div>
<p>WLLN는 표본의 크기가 커짐에 따라 표본평균이 모평균으로 확률수렴하는 것을 의미한다.</p>
<p>이러한 WLLN가 Bayes estimator를 시뮬레이션을 이용하여 구하는데 사용될 수 있는 이유는 squared error loss function 하에서 Bayes estimator가 사후분포의 기댓값이기 때문이고,</p>
<p>기댓값은 적분으로 표현할 수 있기 때문이다. 따라서 위에서 구한 logistic prior의 예시같은 적분 계산의 어려움을 해결하기 위해 충분통계량의 pdf를 <span class="arithmatex">\(\theta\)</span>에 대한 함수 <span class="arithmatex">\(w(\theta)\)</span>로 취급하여 <span class="arithmatex">\(\frac{E[\Theta w(\Theta)]}{E[w(\Theta)]}\)</span>으로 <span class="arithmatex">\(\delta(y)\)</span>를 나타내어 <span class="arithmatex">\(\Theta_i\)</span>만 시뮬레이션 할 수 있으면, WLLN에 의해 m이 커짐에 따라 <span class="arithmatex">\(\theta(y)\)</span>에 대한 consistent estimator로의 <span class="arithmatex">\(\frac{m^{-1}\sum_{i=1}^m\Theta w(\Theta_i)}{m^{-1}\sum_{i=1}^mw(\Theta_i)}\)</span>를 계산할 수 있다.</p>
<h3 id="5-hmc-1145">5. HMC 11.4.5</h3>
<p>Suppose (X,Y) has the mixed discrete-continuous pdf</p>
<div class="arithmatex">\[f(x,y)= \frac{1}{\Gamma(\alpha)}\frac{1}{x!}y^{\alpha+x-1}e^{-2y}, y&gt;0,\ x=0,1,2,..,\ \alpha &gt;0\]</div>
<ol>
<li>Prove Y ~ Gamma(<span class="arithmatex">\(\alpha\)</span>,1)</li>
</ol>
<p><span class="arithmatex">\(f_Y(y)= \sum_{x=0}^\infty \frac{1}{\Gamma(\alpha)}\frac{1}{x!}y^{\alpha+x-1}e^{-2y}= \frac{1}{\Gamma(\alpha)}e^{-2y}y^{\alpha-1}\sum_{x=0}^{\infty}\frac{1}{x!}y^x\)</span>,</p>
<p><span class="arithmatex">\(e^x=\sum_{k=0}^{\infty}\frac{1}{k!}x^k\)</span>이므로, </p>
<p><span class="arithmatex">\(f_Y(y)=\frac{1}{\Gamma(\alpha)}e^{-y}y^{\alpha-1},\ y&gt;0,\ \alpha&gt;0\)</span> 이고 이는 </p>
<p><span class="arithmatex">\(Gamma(\alpha,1)\)</span>의 pdf이다.</p>
<ol>
<li>Prove X is negative Binomial </li>
</ol>
<p><span class="arithmatex">\(fX(x)= \int_0^{\infty}f(x,y)\)</span></p>
<p>여기서 <span class="arithmatex">\(y^{\alpha+x-1}e^{-2y}\)</span>는 <span class="arithmatex">\(\Gamma(\alpha+x,\frac{1}{2})\)</span>의 kernel이다.</p>
<p>고로 <span class="arithmatex">\(f_X(x)= \frac{1}{\Gamma(\alpha)}\frac{1}{x!}\Gamma{(\alpha+x)2^{-(\alpha+x)}}\)</span></p>
<p><span class="arithmatex">\(= \frac{(\alpha+x-1)!}{x!(\alpha-1)!}2^{-(\alpha+x)}\)</span>이다.</p>
<p>이는 성공횟수가 x고 실패횟수가 <span class="arithmatex">\(\alpha\)</span>, 성공확률이 <span class="arithmatex">\(\frac{1}{2}\)</span>인 음이항분포의 pdf이다.</p>
<h3 id="6-hogg-648p-gibbser2s">6. 각자 Hogg 책 648p에 있는 gibbser2.s라는 프로그램을 실행하여, 유사한 결과가 나오는지 확인하시오.</h3>
<pre><code class="language-r">gibbser2 = function(alpha,m,n){ x0 = 1
yc = rep(0,m+n)
xc = c(x0,rep(0,m-1+n))
for(i in 2:(m+n)){yc[i] = rgamma(1,alpha+xc[i-1],2)
  xc[i] = rpois(1,yc[i])}
y1=yc[1:m]
y2=yc[(m+1):(m+n)]
x1=xc[1:m]
x2=xc[(m+1):(m+n)]
list(y1 = y1,y2=y2,x1=x1,x2=x2)
}

set.seed(2013122059)
result=gibbser2(10,3000,3000)
yhat &lt;- result[['y2']]
xhat &lt;- result[['x2']]

yse &lt;- 1.96*(sd(yhat)/sqrt(3000))
yCI &lt;- paste0('(',round(mean(yhat)-yse,3),',',
  round(mean(yhat)+yse,3),')')


xse &lt;-1.96*(sd(yhat)/sqrt(3000))               
xCI &lt;- paste0('(',round(mean(xhat)-xse,3),',',
  round(mean(xhat)+xse,3),')')

mean(xhat) ; mean(yhat)
</code></pre>
<pre><code>## [1] 10.15433
</code></pre>
<pre><code>## [1] 10.10725
</code></pre>
<pre><code class="language-r">var(xhat) ; var(yhat)
</code></pre>
<pre><code>## [1] 20.31862
</code></pre>
<pre><code>## [1] 10.46886
</code></pre>
<pre><code class="language-r">xCI ; yCI
</code></pre>
<pre><code>## [1] &quot;(10.039,10.27)&quot;
</code></pre>
<pre><code>## [1] &quot;(9.991,10.223)&quot;
</code></pre>
<p>책에 있는 gibbs sampler 프로그램을 실행하여 <span class="arithmatex">\(\alpha=10,\  m=3000,\ n=6000\)</span>로 시뮬레이션을 해본 결과 lecture note의 값과 비슷하게 나오는 것을 확인할 수 있다.</p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
